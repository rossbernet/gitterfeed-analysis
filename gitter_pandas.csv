user,text,date,count
lossyrob,Hey everyone. Welcome to the GeoTrellis room!,3/31/15,1
non,hello!,3/31/15,1
non,(and goodbye -- it is quitting time now),3/31/15,1
lossyrob,adios!,3/31/15,1
lossyrob,@non btw did you get my email? I heard your coming to philly...would be great if you stopped in.,3/31/15,1
echeipesh,Not all that gitters is gold,3/31/15,1
lossyrob,hahaha,3/31/15,1
lossyrob,@echeipesh was there anything in that Catalog PR that we should keep?,4/1/15,1
echeipesh,"Looking over it again, but almost everything there had to do with refactor plan that is out the window now. So very unlikely.",4/1/15,1
lossyrob,What was that refactor plan about again?,4/1/15,1
lossyrob,"Also, I can_Ñét name things. Catalog is really just a key value store. Key is the layer ID. But also loading tiles is just a key value store, where the key is the TileID. but that one is read only. So what should the abstractions be named?",4/1/15,1
lossyrob,Reader/Writer traits.,4/1/15,1
lossyrob,Store/ReadOnlyStore ick,4/1/15,1
echeipesh,"That Catalog should not have params, only layer name is traded for RDD. Plus it was based on the type-class design that we_Ñére throwing out.",4/1/15,1
lossyrob,right,4/1/15,1
lossyrob,yeah that makes sense to just throw it out then. It was hanging around for a while and I wasn_Ñét sure if it had value,4/1/15,1
echeipesh,"We can still reference it for catalog work, but there is no way it was going to get merged now.",4/1/15,1
echeipesh,"Ok, I_Ñém going to start working on S3 catalog and sketch it out.",4/1/15,1
lossyrob,nice,4/1/15,1
lossyrob,"Basically a function that calls load(layerId: LayerId): RasterRDD[K] and save(layerId: LayerId, rdd: RasterRDD[K]): Unit for some K",4/1/15,1
echeipesh,yup,4/1/15,1
lossyrob,and perhaps a load(key: K): Tile for some TileLoader that_Ñés based on the LayerId,4/1/15,1
lossyrob,(which still not sure if we will support in HDFS),4/1/15,1
lossyrob,do you think S3 would be fine for supporting that? Probably yeah?,4/1/15,1
echeipesh,Probably,4/1/15,1
lossyrob,the other capability is to load off of a filter set,4/1/15,1
echeipesh,"yes, that_Ñés the tricky bit",4/1/15,1
lossyrob,"or like, you can create just a straight up load(key: K): V function based on injecting filters or whatever into the class that_Ñés providing that function",4/1/15,1
lossyrob,still trying to figure out if that looks ok in practice,4/1/15,1
echeipesh,"yes, I_Ñém just going to make some functions and pass things around as arguments to make it super clear for now. Will plug it into whatever makes sense.",4/1/15,1
lossyrob,```,4/1/15,1
lossyrob,woot,4/1/15,1
lossyrob,"```woot```",4/1/15,1
echeipesh,wtc ?,4/1/15,1
lossyrob,Trying to see how this markdown thing works,4/1/15,1
lossyrob,"```This is code and such```",4/1/15,1
echeipesh,```this is *code* ```,4/1/15,1
notthatbreezy,It works like slack and github,4/1/15,1
lossyrob,![sfcurve-space-diagram](https://cloud.githubusercontent.com/assets/2320142/6543539/449db6e2-c4ed-11e4-865a-584e056b5469.png),4/1/15,1
lossyrob,nice,4/1/15,1
lossyrob,can_Ñét do that on freenode,4/1/15,1
echeipesh,This will also tell you who read the comments,4/1/15,1
lossyrob,oh I didn_Ñét realize that,4/1/15,1
lossyrob,nice. so now I can tell who_Ñés reading my nonsense ,4/1/15,1
notthatbreezy,that's kind of creepy,4/1/15,1
lossyrob,yeah now I can_Ñét pretend that I haven_Ñét read a thing,4/1/15,1
echeipesh,Merging https://github.com/geotrellis/geotrellis/pull/1042 in since its +1,4/1/15,1
lossyrob,surely,4/1/15,1
notthatbreezy,I'm trying to run tests on the spark subproject before I start cleaning up the op stuff there -- but tests are failing with an error: `java.lang.UnsatisfiedLinkError: no gdaljni in java.library.path`,4/1/15,1
lossyrob,that_Ñés fun,4/1/15,1
notthatbreezy,is there a dependency I missed in installing?,4/1/15,1
lossyrob,it would be GDAL,4/1/15,1
lossyrob,but I thought we fixed it so if GDAL wasn_Ñét installed it wouldn_Ñét fail?,4/1/15,1
echeipesh,I don_Ñét recall that.,4/1/15,1
lossyrob,e.g. travis has no gdal,4/1/15,1
lossyrob,oh but it also has no spark. and doesn_Ñét run it ,4/1/15,1
echeipesh,"Yo, Rob, get back in the office",4/1/15,1
echeipesh,Monsuer wants to talk to us,4/1/15,1
lossyrob,shit,4/1/15,1
notthatbreezy,"cool - I'll try that, thanks",4/1/15,1
lossyrob,"@notthatbreezy there should be a way to  check if it_Ñés installed. I either tried to do that and succeeded and for some reason that test isn_Ñét doing it, or I failed, lemme check",4/1/15,1
lossyrob,there should be a way to catch that unsatisfied link error and report that gdal_Ñés not there,4/1/15,1
notthatbreezy,"ok, I can look into it too -- I install libgdal1-dev, but that doesn't seem to have fixed it either -- I can mess around with it now that I understand what's going on, this might be ubuntu specific",4/1/15,1
lossyrob,not sure the dev package installs java bindings,4/1/15,1
lossyrob,hector_Ñés gdal java ansible role installs it,4/1/15,1
notthatbreezy,"got it, I'll look at what that's doing",4/1/15,1
lossyrob,"I wonder if there_Ñés a call we can make on that Gdal object, or some init all, that we can catch the UnsatisfiedLinkError from",4/1/15,1
lossyrob,that would just check if it_Ñés working,4/1/15,1
lossyrob,"and if not, we can just return false to some gdalBindingsInstalled call",4/1/15,1
lossyrob,and skip tests based on if they_Ñére not there,4/1/15,1
notthatbreezy,"yeah, so that it would work similar to how the `ifCanRunSpark` check works?",4/1/15,1
echeipesh,"yeap, something like that. You would just take a function and check the precondition before you eval.",4/1/15,1
lossyrob,@notthatbreezy ,4/1/15,1
lossyrob,"```def isGdalInstalled = try { gdal.AllRegister() ; true } catch { case e: java.lang.UnsatisfiedLinkError => false }```",4/1/15,1
notthatbreezy,"cool - also, I installed the libgdal-java (following from the playbook), but that didn't fix the problem",4/1/15,1
notthatbreezy,"I also used the gdal-netcdf playbook since that was the test that was throwing the error, but that didn't fix things either",4/1/15,1
lossyrob,are the library binaries installed in /usr/local/lib?,4/1/15,1
lossyrob,that_Ñés what is put into the path to search in the build file,4/1/15,1
lossyrob,https://github.com/geotrellis/geotrellis/blob/master/project/Build.scala#L413,4/1/15,1
lossyrob,java.library.path is what it would be looking to to load the linked binary libs,4/1/15,1
lossyrob,is something called `gdaljni` in there?,4/1/15,1
notthatbreezy,ah no,4/1/15,1
notthatbreezy,I'll try and figure out where that is,4/1/15,1
notthatbreezy,it got put in /usr/lib/jni,4/1/15,1
notthatbreezy,actually - the only thing I could find is libgdaljni.so,4/1/15,1
lossyrob,that might just be it,4/1/15,1
lossyrob,i wonder why it got put there?,4/1/15,1
lossyrob,you could try to modify the java.library.path in the build file,4/1/15,1
notthatbreezy,"yeah, I'm in the process of testing that",4/1/15,1
lossyrob,"perhaps that should be pushed out to an optional environment var, like the thigns in Version.scala",4/1/15,1
lossyrob,nice,4/1/15,1
echeipesh,that_Ñés not a bad idea,4/1/15,1
notthatbreezy,woohoo - that worked,4/1/15,1
notthatbreezy,"so yeah, seems like it's necessary to make that configurable through an environment var",4/1/15,1
lossyrob,nice,4/1/15,1
lossyrob,"also if you_Ñére gonna do a PR with that, would be good to include the isGdalInstalled stuff too",4/1/15,1
notthatbreezy,"yeah, I'll work on that now -- though I do have some questions about the ops stuff that might be quicker to discuss in person when you have a second",4/1/15,1
lossyrob,yeah_Ñ_I_Ñém caught in some coding now,4/1/15,1
lossyrob,"would be quicker, but want to do it through here for posterity_Ñés sake?",4/1/15,1
lossyrob,slash I can still focus on code,4/1/15,1
notthatbreezy,ok sure - yeah good point,4/1/15,1
notthatbreezy,"Let me work on this gdal stuff real quick first, then when I get to actually making changes and look at that stuff again I'll pop back on here",4/1/15,1
lossyrob,ok cool,4/1/15,1
notthatbreezy,"is there a spot where environment variables are being documented in the project? -- thought about putting it in gdal/README.md, but then it'd be missed if someone was just interested in spark so the top-level README might be the best",4/1/15,1
echeipesh,I think it might be worth while breaking it out into Defaults.scala under build ?,4/1/15,1
lossyrob,makes sense to break it out_Ñ_though not sure what that would look like,4/1/15,1
lossyrob,need to default versions and also other properties,4/1/15,1
echeipesh,"Just make it project/Defaults.scala, link to it from README",4/1/15,1
notthatbreezy,ah ok - for now this would be the only thing in there I think,4/1/15,1
echeipesh,"No, the default versions for spark and hadoop would be there too I would think.",4/1/15,1
notthatbreezy,"oh, did you mean Version.scala then?",4/1/15,1
lossyrob,if you look at Versions.scala you_Ñéll see the _ÑÒeither_Ñù method for either using an env variable or a default,4/1/15,1
lossyrob,but it doesn_Ñét belong in Version.scala if it_Ñés not only versions,4/1/15,1
lossyrob,which we_Ñéd be adding a thing that isn_Ñét a Version,4/1/15,1
lossyrob,So it_Ñés almost like a Config object that takes either from the environment or some default,4/1/15,1
echeipesh,"yeap, so move that out to Defaults.scala, and from Version you can use _ÑÒDefault.sparkVersion, Default.hadoopVersion""",4/1/15,1
notthatbreezy,"yeah, so right now I just have that in Build.scala `val javaGdalDir = scala.util.Properties.envOrElse(""JAVA_GDAL_DIR"", ""/usr/local/lib"")`",4/1/15,1
notthatbreezy,@echeipesh that seems reasonable,4/1/15,1
lossyrob,but shoudl we have Default,4/1/15,1
echeipesh,Is it Default or Environmental ?,4/1/15,1
lossyrob,or have the either() or (envOrElse) in there too,4/1/15,1
notthatbreezy,"I think keeping ""/usr/local/lib"" as a default makes sense, else existing use would break",4/1/15,1
lossyrob,yeah and that_Ñés where it usually installs to_Ñ_.I_Ñém suprised yours doesn't,4/1/15,1
lossyrob,this is on ubuntu?,4/1/15,1
notthatbreezy,@echeipesh it's not environmental :smile: ,4/1/15,1
notthatbreezy,"yeah, this is on ubuntu",4/1/15,1
lossyrob,hahaha,4/1/15,1
lossyrob,"well actually, the _ÑÒal_Ñù might make sense there",4/1/15,1
lossyrob,or maybe I_Ñéve just caught the _ÑÒEnvironmental Variable_Ñù bug,4/1/15,1
echeipesh,"Added ```\w+(\.\w+){2,}``` as keyword to highlight in white, and it_Ñés dope",4/1/15,1
echeipesh,to iTerm2,4/1/15,1
notthatbreezy,ok - I think I have some questions now about the spark op cleanup task -- the only really global method implemented is vertical flip; viewshed is half-implemented -- the plan was to nix both of these right?,4/1/15,1
moradology,"me too, me too",4/1/15,1
moradology,I'm looking through geotrellis.raster and i'm having a hard time remembering why the operations in histogram aren't in op/global,4/1/15,1
lossyrob,@moradology what operations in histogram?,4/1/15,1
lossyrob,"@notthatbreezy well, viewshed seems really hard to parallelize, so probably won_Ñét get that out before the release",4/1/15,1
lossyrob,but verticle flip_Ñ_that seems doable yeah?,4/1/15,1
lossyrob,some clever think about key switching or something,4/1/15,1
notthatbreezy,"yeah, that's what it looks like its doing now",4/1/15,1
lossyrob,oh your saying that_Ñés the only one,4/1/15,1
notthatbreezy,"flips each individual tile, then re-arranges the keys",4/1/15,1
lossyrob,"right. anyhting that isn_Ñét real goes away, and is marked i guess with  a github issue for actually doing it",4/1/15,1
notthatbreezy,ok got it,4/1/15,1
lossyrob,"and you can mark it as 0.10 milestone, and I_Ñéll go through and see what to try to get in for the release",4/1/15,1
moradology,i was just wondering about any of the operations in `geotrellis.raster.histogram` that use the entire raster. ,4/1/15,1
moradology,"so, ArrayHistogram.fromTile",4/1/15,1
moradology,for instance,4/1/15,1
moradology,i remembered having discussed this,4/1/15,1
moradology,but don't recall what the answer was that we came to at the code sprint,4/1/15,1
lossyrob,ArrayHistogram is something that is never used,4/1/15,1
lossyrob,we always just use FastMapHistogram,4/1/15,1
lossyrob,and I think that has an operation stats,4/1/15,1
lossyrob,but yeah those are stats operations,4/1/15,1
lossyrob,and how an ArrayHistogram is constructed from a tile makes sense to be owned by the ArrayHistogram object,4/1/15,1
lossyrob,"it might be useful to expose it as an op on tile, but again we just have `.histogram` and it creates a FastMapHistogram",4/1/15,1
moradology,mhmmm.,4/1/15,1
moradology,i'm going to mostly punt on the documentation there if it isn't important for usage,4/1/15,1
moradology,i'm only going so deep anyway.,4/1/15,1
lossyrob,Yeah that makes sense. target Histogram and FastMapHistogram,4/1/15,1
lossyrob,I was thinking the Defaults.scala wouldn_Ñét change Version.scala,4/1/15,1
lossyrob,because the versions that are hard coded and don_Ñét have things that can be changed in the environment aren_Ñét really defaults,4/1/15,1
notthatbreezy,"oh, true",4/1/15,1
notthatbreezy,sorry - thought the conversation gravitated towards a defaults.scala and appending Version to it,4/1/15,1
lossyrob,"```object Defaults {  def either(environmentVariable: String, default: String): String =    Properties.envOrElse(environmentVariable, default)  val geotrellisVersion   = ""0.10.0-SNAPSHOT""  lazy val hadoopVersion  = either(""SPARK_HADOOP_VERSION"", ""2.5.0"")  lazy val sparkVersion   = either(""SPARK_VERSION"", ""1.2.0"")  lazy val javaGdalDir    = either(""JAVA_GDAL_DIR"", ""/usr/local/lib"")}```",4/1/15,1
lossyrob,and then in version,4/1/15,1
echeipesh,why are those versions lazy ?,4/1/15,1
echeipesh,\\\,4/1/15,1
lossyrob,well_Ñ_also the Defaults doesn_Ñét encapsulate that it_Ñés actually pulling from the env too,4/1/15,1
echeipesh,"```object Sweet { def apply() = ???}```",4/1/15,1
lossyrob,"so I would say, `object Environment`",4/1/15,1
notthatbreezy,"that makes sense too -- I don't really have a preference, other than making it clear for users what can be set from the environment",4/1/15,1
lossyrob,or `EnvOrDefault` ?,4/1/15,1
lossyrob,I hate names,4/1/15,1
echeipesh,+1 Environment,4/1/15,1
lossyrob,So then version would look like,4/1/15,1
lossyrob,"```object Version {  val geotrellis  = ""0.10.0-SNAPSHOT""  val scala       = ""2.10.4""  val geotools    = ""11.0""  val akka        = ""2.2.4""  val spray       = ""1.2.1""  val jackson     = ""1.6.1""  lazy val hadoop      = Environment.hadoopVersion  lazy val spark       = Environment.sparkVersion}```",4/1/15,1
lossyrob,(and loses the `either` function),4/1/15,1
lossyrob,oooohh i can edit eh,4/1/15,1
notthatbreezy,"cool, updated the PR",4/1/15,1
lossyrob,gratsi,4/1/15,1
hectcastro,Oh damn. Yet another scrollback!,4/1/15,1
lossyrob,"I_Ñém playing chat client bingo_Ñ_I have Kato, Slack, IRC and Gitter on separate desktops",4/1/15,1
lossyrob,I guess I could fullscreen a google chat and make it 5,4/1/15,1
hectcastro,GitHub integration is nice though.,4/1/15,1
lossyrob,@kyeah did you get a successful ingest of that spatial raster into Cassandra?,4/2/15,1
pomadchin,"Hi guyz! Hm, mb it is question more about spark ): I'm trying to run gt-admin server (compiled it (fixed it up to new geotrellis) and it seems to run successfully), but cant it ping by 0.0.0.0:4444 (fixed port in sources) how is it possible to find logs I wonder? ._.' in console it seems to be everything ok. ",4/2/15,1
lossyrob,"hey @pomadchin, can you link to where it is in the sources that that port is fixed?",4/2/15,1
lossyrob,"There_Ñés a team from UT Austin working on gt-admin right now, and it_Ñés in early/active development",4/2/15,1
lossyrob,and can_Ñét remember where that port is used,4/2/15,1
pomadchin,"It is here (spary start server, just changed to my own (hadoop using it)): https://github.com/geotrellis/gt-admin/blob/master/server/src/main/scala/geotrellis/admin/server/CatalogService.scala#L279-L281It is called from ui here: https://github.com/geotrellis/gt-admin/blob/master/viewer/src/scripts/components/RasterViewer.jsx#L20Btw, dont clearly understand how is it possible to run server via spark submit :D Before just worked with hadoop & scalding (so it was a simple writing jobs): https://github.com/geotrellis/gt-admin/blob/master/run-server.sh little bit embarrasing ): ",4/2/15,1
pomadchin,here is a fork (to make it compilable with geotrellis master branch): https://github.com/pomadchin/gt-admin,4/2/15,1
lossyrob,are you able to hit localhost:4444/catalog/,4/2/15,1
lossyrob,?,4/2/15,1
lossyrob,"you run the server via the spark submit so that it runs it with the spark context. And it_Ñés a forever-lasting spark job (until you ctrl-c out of it), which listens on the port",4/2/15,1
pomadchin,"No, cant access 4444 port at all ): (tryed other ports).Btw console output: https://gist.github.com/pomadchin/f1946fd8f13d5da34dbaAnd no warns about not working smth. ):Hm, didn't know that it is possible to run server in a such interesting way (yes, I guessed that it is listening cause why job not stopping).",4/2/15,1
lossyrob,"hm looks like it_Ñés stalling, let me see what would come next in the log output",4/2/15,1
lossyrob,the last line should say that it_Ñés listening on the port,4/2/15,1
pomadchin,"Much thanks! Look it tomorrow (~11p.m. now) Is it possible to find spark logs wile stalling? (mb here some errors) ?",4/2/15,1
lossyrob,here_Ñés the last bit of my output:,4/2/15,1
lossyrob,"```INFO  ZooKeeper - Client environment:user.home=/Users/robINFO  ZooKeeper - Client environment:user.dir=/Users/rob/proj/gt/gt-adminINFO  ZooKeeper - Initiating client connection, connectString=localhost sessionTimeout=30000 watcher=org.apache.accumulo.fate.zookeeper.ZooSession$ZooWatcher@57faec51INFO  ClientCnxn - Opening socket connection to server fe80:0:0:0:0:0:0:1%1/fe80:0:0:0:0:0:0:1%1:2181. Will not attempt to authenticate using SASL (unknown error)INFO  ClientCnxn - Socket connection established to fe80:0:0:0:0:0:0:1%1/fe80:0:0:0:0:0:0:1%1:2181, initiating sessionINFO  ClientCnxn - Session establishment complete on server fe80:0:0:0:0:0:0:1%1/fe80:0:0:0:0:0:0:1%1:2181, sessionid = 0x14c7bc5caa40006, negotiated timeout = 30000[INFO] [04/02/2015 16:16:38.567] [spray-system-akka.actor.default-dispatcher-4] [akka://spray-system/user/IO-HTTP/listener-0] Bound to /0.0.0.0:8088```",4/2/15,1
lossyrob,So it_Ñés really just not outputting the spray start log,4/2/15,1
lossyrob,interesting...,4/2/15,1
lossyrob,Ok! thanks for chatting. I_Ñéll be on tomorrow and we can keep looking at it then if you_Ñére on.,4/2/15,1
lossyrob,@echeipesh got what I was talking about working with the Accumulo side of things. Check it out: https://github.com/geotrellis/geotrellis/pull/1051,4/3/15,1
pomadchin,"@lossyrob mb smth with my accumulo / zookeeper configuration :\ no ideas thoughtried to install it again (clean) but nothing; mb some special cfgs needed?",4/3/15,1
pomadchin,"find out that is it clearly smth wrong with my accumulo / zookeper; sry for it; my mind is blowing with this new stack for me, very interesting. :D trying to fix it",4/3/15,1
pomadchin,* trying on master branch of geotrellis & my gt-admin fixed up to it (to make it compilable).,4/3/15,1
pomadchin,"Hm, problems here and with a simple ingest job, btw in accumulo logs connection persists:```2015-04-03 17:11:23,353 [Audit   ] INFO : operation: permitted; user: root; client: 127.0.0.1:37597; ```",4/3/15,1
pomadchin,mb here is a version issue? accumulo 1.6 is ok?,4/3/15,1
lossyrob,Hmmm perhaps it is that issue,4/3/15,1
lossyrob,I believe gt-admin uses 1.5,4/3/15,1
lossyrob,"@kyeah the change in PR #1051 will be of interest of you too, once it gets merged in and we fit the Cassandra stuff to it. Feel free to give feedback or input.",4/3/15,1
kyeah,"Yep, saw that; I'll look over the changes.",4/3/15,1
lossyrob,nice,4/3/15,1
lossyrob,how long did that Cassandra ingest of the one-month tiles take?,4/3/15,1
kyeah,"Not long at all; turns out I just didn't close the cassandra session afterwards, so it hung after ingesting.",4/3/15,1
lossyrob,oh gotcha,4/3/15,1
lossyrob,did you get to see it through the viewer?,4/3/15,1
kyeah,"Yep; I don't have accumulo setup to compare speeds, but the tiles seemed to load near-instantly for the most part.",4/3/15,1
lossyrob,That_Ñés a big deal,4/3/15,1
lossyrob,[Celebration gif],4/3/15,1
lossyrob,very exciting,4/3/15,1
lossyrob,do you have a branch of the gt-admin tool that I could checkout and use to see it?,4/3/15,1
kyeah,https://github.com/kyeah/gt-admin,4/3/15,1
lossyrob,I immediately regretted putting a gif there haha. Only so much I can watch a loop.,4/3/15,1
lossyrob,it_Ñés just on master?,4/3/15,1
kyeah,haha,4/3/15,1
kyeah,yeah,4/3/15,1
lossyrob,ok awesome. Hopefully I find some time over the weekend to play around with that.,4/3/15,1
lossyrob,Should the cassandra stuff also be able to handle SpaceTimeKey rasters?,4/3/15,1
lossyrob,You could use the NexIngest.scala as a template to build an ingest around reading up the temporal raster set that I gave the viewer team,4/3/15,1
kyeah,"I just got the SpaceTime driver working with the viewer last night, so I'll push that up to my geotrellis / gt-admin repos",4/3/15,1
kyeah,I'll look into Nexingest and get that temporal set from the team,4/3/15,1
lossyrob,wooot,4/3/15,1
lossyrob,at this pace Cassandra support might make it into the 0.10 release,4/3/15,1
notthatbreezy,"Chiming in here because I haven't done it before, but when you're talking about the one month tiles, what/where are they?",4/3/15,1
lossyrob,"it was a set of NEX data that I clipped down to one month (so it didn_Ñét need a time component, just a SpatialKey) that I gave to the UT Austin students to work with",4/3/15,1
lossyrob,lemme see if I can rustle them up and share them,4/3/15,1
notthatbreezy,"ah got it, yeah that'd be cool to have something more manageable to play around with",4/3/15,1
lossyrob,Uploading now,4/3/15,1
notthatbreezy,Are you just putting them on S3?,4/3/15,1
lossyrob,https://drive.google.com/folderview?id=0B_qGBN8rOJmOfnliVjAyb0gwQzJFRlJCaEtLbTJHWDNDVWl6MnVIRlN6UlJzVC1KX2NnaHc&usp=sharing,4/3/15,1
notthatbreezy,oh nice,4/3/15,1
notthatbreezy,thanks,4/3/15,1
lossyrob,"no problem, let me know if you have trouble working with them",4/3/15,1
lossyrob,but you should be able to ingest those through the current gt-admin,4/3/15,1
lossyrob,although I_Ñém not sure if there_Ñés issues with using geotrellis master,4/3/15,1
lossyrob,it might need some updating since Eugene_Ñés change to how Ingest works,4/3/15,1
lossyrob,slight updates but updates nontheless,4/3/15,1
notthatbreezy,"got it, I'll play around with it and see if there are issues (probably over the weekend)",4/3/15,1
pomadchin,"@lossyrob tried 1.5.2 accumulo and ingesting works (with geotrellis master assembly) perfect; btw still got errors while spary starting (gt-admin) :D but, it seems to me I know where to look.Dat incompatibility ):",4/4/15,1
lossyrob,"@pomadchin yeah, there_Ñés problems with upgrading to 1.6_Ñ_cloudera doesn_Ñét have a package for it, and so getting a standard install is tough. Is there a particular reason for you to use 1.6 instead of 1.5?",4/4/15,1
pomadchin,"@lossyrob nope, the only reason I used 1.6.x that I'm new to it, and didn't know what to use",4/4/15,1
lossyrob,"We have a PR to support 1.6, but are also worried about deployment support for 1.6",4/4/15,1
lossyrob,so it_Ñés an ongoing debate. We_Ñéll see!,4/4/15,1
lossyrob,Strange about the spray not working. I wonder if you can get another spray project to run?,4/4/15,1
lossyrob,Here_Ñés one to try https://github.com/geotrellis/geotrellis-chatta-demo,4/4/15,1
DarkManiel,"Has anyone else had trouble accessing localhost:8088/vector? My server is running, but I keep getting an access error:",4/6/15,1
DarkManiel,"GET http://localhost:8088/vector/ localhost/:1 XMLHttpRequest cannot load http://localhost:8088/vector/. No 'Access-Control-Allow-Origin' header is present on the requested resource. Origin 'http://localhost:8000' is therefore not allowed access. The response had HTTP status code 500.",4/6/15,1
DarkManiel,* for gt-admin,4/6/15,1
lossyrob,Hmm_Ñ_that was one of the routes that was to be deleted,4/6/15,1
DarkManiel,"ok, thanks",4/6/15,1
lossyrob,but the Access Control error shouldnt happen since we set the CORS headers,4/6/15,1
lossyrob,so that_Ñés interseting,4/6/15,1
DarkManiel,yeah,4/6/15,1
lossyrob,what browser?,4/6/15,1
DarkManiel,it was in chrome. ,4/6/15,1
DarkManiel,deleted now though,4/6/15,1
pomadchin,"yep, about spary not working in gt-admin, think that some versions mismathes (dunno where): ```Exception in thread ""main"" java.lang.NoSuchMethodError: akka.actor.ActorSystem.dispatcher()Lscala/concurrent/ExecutionContext;```",4/6/15,1
st028,"Hey @lossyrob, just made the fix that you suggested on #1044, tests are still failing unfortunately",4/6/15,1
lossyrob,@st028 the tests that were failing because of the varying resolution are still failing after the extent reprojection fix? ,4/6/15,1
pomadchin,"//offtop// Is it possible to get out already ingested tiff, and save it somewhere else (out of hdfs)? (I loaded it, and the to get it out? noticed png render functions) In other words got misunderstand a little what exactly stores accumulo, only metadata or the whole image data?",4/6/15,1
st028,@lossyrob Yep,4/6/15,1
lossyrob,"@st028 ok, something to debugged then. Is there a test around extent reprojection that maybe mimics what we were doing with the JTS builder that will help you debug?",4/6/15,1
lossyrob,"@pomadchin accumulo or hdfs stores all the image data (in a format that I specific to he back end, mainly a byte array) as well as metadata. You can load a layer out of one catalog and save it to another. Rendering PNGs happens at the Tile level (e.g. for returning PNGs from a TMS web request).",4/6/15,1
DarkManiel,"It looks like I'm missing some of the tiles. I get ""Error during processing of request HttpRequest(GET,http://localhost:8088/tms/nexmonth_gtadmin/3/3/4?breaks=278,280,282,283,284,285,286,288,289,291,293,300 ... Tile with key SpatialKey(3,4) not found for layer Layer(name = ""nexmonth_gtadmin"", zoom = 3)"" for some of the zoom levels of nexmonth in gt_admin. Has anyone else had this problem? It works okay for zoomlevels > 3. I was only able to ingest the one-month-tiles, so wondering if I need to ingest ccsm4-rcp85 for it to work properly.",4/6/15,1
lossyrob,"that might just be that that tile doesn_Ñét exist. Like, the TMS request doesn_Ñét know whether or not the tile (say it_Ñés requqesting china or something) doesn_Ñét exist in the layer. So the request goes through and you get that _ÑÉerror_Ñé message",4/6/15,1
lossyrob,perhaps gt-admin shouldn_Ñét throw the error and just either report the tile doesn_Ñét exist and return a transparent PNG. maybe just log in for DEBUG level,4/6/15,1
lossyrob,"hey @Pshrub, so you got into the leader instance ok now?",4/6/15,1
lossyrob,Pasting in some chat I_Ñém having with @notthatbreezy about HDFS support:,4/6/15,1
lossyrob,"@lossyrobwe have this in master: https://github.com/geotrellis/geotrellis/blob/master/spark/src/main/scala/geotrellis/spark/io/hadoop/PreFilteredHadoopRDD.scalawhich is a hacked together thing that_Ñés basically a copy of Spark codethat is set to private in spark, which is annoying and makes it harder to extentso maybe there_Ñés a better way to do it? or maybe that_Ñés just what we do? or maybe we don_Ñét support filtering? The latter would be the way to do it if really there_Ñés no good way around it.",4/6/15,1
notthatbreezy,"ok yeah, that makes sense -- so this would something similar to how predicate push downs/filters work with like Cassandra, just directly on top of HDFS?",4/6/15,1
lossyrob,"this is RE: whether or not we can support filtering our RasterRDD_Ñés on load with the HDFS backend (geotrellis.spark.io.hadoop). We can do that in Accumulo, but that_Ñés because we index the tiles and Accumulo can handle the range queries",4/6/15,1
lossyrob,yeah_Ñ_like how we do in Accumulo,4/6/15,1
lossyrob,"but the mechanism would necessarily have to be different_Ñ_the main problem is, we don_Ñét want to load up a partition if the partition doesn_Ñét have any keys in our query",4/6/15,1
lossyrob,and we don_Ñét want to load any tiles with keys that don_Ñét match out query,4/6/15,1
lossyrob,the crux is here: https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/rdd/NewHadoopRDD.scala#L157,4/6/15,1
lossyrob,"in order to read the key, we need to read the value as well. In NewHadoopRDD",4/6/15,1
echeipesh,In this context partition would be an HDFS block (which will have multiple key-tile pairs),4/6/15,1
lossyrob,our PreFilteredHadoopRDD is a version of that which doesn_Ñét necessarily load a partition or a value,4/6/15,1
notthatbreezy,"ok, yeah the use-case/need makes sense",4/6/15,1
echeipesh,Now that I know more about Hadoop I think it_Ñés likely we can push the filter down into InputFormat somehow. Maybe extend hadoop FileInputFormat.,4/6/15,1
lossyrob,hmmm truth,4/6/15,1
lossyrob,right. Ok I think that_Ñés a really good option,4/6/15,1
lossyrob,It_Ñés a Reader per map file right?,4/6/15,1
echeipesh,correct,4/6/15,1
lossyrob,"And the input format could get the splits, and check if the mapfile has any keys at all, or could contain tiles in the range?",4/6/15,1
echeipesh,"Yeah, that should happen at the point of creating splits. It should have the _ÑÒsplits_Ñù array available to it at that point.",4/6/15,1
echeipesh,"That_Ñés the part I haven_Ñét looked into yet though, but sounds like it slots into that part of API overall.",4/6/15,1
notthatbreezy,"ok, that makes sense too -- so I guess my task then is to check this out tomorrow?",4/6/15,1
lossyrob,"@notthatbreezy ok I think the support can happen, but it requires some deeper knowledge of InputFormat and RecordReaders",4/6/15,1
notthatbreezy,yeah - I can't really say I have either of those at the moment,4/6/15,1
lossyrob,so I_Ñém wondering if one of us should do it,4/6/15,1
echeipesh,I keep forgetting notthatbreezy is Chris ...,4/6/15,1
lossyrob,haha,4/6/15,1
notthatbreezy,"yeah, just not that chris brown",4/6/15,1
lossyrob,I think with one day it would be a tough task to  take on. perhaps next sprint? Unless one of us solves it before that,4/6/15,1
lossyrob,or you take on some extracarricular studies :),4/6/15,1
echeipesh,"It would be optimistic for it to be a day TBH, FileInputFormat is a work-horse so there is a lot around it.",4/6/15,1
lossyrob,yeah. Hopefully extensible,4/6/15,1
lossyrob,seems like somethign someone should have done at some point,4/6/15,1
notthatbreezy,"yeah, I didn't mean I could finish it in a day, but maybe get a good jump on it",4/6/15,1
lossyrob,"when we were researching it, we were looking at a spark specfic solution",4/6/15,1
lossyrob,which is why the PreFilteredHadoopRDD came into being,4/6/15,1
lossyrob,but an InputFormat solution may already be kicking around in the open-source-o-sphere,4/6/15,1
echeipesh,That_Ñés optimistic too : ),4/6/15,1
lossyrob,"We_Ñére looking for a `org.apache.hadoop.mapreduce.lib.input.SequenceFileInputFormat[K, V]` which allows keys to be filtered out such that the value is not actually read",4/6/15,1
lossyrob,"@echeipesh, so as far as the GeoTiff reader thing goes. I had a long convo with Johan, found out a bunch of stuff_Ñ_what were the questions you had around that?",4/6/15,1
notthatbreezy,Well I can start on the spark documentation too tomorrow -- is there a good place to start (bottom-up?) that won't become irrelevant after some branches out there get merged?,4/6/15,1
echeipesh,Is it possible to back a Tile with GeoTiff bytearray like we talked about and do you know about how to do it or should I continue my reading on the topic.,4/6/15,1
notthatbreezy,"Like I know the catalog stuff, etc. is still in flux so nothing there -- but I'm not sure what else is kicking around on other branches",4/6/15,1
lossyrob,@notthatbreezy catalog stuff is in flux for sure,4/6/15,1
lossyrob,the operations aren_Ñét as much,4/6/15,1
lossyrob,Nor is RasterRDD,4/6/15,1
lossyrob,and what it means to have different key types_Ñ_which really are just SpatialKey and SpaceTimeKey,4/6/15,1
notthatbreezy,"ok cool, yeah - that sounds like a good place to start",4/6/15,1
lossyrob,and what the difference is between RasterRDD[SpatialKey] and RasterRDD[SpaceTimekey],4/6/15,1
lossyrob,@echeipesh yeah I think so,4/6/15,1
lossyrob,so GeoTiffs can either be stored as strips or tiles,4/6/15,1
lossyrob,each of those could be decompressed independently ,4/6/15,1
lossyrob,although there would have to be some serious refactors to the reader to get that to happen,4/6/15,1
lossyrob,a `get and `getDouble` could be implemented,4/6/15,1
lossyrob,"but mainly we_Ñéd encourage the use of `foreach( z => z)` or `foreach( (col, row, z) => z)`",4/6/15,1
lossyrob,and do the decompress in a smart way,4/6/15,1
lossyrob,then there_Ñés the way that the strips and tiles of the geotiff bytes deal with multiple bands,4/6/15,1
lossyrob,"We couldn_Ñét nail that down by the end of our talk, but I have some tests that can shake that stuff out",4/6/15,1
lossyrob,which I wanna get to once the catalog stuff is done,4/6/15,1
echeipesh,"cool, much better place to be than originally.",4/6/15,1
lossyrob,but yeah there should be a way to have a Tile backed by a set of compressed pieces of the raster,4/6/15,1
lossyrob,which I think is a pretty cool thing to move towards anyway,4/6/15,1
echeipesh,"Yes, I can see it having a lot of magical properties.",4/6/15,1
lossyrob,"We_Ñéd have to get the geotiff writer up to speed with the writing. Even if we only supported like ZLib compression on write. Then if you do a tile.map call, it can return a likewise compressed tile",4/6/15,1
lossyrob,"@echeipesh am I write in my code reading, if there are no SpatialFilters set for an Accumulo load (on the try3 branch), then no time range will be set, even if hter eare time filters?",4/6/15,1
lossyrob,write = right. ha.,4/6/15,1
echeipesh,"Not supposed to be that way, not really a widely tested case though.",4/6/15,1
lossyrob,ok cool. Just making sure I wasn_Ñét reading it wrong.,4/7/15,1
lossyrob,@echeipesh why does `S3InputSplit` extend `Writable`? https://github.com/geotrellis/geotrellis/blob/44a22acd771451ea59e7228c96423b1b93e94733/spark/src/main/scala/geotrellis/spark/io/s3/S3InputSplit.scala#L12,4/7/15,1
echeipesh," It gets serialized through Hadoop api, made on the driver, sent to workers. Not sure where that mechanism is specifically though.",4/7/15,1
lossyrob,hmm ok. So I_Ñéll know if I need it for the Filtered split (if that_Ñés the route I go) if it tries to serailize it and it blows up I guess,4/7/15,1
echeipesh,"Would be good to know what blows up, you working on HDFS catalog?",4/7/15,1
lossyrob,"yeah. I have a bead on the filtered thing, trying to knock it out",4/7/15,1
lossyrob,there_Ñés actually something very close to what we need,4/7/15,1
lossyrob,https://github.com/apache/hadoop/blob/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/input/SequenceFileInputFilter.java,4/7/15,1
lossyrob,But it doesn_Ñét have a mechanism to know what key ranges are in the split so it can just wholly skip the file,4/7/15,1
lossyrob,which is what I_Ñéd want to add as an InputSplit,4/7/15,1
lossyrob,and inputSplit that knew it_Ñés key range. so hasNext would just return nothing if there_Ñés no keys in range,4/7/15,1
lossyrob,or getSplits could just filter those out on the InputFormat,4/7/15,1
lossyrob,confused about how I get the split ranges to the getSplits call,4/7/15,1
echeipesh,"It's really hard to read on the phone, but looks promising ",4/7/15,1
lossyrob,"yeah at this point I think there_Ñés a way to at least do it without loading Tiles, but pretty sure there_Ñés a way to do it with skipping whole files as well",4/7/15,1
pomadchin,"hm, to the question of getting out ingested tiles: i've loaded geotiff (only 1, not map), zommed ingest. It is al ok, catalog.metaDataCatalog.fetchAll gets all metadata, nut how can i get this loaded tile (without rendering to png / with, the whole loaded image I mean)? tried so: ```val layer = LayerId(name, zoom)val (lmd, params) = catalog.metaDataCatalog.load(layer)val md = lmd.rasterMetaDataval crs = md.crsval p = Point(x, y).reproject(LatLng, crs)val key = md.mapTransform(p)val tile = catalog.loadTile(layer, SpatialKey(key.col, key.row))```where x and y coords of loaded geotiff center (which r equal to metadata stored in accumulo);worked for zoom = 1 , 2 so decided to bruteforce, and got very strange results :D ```for (zoom <- 1 to 8; row <- 0 to 256; col <- 0 to 256) Try { ... } ```Moreover, i wrote this tile (if sucess) into file ``` Files.write(Paths.get(s""/tmp/$zoom-$row-$col-mod09a1.png""), tile.renderPng.bytes)```Hm, there (in some success files) not an rgb image, but mono channeled (o_O) ): mb there is an ingesting issue, due to 8 layered geotiff file? And it is interesting to get default tiff (like before accumulo ingesting) back from accumulo (as an experiment, as big tiff (mb tiff byte array))Im missing smth? o:",4/7/15,1
pomadchin,"And what r images that i get such way? https://www.dropbox.com/s/6epnmbzsslovzh4/8-46-111-test.png?dl=0default image: https://www.dropbox.com/s/uag1zqjqzmztl1n/MD20150010706.tif?dl=0 ",4/7/15,1
lossyrob,@pomadchin..are you saying your GeoTIFF file is 8 bands?,4/7/15,1
lossyrob,The logic looks correct (you can use `tile.renderPng.write(path)` though),4/7/15,1
lossyrob,This logic isn_Ñét sound though,4/7/15,1
lossyrob,for (zoom <- 1 to 8; row <- 0 to 256; col <- 0 to 256),4/7/15,1
lossyrob,"SpatialKey(col, row) is the column and row of the tile in the TMS tiling scheme",4/7/15,1
lossyrob,not the pixel col row,4/7/15,1
lossyrob,so `0 to 256` doesn_Ñét make sense in that scheme,4/7/15,1
lossyrob,the ingest process tiles up the raster,4/7/15,1
lossyrob,even if it_Ñés just one raster,4/7/15,1
lossyrob,into 256 x 256 tiles,4/7/15,1
lossyrob,so the tile you pull out with `loadTile` is the one 256x256 tile that matches that point. which at zoom level 8 is going to be a small part of your whole image,4/7/15,1
lossyrob,also to note: we don_Ñét currently support multiband,4/7/15,1
lossyrob,so you would need to export the 8 band geotiff into single band geotiffs (with GDAL perhaps),4/7/15,1
lossyrob,and then ingest each band as it_Ñés own layer,4/7/15,1
lossyrob,"`LayerId(_ÑÒimage-band1_Ñù, 8)`, `LayerId(_ÑÒimage-band2_Ñù, 8)` and whatnot",4/7/15,1
lossyrob,some changes upcoming are going to support multiband in a better way. but for now you have to view them as separate layers,4/7/15,1
lossyrob,i.e. separate ingests,4/7/15,1
lossyrob,"__Note__ I made a feature branch on the `geotrellis` org repo called `feature/catalog-refactor`, which has my catalog changes. They_Ñére pretty breaking, but we_Ñére going to be merging that in at some point in the near future, so be aware.",4/7/15,1
pomadchin,"@lossyrob thx alot for answers; about multi band yes, wanted to ask a little later. Thx for cols & rows in a spatial key explanation! awesome. It clears eveythiexactly the files (images avobe): the source is geotiff (rgb composite (1 layer)). and the output is the result (some part of the source tiff)what this output png means? (i mean, is it possiblle to restore the png / tiff to make it colored like before ingesting (rgb)?) ",4/7/15,1
pomadchin,"p.s. this tiff is a modis composite tile (rgb result tiff of modis product mod09a1, projected to web mercator)",4/7/15,1
pomadchin,"About gt-admin: if it is interesting, there is an issue running spray with spark 1.2.x and 1.3.x, due to akka incapability. Though, works well with spark 1.1.x; Apache spark mailing list (with possible solution ~ using spark akka and some additional information): http://apache-spark-user-list.1001560.n3.nabble.com/Spray-client-reports-Exception-akka-actor-ActorSystem-dispatcher-Lscala-concurrent-ExecutionContext-td17475.html#a17566",4/8/15,1
DarkManiel,I think 1.2.1 works,4/8/15,1
ejc123,"Hey @lossyrob What is the status of multiband rasters?  Stuff has really changed since the last time I had a chance to dig into geotrellis!  Great to see all the work with spark.  I keep hoping I can find some time to help out again, but things have not slowed :-(",4/8/15,1
pomadchin,"@DarkManiel nope, just tried now with 1.1.1 and 1.2.1; 1.2.1 fails ): think the only solution is to build spray with ```""org.spark-project.akka""   %%  ""akka-actor"" % ""2.3.4-spark""```",4/8/15,1
pomadchin,or to use spark 1.1.x,4/8/15,1
lossyrob,"@DarkManiel can you verify which spark-submit version your scripts to run gt-admin are working with? I know we hit against the 1.1.x thing, and my scripts are downgraded to use that version, but I_Ñém not sure if we worked around that in another way.",4/8/15,1
lossyrob,"@ejc123 hey! long time no talk, good to hear from you again. Multiband is direclty next on the docket_Ñ_I_Ñém working on polishing up the HDFS and Accumulo catalog fo the spark work, and then I_Ñém going to move to supporting the ingesting and operating on MultiBand rasters. So instead of a `RasterRDD[K]` which is an `RDD[(K, Tile)]` under the hood, we_Ñéll have some `MultiBandRasterRDD[K]` which is an `RDD[(K, MultiBandTile)]` under the hood (that_Ñés the plan anyway).",4/8/15,1
lossyrob,@echeipesh are you around to talk about that PR comment? I want to hear more about what you mean.,4/8/15,1
ejc123,"I really like the proposed move to Gitter, btw.  a lot easier to move between work and home w/out missing stuff.",4/8/15,1
lossyrob,"@ejc123 yeah me too, thanks to @moradology for pushing for the switch.",4/8/15,1
lossyrob,@pomadchin were you saying that the gt-admin works with `1.2.1` if we use the `2.3.4-spark` akka projects?,4/9/15,1
lossyrob,because those might conflict with the `spray` dependencies we have as well,4/9/15,1
lossyrob,"@pomadchin also, PNG output = whatever TMS tiles are. A flat PNG file, should open in an image viewer. If you want RGB tiff, we don_Ñét have that yet. We do have a GeoTIFF writer, which can write a single band GeoTIFF. The solution for working with a RGB raster at scale would be to export the bands into single GeoTIFFS (and perhaps tile them for easier ingest), ingest them as different layers (as described), then work with them, and when it_Ñés time to export chunks of it, export each band by writing out the single band GeoTiff for that band. Afterwards you can combine the bands using GDAL.",4/9/15,1
lossyrob,"To write one band, here_Ñés some code that I would guess would work (didn_Ñét compile):",4/9/15,1
lossyrob,"```val layer = LayerId(name, zoom)val (lmd, params) = catalog.metaDataCatalog.load(layer)val md = lmd.rasterMetaDataval crs = md.crsval p = Point(x, y).reproject(LatLng, crs)val key = md.mapTransform(p)val tile = catalog.loadTile(layer, SpatialKey(key.col, key.row))val extent = md.mapTransform(key) // Get extent from keygeotrellis.raster.io.geotiff.GeoTiffWriter.write(path, tile, extent, crs)```",4/9/15,1
DarkManiel,@lossyrob how do i check the spark-submit version? I'm using spark-core 1.2.1 in my Build.scala,4/10/15,1
DarkManiel,"I just made a bunch of changes to CatalogService.scala but my changes aren't invoked when I run it, even if I restart the server. How does it load the files? Is an old version cached or something?",4/10/15,1
pomadchin,"@DarkManiel to check spark version possible by runing ```spark-shell``` In the output (while shell init) you will see smth like ```Welcome to      ____              __     / __/__  ___ _____/ /__    _\ \/ _ \/ _ `/ __/  '_/   /___/ .__/\_,_/_/ /_/\_\   version 1.1.1      /_/```",4/10/15,1
pomadchin,"@lossyrob I tried to make smth like that (tried 2.3.4-spark and 2.2.3-shaded-protobuf): https://gist.github.com/pomadchin/48d57b8f5b3525c96f69But nothing helped. Mb the problem is deeper. Or the problem in my curves hands. P.S. tried to exclude transitive dependencies from all libs.",4/10/15,1
pomadchin,"Just have no time to investigate it now (have to learn lots in geotrellis / sprak / hadoop / accumulo), but later planing to use spray with spark, so it should be solved later, or mb to use another framework (not spray)",4/10/15,1
pomadchin,"@lossyrob wow that works with tiff. amazing!Hm, and what rendering done with loaded tiff (seems to me wile ingesting)? quality loss?",4/10/15,1
pomadchin,Is it possible to get the original quality tiff?,4/10/15,1
pomadchin,and about src projections: noticed that geotiff writer not supporting normal mercator projection?,4/10/15,1
DarkManiel,"Ok, yeah I'm on 1.2.1",4/10/15,1
DarkManiel,"      ____              __     / __/__  ___ _____/ /__    _\ \/ _ \/ _ `/ __/  '_/   /___/ .__/\_,_/_/ /_/\_\   version 1.2.1      /_/",4/10/15,1
notthatbreezy,@DarkManiel - have you re-run assembly after making your changes? I've done that before after making changes to code and wondering why spark submit wasn't using them,4/10/15,1
DarkManiel,"thanks, yeah that was a dumb mistake on my part",4/10/15,1
lossyrob,"Hey @echeipesh, here_Ñés that index work that I did: https://gist.github.com/lossyrob/04acaf1d6e98c5dcfc2d",4/10/15,1
lossyrob,I_Ñém going to concentrate on the hadoop filter beyond the index stuff if you wanna pick that up.,4/10/15,1
echeipesh,"kk, will check it out, thanks.",4/10/15,1
lossyrob,"@pomadchin Not sure what you mean that geotiff writer does not support normal mercator projection? You pass in a CRS, and so it should be able to write any raster of any projection.",4/10/15,1
lossyrob,"@pomadchin also, the degredation you mention has to do with our ingest process, where we reproject, resample and tile the input raster according to the TMS zoom layout we_Ñére ingesting at, and the projection that is the target projection of the ingested raster",4/10/15,1
lossyrob,watching the portion of this video: https://www.youtube.com/watch?v=802-vNEFQtg where @echeipesh describes the ingest process might help,4/10/15,1
echeipesh,"Yeay, it is useful : )",4/10/15,1
lossyrob,"@echeipesh whenever you go over it, notice https://gist.github.com/lossyrob/04acaf1d6e98c5dcfc2d#file-index-gt-uzaygezen-scala-L211 line 211_Ñ_.we can_Ñét have the index invertable to a SpaceTimeKey since the SpaceTimeKey is an instant inside of a binned block of time; perhaps have to have some intermediate key index like we were talking about, where time has a layout also. Or perhaps we don_Ñét need index to really have that function. Not sure the solution there. SpatialIndex translates to a SpatialKey, but the index of N-dimensional space translates to a specifc col, row, but also a block of time that shouldn_Ñét be represented as a single instant (I know you know that, repeating for the crowd)",4/10/15,1
echeipesh,love the picture placement there,4/10/15,1
lossyrob,grrrrr,4/10/15,1
lossyrob,"I don_Ñét know if I have the original pic that_Ñés from but it_Ñés pretty great, I_Ñéll have to look it up",4/10/15,1
echeipesh,"Yes, I don_Ñét really know how to deal with that problem in the abstract. Unless I find a way to squerl away abritrary information in the mapping process I_Ñéll probably explore the Cube raster instead of raster stack idea where the cube could be sparse.",4/10/15,1
lossyrob,well we don_Ñét have to necessarily have the invertable function produce a key,4/10/15,1
lossyrob,"I def don_Ñét want to slog off the stack support though, or go in a direction where we_Ñére not coding to that use case from the start",4/10/15,1
echeipesh,"Right, we just need to have some mechanism to restore the full metadata",4/10/15,1
lossyrob,"that_Ñés like the column qualifier in Accumulo, where the actual key is created off of more data then the reverse of the index",4/10/15,1
echeipesh,Exactly,4/10/15,1
lossyrob,"it just turns out that for spatial tiles, the reverse of hte index is sufficient to hydrate the hwole key",4/10/15,1
lossyrob,"so really it_Ñés like there_Ñés an intermediate thing in the index, that it inverts to",4/10/15,1
lossyrob,liek a grid coord,4/10/15,1
lossyrob,"@echeipesh when you were saying that the getSplits call should take in the key splits, or read in the file that has the key splits, is there a good way to get that path to getSplits that you know of",4/10/15,1
lossyrob,seems like it_Ñés just going to be a lot of setting things into the job config,4/10/15,1
echeipesh,"Thats pretty much the way. But why a lot? That_Ñés just splits path appended to input path, right?",4/10/15,1
lossyrob,because the filter,4/10/15,1
lossyrob,"and we don_Ñét get input path, we get input paths",4/10/15,1
lossyrob,"well, at least how we_Ñére doing it now. with the wildcarding",4/10/15,1
lossyrob,"`final val SEQFILE_GLOB = ""/*[0-9]*/data_Ñù` is what we attach to the input path when we_Ñére setting it into the config, so inputsplits gets a bunch of data file paths",4/10/15,1
echeipesh,"But for one raster, right ?",4/10/15,1
lossyrob,yeah,4/10/15,1
echeipesh,"So there should only be one splits file, you can just place that into the config.",4/10/15,1
lossyrob,right,4/10/15,1
lossyrob,but also I_Ñéll have to place filter information,4/10/15,1
lossyrob,"so that_Ñés why I said, a lot of putting stuff into the config",4/10/15,1
echeipesh,"Ah, right",4/10/15,1
lossyrob,it_Ñés the only way to actually communicate with the InputFormat,4/10/15,1
lossyrob,which I just need to get over and lean on haha,4/10/15,1
echeipesh,I haven_Ñét seen anything else. Like InputFormat gets instantiated through reflection so it can_Ñét even have a constructor.,4/10/15,1
lossyrob,yeah. shouldn_Ñét be too bad once I get used to how to pass stuff through the conf,4/10/15,1
lossyrob,I_Ñém gonna have a similar challenge to the serialization/deserialization of the index when I need to store and retreive the FilterSet,4/10/15,1
lossyrob,I_Ñém wondering if we can solve both of those in one shot,4/10/15,1
lossyrob,or have some abstraction over the mechanism to do that sort of thing_Ñ_type based instantiation of different subclasses of a thing,4/10/15,1
echeipesh,"Could you handle index through reflection, in similar manner to InputFormat? Or do you have a better idea ?",4/10/15,1
lossyrob,no I mean just JSON serialization/deserialization,4/10/15,1
lossyrob,we_Ñéd have to ship it to the config by the same method if we needed it in the InputFormat,4/10/15,1
echeipesh,"Yeah, as long as it_Ñés an untyped interface, right ?",4/10/15,1
lossyrob,don_Ñét know what you mean about hta,4/10/15,1
echeipesh,When you deserialize the JSON you need to produce an object with an interface that does not have type parameters.,4/10/15,1
lossyrob,That_Ñés not necessarily true,4/10/15,1
lossyrob,I do that with the KeyBounds in accumulo package,4/10/15,1
echeipesh,I_Ñéll check that out,4/10/15,1
lossyrob,"```object KeyBounds {  implicit def keyBoundsFormat[K: JsonFormat]: RootJsonFormat[KeyBounds[K]] =    new RootJsonFormat[KeyBounds[K]] {      def write(keyBounds: KeyBounds[K]) =        JsObject(          ""minKey"" -> keyBounds.minKey.toJson,          ""maxKey"" -> keyBounds.maxKey.toJson        )      def read(value: JsValue): KeyBounds[K] =        value.asJsObject.getFields(""minKey"", ""maxKey"") match {          case Seq(minKey, maxKey) =>            KeyBounds(minKey.convertTo[K], maxKey.convertTo[K])          case _ =>            throw new DeserializationException(""${classOf[KeyBounds[K]] expected"")        }    }}```",4/10/15,1
lossyrob,it_Ñés just that the type bounds also have to have a JsonFormat,4/10/15,1
echeipesh,"Oh right, but you know to ask for specific K, sure",4/10/15,1
lossyrob,"right. and in the index and filter case, I_Ñéll know what the K is",4/10/15,1
lossyrob,because it_Ñéll be worked with inside the `reader[K]` or `writer[K]` functions,4/10/15,1
echeipesh,"ok, makes sense.",4/10/15,1
lossyrob,"here_Ñés a concern I_Ñém stumbling on_Ñ_.MapFiles don_Ñét provide a way to just read the next key, need to read the key and value at the same time",4/10/15,1
lossyrob,"it_Ñés like, MapFile is a wrapper over a sequence file, but doesn_Ñét provide all the functionality of a sequence file",4/10/15,1
lossyrob,so fine grain filtering could be a problem,4/10/15,1
lossyrob,Uzaygezen classes aren_Ñét serializable. So mapping a key to an index inside a rdd `map` function fails. fun fun fun,4/10/15,1
kyeah,"@lossyrob I'm trying to run the tests for the refactored catalogs, but I'm getting a `spray.json.DeserializationException: HadoopLayerMetaData expected` error when I try to generate the test files (`test:run` under the spark project).",4/12/15,1
kyeah,Is there anything else I should be doing to get this to work?,4/12/15,1
lossyrob,"hm, I_Ñém not sure. test:run should just blow away the old test files and create new ones",4/12/15,1
lossyrob,@kyeah what_Ñés the stack trace?,4/12/15,1
kyeah,"```[error] Exception in thread ""main"" spray.json.DeserializationException: HadoopLayerMetaData expected[error] 	at geotrellis.spark.io.hadoop.HadoopLayerMetaData$HadoopLayerMetaDataFormat$.read(HadoopLayerMetaData.scala:31)[error] 	at geotrellis.spark.io.hadoop.HadoopLayerMetaData$HadoopLayerMetaDataFormat$.read(HadoopLayerMetaData.scala:18)[error] 	at spray.json.JsValue.convertTo(JsValue.scala:31)[error] 	at geotrellis.spark.io.hadoop.HadoopLayerMetaDataCatalog$$anonfun$geotrellis$spark$io$hadoop$HadoopLayerMetaDataCatalog$$recurse$1$1.apply(HadoopLayerMetaDataCatalog.scala:57)[error] 	at geotrellis.spark.io.hadoop.HadoopLayerMetaDataCatalog$$anonfun$geotrellis$spark$io$hadoop$HadoopLayerMetaDataCatalog$$recurse$1$1.apply(HadoopLayerMetaDataCatalog.scala:35)[error] 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)[error] 	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108)[error] 	at geotrellis.spark.io.hadoop.HadoopLayerMetaDataCatalog.geotrellis$spark$io$hadoop$HadoopLayerMetaDataCatalog$$recurse$1(HadoopLayerMetaDataCatalog.scala:35)[error] 	at geotrellis.spark.io.hadoop.HadoopLayerMetaDataCatalog$$anonfun$geotrellis$spark$io$hadoop$HadoopLayerMetaDataCatalog$$recurse$1$2.apply(HadoopLayerMetaDataCatalog.scala:67)[error] 	at geotrellis.spark.io.hadoop.HadoopLayerMetaDataCatalog$$anonfun$geotrellis$spark$io$hadoop$HadoopLayerMetaDataCatalog$$recurse$1$2.apply(HadoopLayerMetaDataCatalog.scala:67)[error] 	at scala.collection.immutable.List.foreach(List.scala:318)[error] 	at scala.collection.generic.TraversableForwarder$class.foreach(TraversableForwarder.scala:32)[error] 	at scala.collection.mutable.ListBuffer.foreach(ListBuffer.scala:45)[error] 	at geotrellis.spark.io.hadoop.HadoopLayerMetaDataCatalog.geotrellis$spark$io$hadoop$HadoopLayerMetaDataCatalog$$recurse$1(HadoopLayerMetaDataCatalog.scala:67)[error] 	at geotrellis.spark.io.hadoop.HadoopLayerMetaDataCatalog$$anonfun$geotrellis$spark$io$hadoop$HadoopLayerMetaDataCatalog$$recurse$1$2.apply(HadoopLayerMetaDataCatalog.scala:67)[error] 	at geotrellis.spark.io.hadoop.HadoopLayerMetaDataCatalog$$anonfun$geotrellis$spark$io$hadoop$HadoopLayerMetaDataCatalog$$recurse$1$2.apply(HadoopLayerMetaDataCatalog.scala:67)[error] 	at scala.collection.immutable.List.foreach(List.scala:318)[error] 	at scala.collection.generic.TraversableForwarder$class.foreach(TraversableForwarder.scala:32)[error] 	at scala.collection.mutable.ListBuffer.foreach(ListBuffer.scala:45)[error] 	at geotrellis.spark.io.hadoop.HadoopLayerMetaDataCatalog.geotrellis$spark$io$hadoop$HadoopLayerMetaDataCatalog$$recurse$1(HadoopLayerMetaDataCatalog.scala:67)[error] 	at geotrellis.spark.io.hadoop.HadoopLayerMetaDataCatalog.<init>(HadoopLayerMetaDataCatalog.scala:71)[error] 	at geotrellis.spark.io.hadoop.HadoopRasterCatalog$.apply(HadoopRasterCatalog.scala:55)[error] 	at geotrellis.spark.testfiles.TestFiles$.catalog(TestFiles.scala:25)[error] 	at geotrellis.spark.testfiles.GenerateTestFiles$.main(GenerateTestFiles.scala:151)[error] 	at geotrellis.spark.testfiles.GenerateTestFiles.main(GenerateTestFiles.scala)```",4/12/15,1
kyeah,sorry for the big block haha,4/12/15,1
lossyrob,ok. so I guess we need to change that a bit_Ñ_I_Ñém refactoring the test files right now anyway so I can take care of that,4/12/15,1
lossyrob,but the workaround is,4/12/15,1
lossyrob,just `rm -r spark/src/test/resources/test-catalog`,4/12/15,1
lossyrob,it seems like it_Ñés trying to read in the catalog that_Ñés already there instead of just creating a new one whether or not one exists,4/12/15,1
kyeah,seems like it; worked like a charm afterwards,4/12/15,1
kyeah,thanks!,4/12/15,1
lossyrob,np!,4/12/15,1
lossyrob,note: the CoordinateSpaceTimeTestFileValues isn_Ñét working right,4/12/15,1
lossyrob,so the values will be not as advertised in the comment,4/12/15,1
lossyrob,fixing that now,4/12/15,1
kyeah,"alright, I'll avoid that one for now.",4/12/15,1
lossyrob,I_Ñéll ping you when I get a better set of TestFiles in a PR_Ñ_it_Ñés in the middle of a bunch of other work but I think I can pull it out it_Ñés own commit.,4/12/15,1
lossyrob,Filtering works for Hadoop. sweet.,4/13/15,1
lossyrob,A substation refactor of the catalog API in geotrellis-spark happened. You can see the changes here: https://github.com/geotrellis/geotrellis/pull/1058,4/13/15,1
pomadchin,"@lossyrob awesome, and new indexes",4/14/15,1
pomadchin,Hm and multi band would be available not in the nearest release? :p waiting a lot,4/14/15,1
lossyrob,"@pomadchin that's next on my list, so it'll be in 0.10",4/14/15,1
lossyrob,"@kyeah RE: combining where clauses using logical operators_Ñ_it looks like cassandra doesn_Ñét support _ÑÒOR_Ñù operators in WHERE clauses, so that idea is bunk.",4/14/15,1
lossyrob,"another thing that I_Ñém reading is that LIMIT defaults to 10,000, which might get in the way if we want to pull back a lot of tiles",4/14/15,1
ejc123,"The is probably a bad time to ask, but is there something like cassandra that does support OR in WHERE?   ",4/14/15,1
lossyrob,well we get that functionality from Accumulo,4/14/15,1
lossyrob,"So if we make a space time query, like, the extent of Kansas from 2010 - 2015, we can turn that into a set of index ranges, from our Space Filling Curve",4/14/15,1
lossyrob,"now we have a potentially large set of  { (i0, I0), (i1, I1), _Ñ_, (iN, IN) } index ranges",4/14/15,1
lossyrob,and we want to return values that can be in range0 OR range1 OR...,4/14/15,1
lossyrob,accumulo can do that for us,4/14/15,1
ejc123,"ok, i'm not following where cassandra is fitting in then.  probably due to my skimming discussions :-)",4/14/15,1
lossyrob,we_Ñére trying to provide Cassanda as an alternative backend to Accumulo.,4/14/15,1
lossyrob,"What we_Ñéll have is a set of backends supported that you can choose from: HDFS, Accumulo, Cassandra, and S3",4/14/15,1
lossyrob,"Accumulo and HDFS are the onese that are the most robust/tested now. @kyeah is working on the Cassandra backend and is able to save a RasterRDD and pull out individual tiles, or the whole RasterRDD",4/14/15,1
lossyrob,the trouble we_Ñére having now is how do we do filtering on load?,4/14/15,1
lossyrob,Which Accumulo (and recently HDFS) can do well now.,4/14/15,1
ejc123,that makes sense.,4/14/15,1
ejc123,"like I said, I've been skimming",4/14/15,1
lossyrob,good to know :),4/14/15,1
lossyrob,"I just made an issue about boxing and Function3, and how I think we can lean on Macros to solve the issue. https://github.com/geotrellis/geotrellis/issues/1060",4/15/15,1
lossyrob,"@non like I said in the issue comment, I_Ñéd be interested in your thoughts, given you are a macro magician.",4/15/15,1
lossyrob,"When I was writing the macros we have currently, I based my start off of a project of yours that I believe was doing something very similar to this; I can_Ñét seem to find it.",4/15/15,1
lossyrob,something about inlining anonymous functions,4/15/15,1
moradology,,4/15/15,1
lossyrob,"@moradology pointed out this, which seems like a possible start to a solution: http://stackoverflow.com/questions/12087259/avoiding-boxing-unboxing-within-function",4/15/15,1
lossyrob,"not using quasiquotes, which is painful",4/15/15,1
lossyrob,but could be a good start,4/15/15,1
lossyrob,"Erik said he might cook up a proof of concept, which would be awesome",4/15/15,1
lossyrob,"@non created a playground project to try some stuff out. Basically just stole the Inliner from spire and got something like `tile.map(I { (col, row, z) => col + row + z })` to work. But really looking for `tile.map { (col, row, z) => col + row + z }` to work, not sure how to pull that off.",4/16/15,1
lossyrob,https://github.com/lossyrob/macromagic,4/16/15,1
lossyrob,It seems like I_Ñéd need to write the macro as such:,4/16/15,1
lossyrob,"```object Macros {  def intMapper_impl(c: Context)(f: c.Expr[(Int, Int, Int) => Int]): c.Expr[Tile] = ???}```with the call like```trait Tile {  def map(f: (Int, Int, Int) => Int): Tile = macro tinMapper_impl}```but I don_Ñét have access to the Tile type in the macro project. Not sure if there_Ñés a trick to get around that.",4/16/15,1
echeipesh,Is it possible to pull out self type from Context since this is a method on Tile returning Tile ?,4/16/15,1
lossyrob,hmmmm good suggestion,4/16/15,1
lossyrob,"i_Ñéd need to get _ÑÉthis_Ñé in the macro, since it_Ñés a method on Tile, and then use some trait to call the other instance of map that takes the IntMapper",4/16/15,1
echeipesh,Looking at indexing code in context of catalog and coming to conclusion that it should be catalog implemintation detail. Perhaps set it up so it can be changed easily with subclassing.,4/16/15,1
lossyrob,Catalog provides different mechanisms that are indexed diffently per key,4/16/15,1
lossyrob,"would it make sense to have it as a implicit tag on the key type, that the user can change if they `implici val index: KeyIndex[SpaceTimeKey] = ???`",4/16/15,1
echeipesh,"Doesn_Ñét look like it, for instance HilbertIndex needs `class HilbertSpatialKeyIndex(keyBounds: KeyBounds[SpatialKey], xResolution: Int, yResolution: Int) extends KeyIndex[SpatialKey]`",4/16/15,1
echeipesh,which only becomes available to the provider in the right context.,4/16/15,1
echeipesh,"So if you want different indexing, you need a different provider. That seems ok to me.",4/16/15,1
lossyrob,That_Ñés why there are the Methods,4/16/15,1
lossyrob,`HilbertKeyIndexMethod` takes user params,4/16/15,1
lossyrob,and then the catalog can use that to create the actual KeyIndex,4/16/15,1
echeipesh,"Hmm, let me see if I can use them",4/16/15,1
lossyrob,those are the ones that are currently being explicitly passed into the write methods,4/16/15,1
lossyrob,"which we can make implicit for defaults. not sure whether they should also be available explicitly, because they can be user defined, like defining the year bounds.",4/16/15,1
lossyrob,or time resolutions,4/16/15,1
lossyrob,looks like your generic typing idea is working @echeipesh ,4/16/15,1
echeipesh,with the maco? sweet beans,4/16/15,1
lossyrob,"```trait Mapper[T] {  def mapper(mapper: IntMapper): T}trait IntMapper {  def apply(col: Int, row: Int, z: Int): Int}object Macros {  def intMapper_impl2[T](c: Context)(f: c.Expr[(Int, Int, Int) => Int]): c.Expr[T] = {    import c.universe._    val self = c.Expr[Mapper[T]](c.prefix.tree)    val tree = q""""""$self.mapper(new IntMapper { def apply(col: Int, row: Int, z: Int): Int = $f(col, row, z) })""""""    new InlineUtil[c.type](c).inlineAndReset[T](tree)  }}```",4/16/15,1
lossyrob,"does_Ñént work with overloading the method name `map`, but it_Ñés a pretty good solution I think",4/16/15,1
echeipesh,And so readable too,4/16/15,1
lossyrob,well the call looks like this:,4/16/15,1
lossyrob,_ÑÉ'',4/16/15,1
lossyrob," ```val x = w.map { (col, row, z) => z + 1 }```",4/16/15,1
lossyrob,ug,4/16/15,1
echeipesh,*,4/16/15,1
lossyrob,lol. slowest loading animation ever,4/16/15,1
lossyrob,that_Ñés making my desktop notifications go nuts,4/16/15,1
echeipesh,"oh shit, sorry, didn_Ñét think of that",4/16/15,1
lossyrob,lol,4/16/15,1
lossyrob,"ok, naming time",4/16/15,1
lossyrob,here_Ñés what I would want in an ideal world,4/16/15,1
lossyrob,"```trait Tile {  def map(f: Int => Int): Tile  def map(f: Double => Double): Tile  def map(f: (Int, Int, Int) => Int): Tile  def map(f: (Int, Int, Double) => Double): Tile  def combine(f: (Int, Int) => Int): Tile  def combine(f: (Int, Int, Int, Int) => Int): Tile  def combine (f: (Double, Double) => Double): Tile  def combine(f: (Int, Int, Double, Double) => Double): Tile}```",4/16/15,1
lossyrob,"`map` and `combine` over ints and double, with our without `(col, row)` information",4/16/15,1
lossyrob,"we already have `map` and `mapDouble`, `combine` and `combineDouble` to get around the Int or Double thing",4/16/15,1
lossyrob,"oh and we can_Ñét have the same names because type erasure makes teh compiler confused, so you_Ñéd have to put type annotations on all your funtions which is annoying. e.g. can_Ñét write `map { z => z + 1}`, have to write `map { z: Int => z + 1 }`",4/16/15,1
lossyrob,any suggestions?,4/16/15,1
echeipesh,"wait, what is `map(f: (Int, Int, Int) => Int): Tile` doing exactly ?",4/16/15,1
lossyrob,mapping with col and row in mind,4/16/15,1
lossyrob,"there_Ñés also a `map[T](f: (Int, Int, Int) => T)` needed. which was Monsour_Ñés use case",4/16/15,1
lossyrob,which we can macro-ize similarly,4/16/15,1
lossyrob,currently if you need row and column stuff the way to do it is,4/16/15,1
lossyrob,"```cfor(0)(_ < tile.rows, _ + 1) { row =>  cfor(0)(_ < tile.cols, _ + 1) { col =>    val z = tile.get(col, row)    // do stuff with (col, row, z)  }}```",4/16/15,1
lossyrob,"but since we_Ñére hiding the data in compressed strips or tiles with the GeoTiffTile, we need the implementation of the Tile to preform loops over itself, because only it knows how to optimally do that",4/16/15,1
echeipesh,right,4/16/15,1
lossyrob,"so really it_Ñés just trying to get read of `get` and `getDouble`, in any way possible",4/16/15,1
lossyrob,"which we won_Ñét be able to do always I don_Ñét think, but this is a step towards that",4/16/15,1
non,@lossyrob i think you will need for your macro to be able to see the target type,4/16/15,1
non,i had created my own project to play in but i can move to macromagic if you want,4/16/15,1
non,but i'm pretty sure you'll need some kind of interface available there,4/16/15,1
lossyrob,That_Ñés what I have so far,4/16/15,1
lossyrob,lemme point you at it,4/16/15,1
lossyrob,"I could move to yoru project as well, theres_Ñé not much code",4/16/15,1
lossyrob,https://github.com/lossyrob/macromagic/blob/master/macros/src/main/scala/macros.scala#L6,4/16/15,1
non,hm ok,4/16/15,1
non,"so you have `I { (x, y, z) => ... }` working?",4/16/15,1
lossyrob,yeah,4/16/15,1
lossyrob,"but also, with the other macro",4/16/15,1
lossyrob,"it works with `map { (x,y,z) =. _Ñ_ }`",4/16/15,1
non,great.,4/16/15,1
lossyrob,https://github.com/lossyrob/macromagic/blob/master/macros/src/main/scala/macros.scala#L15,4/16/15,1
non,do you have any remaining issues? seems like you figured it out.,4/16/15,1
lossyrob,Does that seem right? Is it really that easy? Haha was expecting it to be more painful :),4/16/15,1
non,"ok so -- i will try to do a more detailed review. but yeah, i think it's pretty easy.",4/16/15,1
non,at least once you have that `InlineUtil` stuff,4/16/15,1
lossyrob,"one question I have_Ñ_.what are the limitations of the Inliner? Like, if I have a reference to a outside val closed over, will that break the inlining?",4/16/15,1
non,no i think that should be ok,4/16/15,1
lossyrob,yeah `InlineUtil` made it super easy,4/16/15,1
lossyrob,I_Ñéll have to go over that more closely to see how it_Ñés doing what it_Ñés doing. But it seems to work like a charm,4/16/15,1
non,so -- here is what i will say -- i think that there are very rare cases where ownership will get messed up and the compiler will fail after inlining,4/16/15,1
non,i don't hit those -- and i'm not sure how you would hit those issues -- but i know eugene told me it was possible.,4/16/15,1
non,one thing i was thinking of doing was publishing the inlining stuff in a standalone project,4/16/15,1
non,that supports 2.10 and 2.11,4/16/15,1
non,"that way spire, geotrellis, and anyone else doing this could use a shared startegy, and share bug fixes",4/16/15,1
lossyrob,"i_Ñém currently pointing at _ÑÒorg.spire-math_Ñù %% ""spire-macros""",4/16/15,1
non,gotcha.,4/16/15,1
non,yeah that's a fine solution for now too.,4/16/15,1
non,"but eventually it might be nice to split it out more, as we did with machinist.",4/16/15,1
lossyrob,"ok, yeah I_Ñém cool with using that/ pushing anything I find to htat repo",4/16/15,1
lossyrob,gotcha,4/16/15,1
non,thanks!,4/16/15,1
non,good job! :),4/16/15,1
non,(and i will try to take a more detailed look. but to my eyes it looks like what i would have done.),4/16/15,1
lossyrob,should I be pointing to machinist?,4/16/15,1
lossyrob,or does it not have the Inline stuff,4/16/15,1
non,no -- the inlining stuff is different.,4/16/15,1
lossyrob,ok gotcha,4/16/15,1
non,you only need machinist if you are doing the symbolic operator -> method rewriting for type classes,4/16/15,1
non,"that's something i'd expect scalaz to use, but not geotrellis",4/16/15,1
non,"(for example, cats is using machinist)",4/16/15,1
lossyrob,"awesome. well I_Ñéll be on the lookout if you break out InlineUtil & co, but otherwise I_Ñéll be pointing at spire-macro",4/16/15,1
non,great,4/16/15,1
lossyrob,"thanks for that! glad there was something to lean on out in the wild, would have spent many an hour otherwise",4/16/15,1
non,you're very welcome! glad it worked out,4/16/15,1
echeipesh,,4/16/15,1
lossyrob,,4/16/15,1
non,@joshmarcus http://snapsat.org/,4/17/15,1
lossyrob,that_Ñés pretty awesome,4/17/15,1
non,yeah i was thinking it's the sort of thing that geotrellis could do pretty well,4/17/15,1
lossyrob,we_Ñére working on it :),4/17/15,1
non,:),4/17/15,1
lossyrob,actually working off of a DOE grant to do something very similar,4/17/15,1
lossyrob,more in line with mosaicing over multiple images,4/17/15,1
lossyrob,"this is compositing on one image. which is pretty cool. aweomse that Frank Warmerdam helped put up that Lansat 8 imagery on S3, and maintains that process",4/17/15,1
echeipesh,"I would really like to figure out why geotrellis project needs to do full dependency resolution on each compile, even if nothing has changed. It doesn_Ñét look like other sbt projects need to do this.",4/17/15,1
non,is geotrellis using a SNAPSHOT ?,4/17/15,1
lossyrob,just caliper,4/17/15,1
lossyrob,which we_Ñére pulling from your server,4/17/15,1
lossyrob,"```  val caliper       = (""com.google.code.caliper"" % ""caliper"" % ""1.0-SNAPSHOT""    from ""http://plastic-idolatry.com/jars/caliper-1.0-SNAPSHOT.jar"")```",4/17/15,1
non,"hm. i have a lot of other projects using that too, which i don't think have the same issue.",4/17/15,1
non,@echeipesh so -- i'm not sure why it happens but it might be worth looking into http://www.scala-sbt.org/0.13/docs/Cached-Resolution.html,4/17/15,1
lossyrob,I_Ñém actually not sure the issue. @echeipesh I don_Ñét think I_Ñém experiencing the same thing. Are you saying it_Ñés doing the list of dep resolution for each compile? That isn_Ñét happening on my copy,4/17/15,1
echeipesh,"really? Ok, I_Ñéll check with Nathan to see if it happens there. Maybe I_Ñém just SBT cursed.",4/17/15,1
moradology,"yeah, i'm not seeing that behavior on every compile",4/17/15,1
lossyrob,"@echeipesh been trying to think of why typing RasterMetaData was a bad idea when I tried it, but can_Ñét think of it. Maybe it was something with the old architecture, I remember thinking that before the catalog work started coming together. So maybe there_Ñés no reason not to type it on K",4/17/15,1
echeipesh,"cool, that_Ñés encouraging.",4/17/15,1
echeipesh,"re: resolution, it seems blowing away ~/.m2 made the problem go away",4/17/15,1
lossyrob,hmm. weird_Ñ_wonder what you had in there before you dropped it?,4/17/15,1
echeipesh,world will never know,4/17/15,1
lossyrob,"@moradology you had a question about `getSplits` in the writer providers, that_Ñés for Hadoop right?",4/17/15,1
moradology,it was for accumulo,4/17/15,1
lossyrob,I wrote a simplified version of one of them. So there_Ñés two different ways of doing it.,4/17/15,1
lossyrob,oh right,4/17/15,1
moradology,but @echeipesh helped me out with it,4/17/15,1
lossyrob,but really they don_Ñét need to take in keys at all. they could just be based on index,4/17/15,1
lossyrob,and just use the simplified version,4/17/15,1
lossyrob,is that what you did?,4/17/15,1
moradology,"yeah, i put the simpler one on the LUB",4/17/15,1
lossyrob,cool,4/17/15,1
moradology,and got rid of the other,4/17/15,1
lossyrob,the simpler one just splits the index space into even segments right?,4/17/15,1
moradology,"yeah, uses the cfor loop over split size and the min/max ",4/17/15,1
lossyrob,cool,4/17/15,1
lossyrob,"fyi I left a set of TODO_Ñés scattered about the code. So you could grep for those. I should have left one there, sorry about that.",4/17/15,1
st028,@lossyrob I think I may have tracked down the troubles with the interpolation methods,4/19/15,1
lossyrob,Oh yeah? All of them?,4/19/15,1
st028,For average it looks like they translate a 2 x 2 box to the source raster,4/19/15,1
st028,Which results in 2x1 2x2 and 2x3 pixel boxes that it uses to calculate the average,4/19/15,1
st028,"I think because we don't do this exactly, this is causing the methods to fail",4/19/15,1
lossyrob,Ok. Are all resampling methods in gdal doing tht?,4/19/15,1
st028,https://github.com/OSGeo/gdal/blob/trunk/gdal/alg/gdalwarpkernel.cpp#L4682,4/19/15,1
st028,Yep. I pulled down the gdal code and added some prints,4/19/15,1
st028,I think the line that I linked (as well as the next one) is what's causing the difference (at least with average),4/19/15,1
lossyrob,Ok awesome. So if you imitate that method that should fix it?,4/19/15,1
st028,I think so,4/19/15,1
st028,"It's a little strange though, because for average their documentation gives me the impression that the entire raster becomes the average",4/19/15,1
st028,but from their code this is not the case,4/19/15,1
lossyrob,The entire raster becoming the average would be pretty useless for resampling...maybe a big in the focs,4/19/15,1
lossyrob,Taking a 2x2 neighborhood around the projected point sounds right,4/19/15,1
st028,The only assignment that I can find for that function pointer on gdalwarpkernel.cpp:4682 is to this function: https://github.com/OSGeo/gdal/blob/70c634b835530d5d184b76b7de71f2312af8466e/gdal/alg/gdal_tps.cpp#L292,4/19/15,1
st028,Actually I may have spoken too soon,4/19/15,1
st028,"Added some prints, looks like this is the function: https://github.com/OSGeo/gdal/blob/70c634b835530d5d184b76b7de71f2312af8466e/gdal/alg/gdaltransformer.cpp#L1724",4/19/15,1
lossyrob,"Not at a comp right now, I'll take a look later. Seems like your on the right path tho",4/19/15,1
lossyrob,"@st028 Here_Ñés an issue you might run into: Make sure the GeoTiffs you are reading in aren_Ñét of an Unsigned type_Ñ_if you `gdalinfo` and next to the band information, you should make sure it doesn_Ñét say `UInt16` or `UInt64`. The GeoTiff reader currently does not handle unsigned types correctly. This is a problem for the NLCD tile.",4/20/15,1
lossyrob,"I_Ñém working on some big changes to GeoTiff reading which will fix that. But I just saw a test fail because of the unsigned stuff, so be weary. You can do a format change with `gdal_translate -ot Int32` or something to get around leaning on unsigned types",4/20/15,1
lossyrob,"getting some great speedup on the GeoTiff reader, e.g.",4/20/15,1
lossyrob,"```[info]    benchmark   us linear runtime[info]   NativeZLib  666 =====[info] GeotoolsZLib 3365 ==============================[info]      NewZLib  420 === ```",4/20/15,1
non,nice :),4/20/15,1
sandan,is there a way to build and test just a subproject in the geotrellis project? I run the sbt script and build and test that way but it takes up a lot of space.,4/20/15,1
pomadchin,it is possible to select project in an sbt console (,4/20/15,1
echeipesh,```./sbt _ÑÒproject raster_Ñù compile``` will just compile the subproject.,4/20/15,1
lossyrob,or inside the sbt console,4/20/15,1
lossyrob,"```./sbtproject raster-test~test-only *.TestYouWantToRunSpec```",4/20/15,1
lossyrob,"@ejc123 hey, I_Ñém starting to work a MultiBandTile trait. And I_Ñém wondering if you have input on what things you would want on the trait.",4/20/15,1
lossyrob,Some ideas that I have are,4/20/15,1
lossyrob,"```trait MultiBandTile {  def band(i: Int): SingleBandTile  def map(band1: Int)(f: Int => Int): SingleBandTile  def map(band1: Int, band2: Int)(f: (Int, Int) => Int): SingleBandTile}```",4/20/15,1
lossyrob,where mapping would be over one or more band numbers,4/20/15,1
lossyrob,`combine` would behave similarly,4/20/15,1
lossyrob,"so if you wanted to do NDVI you could do something like `multiBandTile.combineDouble(IRIndex, VISIndex) { (ir, vis) => (ir - vis) / (ir + vis) }`",4/20/15,1
lossyrob,looking for spitballed ideas by anyone here,4/20/15,1
ejc123,"I'll have to think about it a bit more, but that looks good so far.  Any way to map over > 2 bands? ",4/20/15,1
lossyrob,can you describe a use case example?,4/20/15,1
lossyrob,"just wondering what your mapping from/to, what the result is",4/20/15,1
ejc123,"For example, from snapsat.org (cool site btw), it seems pretty common to map 3 bands to RGB  -- don't know that it's a computational thing, but it would be pretty cool.",4/20/15,1
ejc123,I'm far from an expert on this stuff :-),4/20/15,1
lossyrob,Hmm right. So maybe you_Ñére actually creating a 3 band raster out of a larger band raster?,4/20/15,1
lossyrob,"or would it be mapping the 3 bands to a single value? Like `combine(2, 3, 5) { (b1, b2, b3) => combineIntoRGB(b1, b2, b3) }`",4/20/15,1
lossyrob,"**NOTE**: Hector's EC2 deployment scripts, which was located in his github account as hectcastro/vagrant-geotrellis-mesos-spark, has moved to geotrellis/geotrellis-ec2-cluster. Please switch your remotes to: https://github.com/geotrellis/geotrellis-ec2-cluster.git @Pshrub et al who were using it",4/20/15,1
ejc123,"I was thinking of the latter; mapping 3 bands to 1 value.  reducing the number of bands may be useful, though.  depends on exactly how we load these MultiBandTiles.  Not that this is limited to landsat (that's my limited experience), but IIRC they are packaged as 1 band per .tiff  In this case, it might be easier to only load the ones you want.  however, if we have some way to slurp in the whole directory with all the bands, it might be useful to have an option to reduce them.  does that make sense?",4/20/15,1
lossyrob,"I think in that case you would just load the single band tiles. Maybe there would be an operation for creating a mulitband tile out of tiles, but we already have a lot of support for combining separate tiles",4/20/15,1
lossyrob,"we can reduce a Sequence of tiles, like Seq(band1, band2, band3).localAdd et al",4/20/15,1
lossyrob,band1.combine(band2) { _Ñ_ ,4/20/15,1
lossyrob,},4/20/15,1
lossyrob,"but the thing about reading in some MultiBand GeoTiffs is that there is pixel interleaving, and so to avoid having to separate them into their own bands we can operate on them just as is",4/20/15,1
lossyrob,"if we were to read multiband out of a directory, then there_Ñés the logic of assigning file names to band numbers. Which I guess could be put in place in the library. but also seems a bit like a client concern...",4/20/15,1
lossyrob,"perhaps there is some benefit though to saying MutliBandTile(band1, band2, band3)",4/20/15,1
lossyrob,"and then utilizing functions on it like `combineBands(1, 2, 3) { (z1, z2, z3) => RGB(z1, z2, z3) }: SingleBandTile`",4/20/15,1
ejc123,"I agree, that does sounds useful.",4/20/15,1
lossyrob,@johanstenberg92 : I_Ñém stumped on why we invert Bytes here in the Bit reading of the GeoTiff reader. Why do we need to do that? https://github.com/geotrellis/geotrellis/blob/67a1a43cb9b64d0d7550fd88686b29e891e97f55/raster/src/main/scala/geotrellis/raster/io/geotiff/reader/ImageConverter.scala#L184,4/20/15,1
lossyrob,"@moradology @echeipesh if filterSet.isEmpty, we should have a simpler code path that doesn_Ñét actually set any unneeded range filters on Accumulo, right? https://github.com/geotrellis/geotrellis/blob/feature/catalog-refactor/spark/src/main/scala/geotrellis/spark/io/accumulo/spacetime/SpaceTimeRasterRDDReaderProvider.scala#L123",4/21/15,1
lossyrob,In Hadoop we do a simpler thing if there are no filters:https://github.com/geotrellis/geotrellis/blob/feature/catalog-refactor/spark/src/main/scala/geotrellis/spark/io/hadoop/spacetime/SpaceTimeRasterRDDReaderProvider.scala#L79,4/21/15,1
echeipesh,That_Ñés true.,4/21/15,1
lossyrob,cool. I_Ñéll mark it down on the catalog-refector-refactor pull request as a TODO,4/21/15,1
st028,@lossyrob https://github.com/OSGeo/gdal/blob/70c634b835530d5d184b76b7de71f2312af8466e/gdal/alg/gdaltransformer.cpp#L1724,4/21/15,1
lossyrob,https://github.com/OSGeo/gdal/blob/70c634b835530d5d184b76b7de71f2312af8466e/gdal/alg/gdalwarpkernel.cpp#L3489,4/21/15,1
sandan,Anyone know of a way to alter the test environment for spark so I can run some tests? I seem to not have enough heap space. I'm in the catalog-refactor branch. Here's a gist: https://gist.github.com/sandan/cca37b9f87589033e2ea,4/21/15,1
echeipesh,"That_Ñés strange, spark subproject is already configrued to use up to 8G, that should apply to test as well: https://github.com/geotrellis/geotrellis/blob/master/project/Build.scala#L368 Does your machine have free momemory?",4/21/15,1
lossyrob,@sandan can you point to the test that it_Ñés trying to run? Is there some run-away memory collection happening?,4/21/15,1
sandan,@echeipesh I should have enough free memory to run 8G comfortably. @lossyrob I'm trying to run a test I moved from geotrellis/index/ to the spark subproject. I moved everything so it compiles fine. ,4/21/15,1
lossyrob,strange. can you run other tests?,4/21/15,1
sandan,@lossyrob I run into the same problem when I just do test: https://gist.github.com/sandan/6afa8a89df3503bc7197,4/21/15,1
lossyrob,can you run the tests in project `raster-test`?,4/21/15,1
sandan,they run fine: https://gist.github.com/sandan/d057faff4d15c9321bf9,4/21/15,1
sandan,@lossyrob I tried running the spark tests on a fresh clone on the catalog-refactor branch and I get the same error,4/21/15,1
lossyrob,weird,4/21/15,1
lossyrob,what_Ñés yours os?,4/21/15,1
sandan,linux,4/21/15,1
sandan,Could I just isolate the files needed to test the classes in the spark subproject under index/ ?,4/21/15,1
lossyrob,can you run test:run?,4/21/15,1
sandan,in raster-test?,4/21/15,1
lossyrob,no in spark,4/21/15,1
sandan,https://gist.github.com/sandan/2e6a62168cb2b4dc35b5,4/21/15,1
lossyrob,Cna you change this 8g to 3g here https://github.com/geotrellis/geotrellis/blob/master/project/Build.scala#L368,4/21/15,1
lossyrob,"it looks like there_Ñés more heap space being requested then available. but you said that there_Ñés a lot of free memory, so I_Ñém a bit confused",4/21/15,1
sandan,not sure what to make of it either,4/21/15,1
sandan,that did the trick,4/21/15,1
lossyrob,ok. for some reason if you have enough free memory Java doesn_Ñét have access to it,4/21/15,1
lossyrob,are you on a VM?,4/21/15,1
sandan,nope,4/21/15,1
sandan,for the record: https://gist.github.com/sandan/44411f9c86ce2a76448b,4/21/15,1
sandan,"Ahh, since I'm using a machine on campus it has restrictions on RAM usage ... I should be good up to 6Gb",4/21/15,1
lossyrob,Oh haha mystery solved.,4/21/15,1
sandan,"haha yup, thanks for the help though",4/21/15,1
lossyrob,@nzimmerman: can you explain your comment on your PR more? I'm not sure I get why Accumulo can't skip filtering when `filterSet.isEmpty` is true,4/22/15,1
moradology,"on accumulo, you have to deal with this: ```scala    if (timeFilters.isEmpty) {      val minKey = keyBounds.minKey.temporalKey      val maxKey = keyBounds.maxKey.temporalKey      timeFilters = Seq((minKey.time, maxKey.time))    }    for ( (start, end) <- timeFilters) {      val props =  Map(        ""startBound"" -> start.toString,        ""endBound"" -> end.toString,        ""startInclusive"" -> ""true"",        ""endInclusive"" -> ""true""      )      InputFormatBase.addIterator(job,        new IteratorSetting(2, ""TimeColumnFilter"", ""org.apache.accumulo.core.iterators.user.ColumnSliceFilter"", props))    }```",4/22/15,1
moradology,that didn't quite work as anticipated.,4/22/15,1
moradology,"Anyway, the point is that there's a default behavior for column filters in accumulo when filters are empty and you have a spacetimekey - there's nothing similar going on for accumulo w/r/t spatialkeys",4/22/15,1
lossyrob,is the last _ÑÒaccumulo_Ñù supposed to be hadoop?,4/22/15,1
moradology,"nope, the point is that you can't abstract over default behavior so easily - either accumuloreader needs to implement a fallback behavior (noop in the case of spatialkeys)",4/22/15,1
moradology,OR we push all logic into the concrete classes,4/22/15,1
moradology,the code i tried to post above is code that only happens for spacetime keys,4/22/15,1
lossyrob,do some sort of default filters need to be applied if there are no filters?,4/22/15,1
moradology,"yeah, for spacetime keys on accumulo only",4/22/15,1
lossyrob,I know that defalut space filters need to be applied if there are Space filters and no time filters. Or that default space filters need to be applied if there are no space filters and no time filters,4/22/15,1
lossyrob,"but if there are no filters, I don_Ñét think there should be defaults necessary",4/22/15,1
echeipesh,zoom level,4/22/15,1
moradology,https://github.com/moradology/geotrellis/blob/feature/catalog-refactor/spark/src/main/scala/geotrellis/spark/io/accumulo/spacetime/SpaceTimeRasterRDDReader.scala#L53,4/22/15,1
echeipesh,basically if there are no filters it needs to know to snap to the zoom level for the row filter,4/22/15,1
lossyrob,where is the spatial filter in the code you linked to?,4/22/15,1
lossyrob,"Also I see how it *is* doing default filtering, I just don_Ñét think it needs to",4/22/15,1
moradology,ah. cool,4/22/15,1
lossyrob,we do need to do the zoom level filter,4/22/15,1
lossyrob,"but that_Ñés actually always the case, and the same between keys. So perhaps it_Ñés if(filterSet.isEmpty { do the zoom filter } else { do the key specific filtering }",4/22/15,1
lossyrob,oh right I see the `FilterRanges.spatioTemporal` does the space filter thing,4/22/15,1
lossyrob,and then the temoral filter for the accumulo filterable iterator is set afer taht,4/22/15,1
lossyrob,"and yeah, if it_Ñés `None`, wouldn_Ñét have to actually set that iterator",4/22/15,1
moradology,"okay, cool",4/22/15,1
moradology,"i can totally do that, i've been erring conservative with the refactor",4/22/15,1
lossyrob,"gotcha, yeah that would be good. thanks.",4/22/15,1
lossyrob,actually,4/22/15,1
lossyrob,we were talking about making zoom level tables,4/22/15,1
lossyrob,"like, because of the way things are indexed, you really shouldn_Ñét have tiles in the same table that are of different zoom levels",4/22/15,1
lossyrob,so we need to make tables per zoom level,4/22/15,1
moradology,that makes sense to me,4/22/15,1
lossyrob,not sure what the best way to do that is.,4/22/15,1
moradology,might be a good thing to have for RRP tomorrow,4/22/15,1
pomadchin,"@lossyrob lil bit off top but about zoom in a ```scala LayerId```what about this pyramid parameter? https://gist.github.com/lossyrob/fda457994a4be6db598c#file-ingest-spatial-sh-L22how to get tiles loaded with pyramid false?",4/22/15,1
lossyrob,"if you don_Ñét pyramid, it still imports at a specific zoom level",4/22/15,1
lossyrob,it finds the closest zoom level to the native resolution N,4/22/15,1
lossyrob,and then ingests it,4/22/15,1
pomadchin,oh,4/22/15,1
pomadchin,ty,4/22/15,1
lossyrob,"if Pyramiding, then it does the same for N - 1, N - 2, _Ñ_, 1",4/22/15,1
pomadchin,thx alot! (:,4/22/15,1
lossyrob,your very welcome!,4/22/15,1
notthatbreezy,@lossyrob figured out the setting -- spark.mesos.extra.cores,4/23/15,1
notthatbreezy,"bumped it up just now, got the ingest down to 12 minutes -- still hadn't maxed out the CPU so I'm going to push it even further",4/23/15,1
lossyrob,wow,4/23/15,1
lossyrob,this is still on one node?,4/23/15,1
notthatbreezy,"2 nodes, 16 cores actually",4/23/15,1
lossyrob,oh ok,4/23/15,1
lossyrob,how long did it take without that setting?,4/23/15,1
notthatbreezy,24 minutes,4/23/15,1
lossyrob,nice,4/23/15,1
notthatbreezy,"I bumped it up just now so it was running with 35 tasks in parallel, average time per task was still 3.5 minutes per second so I think there's still room to go, going to try again",4/23/15,1
notthatbreezy,"sorry, 3.5 minutes per task",4/23/15,1
lossyrob,"so, if there_Ñés more cores in that setting then actual cores, and there_Ñés CPU not being utilized, it_Ñéll actually do the context switching on a thread pool I guess?",4/23/15,1
notthatbreezy,"yeah, that's my understanding, I'm not actually sure how it works 100 percent on mesos -- on standalone cluster, the number of cores you set is the number of executors that get launched by the worker node",4/23/15,1
notthatbreezy,if you're on the Azavea VPN you can check the difference in CPU utilization here: http://ec2-52-8-4-42.us-west-1.compute.amazonaws.com:8090/#dashboard/temp/AUzj9qsErrpZpZjH7yTs,4/23/15,1
lossyrob,that_Ñés a pretty sweet visualization,4/23/15,1
notthatbreezy,"the first group is the 13 minute run, the second group is the run now",4/23/15,1
notthatbreezy,that time it took 4 minutes,4/23/15,1
lossyrob,![ClusterUsage](http://i.imgur.com/RKPr7F6.png),4/23/15,1
notthatbreezy,and was pretty much as parallel as it could get - it started all 111 tasks at the same time,4/23/15,1
notthatbreezy,"nice, yeah good idea with the screenshot",4/23/15,1
lossyrob,what do you mean _ÑÒgroup_Ñù?,4/23/15,1
notthatbreezy,the first set from 9:30ish until 9:40ish was when I ran it with 36 tasks in parallel -- total time was about 13 minutes (first group),4/23/15,1
notthatbreezy,"the second group, that started at 9:45ish was when I ran it will 111 tasks in parallel for the spark job and took about 4 minutes from start to finish",4/23/15,1
lossyrob,![better-shot](http://i.imgur.com/9CaBXVE.png),4/23/15,1
lossyrob,there_Ñés a better shot of the difference,4/23/15,1
notthatbreezy,yeah,4/23/15,1
lossyrob,"you mean, a whole ingest?",4/23/15,1
lossyrob,in 4 minutes?,4/23/15,1
notthatbreezy,it's the ingest command I got from eugene,4/23/15,1
lossyrob,how many nodes?,4/23/15,1
notthatbreezy,"```spark-submit \--class climate.ingest.NEXIngest \--master mesos://zk://zookeeper.service.geotrellis-spark.internal:2181/mesos \--conf spark.mesos.coarse=true \--conf spark.executor.memory=50g \--conf spark.mesos.extra.cores=30 \--conf spark.executorEnv.SPARK_LOCAL_DIRS=""/media/ephemeral0,/media/ephemeral1"" \--driver-library-path /usr/local/lib $JAR \--crs EPSG:4326 --instance geotrellis-accumulo-cluster --user root --password secret --zookeeper zookeeper.service.geotrellis-spark.internal \--input s3n://$AWS_ID:$AWS_KEY@nex-bcsd-tiled-geotiff/rcp26/tasmax/CCSM4 \--layerName tasmax-rcp26-ccsm4  --table tas --clobber true --s3PageSize 1000```",4/23/15,1
notthatbreezy,"ugh, can't get the formatting right",4/23/15,1
lossyrob,haha that_Ñés fine. i know hte command,4/23/15,1
lossyrob,just suprised at the time,4/23/15,1
lossyrob,but yeah are you still on 2 nodes?,4/23/15,1
notthatbreezy,"yeah, 2 nodes, 8 cores each",4/23/15,1
lossyrob,what happens if you bring it up to like 8? I wonder how fast you can get it,4/23/15,1
lossyrob,We were doing 20 nodes at some point too. I think the ingest still took a couple of minutes,4/23/15,1
notthatbreezy,"I'm not sure much faster, it's still not using all of the CPU for the 2 nodes I have up now ... I guess there's only one way to find out",4/23/15,1
lossyrob,yeah. pedal to the metal.,4/23/15,1
echeipesh,That's impressive,4/23/15,1
lossyrob,"We_Ñére eventually going to want to do parallel ingests, say of two layers simultaniously that we_Ñéll want to run operations on. not sure if they woudl be part of the same or different spark submit job, or what the consequences would be of having the two jobs running simultaniously. something to test out",4/23/15,1
lossyrob,yeah but ingesting 1 layer 2 nodes 4 minutes is great. Are you recording these? We_Ñéll need ingest timing stats for the whitepaper,4/23/15,1
notthatbreezy,I'll take the numbers down now,4/23/15,1
notthatbreezy,"Yeah, over-subscribing the CPU so much may not make sense if there are multiple spark jobs being submitted, though mesos will at least hold off on simultaneous jobs if there isn't enough resources in the cluster to satisfy the offer",4/23/15,1
lossyrob,hmm yeah_Ñ_wondering how Mesos would behave as we increase the number of submitted ingest jobs from 1 -> crazy,4/23/15,1
notthatbreezy,would also want to play around with fine-grained vs. coarse-grained mode in a multi-user/multi-job environment I think so one long-running job doesn't block all other jobs from running,4/23/15,1
lossyrob,"I remember there being a lot of frustrated talk about fine grained vs coarse grained mode by @hectcastro and @echeipesh, though that may have gotten sorted out",4/23/15,1
notthatbreezy,"yeah, so I have the number of cores it's requesting == to the number of the cores in the cluster now so other jobs would be blocked until it finishes",4/23/15,1
lossyrob,there was a problem with running fine grained mode. it_Ñés running coarse grain right?,4/23/15,1
lossyrob,not sure what that issue was or if we can do fine grained now,4/23/15,1
notthatbreezy,"I think fine-grained mode makes each task a new mesos job, which there's quite a bit of overhead for something like this in that case, but may be better in a multi-user setting -- yeah, it's coarse-grained",4/23/15,1
notthatbreezy,would be interested to learn more about @echeipesh and @hectcastro experience with that though,4/23/15,1
echeipesh,"That was solved, it's 5% slower to run in fine grained, but it works",4/23/15,1
lossyrob,ah gotcha,4/23/15,1
lossyrob,"so yeah as  we go 1 -> crazy that % may change from negative to positive, I guess we_Ñéll see",4/23/15,1
hectcastro,Nice. IRC gateway for Gitter works.,4/23/15,1
lossyrob,Nice. but then you_Ñéll miss out on all the sweet emoji_Ñés yeah? :poop: ,4/23/15,1
hectcastro,Some decisions are easier to make than others.,4/23/15,1
lossyrob,"FYI, implicit conversions don_Ñét get picked up for unapply. Something I might have guess but didn_Ñét know until I tried",4/23/15,1
lossyrob,"```scala> case class Foo(a: Int)defined class Fooscala> class Bar() { val x: Int = 5 }defined class Barscala> implicit def barToFoo(b: Bar): Foo = Foo(b.x)barToFoo: (b: Bar)Fooscala> val f: Foo = new Bar()f: Foo = Foo(5)scala> val Foo(i) = new Bar()<console>:11: error: constructor cannot be instantiated to expected type; found   : Foo required: Bar       val Foo(i) = new Bar()```",4/23/15,1
lossyrob,@moradology looks like your PR is failing,4/23/15,1
lossyrob,"```/home/travis/build/geotrellis/geotrellis/spark/src/test/scala/geotrellis/spark/io/accumulo/AccumuloRasterCatalogSpec.scala:67: value tileReader is not a member of geotrellis.spark.io.accumulo.AccumuloRasterCatalog[error]           val tile = catalog.tileReader[SpatialKey](layerId).read(key)[error]                              ^[error] ```",4/23/15,1
lossyrob,":question: My idea is that we fit the multi band API to what GDAL does and what I think is generally used, where the first band is Band 1. So the bands would not be 0 index, i.e. `multBandTile.band(0)` would be invalid. That goes against Scala notation but fits geospatial. Good idea or bad?",4/23/15,1
lossyrob,I can_Ñét tell if that_Ñés a bad idea or not. @echeipesh @ejc123 @Pshrub thoughts?,4/23/15,1
echeipesh,@lossyrob did we ever agree if FilterSet can have disjoint spaces? ,4/23/15,1
echeipesh,So for instance can we have multiple time fitlers ?,4/23/15,1
lossyrob,I don_Ñét think we explicitly agreed. But I did read the code that that was possible,4/23/15,1
lossyrob,like you could specify multiple ranges,4/23/15,1
echeipesh,"re: band indexing, unless there is some encoding reason, like we need to match some metadata with band index I would prefer to stick to 0-index. Every time I_Ñéve seen a library trying to match a specific domain it was confusing as all hell.",4/23/15,1
lossyrob,I don_Ñét know how useful that is. but the current functionality lets that happen,4/23/15,1
echeipesh,"Ok, then we have a problem with that.",4/23/15,1
lossyrob,"If we don_Ñét allow that, then the FilterSet is kind of weird. Like it_Ñés a collection when it_Ñés really not, sort of",4/23/15,1
lossyrob,"I was thinking about how to change that, or if it should change. and then I stopped thinking about it.",4/23/15,1
lossyrob,but it_Ñés a good question,4/23/15,1
echeipesh,this will never be true for disjoint ranges: https://github.com/geotrellis/geotrellis/blob/feature/catalog-refactor/spark/src/main/scala/geotrellis/spark/FilterSet.scala#L17,4/23/15,1
lossyrob,that is very true,4/23/15,1
echeipesh,I think I know how to fix it actually,4/23/15,1
echeipesh,It_Ñés super simple,4/23/15,1
lossyrob,"it_Ñés almost like you want an || between filters that have the same key, and && for filters that have different keys, ya know?",4/23/15,1
echeipesh,So I think FilterSet actually maps to a set of KeyBounds which is a cartisian product of spatial filters and temporal filters.,4/23/15,1
lossyrob,"well, if we type it specifically to spatial and temporal, then we should just hose the whole generic thing",4/23/15,1
lossyrob,the thing about this setup is that it is generic on K,4/23/15,1
echeipesh,Might be able to do it generic.,4/23/15,1
lossyrob,"ok, interested",4/23/15,1
echeipesh,"thinking, please wait : )",4/23/15,1
echeipesh,I think I_Ñéd like to merge @moradology catalog refactor refactor refactor into my s3 catalog branch.,4/23/15,1
echeipesh,"I ran into this when finishing those time filters on s3 catalog, so I need it to finish that anyway.",4/23/15,1
lossyrob,I want to merge it into catalog-refactor,4/23/15,1
echeipesh,"Ok, that_Ñés cool too",4/23/15,1
lossyrob,but it_Ñés broken,4/23/15,1
lossyrob,I_Ñéd also like to get catalog-refactor mergd in. I think it_Ñés time.,4/23/15,1
echeipesh,"I think we might still end up tweaking with some details of the catalogs, like the constructors, but that shouldn_Ñét prevent a merge.",4/23/15,1
lossyrob,"yeah. It can still be a work in progress, but I think it has enough fixes and stuff that I want everyone to be working off of it",4/23/15,1
pomadchin,"// offtop //guyz, how to deal in a standart way with web mercator _ÑÉblack zones_Ñé ? I mean tile in this projection has dark areas in the corners, in rgba tile it is possible to make it transparent; but what to do with 1 band grayscaled (int16) tiff ? some standard ways? p.s. noticed that tiff has _ÑéNODATA_Ñé pixels",4/23/15,1
lossyrob,"When we render we do so in RGBA, and make NODATA transparent like you said",4/23/15,1
lossyrob,not sure what else can be done about NODATA when you need it to be colored in RGB,4/23/15,1
pomadchin,"hm, and if our tiff is one banded, how to make it (such areas) transparent? ._.'",4/23/15,1
lossyrob,"well, we render to PNG, which writes RGBA",4/23/15,1
pomadchin,oh,4/23/15,1
pomadchin,mm,4/23/15,1
lossyrob,"if we write a GeoTIFF, it has no data values, and I think whatever actually does the rendering should know how to make them transparent",4/23/15,1
lossyrob,what are you using to look at the tiff images?,4/23/15,1
pomadchin,qgis,4/23/15,1
lossyrob,Writing an RGBA multiband geotiff is probably a good thing to have. And I might end up taking that on as part of my multi band stuff,4/23/15,1
lossyrob,stuff = feature,4/23/15,1
pomadchin,: D,4/23/15,1
pomadchin,yep ok,4/23/15,1
lossyrob,does the geotiff have an explicit NODATA value set?,4/23/15,1
lossyrob,and is it the correct value? Int.MinValue in scala,4/23/15,1
lossyrob,I feel like QGIS would be smart enough to make NoData pixels transparent,4/23/15,1
pomadchin,yep it makes so,4/23/15,1
lossyrob,"there might be an issue in the GeoTiffWriter_Ñ_I got into that code a bit last night, and there were a couple of issues",4/23/15,1
lossyrob,e.g. double values were not being written properly,4/23/15,1
lossyrob,also some tags were off,4/23/15,1
lossyrob,so it might be an issue with the GeoTiffWriter,4/23/15,1
lossyrob,"one thing to check is, what happens if you run it through `gdal_translate` and set the nodata  explicitly",4/23/15,1
lossyrob,`gdal_translate  -a_nodata -2147483648 in.tif out.tif`,4/23/15,1
lossyrob,"not sure that would work, something to try tho",4/23/15,1
pomadchin,wow ok ty,4/23/15,1
pomadchin,"will try tommorrow landsat tiles, there is mb an issue with them too; thx alot!",4/23/15,1
pomadchin,): mb tommorow there would be strange questions :D sorry ,4/23/15,1
lossyrob,"your very welcome, enjoying hearing about your work! the stranger the questions the better :)",4/23/15,1
lossyrob,@echeipesh re: indexing bands. Yeah I agree. Let_Ñés keep it to 0 indexing.,4/23/15,1
lossyrob,I merged @moradology refactor-refactor-refactor,4/23/15,1
moradology,woo!,4/23/15,1
lossyrob,"then we just need to merge Eugene_Ñés S3, and then we_Ñére good to merge it into master",4/23/15,1
notthatbreezy,[![doing-something.png](https://files.gitter.im/geotrellis/geotrellis/7tvc/thumb/doing-something.png)](https://files.gitter.im/geotrellis/geotrellis/7tvc/doing-something.png),4/23/15,1
notthatbreezy,"now that it's actually doing something, it doesn't have any issues with using all the CPU",4/23/15,1
lossyrob,haha,4/23/15,1
lossyrob,too good to be true,4/23/15,1
notthatbreezy,"yeah, ugh",4/23/15,1
lossyrob,ah if only settings were as magical as that,4/23/15,1
lossyrob,"to be clear to others, the conversations we were having about ingest times last night were bunk, because when Pyramiding=FALSE it wasn_Ñét actually saving. This is a bug that was fixed in the catalog-refactor branch, which is a good reason to merge that feature branch sooner rather than later. @pomadchin I think you were maybe running into that issue",4/23/15,1
hectcastro,Damn.,4/23/15,1
notthatbreezy,Someday I'm going to find this magical speed everything up setting,4/23/15,1
hectcastro,Just put it in MongoDB.,4/23/15,1
echeipesh,I hear CouchDB is even better,4/23/15,1
pomadchin,"Hello again guyz! : Dgot such issue with geotiff writer:```javaException in thread ""main"" geotrellis.raster.io.geotiff.GeoTiffWriterLimitationException: This GeoTiff writer does either not support the projection merc or it is malformed.    at geotrellis.raster.io.geotiff.Proj4StringParser.projProps$lzycompute(Proj4StringParser.scala:124)    at geotrellis.raster.io.geotiff.Proj4StringParser.projProps(Proj4StringParser.scala:119)    at geotrellis.raster.io.geotiff.Proj4StringParser.parse$lzycompute(Proj4StringParser.scala:93)    at geotrellis.raster.io.geotiff.Proj4StringParser.parse(Proj4StringParser.scala:86)    at geotrellis.raster.io.geotiff.Encoder.writeGeoKeyDirectory(Encoder.scala:344)    at geotrellis.raster.io.geotiff.Encoder.write(Encoder.scala:327)    at geotrellis.raster.io.geotiff.Encoder$.writePath(Encoder.scala:393)    at geotrellis.raster.io.geotiff.GeoTiffWriter$.write(GeoTiffWriter.scala:30)    at geotrellis.admin.ingest.AccumuloIngestOutCommand$.main(AccumuloIngestOutCommand.scala:77)    at geotrellis.admin.ingest.AccumuloIngestOutCommand$.main(AccumuloIngestOutCommand.scala:44)    at com.quantifind.sumac.ArgMain$class.mainHelper(ArgApp.scala:45)    at com.quantifind.sumac.ArgMain$class.main(ArgApp.scala:34)    at geotrellis.admin.ingest.AccumuloIngestOutCommand$.main(AccumuloIngestOutCommand.scala:44)    at geotrellis.admin.ingest.AccumuloIngestOutCommand.main(AccumuloIngestOutCommand.scala)    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)    at java.lang.reflect.Method.invoke(Method.java:497)    at org.apache.spark.deploy.SparkSubmit$.launch(SparkSubmit.scala:329)    at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:75)    at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)``` on merc espg:```+proj=merc +a=6378137 +b=6378137 +lat_ts=0.0 +lon_0=0.0 +x_0=0.0 +y_0=0 +k=1.0 +units=m +nadgrids=@null +wktext +no_defs```https://github.com/geotrellis/geotrellis/blob/68ef5edfa57bd44f8071896a4892a297472a6c6b/raster/src/main/scala/geotrellis/raster/io/geotiff/Proj4StringParser.scala#L119-L129",4/24/15,1
pomadchin,"hm, seems to me some problems here with web mercator projection?",4/24/15,1
pomadchin,"we_Ñéve also tried to ingest landsat tiles in webmercator with the nodatathe result was ~ https://files.slack.com/files-pri/T03GQ99SH-F04HUU43V/6-38-22.pngThe main thing that it seems to me, overlapping areas are overwriting on the same layer (even NODATA pixels)",4/24/15,1
pomadchin,http://oi59.tinypic.com/32ziasm.jpg,4/24/15,1
lossyrob,@pomadchin is your code somewhere I can pull down and run? If you tell me what landsat images to DL I can run your process and try to see what_Ñés going on with it.,4/24/15,1
lossyrob,http://www.meetup.com/geomeetup/events/17034143/,4/24/15,1
lossyrob,wonder how far they got.,4/24/15,1
ejc123,@lossyrob did you check out the slides?,4/24/15,1
lossyrob,yup looking at them now,4/24/15,1
lossyrob,"we have a FacebookOpenAcademy student who_Ñés created a Cassanrda catalog @kyeah. Killed it, so we have most features_Ñ_but the catalog has a really slow mechanism for supporting range queries. We have a GSoC student that_Ñés going to pick up the ball and keep running with it. So we need to figure out how to do geospatial/spatiotemporal queires on Cassandra",4/24/15,1
lossyrob,there_Ñés apparently work on a kd-tree on Cassandra that does this. I think by SimpleGeo. Trying to find that work...,4/24/15,1
lossyrob,@allixender is the GSoC student btw,4/24/15,1
ejc123,"cool.  looks like kd-tree is the way to go, has nice properties.",4/24/15,1
mojodna,"oh, ha.  i worked on that @ SimpleGeo.",4/24/15,1
mojodna,(hi @lossyrob),4/24/15,1
lossyrob,Oh yeah? Is that code anywhere?,4/24/15,1
lossyrob,"There_Ñés a problem we have with Cassandra, in that it can_Ñét do multiple range queries all at once",4/24/15,1
lossyrob,"we have a spatial index on it, can fetch tiles really nicely",4/24/15,1
mojodna,i don't think so.  it got subsumed by Urban Airship,4/24/15,1
mojodna,there were some 0.7-era patches to do that though.,4/24/15,1
mojodna,let me see if i can dig them up,4/24/15,1
lossyrob,nice!,4/24/15,1
lossyrob,hey I_Ñém trying to find that node-js code you were showing me at FOSS4G NA. that was for SWF right?,4/24/15,1
mojodna,"yes, it's still just on my laptop",4/24/15,1
mojodna,partially because it needs a name,4/24/15,1
lossyrob,"I need to take those zipped 30 metere DEM files, unzip, chunk them out into compressed GeoTiffs and put them back on S3. I forget exactly how vapor-clock works, but that_Ñés pretty much what it_Ñés for yeah?",4/24/15,1
lossyrob,just need a thing that runs some gdal commands. I have some python stuff hacked together to do a similar thing here: https://github.com/lossyrob/nex-chunker-worker,4/24/15,1
mojodna,yup,4/24/15,1
lossyrob,but it_Ñés not generalized too much_Ñ_if there_Ñés an effort that you_Ñéve done for a generalized version of that I_Ñéd rather use it,4/24/15,1
mojodna,"yeah, look at vapor-clock",4/24/15,1
mojodna,there's already a command type that does that,4/24/15,1
lossyrob,perfect,4/24/15,1
lossyrob,what_Ñés the deploy look like for a cluster to run vapor-clock operations?,4/24/15,1
mojodna,those Cassandra patches are lost to me,4/24/15,1
lossyrob,"dang. well, the slides from that presentation spell it out pretty well. We can just recreate it",4/24/15,1
mojodna,cool.  the thrift file had an additional rpc call or two defined and then the internals just called individual range queries within an executor pool (or similar),4/24/15,1
mojodna,vapor clock operations: https://github.com/stamen/vapor-clock/blob/master/aws/cloudformation.json,4/24/15,1
mojodna,that spins up a single g2.xlarge (not necessary to have a GPU) as a spot instance when stuff is present in the SQS queue,4/24/15,1
mojodna,"once CF configures the autoscaling group, you can manually increase the number of workers to boost your throughput",4/24/15,1
lossyrob,"ok, got it. and that AMI is public, and has GDAL installed",4/24/15,1
mojodna,"no, but the user-data installs it after the fact",4/24/15,1
lossyrob,"ah, I see it.",4/24/15,1
mojodna,:+1:,4/24/15,1
mojodna,specially compiled w/ OpenCL support,4/24/15,1
lossyrob,nice_Ñ_to take advantage of the gpu yeah? is that jsut for hill shade?,4/24/15,1
mojodna,"no, just for reprojection / resampling",4/24/15,1
mojodna,"it makes a difference, but not a big one",4/24/15,1
lossyrob,"hm, interesting",4/24/15,1
mojodna,Frank + Seth @ Planet apparently disagree on the value,4/24/15,1
lossyrob,How much is not a big difference?,4/24/15,1
lossyrob,have you run any reproj benchmarks on GPU/non GPU to get a sense?,4/24/15,1
mojodna,1s,4/24/15,1
mojodna,http://trac.osgeo.org/gdal/ticket/5772#comment:2,4/24/15,1
mojodna,"```# OpenCL-enabled$ time $(sudo LD_LIBRARY_PATH=/usr/local/lib /usr/local/bin/gdalwarp -q -wo ""USE_OPENCL=TRUE"" -t_srs ""EPSG:3857"" -r lanczos /vsizip/n43w125.zip/n43w125/floatn43w125_13.flt test-gpu.tiff)real	0m26.536suser	0m22.908ssys	0m3.664s# single-core CPU$ time $(sudo LD_LIBRARY_PATH=/usr/local/lib /usr/local/bin/gdalwarp -q -wo ""USE_OPENCL=FALSE"" -t_srs ""EPSG:3857"" -r lanczos /vsizip/n43w125.zip/n43w125/floatn43w125_13.flt test-cpu.tiff)real	0m56.892suser	0m55.180ssys	0m1.404s# multi-processor CPU$ nproc8$ time $(sudo LD_LIBRARY_PATH=/usr/local/lib /usr/local/bin/gdalwarp -q -wo ""USE_OPENCL=FALSE"" -t_srs ""EPSG:3857"" -multi -r lanczos /vsizip/n43w125.zip/n43w125/floatn43w125_13.flt test-cpu-multi.tiff)real	0m45.119suser	0m55.120ssys	0m1.720s# multi-processor, OpenCL-enabled$ time $(sudo LD_LIBRARY_PATH=/usr/local/lib /usr/local/bin/gdalwarp -q -t_srs ""EPSG:3857"" -multi -r lanczos /vsizip/n43w125.zip/n43w125/floatn43w125_13.flt test-cpu.tiff)real	0m13.018suser	0m14.896ssys	0m3.696s```",4/24/15,1
mojodna,3-4x in a non-scientific test,4/24/15,1
mojodna,(using NED 1/3 arcsec),4/24/15,1
lossyrob,"yeah. that_Ñés pretty significant, if you_Ñére chunking a lot of data",4/24/15,1
mojodna,Frank was saying that the main difference is splitting the IO up between multiple processors,4/24/15,1
mojodna,you can also run multiple gdal instances in parallel,4/24/15,1
mojodna,though you do need to watch for running out of GPU memory,4/24/15,1
mojodna,i keep tweaking concurrency here as a result: https://github.com/stamen/vapor-clock/blob/master/server.js#L34,4/24/15,1
mojodna,"for io-bound stuff, 2*nproc makes sense",4/24/15,1
lossyrob,so currently the sweet spot is running half the number of gdal jobs as processesors you have_Ñ_otherwise GPU memory gest swamped?,4/24/15,1
mojodna,or something ;-),4/24/15,1
lossyrob,oh 2 times,4/24/15,1
mojodna,i was using 1/2 for GPU-backed jobs,4/24/15,1
lossyrob,gotcha,4/24/15,1
mojodna,https://github.com/stamen/vapor-clock/commit/a7a83607ba4a53018774a7be3b461a17ca50938b ;-),4/24/15,1
mojodna,(includes a couple of other knobs i was tweaking),4/24/15,1
lossyrob,ha nice. good example of the trouble of keeping comments up to date with code :),4/24/15,1
mojodna,exactly,4/24/15,1
mojodna,there's also https://github.com/stamen/vapor-clock/blob/master/lib/operations/resample.js#L52,4/24/15,1
mojodna,`// TODO declare memory requirements`,4/24/15,1
mojodna,i need to disappear back into Field Papers land...  yell if you have more vapor-clock questions.,4/24/15,1
lossyrob,"Nice, I_Ñém going to kick on it for a while and I_Ñém sure I_Ñéll have questions for you later. Thanks for the help @mojodna!",4/24/15,1
lossyrob,This is pretty cool: http://dwtkns.com/srtm/,4/24/15,1
lossyrob,@mojodna is there a tool to feed a vapor-clock queue based on a s3 bucket listing?,4/25/15,1
mojodna,not out of the box. i_Ñéve been using `aws s3 ls s3://_Ñ_ > files.list` and then piping that through something like https://github.com/stamen/vapor-clock/blob/master/bin/queue.js,4/25/15,1
mojodna,https://gist.github.com/mojodna/8aada8afa344ab2e39c4,4/25/15,1
lossyrob,"sweet. I_Ñéll use that, perhaps try to abtract over it a bit and throw it up on github.",4/25/15,1
mojodna,excellent,4/25/15,1
lossyrob,"I_Ñém a nodejs newbie, will give me a good chance to use the language in anger.",4/25/15,1
mojodna,:+1:,4/25/15,1
lossyrob,@mojodna I wrote up an issue on vapor-clock (https://github.com/stamen/vapor-clock/issues/1),4/25/15,1
lossyrob,"I_Ñém going to attack it, if I_Ñém headed in a bad direction, let me know!",4/25/15,1
notthatbreezy,"not sure if this is helpful, but I've been playing with rasterio to use with spark/python -- found this on doing windowed reading of large DEM file http://sgillies.net/blog/2014/02/14/rasterio-cookbook-recipe-number-one.html -- I know you used rasterio before, but not sure if you saw this http://sgillies.net/blog/2014/02/14/rasterio-cookbook-recipe-number-one.html",4/25/15,1
lossyrob,@notthatbreezy nice. I used rasterio for some windowing stuff here: https://github.com/lossyrob/nex-chunker-worker/blob/master/src/chunker/tiler.py,4/25/15,1
notthatbreezy,"oh cool, yeah I knew you said you had used it before, but forgot what for",4/25/15,1
lossyrob,"for the netCDF chunking. Rasterio was good for that. I_Ñéd probably keep using it and working with that project, but `vapor-clock` already does some of the task/SQS handling I would have to build, so trying to lean on that (which just does shell commands to GDAL)",4/25/15,1
notthatbreezy,"yeah, was just looking at that, that's a nice bonus (SQS)",4/25/15,1
lossyrob,"yeah I def used it in anger and have been throught the code base, so if there_Ñés any rasterio stuff you run into would be into having conversations about it",4/25/15,1
lossyrob,e.g. they didn_Ñét support reading paths with driver prefixes (which is why I had to do https://github.com/lossyrob/nex-chunker-worker/blob/master/src/chunker/tiler.py#L9,4/25/15,1
lossyrob,"I should have really packaged that up into a PR, but ya know, time.",4/25/15,1
lossyrob,@notthatbreezy also @mojodna is working on a NodeJS library for working with SWF that pretty awesome. You guys use SWF a lot right? Do you use the ruby library for doing stuff with it?,4/25/15,1
notthatbreezy,"definitely - my use case is pretty simple -- I have a bunch of lat/lngs (27gb worth), I just want to put them into a rasters and write them to disk",4/25/15,1
notthatbreezy,"but its for the python meetup, so I'm doing everything in python, then I'm going to do some machine learning and analysis in spark with the data",4/25/15,1
notthatbreezy,"yea, we use SWF a lot -- the ruby library has been a major pain point for us",4/25/15,1
lossyrob,"streaming points into rasters is actually something we_Ñéve been meaning to handle in GeoTrellis, so what your doing will be really good to inform that work, whenever we get to it.",4/25/15,1
lossyrob,"yeah, you should def check out Seth_Ñés work_Ñ_whenever he puts it up on github ;)",4/25/15,1
lossyrob,Help him come up with a name for it and it_Ñéll probably happen quicker lol,4/25/15,1
notthatbreezy,"ha, I'm not good with names",4/25/15,1
lossyrob,"but he stepped through it with me at FOSS4G NA, looks solid",4/25/15,1
notthatbreezy,"that's cool - SWF itself has been pretty solid, the main issue for us is the ruby library support has been kind of iffy -- it's an official AWS library, but releases usually introduce bugs that require workarounds so we've been skittish to keep up to date with it",4/25/15,1
mojodna,"@notthatbreezy Amazon's Ruby and Java Flow libraries (for SWF) caused me 2 days of pain without managing to put together a reasonable mental model of how they work under the hood (beyond ""Fibers"", ""Promises""), in part because they would swallow errors that prevented me from fixing my own code (or actually log them to undocumented files in `/tmp`)",4/26/15,1
mojodna,"so i spent a day spiking a Node/JavaScript/Promise (Bluebird) implementation that tries to avoid magic and that i understand (the polling stuff is similar to https://github.com/stamen/exquisite, but the replay stuff relies on Bluebird heavily).",4/26/15,1
mojodna,the bonus is that it works under both SWF and locally,4/26/15,1
mojodna,"the thing that's keeping me from posting it is the lack of a name better than ""swfr"" / ""swiffer"" and the fact that i haven't touched it since before i showed it to rob",4/26/15,1
mojodna,@mojodna reads @lossyrob's comment about naming _now_,4/26/15,1
lossyrob,I_Ñém pretty into swiffer_Ñ_.but this :/ https://www.npmjs.com/package/swiffer,4/26/15,1
st028,@lossyrob about the interpolation methods: It seems like gdal is doing something other than a transformation when interpolating,4/27/15,1
lossyrob,@st028 how do you mean?,4/27/15,1
st028,"So when they do the interpolation they take the top left and bottom right coordinate (which creates the box to interpolate over) and do what they describe as transforming the points to destination to source coordinates. However when I added the code to do this, our boxes still don't seem to line up with theirs",4/27/15,1
lossyrob,"hmm, how far off is it?",4/27/15,1
lossyrob,how they go from dest grid coords -> source grid coords might not be through the map coordinates like we do,4/27/15,1
st028,Every once and a while it gets off by 1,4/27/15,1
lossyrob,is it consistent about where it goes off by 1,4/27/15,1
st028,It doesn't seem to be :/,4/27/15,1
lossyrob,hmmm. ok. well off by not more than one is encouraging,4/27/15,1
lossyrob,how many instances is it dead on vs how many off-by-1,4/27/15,1
lossyrob,"I_Ñém wondering if we can still ask the question we wanted to ask, questioning GDAL_Ñés method, even if we don_Ñét completely nail down the same results in a port of what they_Ñére doing.",4/27/15,1
st028,Is there an easy way to send the sbt output to a file? I can check,4/27/15,1
lossyrob,"./sbt _ÑÒproject raster-test_Ñù ""test-only *YourSpec_Ñù > output.txt",4/27/15,1
DarkManiel,"I'm having some trouble making a rest call on the pixel path in CatalogService.scala of gt_admin. If I call http://localhost:8088/pixel?name=nexmonth_gtadmin&zoom=5&x=-95.75291207074649&y=36.99640208444883 I get json back without any data, but it seems that should exist based on the results from clicking zoom level 5. Am I understanding it correctly that x should be longitude and y is latitude?",4/27/15,1
notthatbreezy,"@mojodna swfr is a good name, avoids the conflict with the other swiffer project too",4/27/15,1
lossyrob,"@DarkManiel yes, but the tile coordinates are in web mercator, so you_Ñéll have to reproject them (like the admin subproject does). If you have some code I can look at it and see if I can see what_Ñés going wrong.",4/27/15,1
lossyrob,"@mojodna I worked with vapor-clock over the weekend. I got something that allows operations to produce multiple outputs, as well as chain operations so that each N+1 operation takes on the ouputs of the Nth operation as inputs. Would appreciate any feedback: https://github.com/lossyrob/vapor-clock/blob/feature/muliple-outputs/lib/worker.js",4/27/15,1
lossyrob,"I need to figure out how the final output should be specified in the task for the case where there_Ñés multiple inputs_Ñ_is it that the specified task output is a directory? or template file name, and the final outputs are some sort of modifier to that? Not sure the best way.",4/27/15,1
lossyrob,"Also, realized it would be really great to have vapor-clock not just run shell, but run python or scala. Would be useful in some GeoTrellis processes, or for running some rasterio code for when I want to read the metadata from the input as part of the operation.",4/27/15,1
DarkManiel,"Well, I'm trying to get the values as they do in the old admin with :",4/27/15,1
DarkManiel,"rs         .converge       .map { rast =>         val (col,row) = rasterExtent.mapToGrid(x,y)        for(r <- (row - size) to (row + size);              c <- (col - size) to (col + size)) yield {           if(0 <= c && c <= rast.cols &&               0 <= r && r <= rast.rows) {             ""\""%.2f\"""".format(rast.getDouble(c,r))             } else {                ""\""\""""               }           }           }        .map { values =>           s"""""" { ""success"" : ""1"", ""values"" : [ ${values.mkString("","")} ] } """"""          }",4/27/15,1
DarkManiel,"It looks like pixelRoute is doing something similar, so I want to access it to get the values",4/27/15,1
DarkManiel,How do access the web mercator and reproject them?,4/27/15,1
lossyrob,"when you paste in code, surround it by _ÑÒ```_Ñù (triple bak ticks) to format it",4/27/15,1
lossyrob,"```rs  .converge  .map { rast =>    val (col,row) = rasterExtent.mapToGrid(x,y)    for(r <- (row - size) to (row + size);        c <- (col - size) to (col + size)) yield {      if(0 <= c && c <= rast.cols &&         0 <= r && r <= rast.rows) {        ""\""%.2f\"""".format(rast.getDouble(c,r))      } else {        ""\""\""""      }    }   }  .map { values =>    s"""""" { ""success"" : ""1"", ""values"" : [ ${values.mkString("","")} ] } """"""  }```",4/27/15,1
lossyrob,"so, for gt-admin, you get a single tile based on the point",4/27/15,1
lossyrob,"to get that tile, you have to figure out how the point maps to a tile key based on the (lng, lat) x y values coming in from leaflet, and if there is a time band, the time band that is selected",4/27/15,1
lossyrob,"for a spatial raster, let me see if I can off-the-cuff code something that could guide",4/27/15,1
lossyrob,"```val (x: Doulbe, y: Double) = ??? // The coords coming in from the clickval layer: String = ???val zoom: int = ???// Assuming that the zoom level exists for this layer. If not, we might want to do the same logic that the tmsRoute is doing// for loading the highest zoom level tile and resampling.val layerId = LayerId(layer, zoom)val rasterMetaData = catalog.metaDataCatalog.load(layerId)import goetrellis.vector._import goetrellis.vector.reproject._import geotrellis.proj4._val clickPoint = Point(x, y).reproject(LatLng, rasterMetaData.crs)val spatialKey = rasterMetaData.mapTransform(clickPoint)val tile = catalog.loadTile(layerId, spatialKey)// Figure out the extent of this tile by transforming the key to a geotrellis.vector.Extentval tileExtent = rasterMetaData.mapTransform(spatialKey)// Get the object that can turn points into tile coordinates.val rasterExtent = RasterExtent(tileExtent, tile.cols, tile.rows)// At this point, you have a geotrellis.raster.Tile, and know it's extent. Iterate over the desired values and build up the json.val (col, row) = rasterExtent.mapToGrid(clickPoint.x, clickPoint.y) // Make sure to use the tile's CRS, not LatLng// Always use cfor instead of for when iterating over tiles/arrays. It's much much faster.import scala.collection.mutableval values = mutable.ListBuffer[String]()cfor(row - size)(_ <= row + size; _ + 1) { row =>  cfor(col - size)(_ <= col + size; _ + 1) { col =>      if(0 <= c && c <= rast.cols &&         0 <= r && r <= rast.rows) {        values += ""\""%.2f\"""".format(rast.getDouble(c,r))      } else {        values += ""\""\""""      }  }}s"""""" { ""success"" : ""1"", ""values"" : [ ${values.mkString("","")} ] } """"""```",4/27/15,1
lossyrob,"that_Ñés not compiled or anything, but that_Ñés the idea",4/27/15,1
DarkManiel,Thanks. That helps a lot. I'll try doing it this way,4/27/15,1
lossyrob,cool. let me know if you hit snags.,4/27/15,1
DarkManiel,"ok, will do",4/27/15,1
mojodna,@lossyrob @notthatbreezy https://github.com/stamen/swfr,4/27/15,1
mojodna,"(sorry, no README yet)",4/27/15,1
mojodna,@lossyrob having a look at your PRs + issues today.  i suspect that the answer to some of them is going to be _ÑÒlet_Ñés push it into swfr_Ñù (since that_Ñés intended for multi-step processes and handles them better),4/27/15,1
lossyrob,"ok. so is swfr trying to be vapor-clock, but for SWF instead of SQS?",4/27/15,1
mojodna,yes,4/27/15,1
lossyrob,"I think SQS works well if you want to do multiple operaitons per input file, or just one operation but isn_Ñét multistep, but has multiple outputs. i.e. running a python or scala program to chunk up the image",4/27/15,1
mojodna,sure,4/27/15,1
lossyrob,"is it a matter of, SWF is the more powerful tool, so let_Ñés just move to that instead of keeping to SQS, and do everything we can do on SQS on SWF?",4/27/15,1
mojodna,"kinda, yeah",4/27/15,1
mojodna,https://github.com/stamen/swfr/blob/master/cgiar-csi-srtm.js,4/27/15,1
mojodna,"that_Ñés a multi-step SRTM workflow that spreads out to operate on individual files (doing multiple things if necessary), then waits for all of those jobs to finish, then does something with all of them, ...",4/27/15,1
mojodna,SWF is likely an abstraction over SQS,4/27/15,1
lossyrob,cool. does it have the same _ÑÒmessage is a JSON object that tells the worker what to do_Ñù idea as vapor-clock? I thought that was pretty handy.,4/27/15,1
notthatbreezy,@mojodna cool - thanks for putting it up -- I'll definitely check it out,4/27/15,1
mojodna,"it uses SWF _ÑÒactions_Ñù instead, but there_Ñés no reason one couldn_Ñét build an action that_Ñés essentially a catch-all w/ a JSON input",4/27/15,1
mojodna,"(btw, in case this wasn_Ñét clear, it _works_, but it hasn_Ñét been cleaned up nearly as much as vapor-clock)",4/27/15,1
lossyrob,yeah totally.,4/27/15,1
mojodna,"iow, feedback is awesome",4/27/15,1
mojodna,in part because there are parts that are still in flux,4/27/15,1
lossyrob,"I_Ñém looking to use this for some internal stuff, and then going to suggest leaning on it for OAM tiler nodes at the HOT Summit code sprint",4/27/15,1
mojodna,cool cool,4/27/15,1
notthatbreezy,"yeah, we've used JSON as input to kick off SWF workflows (basically kick off jobs with posts to an endpoint) -- the Ruby flow framework (and I believe SWF in general based on the SWF console) serializes inputs to workers to json",4/27/15,1
mojodna,"i won_Ñét be there, sadly, but i_Ñéll be online during pacific time",4/27/15,1
lossyrob,"so it seems like I could put effort behind modifying vapor-clock to do the things I need, or just start sort of fresh with swfr. And your saying let_Ñés do swfr, sounds good to me.",4/27/15,1
mojodna,"yeah, cool.  much of what you_Ñéve done can be translated over",4/27/15,1
mojodna,"the core (the parts that don_Ñét involve replaying promises) is very similar to exquisite, which is what underlies vapor-clock",4/27/15,1
mojodna,swfr : exquisite :: ___ : vapor-clock,4/27/15,1
lossyrob,"ok, yeah",4/27/15,1
lossyrob,so should vapor-clock just move to be using swfr instead of exquisite?,4/27/15,1
mojodna,probably,4/27/15,1
lossyrob,ok. that makes sense.,4/27/15,1
mojodna,i was in the process of merging the functionality into swfr while getting it to work with the intention of splitting the functionality back out once i was happy with the abstractions.,4/27/15,1
lossyrob,"so keep vapor-clock around, move it to use swfr, push the two forward in parallel",4/27/15,1
mojodna,"btw, this might also be of interest for OAM: https://github.com/mojodna/tilelive-raster",4/27/15,1
mojodna,:+1:,4/27/15,1
mojodna,it may not make a whole lot of sense since it doesn_Ñét do block-level fetching of remote sources,4/27/15,1
mojodna,but it_Ñés how i_Ñém building the replacement geotiff tiler for field papers,4/27/15,1
mojodna,(since geotiffs will typically be < 10MB in that scenario),4/27/15,1
lossyrob,"one approach is to do block level remote fetching, and have one output per process. the other is to take the entire file off s3 and create all the tiles from it in one swoop. Do you think the one is much better than the other?",4/27/15,1
mojodna,"(to clarify, this is totally separate from the SQS/SWF terrain workflow)",4/27/15,1
mojodna,"since requests to FP files are pretty uncommon / lightweight, my preference is to keep the GeoTIFF as the source of truth and to dynamically serve it up as tiles",4/27/15,1
lossyrob,"right. but now I_Ñém thinking through the tiling approach that I was thinking of, which would be the latter_Ñ_new image comes in, kicks of a cloudwatch notification and submits a job to the SQS (or SWF) queue, then vapor-clock kicks off an operation that tiles that whole image",4/27/15,1
mojodna,(vs. the current approach which involves pre-generating tiles when the image is uploaded_ÑÓthis uses unnecessary amounts of time and space _for this scenario_),4/27/15,1
lossyrob,"oh, so your doing remote sourcing as a dynamic tiling mechanism",4/27/15,1
mojodna,basically,4/27/15,1
lossyrob,gotcha,4/27/15,1
mojodna,"for OAM, yes, i think you_Ñére right",4/27/15,1
mojodna,https://github.com/mojodna/tl is good for generating tiles from tilelive sources,4/27/15,1
mojodna,(like tilelive-raster),4/27/15,1
mojodna,"@hallahan is thinking about an MBTiles-as-a-service built on tl, intended for the Red Cross",4/27/15,1
lossyrob,nice. This is good stuff to research before I get to the code sprint,4/27/15,1
lossyrob,I_Ñém not familiar with tilelive at all,4/27/15,1
mojodna,"it_Ñés quite a good API, even if it wasn_Ñét intended for how i_Ñéve used it",4/27/15,1
mojodna,https://github.com/mojodna/tessera is my tilelive-based tiler,4/27/15,1
notthatbreezy,"not to throw out more aws acronyms/services, but if you want to watch like an s3 folder or bucket, lambda (http://aws.amazon.com/lambda/) sounds like it would be an option (re: @lossyrob your comment about kicking of a job when a new image comes in)",4/27/15,1
lossyrob,"@notthatbreezy nice, that_Ñés good info.",4/27/15,1
mojodna,"it_Ñés intended to have a minimal core and rely on tilelive sources for as much as possible, so it will render images, vector tiles, images from vector tiles, images from a filesystem, mbtiles, ...",4/27/15,1
mojodna,"@notthatbreezy yes, i_Ñéve been thinking about how to incorporate lambda into this workflow since it means not having to provision servers",4/27/15,1
lossyrob,There_Ñés a ton of tilelive projects. Is this the main one? https://github.com/mapbox/tilelive.js/,4/27/15,1
notthatbreezy,"yeah, lambda looks cool, I haven't found a good use-case for myself  yet though",4/27/15,1
mojodna,"when i first played with lambda after it was in preview, the 3 things that kept me from using it for image processing were 1) limited memory (1GB max), 2) limited ephemeral storage (500MB? in /tmp), 3) timeouts (60s)",4/27/15,1
mojodna,"@lossyrob yeah, that defines the API",4/27/15,1
lossyrob,"ok cool, thanks.",4/27/15,1
mojodna,and provides the main plumbing,4/27/15,1
mojodna,"Mapbox Studio uses it under the hood, but not as extensively as i believe they should",4/27/15,1
mojodna,"since all sources can be expressed as urls, practically everything is interchangeable",4/27/15,1
lossyrob,sounds pretty useful,4/27/15,1
notthatbreezy,"yeah, timeouts are the thing that confuses me about it -- but seems like it could be good to kick off an SWF job or something based on things getting dropped in folders or buckets",4/27/15,1
notthatbreezy,"http://aws.amazon.com/about-aws/whats-new/2015/04/aws-cloudformation-supports-aws-lambda-backed-custom-resources/ is pretty cool, but haven't had a chance to use it in any of our cloudformation things",4/27/15,1
mojodna,@notthatbreezy the fact that it supports synchronous calling now is *awesome*,4/27/15,1
mojodna,means that it might be able to make a good pbf -> raster renderer,4/27/15,1
mojodna,haven_Ñét written the poc yet though,4/27/15,1
mojodna,@lossyrob i_Ñém a huge fan of tilelive (probably a given based on the number of modules i_Ñéve written),4/27/15,1
mojodna,@mojodna goes away,4/27/15,1
mojodna,@mojodna goes away,4/27/15,1
lossyrob,"ha yeah, I could tell. Going to have to do some research on it and come back with questions/ideas.",4/27/15,1
lossyrob,"ha yeah, I could tell. Going to have to do some research on it and come back with questions/ideas.",4/27/15,1
lossyrob,"as for swfr/vapor-clock, I_Ñém going to push that forward over the week, since it_Ñés immediately useful for a task I have. If you_Ñéll be hacking on it as well we should try to coordinate.",4/27/15,1
lossyrob,"as for swfr/vapor-clock, I_Ñém going to push that forward over the week, since it_Ñés immediately useful for a task I have. If you_Ñéll be hacking on it as well we should try to coordinate.",4/27/15,1
mojodna,great great,4/27/15,1
mojodna,great great,4/27/15,1
mojodna,"hoping i do have time this week to do work, if not, to respond to things ;-)",4/27/15,1
mojodna,"hoping i do have time this week to do work, if not, to respond to things ;-)",4/27/15,1
lossyrob,awesome.,4/27/15,1
lossyrob,awesome.,4/27/15,1
st028,@lossyrob I have a number of off-by-1 vs dead on. It's not pretty,4/27/15,1
st028,@lossyrob I have a number of off-by-1 vs dead on. It's not pretty,4/27/15,1
mojodna,@st028 don't mind me jumping in--is the off-by-1 a GDAL thing or geotrellis?,4/27/15,1
mojodna,@st028 don't mind me jumping in--is the off-by-1 a GDAL thing or geotrellis?,4/27/15,1
st028,It's a difference between our interpolation methods and GDAL,4/27/15,1
st028,It's a difference between our interpolation methods and GDAL,4/27/15,1
st028,"I started writing tests by comparing our output to theirs, however we were getting a lot of failed tests",4/27/15,1
st028,"I started writing tests by comparing our output to theirs, however we were getting a lot of failed tests",4/27/15,1
mojodna,"aha.  i've noticed off-by-1s (or even 2) when attempting to tile rasters w/ overlap (using gdalwarp).  the goal is to produce 1026x1026, but they're often 1025x1027",4/27/15,1
mojodna,"aha.  i've noticed off-by-1s (or even 2) when attempting to tile rasters w/ overlap (using gdalwarp).  the goal is to produce 1026x1026, but they're often 1025x1027",4/27/15,1
st028,"It's a geotrellis thing, but there's also an argument that GDAL isn't doing things in correctly",4/27/15,1
st028,"It's a geotrellis thing, but there's also an argument that GDAL isn't doing things in correctly",4/27/15,1
st028,"That may be related, but what I'm working on is mainly looking at how the interpolations are being performed for each pixel. They make a box to do an interpolation operation by transforming a 2 x 2 box from destination and source coordinates",4/27/15,1
st028,"That may be related, but what I'm working on is mainly looking at how the interpolations are being performed for each pixel. They make a box to do an interpolation operation by transforming a 2 x 2 box from destination and source coordinates",4/27/15,1
st028,"Our troubles are that when we transform to destination to source coordinates, our boxes are off by a dimension 80% of the time",4/27/15,1
st028,"Our troubles are that when we transform to destination to source coordinates, our boxes are off by a dimension 80% of the time",4/27/15,1
st028,If they are using the same transformation method it's possible that they are related,4/27/15,1
st028,If they are using the same transformation method it's possible that they are related,4/27/15,1
lossyrob,"@mojodna the off-by-one is pretty specific to how resampling methods like average or mode take values from the source raster. We_Ñére trying to replicate what GDAL is doing, because I think what it_Ñés doing is not optimal, but I want to be clear about how GDAL works and how we would want it to work and pose the question on a mailing list or something. But in trying to match what GDAL is doing, mapping the dest col/row to a set of source col/rows is not behaving exactly the way we think it should (off by 1)",4/27/15,1
lossyrob,"@mojodna the off-by-one is pretty specific to how resampling methods like average or mode take values from the source raster. We_Ñére trying to replicate what GDAL is doing, because I think what it_Ñés doing is not optimal, but I want to be clear about how GDAL works and how we would want it to work and pose the question on a mailing list or something. But in trying to match what GDAL is doing, mapping the dest col/row to a set of source col/rows is not behaving exactly the way we think it should (off by 1)",4/27/15,1
lossyrob,@st028 what are the numbers?,4/27/15,1
lossyrob,@st028 what are the numbers?,4/27/15,1
st028,@lossyrob 80% of the boxes are off by at least one dimension,4/27/15,1
st028,@lossyrob 80% of the boxes are off by at least one dimension,4/27/15,1
st028,So not pretty,4/27/15,1
st028,So not pretty,4/27/15,1
lossyrob,at least one. so there are some that are more off_Ñ_how off is max?,4/27/15,1
lossyrob,at least one. so there are some that are more off_Ñ_how off is max?,4/27/15,1
lossyrob,"i thought you said it was at most off by one, no?",4/27/15,1
lossyrob,"i thought you said it was at most off by one, no?",4/27/15,1
st028,"Oh, sorry there were 3 cases: off by 1 in x, off by 1 in y, off by 1 in x and y",4/27/15,1
st028,"Oh, sorry there were 3 cases: off by 1 in x, off by 1 in y, off by 1 in x and y",4/27/15,1
lossyrob,oh jeesh,4/27/15,1
lossyrob,oh jeesh,4/27/15,1
lossyrob,there maybe is a thing about how we translate coordinates,4/27/15,1
lossyrob,there maybe is a thing about how we translate coordinates,4/27/15,1
lossyrob,"if GDAL isn_Ñét taking center cell, maybe that_Ñés the issue",4/27/15,1
lossyrob,"if GDAL isn_Ñét taking center cell, maybe that_Ñés the issue",4/27/15,1
lossyrob,"in that, when we take the dest (col, row). We find the dest (x, y), which represents the center of the cell. Then find the cell that contains the (x, y) in the source. That_Ñés the col row",4/27/15,1
lossyrob,"in that, when we take the dest (col, row). We find the dest (x, y), which represents the center of the cell. Then find the cell that contains the (x, y) in the source. That_Ñés the col row",4/27/15,1
lossyrob,I_Ñéll have to find the pixel/line -> pixel/line transform code,4/27/15,1
lossyrob,I_Ñéll have to find the pixel/line -> pixel/line transform code,4/27/15,1
lossyrob,(the code that actually runs when that transform function pointer is invoked),4/27/15,1
lossyrob,(the code that actually runs when that transform function pointer is invoked),4/27/15,1
lossyrob,do you know where that is per chance?,4/27/15,1
lossyrob,do you know where that is per chance?,4/27/15,1
st028,I traced the code to this function https://github.com/OSGeo/gdal/blob/trunk/gdal/alg/gdaltransformer.cpp#L1728,4/27/15,1
st028,I traced the code to this function https://github.com/OSGeo/gdal/blob/trunk/gdal/alg/gdaltransformer.cpp#L1728,4/27/15,1
pomadchin,"@lossyrob sry for long answering (questioning :D) my question was about overlapping and web mercator projection: http://oi59.tinypic.com/32ziasm.jpgsources: https://www.dropbox.com/s/63r7sxwf92pst09/LC81770272014081LGN00_B4.tif?dl=0https://www.dropbox.com/s/8w5uqng43pd2dri/LC81770282014081LGN00_B4.tif?dl=0",4/27/15,1
pomadchin,"@lossyrob sry for long answering (questioning :D) my question was about overlapping and web mercator projection: http://oi59.tinypic.com/32ziasm.jpgsources: https://www.dropbox.com/s/63r7sxwf92pst09/LC81770272014081LGN00_B4.tif?dl=0https://www.dropbox.com/s/8w5uqng43pd2dri/LC81770282014081LGN00_B4.tif?dl=0",4/27/15,1
pomadchin,Landsat near located tiles,4/27/15,1
pomadchin,Landsat near located tiles,4/27/15,1
pomadchin,"tried to make black area transparent (nodata), but seems to me it rewrites anyway the normal data",4/27/15,1
pomadchin,"tried to make black area transparent (nodata), but seems to me it rewrites anyway the normal data",4/27/15,1
lossyrob,@pomadchin Ok thanks. Can you describe the processing steps your doing to these two input tiles so I can try to reconstruct them?,4/27/15,1
lossyrob,@pomadchin Ok thanks. Can you describe the processing steps your doing to these two input tiles so I can try to reconstruct them?,4/27/15,1
lossyrob,@st028 I think answers would be found following this code: https://github.com/OSGeo/gdal/blob/trunk/gdal/alg/gdaltransformer.cpp#L926,4/27/15,1
lossyrob,@st028 I think answers would be found following this code: https://github.com/OSGeo/gdal/blob/trunk/gdal/alg/gdaltransformer.cpp#L926,4/27/15,1
lossyrob,"I_Ñém currently porting code from https://github.com/OSGeo/gdal/blob/trunk/gdal/frmts/gtiff/libtiff/tif_predict.c, so I hope you find some solace in the knowledge of shared the pain of digging through C code and porting to scala :)",4/27/15,1
lossyrob,"I_Ñém currently porting code from https://github.com/OSGeo/gdal/blob/trunk/gdal/frmts/gtiff/libtiff/tif_predict.c, so I hope you find some solace in the knowledge of shared the pain of digging through C code and porting to scala :)",4/27/15,1
lossyrob,"@mojodna saw that the reproject/resample operations were using lzw predictor=2. Sent me down a path, now I_Ñém implementing predictors in our GeoTiff writer (an important feature we are missing), and also took a look at compression ratios. For one of the NED files, I get these measures:```~/proj/ec2 du -h *.tif271M	deflate-predictor2.tif212M	deflate-predictor3.tif346M	deflate.tif318M	lzw-predictor2.tif255M	lzw-predictor3.tif432M	lzw.tif```",4/27/15,1
lossyrob,"@mojodna saw that the reproject/resample operations were using lzw predictor=2. Sent me down a path, now I_Ñém implementing predictors in our GeoTiff writer (an important feature we are missing), and also took a look at compression ratios. For one of the NED files, I get these measures:```~/proj/ec2 du -h *.tif271M	deflate-predictor2.tif212M	deflate-predictor3.tif346M	deflate.tif318M	lzw-predictor2.tif255M	lzw-predictor3.tif432M	lzw.tif```",4/27/15,1
mojodna,care to guest post that on openterrain.tumblr.com?,4/27/15,1
mojodna,care to guest post that on openterrain.tumblr.com?,4/27/15,1
lossyrob,"wondering if there_Ñés a reason to use lzw over deflate (speed of compression could be the case, deflate is a lot slower), or not use float predictors",4/27/15,1
lossyrob,"wondering if there_Ñés a reason to use lzw over deflate (speed of compression could be the case, deflate is a lot slower), or not use float predictors",4/27/15,1
lossyrob,sure thing,4/27/15,1
lossyrob,sure thing,4/27/15,1
mojodna,it was an arbitrary choice on my part,4/27/15,1
mojodna,it was an arbitrary choice on my part,4/27/15,1
lossyrob,ok cool. yeah so that info might be useful. How do I guest post?,4/27/15,1
lossyrob,ok cool. yeah so that info might be useful. How do I guest post?,4/27/15,1
mojodna,tumblr invitation sent,4/27/15,1
mojodna,tumblr invitation sent,4/27/15,1
lossyrob,cool. Something short and sweet alright?,4/27/15,1
lossyrob,cool. Something short and sweet alright?,4/27/15,1
mojodna,it_Ñés a tumblr_ÑÓperfect!,4/27/15,1
mojodna,it_Ñés a tumblr_ÑÓperfect!,4/27/15,1
mojodna,what_Ñés the pixel type on the source?,4/27/15,1
mojodna,what_Ñés the pixel type on the source?,4/27/15,1
lossyrob,Float32,4/27/15,1
lossyrob,Float32,4/27/15,1
lossyrob,predictor=3 needs floating point,4/27/15,1
lossyrob,predictor=3 needs floating point,4/27/15,1
lossyrob,predictor=2 can_Ñét do doubles,4/27/15,1
lossyrob,predictor=2 can_Ñét do doubles,4/27/15,1
lossyrob,(float64),4/27/15,1
lossyrob,(float64),4/27/15,1
lossyrob,so it_Ñés the only type they can both handle,4/27/15,1
lossyrob,so it_Ñés the only type they can both handle,4/27/15,1
mojodna,"oh, huh",4/27/15,1
mojodna,"oh, huh",4/27/15,1
mojodna,great info,4/27/15,1
mojodna,great info,4/27/15,1
mojodna,@mojodna envisions a matrix,4/27/15,1
mojodna,@mojodna envisions a matrix,4/27/15,1
mojodna,gitter: that_Ñés annoyingly large.,4/27/15,1
mojodna,gitter: that_Ñés annoyingly large.,4/27/15,1
lossyrob,hahaha,4/27/15,1
lossyrob,hahaha,4/27/15,1
lossyrob,much less subtle then the IRC version ,4/27/15,1
lossyrob,much less subtle then the IRC version ,4/27/15,1
lossyrob,http://openterrain.tumblr.com/post/117553678231/gdal-compression-options-against-ned-data,4/27/15,1
lossyrob,http://openterrain.tumblr.com/post/117553678231/gdal-compression-options-against-ned-data,4/27/15,1
mojodna,"awesome, thanks for doing that!",4/27/15,1
mojodna,"awesome, thanks for doing that!",4/27/15,1
lossyrob,for sure. Weird that floating point predictor goes faster than w/o a predictor for deflate. but I_Ñéd say that_Ñés the best option out of them.,4/27/15,1
lossyrob,for sure. Weird that floating point predictor goes faster than w/o a predictor for deflate. but I_Ñéd say that_Ñés the best option out of them.,4/27/15,1
mojodna,i wonder if this had to do with buffering the file into memory:,4/27/15,1
mojodna,i wonder if this had to do with buffering the file into memory:,4/27/15,1
mojodna,> 0m48.615s deflate-predictor2.tif,4/27/15,1
mojodna,> 0m48.615s deflate-predictor2.tif,4/27/15,1
lossyrob,i tried it a couple of times,4/27/15,1
lossyrob,i tried it a couple of times,4/27/15,1
lossyrob,similar results,4/27/15,1
lossyrob,similar results,4/27/15,1
mojodna,huh,4/27/15,1
mojodna,huh,4/27/15,1
mojodna,cool.,4/27/15,1
mojodna,cool.,4/27/15,1
lossyrob,"I haven_Ñét gotten to implementing floating point predictor in the reader yet, so maybe when I do I_Ñéll find out something that_Ñéll make sense about that. Until then, head scratcher.",4/27/15,1
lossyrob,"I haven_Ñét gotten to implementing floating point predictor in the reader yet, so maybe when I do I_Ñéll find out something that_Ñéll make sense about that. Until then, head scratcher.",4/27/15,1
hectcastro,Gave the README a little love: https://github.com/geotrellis/geotrellis-ec2-cluster/pull/28,4/28/15,1
hectcastro,Gave the README a little love: https://github.com/geotrellis/geotrellis-ec2-cluster/pull/28,4/28/15,1
pomadchin,"@lossyrob just accumulo ingest and tried to make nodata transparent (`gdal_translate  -a_nodata -2147483648 in.tif out.tif` btw, it seems to me it makes `a_nodata 0` due to overflow) and ingest via accumulo ingest. same result ",4/28/15,1
pomadchin,"@lossyrob just accumulo ingest and tried to make nodata transparent (`gdal_translate  -a_nodata -2147483648 in.tif out.tif` btw, it seems to me it makes `a_nodata 0` due to overflow) and ingest via accumulo ingest. same result ",4/28/15,1
DarkManiel,"@lossyrob I got snagged again. In the old admin, the data for e in the code below is an object that has a latlng field. what code is actually mapping the position of the click on the map to the latlng? It seems that there must be a a library that is doing that, but I can't find it, unless it's somewhere in leaflet-src.js.",4/28/15,1
DarkManiel,"@lossyrob I got snagged again. In the old admin, the data for e in the code below is an object that has a latlng field. what code is actually mapping the position of the click on the map to the latlng? It seems that there must be a a library that is doing that, but I can't find it, unless it's somewhere in leaflet-src.js.",4/28/15,1
DarkManiel,"```map.on('click', function(e) {                        console.log('e is : ', e);                        valueViewer.update(e.latlng);                        $('a[href=#layer-info]').tab('show');                    });```",4/28/15,1
DarkManiel,"```map.on('click', function(e) {                        console.log('e is : ', e);                        valueViewer.update(e.latlng);                        $('a[href=#layer-info]').tab('show');                    });```",4/28/15,1
DarkManiel,And is there an equivalent mapping library in gt-admin that I can use?,4/28/15,1
DarkManiel,And is there an equivalent mapping library in gt-admin that I can use?,4/28/15,1
pomadchin,"Hm and a lil bit newbie q, about spark: how to calculate memory for executors / driver for spark submit? (basing on a certain tif size, I guess), and memory is limiting by size available on each worker, or available on the whole cluster?",4/28/15,1
pomadchin,"Hm and a lil bit newbie q, about spark: how to calculate memory for executors / driver for spark submit? (basing on a certain tif size, I guess), and memory is limiting by size available on each worker, or available on the whole cluster?",4/28/15,1
pomadchin,"Hi :D it is me again;trying to ingest in a distributed mode (3 nodes, 1 master, 2 slaves)got a not very informative error: ```com.esotericsoftware.kryo.KryoException: Encountered unregistered class ID: 13994```mb smth special I should pass in arguments to spark-submit ?",4/28/15,1
pomadchin,"Hi :D it is me again;trying to ingest in a distributed mode (3 nodes, 1 master, 2 slaves)got a not very informative error: ```com.esotericsoftware.kryo.KryoException: Encountered unregistered class ID: 13994```mb smth special I should pass in arguments to spark-submit ?",4/28/15,1
pomadchin,"```spark-submit \--class geotrellis.admin.ingest.AccumuloIngestCommand \--supervise --master spark://master1:7077 \--deploy-mode cluster \--driver-memory $DRIVER_MEMORY \--executor-memory $EXECUTOR_MEMORY \$JAR \--instance gis --user root --password secret --zookeeper master1 \--crs $CRS \--pyramid $PYRAMID --clobber $CLOBBER \--input $INPUT \--layerName $LAYER_NAME \--table $TABLE```",4/28/15,1
pomadchin,"```spark-submit \--class geotrellis.admin.ingest.AccumuloIngestCommand \--supervise --master spark://master1:7077 \--deploy-mode cluster \--driver-memory $DRIVER_MEMORY \--executor-memory $EXECUTOR_MEMORY \$JAR \--instance gis --user root --password secret --zookeeper master1 \--crs $CRS \--pyramid $PYRAMID --clobber $CLOBBER \--input $INPUT \--layerName $LAYER_NAME \--table $TABLE```",4/28/15,1
pomadchin,can provide a link to a test master UI o: to show logs or smth else,4/28/15,1
pomadchin,can provide a link to a test master UI o: to show logs or smth else,4/28/15,1
pomadchin,"btw, with default deploy mode `--deploy-mode client` everything is ok",4/28/15,1
pomadchin,"btw, with default deploy mode `--deploy-mode client` everything is ok",4/28/15,1
notthatbreezy,What kind of spark cluster is it? Is it a mesos cluster?,4/28/15,1
notthatbreezy,What kind of spark cluster is it? Is it a mesos cluster?,4/28/15,1
notthatbreezy,"oh whoops, just looked at the master url",4/28/15,1
notthatbreezy,"oh whoops, just looked at the master url",4/28/15,1
pomadchin,:D) spark cluster ye,4/28/15,1
pomadchin,:D) spark cluster ye,4/28/15,1
pomadchin,docker is a power!,4/28/15,1
pomadchin,docker is a power!,4/28/15,1
pomadchin,http://104.155.79.182:8080/,4/28/15,1
pomadchin,http://104.155.79.182:8080/,4/28/15,1
notthatbreezy,Reason I asked was that I know cluster mode isn't supported supported for mesos,4/28/15,1
notthatbreezy,Reason I asked was that I know cluster mode isn't supported supported for mesos,4/28/15,1
pomadchin,heh; but its ok in a client mode normal ):,4/28/15,1
pomadchin,heh; but its ok in a client mode normal ):,4/28/15,1
pomadchin,dev link* for test ,4/28/15,1
pomadchin,dev link* for test ,4/28/15,1
notthatbreezy,"Hmm, I'm not really sure what the problem is (mesos thing was my only haphazard guess) -- someone else might have some ideas though",4/28/15,1
notthatbreezy,"Hmm, I'm not really sure what the problem is (mesos thing was my only haphazard guess) -- someone else might have some ideas though",4/28/15,1
echeipesh,fun fact: Exponential backoff is actually a random choice from (0 _ÑÓ 2^failures) * base multiple,4/28/15,1
echeipesh,fun fact: Exponential backoff is actually a random choice from (0 _ÑÓ 2^failures) * base multiple,4/28/15,1
echeipesh,http://en.wikipedia.org/wiki/Exponential_backoff,4/28/15,1
echeipesh,http://en.wikipedia.org/wiki/Exponential_backoff,4/28/15,1
echeipesh,"@pomadchin, I just came across this blog re spark executor memory tuning. It mostly talks about YARN, but the points are still valid. http://blog.cloudera.com/blog/2015/03/how-to-tune-your-apache-spark-jobs-part-2/",4/28/15,1
echeipesh,"@pomadchin, I just came across this blog re spark executor memory tuning. It mostly talks about YARN, but the points are still valid. http://blog.cloudera.com/blog/2015/03/how-to-tune-your-apache-spark-jobs-part-2/",4/28/15,1
echeipesh,"Trying to imagine how that Kyro error could come to be. AFAIK Kryo will create a mapping from class to ID the first time it encounters a new class to be serialized. So some component is getting a kryo seralized class of the type it has not seen. It_Ñés worth turning on logging and seeing if there is more useful error message there. ```import com.esotericsoftware.minlog.Log  Log.set(Log.LEVEL_TRACE)```",4/28/15,1
echeipesh,"Trying to imagine how that Kyro error could come to be. AFAIK Kryo will create a mapping from class to ID the first time it encounters a new class to be serialized. So some component is getting a kryo seralized class of the type it has not seen. It_Ñés worth turning on logging and seeing if there is more useful error message there. ```import com.esotericsoftware.minlog.Log  Log.set(Log.LEVEL_TRACE)```",4/28/15,1
pomadchin,"@echeipesh g2 provide ~ 10gb's to run an ingest job for tiles ~ 400-1gb sizes; batches by 4, ingesting with temporal spatial key ._. With 8gb's there was a memory error",4/28/15,1
pomadchin,"@echeipesh g2 provide ~ 10gb's to run an ingest job for tiles ~ 400-1gb sizes; batches by 4, ingesting with temporal spatial key ._. With 8gb's there was a memory error",4/28/15,1
pomadchin,Will try tomorrow to set improved logging,4/28/15,1
pomadchin,Will try tomorrow to set improved logging,4/28/15,1
pomadchin,"P.s. Ingesting in a client mode, but using cluster master was successful",4/28/15,1
pomadchin,"P.s. Ingesting in a client mode, but using cluster master was successful",4/28/15,1
lossyrob,@pomadchin we haven_Ñét used cluster mode for our spark submit jobs_Ñ_we just submit them as a process on master. That only means that the job runs as a sync process from where you call it; it still utilizes the whole cluster. https://spark.apache.org/docs/latest/spark-standalone.html,4/28/15,1
lossyrob,@pomadchin we haven_Ñét used cluster mode for our spark submit jobs_Ñ_we just submit them as a process on master. That only means that the job runs as a sync process from where you call it; it still utilizes the whole cluster. https://spark.apache.org/docs/latest/spark-standalone.html,4/28/15,1
pomadchin,"@lossyrob then, what exactly means cluster mode?",4/28/15,1
pomadchin,"@lossyrob then, what exactly means cluster mode?",4/28/15,1
pomadchin,"yep, noticed that it sends tasks on every worker",4/28/15,1
pomadchin,"yep, noticed that it sends tasks on every worker",4/28/15,1
notthatbreezy,cluster mode just means the spark driver is started on one of the workers instead of on your local machine (where you run spark submit from),4/28/15,1
notthatbreezy,cluster mode just means the spark driver is started on one of the workers instead of on your local machine (where you run spark submit from),4/28/15,1
pomadchin,hm,4/28/15,1
pomadchin,hm,4/28/15,1
pomadchin,see it,4/28/15,1
pomadchin,see it,4/28/15,1
notthatbreezy,"On the page @lossyrob linked, there's an explanation at the header ""Launching Spark Applications""",4/28/15,1
pomadchin,ty guys! :p soon will check the quality of ndvi counting with geotrellis,4/28/15,1
DarkManiel,@lossyrob nevermind I figured it out using:,4/28/15,1
DarkManiel,```var mousePoint = this.map.mouseEventToLayerPoint(e); var latLng = this.map.layerPointToLatLng(mousePoint); ```,4/28/15,1
lossyrob,oh nice. That sounds like the way to do it.,4/28/15,1
notthatbreezy,"@lossyrob - we kicked off an ingest a while ago, it seems to be stuck on pushing to accumulo -- is that something you all have come across before?",4/28/15,1
notthatbreezy,[![ingest.png](https://files.gitter.im/geotrellis/geotrellis/NuMC/thumb/ingest.png)](https://files.gitter.im/geotrellis/geotrellis/NuMC/ingest.png),4/28/15,1
echeipesh,pretty,4/28/15,1
notthatbreezy,"for reference, that's what the cluster looks like right now wrt to CPU and network IO -- lots of stuff getting pushed between instances, not much getting written to disk",4/28/15,1
echeipesh,Where is it in the spark job ?,4/28/15,1
notthatbreezy,http://ec2-52-1-248-63.compute-1.amazonaws.com:4040/stages/stage/?id=7&attempt=0,4/28/15,1
echeipesh,calculating a histogram,4/28/15,1
notthatbreezy,well it was stuck on save as new hadoop file for a while,4/28/15,1
lossyrob,"what_Ñés a while, and how big is the cluster?",4/28/15,1
notthatbreezy,1.3 hours,4/28/15,1
notthatbreezy,"32 cores, 170 GB of memory",4/28/15,1
notthatbreezy,4 r3.2xlarges,4/28/15,1
lossyrob,hmmm. Yeah that doesn_Ñét seem right.,4/28/15,1
notthatbreezy,"ok, I have some ideas, haven't messed with caching yet and it seems like a lot got evicted from memory -- just wanted to see what your experiences have been",4/28/15,1
lossyrob,@echeipesh would have a better sense of how bad that is compared to other runs we_Ñéve done,4/28/15,1
notthatbreezy,"cool, I forgot to take off the extra cores parameter which probably didn't help this run",4/28/15,1
lossyrob,that might be messing with it all,4/28/15,1
notthatbreezy,probably,4/28/15,1
lossyrob,things are getting evicted from memory because there_Ñés too many tasks,4/28/15,1
echeipesh,Recorded one for 5 i2.2xlarge `DAGScheduler: Stage 7 (saveAsNewAPIHadoopFile at AccumuloDriver.scala:191) finished in 740.410 s`,4/28/15,1
notthatbreezy,"that makes sense, yeah",4/28/15,1
notthatbreezy,"ha ok, cool, that seems much more reasonable",4/28/15,1
echeipesh,This is a NEX ingest right ?,4/28/15,1
lossyrob,yeah,4/28/15,1
echeipesh,111 tasks is what it always ends up with given 1k keys page size,4/28/15,1
notthatbreezy,"going AFK -- yeah that's what we ended up with too, but since I bumped the number of parallel tasks, that was messing things up",4/28/15,1
echeipesh,"ah, didn_Ñét notice that",4/28/15,1
lossyrob,"hey @non, I spent a whole lot of time yestereday/last night trying to optimize the implementation that Johan had made of LZW decompression. You did LZW compression for GeoTrellis, and I basically learned scala optimization techinques from reading your code, so  I was wondering if you could give a quick eye to see if you have any ideas to move it further.",4/29/15,1
lossyrob,This is what it is currently: https://github.com/lossyrob/geotrellis/blob/geotiff-tile/raster/src/main/scala/geotrellis/raster/io/geotiff/compression/LZWDecompression.scala,4/29/15,1
lossyrob,"That does a decompression in, lets say for relative rating, _ÑÉ10s'",4/29/15,1
lossyrob,"This version: https://github.com/lossyrob/geotrellis/blob/320072b10d64387ca1945bb42ad47f69ce28611b/raster/src/main/scala/geotrellis/raster/io/geotiff/compression/LZWDecompression.scala is what I started with, which was _ÑÉ17s'",4/29/15,1
lossyrob,The ZLib can do it in about 4.5s,4/29/15,1
lossyrob,"GDAL doesn_Ñét seem to have much difference in decompression speed between ZLib and LZW. So yeah, not sure what else we can do with it to not make it so much worse than ZLib",4/29/15,1
lossyrob,"Profiling show most time spent _ÑÒ writeToOutput_Ñù, _ÑÒLZWBitInputStream.get_Ñù, and  _ÑÒaddEntry_Ñù, which have the most invocations, by a lot_Ñ_but I mean bits need to be read and copied to output, so yeah. not sure what else I can do here.",4/29/15,1
notthatbreezy,"@lossyrob - ingest is going much better today, got to writing output stage, though looks like it's pushing all the load to a single accumulo instance in the cluster",4/29/15,1
notthatbreezy,[![accumulo.png](https://files.gitter.im/geotrellis/geotrellis/NxzM/thumb/accumulo.png)](https://files.gitter.im/geotrellis/geotrellis/NxzM/accumulo.png),4/29/15,1
non,@lossyrob taking a look,4/29/15,1
non,i'll be on the train later tonight so i can maybe try to give you some comments then (at work now),4/29/15,1
non,@lossyrob so -- my intuition is that storing and writing data byte-by-byte will be slower than it needs to be,4/29/15,1
non,that may be wrong -- but it's just something that jumped out at me,4/29/15,1
lossyrob,"The initial implementation used byte arrays to hold the tokens_Ñ_but for each token concat, another small array would have to be created and copied, and so that was slow. The latest optimization was to do the linked-list tokens so that each token string didn_Ñét have to be it_Ñés own array.",4/29/15,1
lossyrob,and I_Ñém copying GDAL_Ñés method for that_Ñ_but also they are using structs and pointers and it_Ñés super fast :/ ,4/29/15,1
lossyrob,"not sure if there_Ñés a better method, I hope there is, but that_Ñés why the char by char method is there",4/29/15,1
DarkManiel,"When I run ```val spatialKey = rasterMetaData.mapTransform(clickPoint)``` I get an error that ""value mapTransform is not a member of (geotrellis.spark.io.LayerMetaData, geotrellis.admin.server.CatalogService.catalog.metaDataCatalog.TableName)""",4/29/15,1
DarkManiel,Is there another way to do this? My rasterMetaData is catalog.metaDataCatalog.load(layerId),4/29/15,1
non,@lossyrob is there a simple lzw benchmark i can run?,4/29/15,1
non,(which demonstrates the speed differences you talked about),4/29/15,1
non,(also -- i have to say that launching SBT to pull down everything over my tethered phone on a train is... problematic.),4/29/15,1
DarkManiel,"Ok... I think I got passed that using val (meta, _) , but now I'm having trouble finding what the ""rast"" variable was referring to. I tried ""raster.getDouble(col, row)"" and ""rasterExtent.getDouble(col, row)"", but neither work. Here's my code:",4/29/15,1
DarkManiel,"```scaladef valueRoute = cors{    path(""valuegrid"") {        get{      parameters(        'store,        'layer,        'zoom.as[Int],        'lat.as[Double],        'lng.as[Double],        'x.as[Double],         'y.as[Double],        'size.as[Int] ? 7) { (store,layer,zoom,lat,lng,x,y,size) =>        val layerId = LayerId(layer, zoom)        val (x,y) = (lng,lat).reproject(LatLng, WebMercator)        val (meta, _) = catalog.metaDataCatalog.load(layerId)        val clickPoint = Point(x, y).reproject(LatLng, meta.rasterMetaData.crs)        val rmd = meta.rasterMetaData        val spatialKey = rmd.mapTransform(clickPoint)        val tile = catalog.loadTile(layerId, spatialKey)        val tileExtent = rmd.mapTransform(spatialKey)        val rasterExtent = RasterExtent(tileExtent, tile.cols, tile.rows)        val (col, row) = rasterExtent.mapToGrid(clickPoint.x, clickPoint.y)        val values = mutable.ListBuffer[String]()        cfor(row - size)(_ <= row + size, _ + 1) { row =>  //ask sage??          cfor(col - size)(_ <= col + size, _ + 1) { col =>             if(0 <= col && col <= raster.cols && 0 <= row && row <= raster.rows) {               values += ""\""%.2f\"""".format(raster.getDouble(col,row))             } else {               values += ""\""\""""             }          }        }        s"""""" { ""success"" : ""1"", ""values"" : [ ${values.mkString("","")} ] } """"""                JsArray(JsObject(             ""model"" -> JsString(layer),             ""data"" -> JsArray(               data.map { case (year, value) =>                 JsObject(                   ""year"" -> JsString(year.toString),                   ""value"" -> JsNumber(value)                 )               }: _*             )           ))            }      }    } } ```",4/29/15,1
non,"@DarkManiel i think you need to put the ""\`\`\`scala"" and ""\`\`\`"" on their own lines",4/29/15,1
DarkManiel,okay thanks. Trying to edit it right now,4/29/15,1
non,looks good :),4/29/15,1
DarkManiel,"@lossyrob also, I'm not really sure how to deal with ``s"""""" { ""success"" : ""1"", ""values"" : [ ${values.mkString("","")} ] } """"""``. In the old admin, I see it being mapped into a rastersource, but we don't use rastersource here, so not quite sure how to handle that.",4/29/15,1
non,`567 warnings found` wow geotrellis has a lot of warnings these days,4/29/15,1
non,@lossyrob so i have a bunch of super speculative changes in a branch,4/29/15,1
non,if you can give me a benchmark to run i'm happy to compare it to what you have,4/29/15,1
lossyrob,"@non totally. Was travelling, just got to DC. I_Ñéll get you the benchmarks I was running",4/30/15,1
lossyrob,"Also, all those warning are about failing to inline, because the -optimize flag is set",4/30/15,1
non,thanks :),4/30/15,1
non,right right,4/30/15,1
lossyrob,Any suggestions on how to deal with that?,4/30/15,1
non,"fair warning: if the thing i did improves performance, you may still hate the code",4/30/15,1
lossyrob,I don_Ñét hate better performing code,4/30/15,1
non,"well -- if you were marking things with `@inline` that can't be inlined, removing the annotation is not a bad idea",4/30/15,1
non,but yeah sometimes it just complains about stuff and there's not a lot one can do,4/30/15,1
lossyrob,it_Ñés all just ->,4/30/15,1
lossyrob,which I never use anymore,4/30/15,1
lossyrob,but other people do,4/30/15,1
non,haha yeah,4/30/15,1
non,i might mass-refactor that to remove warnings,4/30/15,1
non,but that's a bit passive-aggressive,4/30/15,1
lossyrob,I_Ñéve been wanting to do that,4/30/15,1
lossyrob,"but yeah. It_Ñés been like, meh I might end up doing that but not yet",4/30/15,1
non,so a few things i'm trying in this commit -- make the bounds-checking on array access more transparent,4/30/15,1
non,to try to help loop optimizations,4/30/15,1
non,"also minimize/reduce calculations in the hot loop, also try to avoid using inner methods which will turn vars into ""Ref"" classes",4/30/15,1
non,"(e.g. if you capture a `var foo` and update it from an inner method, `foo` ends up being an `IntRef` object to allow it to be updated from another method.)",4/30/15,1
non,"not sure if any of it will make a difference, we'll see",4/30/15,1
non,"the overall lines of code are the same, but there is a lot of manual inlining/optimization going on :P",4/30/15,1
lossyrob,"awesome. those sound like good moves. Just uploading the data for the benchmarks, a bit too big to commit",4/30/15,1
non,yeah for sure,4/30/15,1
non,we'll see if my data plan can handle downloading it ;),4/30/15,1
lossyrob,I can get 2 zipped at 815 MB. Is that going to be too much?,4/30/15,1
non,haha i'll wait an hour or two until i'm home,4/30/15,1
non,then it'll be fine,4/30/15,1
non,"i have the code compiling so ideally i can just download the data, run a before/after benchmark, and report the findings",4/30/15,1
lossyrob,yeah. Also if you have a branch I can run the benchmarks against the data here,4/30/15,1
lossyrob,"uploading to gdrive now, will take a bit anyway on my side",4/30/15,1
non,ok,4/30/15,1
non,maybe i'll push my branch and let you try it out,4/30/15,1
non,also fair warning it may be buggy,4/30/15,1
lossyrob,I didn_Ñét know about that IntRef stuff. Any reason that wasn_Ñét showing up on the VisualVM profiler that you_Ñéd know of?,4/30/15,1
lossyrob,yeah for sure. Do the tests pass?,4/30/15,1
lossyrob,~test-only *Predictor* should tell you if it works ,4/30/15,1
lossyrob,\*Predictor\*,4/30/15,1
non,ok,4/30/15,1
non,"i ran them awhile ago, let's try again",4/30/15,1
non,"ugh, well, somehow i'm getting a java.net.UnknownHostException now",4/30/15,1
lossyrob,hmmm,4/30/15,1
lossyrob,that_Ñés a weird one,4/30/15,1
non,anyway -- i can't run tests or even push my branch,4/30/15,1
non,due to bandwidth,4/30/15,1
non,but here's a patch if you are interested: https://gist.github.com/non/f80e0fea16d6fb526df7,4/30/15,1
non,it's a patch against your branch,4/30/15,1
non,pretty extensive but should be easy to apply,4/30/15,1
lossyrob,nice,4/30/15,1
non,the big thing you'll notice is that the loops / ifs are a bit simpler and fewer,4/30/15,1
lossyrob,"cool, testing now",4/30/15,1
non,great :) it's likely i missed something up,4/30/15,1
non,if there are bugs i'll look at them when i'm back home and have a more sane network situation,4/30/15,1
lossyrob,test ran fine,4/30/15,1
non,great,4/30/15,1
lossyrob,hm actually I don_Ñét think I applied the patch right_Ñ_will let you know when I get it working correctly,4/30/15,1
non,ok,4/30/15,1
non,i think `patch -p1 < lzw.patch` should work,4/30/15,1
non,assuming you have you branch checked out,4/30/15,1
lossyrob,yeah thats not doing it for me. at the root of the repo right?,4/30/15,1
non,yeah,4/30/15,1
non,ok here is the entire file: https://gist.github.com/non/7cb35c2dbdd33a0f708f,4/30/15,1
lossyrob,"cool thanks. never got good at patching. I used that, let_Ñés see how that goes.",4/30/15,1
lossyrob,yeah things are failing. I_Ñéll take a look and see if I can sort out what_Ñés going on,4/30/15,1
non,i made a ton of changes,4/30/15,1
non,i'm sure i screwed it up,4/30/15,1
non,like -- each individual change was supposed to preserve overall behavior,4/30/15,1
non,but it's possible that one erroneous change made it seem like other optimizations would be safe (but weren't). so don't spend too much time on it ;),4/30/15,1
lossyrob,"yeah. I ran into that as well in a try I did, before I did the linked list try. ended up reverting it and trying again",4/30/15,1
non,"when i get back tonight i'll take a quick look but maybe just try to ""re-do"" the changes while running tests to ensure correctness (what i should have been doing all along)",4/30/15,1
lossyrob,the logic read clean,4/30/15,1
lossyrob,"ok cool. I_Ñém going to spend a little time with it too, so I_Ñéll let you know if I make any progress",4/30/15,1
non,thanks!  but don't feel like you have to clean up my mess ;),4/30/15,1
lossyrob,"@DarkManiel on your first question_Ñ_when there_Ñés a type issue, you need to inspect the types to see what you mean to say, and what your actually saying in code, and how you can lean on the compiler to figure out what you_Ñére saying wrong. The compiler is telling us, ok, I have a (LayerMetaData, TableName) type, and your asking me for something that has a mapTransform method, and this def isn_Ñét it. You could inspect the types your getting back, and see that LayerMetaData.rasterMetaData actually has what your looking for, so then use the appropriate proerties to get at the mapTransform.",4/30/15,1
lossyrob,"@non please don_Ñét sweat that idea! Getting you to look at the code and make those suggestions, even messy suggestions is super helpful. So thank you!",4/30/15,1
lossyrob,"I always side glanced the inner method calls, but now I have an actual reason to know not to use them",4/30/15,1
lossyrob,and if you did make it faster then that_Ñés going to be an awesome contribution,4/30/15,1
non,yeah hopefully! i'm about 20 minutes away from PVD i think so i'll go offline now but i'll ping you when i'm home,4/30/15,1
lossyrob,"Oh sorry @DarkManiel reading through comments i_Ñéve been missing (traveling this week), so it looks like you sorted that out. The other thing_Ñ_what you get back is a tile. We renamed _ÑÒRaster_Ñù to _ÑÒTile_Ñù when the spark code started in earnest, because the _ÑÉRaster_Ñé was actually the whole layer, and so the individual _ÑÉTiles_Ñé were really what you_Ñéd get values from. For better or worse. So the _ÑÉraster_Ñé you want to get values from is actually the tile that you loaded. So _ÑÉtile.getDouble(col, row)_Ñé should work",4/30/15,1
lossyrob,"@non awesome, I should be on for a while (in DC with nothing to do but all the coding) so talk to you then!",4/30/15,1
lossyrob,"@DarkManiel a good way to have found that out is, if you_Ñére looking for what has a method call, `grep` the codebase (or use [ag, the silver searcher](https://github.com/ggreer/the_silver_searcher), which is super fast and awesome) for that method name, and look for declarations. That perhaps would have landed you on the Tile trait, and let you find that that_Ñés what you want to get values from",4/30/15,1
lossyrob,"@non nice! just took one switch. oldCode = code should happen even if code == ClearCode, that was it.",4/30/15,1
lossyrob,so moving the assignment at line 142 to under the bracket on 143,4/30/15,1
lossyrob,"Ok, so I_Ñém running them through a really crappy version of the benchmark_Ñ_caliper doesn_Ñét like how long it takes to process the big file. I_Ñém going to work on some better benchmkars by gdal_translating, but here_Ñés some initial results:",4/30/15,1
lossyrob,"```Pre Erik changes:[info] Running geotrellis.benchmark.RasterReadProfile [info] Starting...[info] [Reading geotiff lzw time 1] Run 0 took 8773 ms.[info] [Reading geotiff deflate time 1] Run 0 took 5676 ms.[info] [Reading geotiff lzw time 2] Run 0 took 8764 ms.[info] [Reading geotiff deflate time 2] Run 0 took 4507 ms.[info] [Reading geotiff lzw time 3] Run 0 took 8283 ms.[info] [Reading geotiff deflate time 3] Run 0 took 4797 ms.[info] [Reading geotiff lzw time 4] Run 0 took 10419 ms.[info] [Reading geotiff deflate time 4] Run 0 took 5007 ms.Post Erik changes:[info] [Reading geotiff lzw time 1] Run 0 took 8870 ms.[info] [Reading geotiff deflate time 1] Run 0 took 5574 ms.[info] [Reading geotiff lzw time 2] Run 0 took 7972 ms.[info] [Reading geotiff deflate time 2] Run 0 took 4384 ms.[info] [Reading geotiff lzw time 3] Run 0 took 8711 ms.[info] [Reading geotiff deflate time 3] Run 0 took 4633 ms.[info] [Reading geotiff lzw time 4] Run 0 took 7974 ms.[info] [Reading geotiff deflate time 4] Run 0 took 5149 ms.[info] [Reading geotiff lzw time 5] Run 0 took 9578 ms.[info] [Reading geotiff deflate time 5] Run 0 took 4542 ms.[info] [Reading geotiff lzw time 6] Run 0 took 7970 ms.[info] [Reading geotiff deflate time 6] Run 0 took 4486 ms.```",4/30/15,1
lossyrob,"I_Ñéd say, definite improvement",4/30/15,1
lossyrob,so strange. GDAL works so much faster with LZW than with Deflate,4/30/15,1
notthatbreezy,@lossyrob since you're gioing to be on for a while ... maybe we could chat about some of the issues we ran into with the ingest stuff today (with the hope that there's some config change I'm missing that could fix things),4/30/15,1
lossyrob,for sure,4/30/15,1
notthatbreezy,"we ran the ingest on a big cluster (112 cores, about 800GB of memory (don't worry, used spot instances so it was cheap)) -- but the writing to accumulo still took about 40 minutes",4/30/15,1
lossyrob,hm,4/30/15,1
notthatbreezy,"everything else ran really fast -- it looked like the writes were going to a single accumulo instance, which then had to balance it as it got too big",4/30/15,1
notthatbreezy,"in fact, I'm almost positive that's what was happening based on the accumulo UI",4/30/15,1
lossyrob,"well, the way we do it now I believe (and @echeipesh wrote it so he would know best) is that we write to HDFS, and then tell accumulo to vacuum up the tiles from the directory in HDFS",4/30/15,1
lossyrob,have you looked at where it does that in code?,4/30/15,1
notthatbreezy,"kind of (one sec, let me get a link to where I was looking at stuff)",4/30/15,1
lossyrob,"and to be sure, we_Ñére on catalog-refactor, right?",4/30/15,1
lossyrob,https://github.com/geotrellis/geotrellis/blob/feature/catalog-refactor/spark/src/main/scala/geotrellis/spark/io/accumulo/RasterRDDWriter.scala#L63,4/30/15,1
notthatbreezy,"ha yes, didn't make that mistake again",4/30/15,1
lossyrob,hm,4/30/15,1
lossyrob,something seems weird,4/30/15,1
notthatbreezy,https://github.com/geotrellis/geotrellis/blob/feature/catalog-refactor/spark/src/main/scala/geotrellis/spark/io/accumulo/RasterRDDWriter.scala#L58,4/30/15,1
notthatbreezy,"yeah, so that's where I was looking at things",4/30/15,1
notthatbreezy,tried tweaking some things with the BatchConfig --- but ran into more issues,4/30/15,1
lossyrob,this is Eugene_Ñés try3 branch,4/30/15,1
lossyrob,https://github.com/echeipesh/geotrellis/blob/zcurve-try3/spark/src/main/scala/geotrellis/spark/io/accumulo/AccumuloDriver.scala#L168,4/30/15,1
lossyrob,it_Ñés saving it differently,4/30/15,1
lossyrob,it_Ñés possible that this change didn_Ñét make it_Ñés way in,4/30/15,1
notthatbreezy,"ok cool, maybe I'll try that out tomorrow",4/30/15,1
notthatbreezy,what you're saying it _should_ do makes sense,4/30/15,1
lossyrob,yeah. I imagine that_Ñés what_Ñés going on,4/30/15,1
notthatbreezy,ran into another issue when trying to ingest a second layer/set of data,4/30/15,1
notthatbreezy,that I don't think is related,4/30/15,1
lossyrob,which is unfortunate.,4/30/15,1
lossyrob,"ok, what_Ñés that?",4/30/15,1
notthatbreezy,"was getting a weird error about case match (was splitting a java string into an `Array(table, name)` (if that rings a bell) failing ... trying to find the exact spot",4/30/15,1
lossyrob,hm don_Ñét htink i hit that,4/30/15,1
notthatbreezy,https://github.com/geotrellis/geotrellis/blob/feature/catalog-refactor/spark/src/main/scala/geotrellis/spark/io/accumulo/AccumuloLayerMetaDataCatalog.scala#L83,4/30/15,1
notthatbreezy,"I tried ingesting rcp24, which though slow, eventually worked, then since the benchmark needs another layer was going to do rcp60,",4/30/15,1
lossyrob,did you switch codebases (to the refactor branch) between those two ingests?,4/30/15,1
notthatbreezy,"no, everything was on catalog-refactor",4/30/15,1
lossyrob,k,4/30/15,1
lossyrob,did you happen to catch what the string was,4/30/15,1
notthatbreezy,"no, I added a log statement on the last try, but think I messed it up and it never printed the actual string just the StringXXXX memory location stuff",4/30/15,1
lossyrob,"so you loaded rcp60, and it failed on ingest?",4/30/15,1
notthatbreezy,"yeah, it got as far as getting it from s3, then this runs next and it failed with the case match error",4/30/15,1
notthatbreezy,or maybe just matcherror,4/30/15,1
lossyrob,you_Ñéd be able to see what_Ñés going on in the accumulo shell,4/30/15,1
lossyrob,and do a `scan -f 20` on the metadata table,4/30/15,1
lossyrob,and look at the row ids,4/30/15,1
lossyrob,and see if htere_Ñés any that don_Ñét have that `XXX__XXX` name convention,4/30/15,1
notthatbreezy,"got it,  the cluster isn't around any longer, but I'll keep that in mind tomorrow",4/30/15,1
lossyrob,"ok. @echeipesh would be good to work with on that, if you have to get him to pair with you a bit for that accumulo write issue and this that would be helpful.",4/30/15,1
lossyrob,"@non here_Ñés some more results, I quartered the dataset so it can run in caliper ok:",4/30/15,1
lossyrob,"```Pre Erik changes:[info]       LZW 2035 ==============================[info]   DEFLATE  995 ==============Post Erik changes:[info] benchmark   ms linear runtime[info]       LZW 2009 ==============================[info]   DEFLATE  966 ==============```",4/30/15,1
notthatbreezy,"sounds good, yeah he was out sick today -- thanks for the help",4/30/15,1
lossyrob,"no problem. Sorry I_Ñém not more of a help, but I know that Eugene has worked through the guts of these issues more than me so I think he_Ñéd be the best to have definite answers",4/30/15,1
notthatbreezy,"no worries, this is all good info -- that PR @moradology put together was one quick optimization we tried out, which I still think is a good idea, otherwise we're materializing that RDD twice for no reason -- also spent a fair amount of time digging through accumulo docs which will probably be useful at some point",4/30/15,1
non,@lossyrob interesting. so not really a difference,4/30/15,1
notthatbreezy,found some info about how many splits/tablet servers are optimal for pre-splitting too,4/30/15,1
allixender,"Hi everyone, just wanted say Hi and introduce myself quickly :-) I'm Alex and I got accepted for this year's GSoC with GeoTrellis.",4/30/15,1
lossyrob,hey dude! Congrats on getting accepted,4/30/15,1
allixender,"Hi rob, just saw the assigned mentor in Melange would be Chris Brown",4/30/15,1
lossyrob,"That_Ñés fine, that_Ñés just admin stuff.",4/30/15,1
allixender,Thanks Rob :-) I've sent you a big email. Shall I get in touch with Chris via email?,4/30/15,1
lossyrob,"GeoTrellis has 5 students accepted, they all had us two as metnors, so they_Ñére probably trying to spread it out",4/30/15,1
notthatbreezy,"Hey Alex - that's me, nice to meet you -- congrats on getting accepted",4/30/15,1
lossyrob,"but I_Ñéll remain the primary point of contact, as not to drag chris into more work that he agreed to take on ;)",4/30/15,1
allixender,"Oh wow, congrats to you guys, too. That's amazing",4/30/15,1
allixender,"Ok, that's cool, Rob. Hi Chris :-)",4/30/15,1
notthatbreezy,ha thanks @lossyrob :),4/30/15,1
lossyrob,"Apologies for not responding to your email yet, I_Ñéve got a lot of those :/",4/30/15,1
allixender,"tags, alright, I'll get into it @lossyrob @notthatbreezy ",4/30/15,1
lossyrob,but we should set up an initial google hangout to do a meet and greet (which @notthatbreezy it would be awesome if you could join in on that),4/30/15,1
non,"@lossyrob hey, so i'm back in PVD. where can i download the test data?",4/30/15,1
notthatbreezy,"yeah, definitely we should do taht soon",4/30/15,1
allixender,"Yeah no worries @lossyrob , community bonding although needs engagement from student's side :-) ",4/30/15,1
allixender,*also,4/30/15,1
lossyrob,yeah totally. and I_Ñém really glad you_Ñére on here. This so far has been the most effecitive way to communicate with the GeoTrellis community I think,4/30/15,1
allixender,"So, if I am off and log back in would I get reminded if my name has been tagged in messages?",4/30/15,1
lossyrob,"yup, it_Ñéll even annoy you with emails if your mentioned",4/30/15,1
notthatbreezy,I think so -- I get emails if I have unread messages in general after a while,4/30/15,1
allixender,sweet.,4/30/15,1
lossyrob,"I use the OSX client for it, which is pretty great_Ñ_can_Ñét speak to other clients",4/30/15,1
notthatbreezy,"I use the web client, no complaints -- there _is_ an IRC gateway too as well I've played with",4/30/15,1
lossyrob,@non here_Ñés a branch that you can use to benchmark LZW if you_Ñére curious: https://github.com/lossyrob/benchmark/tree/benchmarking-lzw,4/30/15,1
lossyrob,"if you dl this zip and put the two TIF_Ñés in the benchmark/geotiff/data folder, should run for you",4/30/15,1
lossyrob,https://drive.google.com/file/d/0B7ubrF-X8So3V0JHazhHRjJjYXM/view?usp=sharing,4/30/15,1
non,great thanks,4/30/15,1
allixender,"Gitter website will do for starters :-) Hey my timezone is Auckland/New Zealand, so for more formal meeting arrangements my mornings and your afternoons http://www.timeanddate.com/worldclock/meetingtime.html?iso=20150430&p1=198&p2=22",4/30/15,1
allixender,"Let me know what you think @lossyrob @notthatbreezy , in the meanwhile I'll plough through the docs and tutorials and maybe collect documentation upgrades if applicable :-p ",4/30/15,1
lossyrob,ok. And you should check out the feature/cassandra branch on geotrellis,4/30/15,1
lossyrob,I_Ñéll try to schedule something for next week. How early is too early for you?,4/30/15,1
allixender,"yes, I had an eye on that branch for a while.",4/30/15,1
allixender,"umm, from 9am I would have had a coffee and clear thinking. I could do 7am or 8am too (not happy, but could do), but for regular timings I'd prefer 9am :-) if that's alright",4/30/15,1
allixender,8am would be ok too if you wanna leave work early :-),4/30/15,1
lossyrob,9 AM or 10 AM is fine by me,4/30/15,1
allixender,great,4/30/15,1
lossyrob,"I_Ñém a night owl, so my work time tends towards later in the day, so that works out",4/30/15,1
notthatbreezy,that works for me too,4/30/15,1
lossyrob,"cool, we_Ñéll work on getting that together for next week, otherwise we can take scheduling offline and not bug people about it :)",4/30/15,1
allixender,:thumbsup:  ,4/30/15,1
DarkManiel,Thanks! I got it to compile now,4/30/15,1
mojodna,"@hectcastro @lossyrob speaking of Docker (re: vapor-clock provisioning discussion), have you guys had a chance to spend any time with ECS yet?",4/30/15,1
mojodna,it moved up my list a couple weeks ago with the announcement of hooks into ELB,4/30/15,1
lossyrob,"haven_Ñét looked into that yet, but I agree that does sound appealing_Ñ_hope to get to work with it soon",4/30/15,1
mojodna,"we have a bunch of services with higher RAM requirements that don_Ñét get a whole lot of use, so it_Ñés appealing to reduce costs",4/30/15,1
mojodna,"(err, spikey higher RAM requirements)",4/30/15,1
lossyrob,"ECS is like beanstalk but with docker, yeah? or am I misthinking",4/30/15,1
mojodna,that_Ñés beanstalk with docker that_Ñés beanstalk with docker ;-),4/30/15,1
mojodna,ECS is like kubernetes,4/30/15,1
mojodna,and/or mesos (w/ docker),4/30/15,1
lossyrob,nice. so cluster resource management/task execution,4/30/15,1
mojodna,"yup.  the preview release looked great for tasks, but not so good for services w/o having to do a bunch of runtime configuration to ensure that ports are mapped and so on",4/30/15,1
mojodna,i _think_ that_Ñés improved,4/30/15,1
mojodna,i_Ñém hoping to look into it in the next few days in the context of Field Papers (since that now incorporates a few contained units),4/30/15,1
mojodna,"speaking of, is Sunday the sprint day at the HOT summit?",4/30/15,1
lossyrob,"yup, Sunday and Monday",4/30/15,1
mojodna,i won_Ñét be there in person but was hoping to participate remotely,4/30/15,1
mojodna,"ah, great",4/30/15,1
lossyrob,"yeah, I_Ñéll make sure to keep you in the loop",4/30/15,1
mojodna,might duck out for a while saturday to kayak,4/30/15,1
mojodna,thanks!,4/30/15,1
mojodna,"err, sunday too",4/30/15,1
lossyrob,I_Ñéll try to put notices on the OAM gitter or somehting,4/30/15,1
lossyrob,nice. Don_Ñét get much Kayaking in in Philly :),4/30/15,1
mojodna,western PA is awesome,4/30/15,1
mojodna,"(though i_Ñéve only been down that way once for the Yough, the Cheat, and the Savage)",4/30/15,1
lossyrob,I_Ñéve never been. Don_Ñét really know what_Ñés involved with Kayaking tbh. But seems like a nice way to get out of the city/out of the computer,4/30/15,1
mojodna,:+1:,4/30/15,1
mojodna,i_Ñéll take you on some class II sometime,4/30/15,1
lossyrob,that would be awesome,4/30/15,1
lossyrob,Hopefully I_Ñéll find myself in Seattle at some point and do that,4/30/15,1
lossyrob,looking up the different classes_Ñ_what_Ñés the highest class you do?,4/30/15,1
mojodna,V,4/30/15,1
lossyrob,whoa. the videos of class V rivers are pretty intense,4/30/15,1
mojodna,did this saturday: http://www.americanwhitewater.org/content/River/detail/id/2245/,4/30/15,1
mojodna,V is a big range.  i dip into the lower end from time to time,4/30/15,1
mojodna,"(walked around Landslide, which is pictured there)",4/30/15,1
lossyrob,awesome. well if I end up over there I_Ñém definitely taking you up on the offer :),4/30/15,1
mojodna,great!,4/30/15,1
lossyrob,for now I_Ñém headed AFK. cheers!,4/30/15,1
mojodna,have a good night!,4/30/15,1
mojodna,hi to folks in DC,4/30/15,1
DarkManiel,"Hi Rob, I just ran ./sbt ""project server"" clean and update and now get ""ClassNotFoundException: geotrellis.admin.server.CatalogService when I run ./run-server.sh. I tried publishLocal, but still getting the error. Do you know how I can correct this?",4/30/15,1
hectcastro,"mojodna: Haven't messed with ECS yet. So many tools in that space now to evaluate. CoreOS, K8r, Mesos/Marathon, Lattice.",4/30/15,1
lossyrob,"@DarkManiel: don't forget './sbt ""project server"" assembly' whenever you want to run changes through the scripts ",4/30/15,1
lossyrob,"I'm having comnection issues at HOT summit, fyi",4/30/15,1
notthatbreezy,"took a look at the metadata table, definitely some things that don't follow the table __ layer convention```bashLayer(name = ""tasmax keyBounds: []    {""minKey"":{""col"":39,Layer(name = ""tasmax keyIndex: []    {""serialized"":""\\u000tas__tasmax-rcp60-cc 8:histogram []    [[325,5355],[324,270tas__tasmax-rcp60-cc 8:keyClass []    geotrellis.spark.Spatas__tasmax-rcp60-cc 8:metadata []    {""cellType"":""float32```",4/30/15,1
lossyrob,"Looks like a toString is happening somewhere, wrongly. Key bounds and key index",4/30/15,1
notthatbreezy,"got it - I think we're going to pause on the benchmarking stuff -- @echeipesh is still out sick -- I applied the hdfs/accumulo  thing you pointed me to last night, but it's still pretty slow",4/30/15,1
notthatbreezy,"I have a feeling there's probably something we're missing from another branch or something that it makes sense to wait to pair on this once he's feeling better, this way we can save these points for actually doing the benchmarks",4/30/15,1
lossyrob,"Ok, that makes sense.",4/30/15,1
pomadchin,"@lossyrob ty for help about my fault with overlapping areas (if you remember): there was fault with a wrong no value data; tiff band was int16 not int32, so nodata value was  -32768; so there is no bug in a geotrellis ingest. ",4/30/15,1
pomadchin,"hm, got an idea o: is it possible to ingest tiles from hdfs (any fs mounted to hadoop)?",4/30/15,1
lossyrob,Ohh I see,4/30/15,1
lossyrob,"Yes, you can ingest from hdfs or s3",4/30/15,1
pomadchin,"hm, just to make `_ÑÓinput hdfs://` ?",4/30/15,1
lossyrob,Yup!,4/30/15,1
pomadchin,WOW,4/30/15,1
pomadchin,cool,4/30/15,1
pomadchin,:p yee ,4/30/15,1
lossyrob,:),4/30/15,1
ZachSand,"@lossyrob I've gotten the color ramp drop down for gt-admin to change the colors of the layer based upon selection, but it feels a bit too ""hacky"" to me. I'm appending a string onto the layer URL generated in LeafletMap.jsx  that CatalogService.scala is using as optional parameters for rendering. Any ideas for a better/different solution? I pushed some rough code with the color ramp feature to this [branch](https://github.com/ZachSand/gt-admin/tree/colorRamp) ",4/30/15,1
lossyrob,"@ZachSand that's great! Is be happy to take a look. I'll have to do it tonight since I don't have wifi and I'm at a conference. If anyone else is into some JavaScript code review, that would help :)",4/30/15,1
ZachSand,"Sounds good. I haven't done a lot of JS (or front end for that matter) development so I'm sure there are a good amount of things I could be doing better. Anyone feel free to enlighten me! p.s. emphasis on the ""rough code"" in the branch, I was mainly focused on getting it to work so there are still a lot of console logging, poor naming, etc. that hasn't been cleaned up. ",4/30/15,1
DarkManiel,"@lossyrob are there any tiles with spatial keys that match nexmonth_gtadmin layer? I keep getting errors that they don't exist for everywhere on this layer. Unfortunately this is the only layer I have to test on, but just want to make sure I'm not wasting my time on this layer.",5/1/15,1
DarkManiel,Or... is there a way I can check out the tiles so I can test the functionality of the rest of my code with known data?,5/1/15,1
DarkManiel,"Nevermind, I found a workaround",5/1/15,1
DarkManiel," ```javascript render: function() {  var css = {    'width' : '100%'};    var gridTh = _.map([1,2,3,4], function(val){      return <th>{val}</th>;    });    var gridTr = _.map([1,2,3,4], function(){      return <tr>{ gridTh }</tr>    });    return (          <Table striped bordered condensed hover style = {css}>    <tbody>      { gridTr }    </tbody>  </Table>          );  }  ```",5/4/15,1
DarkManiel,"@lossyrob I'm writing a function to display a variable size table for the values, but I'm having difficulty figuring out how to take my array of values with known rowlen and colLen and substituting those values for the dummy values above",5/4/15,1
DarkManiel,"This code gives me a 4X4 grid, but do you have any suggestions for how I can convert the array I have into the grid using react?",5/4/15,1
lossyrob,"so you need to figure out how render can take an argument of your array of values (which is an array of arrays, with the second array being rows, yeah?)",5/4/15,1
lossyrob,do you know how react would give yoru value array to that function?,5/4/15,1
lossyrob,tbh not very familiar with react so not sure which part is the problem area,5/4/15,1
DarkManiel,"well, values is really just an array (from val values = mutable.ListBuffer[String]()), but I think I need to just display it as an array of arrays based on the number of rows and columns, or at least that's what I was planning on doing",5/4/15,1
lossyrob,ok. so the question is how in react do you convert your array to that grid. I_Ñéd have to look at the react code to try to figure that out. do you have it up on a branch?,5/4/15,1
DarkManiel,"No, but I can push it",5/4/15,1
DarkManiel,https://github.com/DarkManiel/gt-admin/blob/modal_grid/viewer/src/scripts/components/ValueGrid.jsx,5/4/15,1
lossyrob,cool looking now.,5/4/15,1
lossyrob,"ps, should have forked from geotrellis/gt-admin, not from my fork of it",5/4/15,1
DarkManiel,"Basically, I have an array of values under this.props.values and want to stick it into elemenets of columns (gridTh)  which are then rendered into rows (gridTr)",5/4/15,1
DarkManiel,"oh okay, thanks",5/4/15,1
DarkManiel,"the code is still a bit messy, so let me know if you have any questions ",5/4/15,1
lossyrob,oh ok,5/4/15,1
lossyrob,so you have access to the values,5/4/15,1
DarkManiel,yeah,5/4/15,1
lossyrob,and you just need to figure out how to turn it into the array of arrays,5/4/15,1
DarkManiel,exactly,5/4/15,1
lossyrob,"oh, ok. fun",5/4/15,1
lossyrob,do you know the number of cols and rows,5/4/15,1
lossyrob,or even jsut number of cols,5/4/15,1
DarkManiel,"but it's a little tricky with react. Yeah... I mean I have to add that in on the scala side and carry it through, but I know how to do it",5/4/15,1
lossyrob,"is the values the same size always from teh service, or when there_Ñés like cropping (i.e. click on a border pixel) it_Ñéll have less values",5/4/15,1
lossyrob,you could just return it as an array of arrays in scala,5/4/15,1
lossyrob,"so you hit that endpoint and get { _ÑÉvalues_Ñé : [ row1, row2 ] }",5/4/15,1
lossyrob,etc,5/4/15,1
lossyrob,but you can totally do it in js,5/4/15,1
DarkManiel,"Yeah, I think the size varies",5/4/15,1
lossyrob,"so just to be clear. are we talking about, how we actually get the values when the pixel is clicked? or just, starting from we have the values, being able to use them in the render func",5/4/15,1
DarkManiel,just being able to use them in the render function,5/4/15,1
lossyrob,ok,5/4/15,1
lossyrob,"so we can boil the problem down to, given an array of values, if that array of values describes the values of rows that are `rows` across, how do we transform that into an array of arrays, which represent the row values",5/4/15,1
DarkManiel,I figured out how to get the values.... I rigged it to always give the same values right now though.... just for testing,5/4/15,1
lossyrob,"so to write it in scala, just because writing typed methods can be helpful```def valuesToRows(vales: Array[Double], rows: Int): Array[Array[Double]]```",5/4/15,1
lossyrob,is what you need,5/4/15,1
lossyrob,right?,5/4/15,1
DarkManiel,"Yeah I think that looks right. so then in the react code above, how would I specify the .map functions using the array of arrays? what would I use as the parameters of the two .map functions?",5/4/15,1
lossyrob,"ok, let_Ñés inspect that",5/4/15,1
lossyrob,"say our values were [1, 2, 3, 4], and our row size was 2",5/4/15,1
lossyrob,"the way you have it, `gridTr` would produce 4 rows",5/4/15,1
lossyrob,if you were to pass the values into that function,5/4/15,1
DarkManiel,okay,5/4/15,1
lossyrob,which doesn_Ñét seem right.,5/4/15,1
lossyrob,"Also, because the `gridTh` is a static var, it would return the same thing for each row",5/4/15,1
lossyrob,"but we want each of our `gridTr`_Ñés to be different, namely the values of each row",5/4/15,1
lossyrob,"so for our example, `[1, 2]` and `[3, 4]`",5/4/15,1
DarkManiel,yeah,5/4/15,1
lossyrob,ok. so what change should be made to gridTr to account for that?,5/4/15,1
DarkManiel,so I don't think the way I set up gridTr and gridTh are correct,5/4/15,1
lossyrob,they_Ñére close for sure,5/4/15,1
DarkManiel,"so, should gridTr be a function that iterates through the array of arrays?",5/4/15,1
lossyrob,it could be,5/4/15,1
lossyrob,"or, you could change `gridTh` into a function",5/4/15,1
lossyrob,that took as an argument the array that represents the row values,5/4/15,1
lossyrob,and then that would be the argument for the map you wrote,5/4/15,1
lossyrob,"so `gridTr` would be something that iterates over the row Array, and feeds those elements (which are arrays) into the `gridTh` function, and handles putting the outputs together between <tr/>s",5/4/15,1
DarkManiel,"Okay, I think I see what you're saying. I'll try to play around with that and see what I can get",5/4/15,1
lossyrob,ok cool,5/4/15,1
lossyrob,here_Ñés a hacked together JS Fiddle you can play around with to scope the example,5/4/15,1
lossyrob,http://jsfiddle.net/nb776mzs/,5/4/15,1
lossyrob,sorry,5/4/15,1
lossyrob,http://jsfiddle.net/nb776mzs/1/,5/4/15,1
lossyrob,"so if you could get the output of that to say 1 - 16 in the right format in the table, you win",5/4/15,1
DarkManiel,"Thanks, by the way, the colNum from scala is a bit strange. I get an array of size 225, but cols = -4095 and rows = 5317. I'm not sure I understand this correctly",5/4/15,1
DarkManiel,are the actual rownum and colnum equal? if so I could always just take the squareroot,5/4/15,1
lossyrob,what do you mean colNum?,5/4/15,1
DarkManiel,number of rows and columngs,5/4/15,1
DarkManiel,columns*,5/4/15,1
lossyrob,not sure why you_Ñére getting negative values,5/4/15,1
lossyrob,for cols,5/4/15,1
lossyrob,"but you should return the number of columns in your value grid. so usually that would be like 16, if it_Ñés a 16 x 16 grid.",5/4/15,1
lossyrob,which is probably always what the endpoint should do. And just return something like Double.NaN for the rest of the values if they fall off the raster,5/4/15,1
lossyrob,like for border pixels,5/4/15,1
DarkManiel,okay... I'll look into that too then,5/4/15,1
lossyrob,@echeipesh have you had a chance to look at the accumulo performance stuff with @notthatbreezy and @moradology? Curious about where that_Ñés at.,5/4/15,1
notthatbreezy,we're testing it out now -- @moradology put together a branch that seems like it will work,5/4/15,1
moradology,"on friday, i talked to @echeipesh and got some guidance about slow accumulo ingest",5/4/15,1
echeipesh,"Yeah, that was performance for an unoptimized batchwriter.  @moradology  PR1076 adds the HDFS ingest that should be much better.",5/4/15,1
moradology,the branch with 'strategies' is where that sits - also uncovered a bug related to keybounds,5/4/15,1
echeipesh,HDFS ingest is what I was benchmarking with to be clear.,5/4/15,1
moradology,"which was just luck to find and, i think, @echeipesh was already on the trail of",5/4/15,1
moradology,"cool, cool",5/4/15,1
lossyrob,ok nice.,5/4/15,1
lossyrob,so the slow ingest that chris was doing was the BatchAccumuloInputFormat ingest? not the write-to-HDFS-and-then-hoover-using-Accumulo-API strategy?,5/4/15,1
echeipesh,correct,5/4/15,1
lossyrob,cool. thanks guys.,5/4/15,1
notthatbreezy,"good news - got an ingest done in about 10 minutes, @echeipesh has a fix in the works for the metadata/layer error in accumulo too so after that's merged in we're looking pretty good",5/4/15,1
lossyrob,woot,5/4/15,1
lossyrob,"10 minutes, what sort of cluster size?",5/4/15,1
notthatbreezy,"10 r3.4xlarges -- 160 cores, 1100 GB of memory",5/4/15,1
lossyrob,"woof, that_Ñés a big cluster",5/4/15,1
notthatbreezy,"yeah, wanted it to fail fast if it was going to fail -- still using spot instances. We're trying out eugene's most recent PR so if we can ingest multiple datasets then we should be good to go",5/4/15,1
lossyrob,did you get to see the utilization stats for that 10 min ingest?,5/4/15,1
lossyrob,and that processing a layer that_Ñés like 70 GB right?,5/4/15,1
notthatbreezy,"yeah, looked pretty good -- I think it wsa about 90 GB (at least that's what was stored in memory once by spark)",5/4/15,1
notthatbreezy,I guess it was more like 17 minutes (looking at how long it took accumulo to ingest the HDFS files),5/4/15,1
notthatbreezy,[![accumulo-ingest-good.png](https://files.gitter.im/geotrellis/geotrellis/fYfO/thumb/accumulo-ingest-good.png)](https://files.gitter.im/geotrellis/geotrellis/fYfO/accumulo-ingest-good.png),5/4/15,1
lossyrob,"ok, so it took 10 minutes to get it into HDFS, 7 min to ingest up into accumulo/",5/4/15,1
lossyrob,?,5/4/15,1
lossyrob,hm that cpu usages is a bit weird yeah?,5/4/15,1
notthatbreezy,"yeah towards the end -- that's accumulo, the spark job was over by then",5/4/15,1
notthatbreezy,I decreased the s3 page size to make sure the number of partitions was at least as big as the cluster,5/4/15,1
lossyrob,so the ingest starts at like 15:24:30?,5/4/15,1
lossyrob,wonder what_Ñés happening at 15:31,5/4/15,1
echeipesh,"looks like a shuffle stage, previous stage dumps to disk and start of next stage sucks up the shuffle files and starts to churn them.",5/4/15,1
notthatbreezy,[![spark.png](https://files.gitter.im/geotrellis/geotrellis/E5nH/thumb/spark.png)](https://files.gitter.im/geotrellis/geotrellis/E5nH/spark.png),5/4/15,1
notthatbreezy,this is from the one I just ran,5/4/15,1
notthatbreezy,so about 13 minutes in spark,5/4/15,1
lossyrob,and about half of that is saving to hdfs. or perhaps that includes some additional processing that doesn_Ñét happen until that action?,5/4/15,1
echeipesh,that looks like +1 on that PR ?,5/4/15,1
notthatbreezy,"I think by that point it's just dumping it -- the RDD has already been materialized and is in cache (hence all the skipped stages), let me see if I can find some disk IO metrics that we can match up too",5/4/15,1
lossyrob,What is going on with travis. ugh.,5/4/15,1
echeipesh,Labor strike looks like.,5/4/15,1
lossyrob,@echeipesh 1076?,5/4/15,1
echeipesh,"1078, but yeah, that one too",5/4/15,1
echeipesh,Chris said he was running code from 1078 on that last ingest,5/4/15,1
notthatbreezy,"yeah, let me do a second ingest here",5/4/15,1
notthatbreezy,the second one was what was failing before,5/4/15,1
notthatbreezy,just started it,5/4/15,1
lossyrob,what_Ñés that caching server that starts with a v?,5/4/15,1
lossyrob,nvm varnish,5/4/15,1
notthatbreezy,yeah,5/4/15,1
echeipesh,merging 1078 since it worked for Chris on the ingest.,5/4/15,1
lossyrob,"Ok, I wanted to reveiw the metadata catalog removal",5/4/15,1
lossyrob,"but I can just do that after the fact, so go for it",5/4/15,1
echeipesh,"Sigh, sorry for the race condition. As a note the attribute does not save the histogram like the metadatacatalog did, it_Ñés on my list of issues to address with further cleanup.",5/4/15,1
lossyrob,will that break the gt-admin?,5/4/15,1
echeipesh,definitely,5/4/15,1
echeipesh,Want me to take a look at gt-admin ?,5/4/15,1
lossyrob,yeah. the FB students need their PR_Ñés to be merged in by Friday. The gt-admin is pointing to the catalog-refactor branch. so if they update and publish local it_Ñéll mess with them,5/4/15,1
echeipesh,"ok, they_Ñéd be messed up either way since there is no longer metaDataCatalog value. I_Ñéll send a PR to gt-admin.",5/4/15,1
lossyrob,"right, ok. Yeah they_Ñéve been updating to that branch, and we haven_Ñét been making it easy on them ha. I guess that_Ñés what happens when you are working off a branch with the name `refactor` in it",5/4/15,1
echeipesh,@lossyrob master on gt-admin ?,5/4/15,1
pomadchin,"/offtop/ wow, 70Gb's ingesting ~10 minutes? with pyramid?",5/4/15,1
echeipesh,"I don_Ñét think they_Ñére pyramiding, just doing the base layer ingest.",5/4/15,1
pomadchin,"anyway, nice speed",5/4/15,1
zifeo,"Hi everyone ! I'm one of the students Rob will supervise under the GSoC program. The project I submitted is ""Combining climate data and elevation to predict flood risk"". To introduce myself: I'm currently studying computer science and communication systems at Ecole Polytechnique Federal de Lausanne (EPFL) in Switzerland. I seeked a Scala-enthusiastic position for summer and got interested in this prediction project.   ",5/5/15,1
lossyrob,hey Teo. nice to see you in the Gitter room!,5/5/15,1
allixender,"Hi @zifeo , ""I seeked a Scala-enthusiastic position for summer"" :-) /me too",5/6/15,1
notthatbreezy,"@lossyrob working on benchmarking -- does https://github.com/notthatbreezy/benchmark/commit/af8af5a3cce1d7328ab9861cc412ce7aefc85681 cover the part of ""Take difference, get average yearly."" in the benchmarking plans?",5/6/15,1
lossyrob,yup that seems right,5/6/15,1
lossyrob,__FYI__ I_Ñém going to be working on merging everything into `catalog-refactor` and then merging that into master today,5/6/15,1
notthatbreezy,woohoo!,5/6/15,1
echeipesh,@notthatbreezy are you guys running spark 1.3 on cluster ?,5/6/15,1
notthatbreezy,yep,5/6/15,1
notthatbreezy,that's what geotrellis-ec2-cluster uses,5/6/15,1
echeipesh,"damn, 1.6 cluster PR went stale, it_Ñés still on 1.2",5/6/15,1
notthatbreezy,cloudera released cdh5.4 and it broke all of our old roles -- it was using 1.2 until that happened,5/6/15,1
notthatbreezy,this one? https://github.com/geotrellis/geotrellis-ec2-cluster/pull/25,5/6/15,1
echeipesh,yep,5/6/15,1
echeipesh,Where does the updated repo provide the spark version? it_Ñés not in group_vars/all,5/6/15,1
notthatbreezy,it's using what is being used in the azavea spark role,5/6/15,1
echeipesh,"ah, it just defaults to role version",5/6/15,1
notthatbreezy,yeah,5/6/15,1
notthatbreezy,*off topic* does gitter get the ordering of messages wrong for anyone else? it's kind of annoying,5/6/15,1
echeipesh,I haven_Ñét noticed anything like that.,5/6/15,1
notthatbreezy,I can take a look if you're running into issues,5/6/15,1
hectcastro,I wonder what data store they use. :),5/6/15,1
echeipesh,Probably something horrible like Riak,5/6/15,1
hectcastro,8),5/6/15,1
hectcastro,That branch should be OK to rebase against develop,5/6/15,1
notthatbreezy,whatever it is it's _eventually_ consistent,5/6/15,1
notthatbreezy,but it throws me through a loop when I come back to this tab and everything is re-arranged in the correct order,5/6/15,1
echeipesh,"@notthatbreezy could you send me your geotrellis-cluster.config, I think the sample has some gaps",5/6/15,1
notthatbreezy,"sure, if something is unclear/not working log an issue too maybe so I can fix the docs so others don't run into the same problem",5/6/15,1
echeipesh,yep,5/6/15,1
notthatbreezy,"there was a bug we discovered yesterday that I have a PR to fix, but have to change something before I merge it",5/6/15,1
notthatbreezy,are you in the office?,5/6/15,1
echeipesh,"ah, no, I_Ñém in Lift for a little bit",5/6/15,1
notthatbreezy,ah ok,5/6/15,1
pomadchin,"guyz, a little /offtop/ : smb worked with modis hdf files? *I mean converting it to geotiff files, without using mrt (modis reprojection tool), using some java/ scala interface?",5/6/15,1
lossyrob,gdal would do it,5/6/15,1
pomadchin,it saves projection information correct?,5/6/15,1
lossyrob,yup!,5/6/15,1
pomadchin,o:,5/6/15,1
pomadchin,wow,5/6/15,1
lossyrob,there_Ñés some GeoTIFF specific options that you_Ñéll want to get used to,5/6/15,1
lossyrob,http://www.gdal.org/frmt_gtiff.html,5/6/15,1
pomadchin,O_O,5/6/15,1
lossyrob,things like `gdal_translate -co compress=deflate -tiled=yes input.hdf out.tif`,5/6/15,1
lossyrob,"gdal is the jam. if you need to do anythign with file conversions, it most probably can do what you need + 100 other things",5/6/15,1
lossyrob,"so for instance, we use GDAL to do a lot of preprocessing of rasters before we ingest it into GeoTrellis, which is specifically good for distributed raster process. but anything as far as, convert this HDF5 to GeoTiff, that_Ñés GDAL",5/6/15,1
lossyrob,"or even using something like `rasterio`, which is a sweet python lib on top of GDAL, to do stuff like, take this netCDF file and tile it into single band GeoTiffs_Ñ_and under the covers that_Ñés all using GDAL functionality",5/6/15,1
pomadchin,o: magic ,5/6/15,1
lossyrob,yeah. GDAL is the giant_Ñés shoulders which all geospatial raster open source code stands on. So thank Frank Warmerdam if you ever get the opportunity.,5/6/15,1
pomadchin,"thought that it would be nice to make all processing inside hdfs (before accumulo ingesting):1. load modis hdf to hdfs. 2. convert to tiff (make a dir insede hdfs) 3. run accumulo ingestit is not possible or it is  a sort of overhead?",5/6/15,1
pomadchin,"now we are making a similar thing: loading hdf files to a host machine, reprojecting via mrt and gdal tools, then ingesting => ....",5/6/15,1
lossyrob,"well, that_Ñés a limitation of GDAL. It_Ñéd be nice if we had an HDF reader, then we could read direct from HDFS, but we dont",5/6/15,1
lossyrob,so what we usually do is do the per-file processing of other formats into a GeoTIFF and push it up to HDFS then,5/6/15,1
lossyrob,or s3,5/6/15,1
lossyrob,"and we_Ñére building systems that actually do that in a scalable way when your data starts in s3, where you create a workflow where a set of 100s of nodes pull data down from s3 and do the GDAL processing, throw it back up on s3, ready to be ingested and used in GeoTrellis",5/6/15,1
lossyrob,"so for the HDFS route, if there_Ñés a similar chain where you could do some of the processing of the modis as you load it into HDFS, that would be good. otherwise, we do have a reader that uses GDAL bindings",5/6/15,1
lossyrob,you can find it in the geotrellis.gdal subproject,5/6/15,1
lossyrob,"it_Ñés a little bit thin, so I haven_Ñét/don_Ñét lean on it too hard",5/6/15,1
pomadchin,"yep, saw it",5/6/15,1
pomadchin,hm,5/6/15,1
pomadchin,thx a lot for responses,5/6/15,1
lossyrob,"but you could use that. the trick would be, because GDAL needs the files to be on the local file system, you_Ñéd have use an InputFormat that copies to local file system before processing",5/6/15,1
lossyrob,we do have one of those I believe,5/6/15,1
lossyrob,https://github.com/geotrellis/geotrellis/blob/master/spark/src/main/scala/geotrellis/spark/io/hadoop/formats/GdalInputFormat.scala,5/6/15,1
lossyrob,it requires that GDAL java bindings are installed on all worker machines,5/6/15,1
lossyrob,"but that could be another path. we found that actually doing a preprocessing step ended up being easier, when we were working with netCDF files (which are backed by HDF5)",5/6/15,1
pomadchin,got to try all these things,5/6/15,1
lossyrob,"definitely the GDAL wrapper could use some work, so if you run into things, let us know!",5/6/15,1
pomadchin,"yep, sure! I'll provide all information I get",5/6/15,1
pomadchin,geotrellis is an awesome thing o:,5/6/15,1
lossyrob,thanks!,5/6/15,1
pomadchin,": D no no, thank you and all team(!) I am only spaming here with questions ",5/6/15,1
lossyrob,"@kyeah trying to run Cassandra unit tests, getting _ÑÒfailed to start Cassandra_Ñù errors. is there some config I_Ñém missing?",5/7/15,1
kyeah,What's the full error? No hosts tried?,5/7/15,1
lossyrob,here_Ñés teh full output http://pastebin.com/GeAKmV2j,5/7/15,1
lossyrob,last 3 lines: ,5/7/15,1
lossyrob,"```20:15:27 StorageService: Node localhost/127.0.0.1 state jump to normal20:15:27 CompactionTask: Compacted 4 sstables to [/var/folders/sv/zr8j0t4j1f726nhlt3vb8c300000gn/T/1430957724679-0/cassandra-driver-spark/cassandra/data/system/local-7ad54392bcdd35a684174e047860b377/system-local-ka-5,].  5,921 bytes to 5,751 (~97% of original) in 37ms = 0.148232MB/s.  4 total partitions merged to 1.  Partition merge counts were {4:1, }[error] Uncaught exception when running geotrellis.spark.io.cassandra.CassandraAttributeStoreSpec: java.io.IOException: Failed to start Cassandra.```",5/7/15,1
kyeah,"hmm, weird. Looks like the CassandraConnector never makes a connection to the cluster?",5/7/15,1
kyeah,"I get ```19:41:24 Cluster: New Cassandra host /127.0.0.1:9042 added19:41:24 CassandraConnector: Connected to Cassandra cluster: Test Cluster```",5/7/15,1
kyeah,Which seems to be missing from your output,5/7/15,1
lossyrob,I_Ñém working with an updated branch.,5/7/15,1
lossyrob,can you try it off of my feature/cassandra branch?,5/7/15,1
lossyrob,I_Ñém doing the merge now,5/7/15,1
kyeah,will do,5/7/15,1
lossyrob,https://github.com/lossyrob/geotrellis.git,5/7/15,1
lossyrob,https://github.com/lossyrob/geotrellis/tree/feature/cassandra,5/7/15,1
kyeah,"I can still start cassandra and run the tests, although a bunch of them are failing now",5/7/15,1
lossyrob,hm. how are you running them?,5/7/15,1
kyeah,test-only,5/7/15,1
lossyrob,I_Ñém doing `~test-only *Cassandra*`,5/7/15,1
kyeah,"I ran them individually, will try that",5/7/15,1
lossyrob,yeah that seems to be it,5/7/15,1
kyeah,yeah still works,5/7/15,1
lossyrob,oh wait. nvm,5/7/15,1
lossyrob,"thought it was working, still isnt",5/7/15,1
lossyrob,does the cassandra tests interact wit hthe local cassandra instance?,5/7/15,1
lossyrob,"like, what_Ñés the diff between my and your environment",5/7/15,1
lossyrob,"i thought it was self contained in tests, no?",5/7/15,1
kyeah,"It *should* set up its own local/embedded instance, but maybe I'm misunderstanding Datastax's embedded cassandra class/trait?",5/7/15,1
lossyrob,ok,5/7/15,1
lossyrob,"so I just ran `cassandra`, and got some output",5/7/15,1
lossyrob,"then ran the test, and it worked (well, failed, but actually ran)",5/7/15,1
kyeah,"okay, so it needs the cassandra service to be running then",5/7/15,1
lossyrob,"hmmm. ok, that_Ñés not ideal",5/7/15,1
kyeah,yeah there didn't seem to be any library to mock out a cassandra instance,5/7/15,1
lossyrob,ok gotcha. we_Ñéll work with it,5/7/15,1
lossyrob,perhaps that_Ñés something that @allixender can start looking into,5/7/15,1
lossyrob,@kyeah can you think of a good way to test for Cassanrda on the system? I don_Ñét want to run the tests if the user doesn_Ñét have cassandra,5/7/15,1
kyeah,"I'm guessing attempting a connection and catching the ""Failed to Start Cassandra"" error is non-ideal?",5/7/15,1
lossyrob,I_Ñém confused about how `EmbeddedCassandra` works with the `SharedEmbeddedCassanrda` object,5/7/15,1
kyeah,The SharedEmbeddedCassandra object does all the work with setting up the test cluster,5/7/15,1
lossyrob,I_Ñém looking at the datastax EmbeddedCassandra and don_Ñét see it doing a lot,5/7/15,1
kyeah,EmbeddedCassandra is only there to recreate the keyspace before every spec is run,5/7/15,1
lossyrob,https://github.com/datastax/spark-cassandra-connector/blob/master/spark-cassandra-connector-embedded/src/main/scala/com/datastax/spark/connector/embedded/EmbeddedCassandra.scala,5/7/15,1
lossyrob,looks like DataStax enterprise has an in-memory version. :angry: ,5/7/15,1
lossyrob,"@kyeah if you do `test:run` to generate new test files, do the unit tests pass for you?",5/7/15,1
lossyrob,I_Ñém getting `java.io.IOException: Failed to open native connection to Cassandra at {192.168.96.102}:9042` for each of the `CassandraRasterCatalogSpec` tests,5/7/15,1
lossyrob,but the `CassandraAttributeStoreSpec` tests pass,5/7/15,1
kyeah,I'm getting RuntimeExceptions instead of the expected LayerNotFound errors,5/7/15,1
kyeah,but yeah otherwise they pass,5/7/15,1
kyeah,argh,5/7/15,1
lossyrob,hmm,5/7/15,1
lossyrob,ok. well I_Ñém going to put it away for a bit. I think we should figure out the Cassanrda test situation before merging it into master,5/7/15,1
kyeah,"yeah, definitely",5/7/15,1
lossyrob,slash get through some of the env config errors.,5/7/15,1
lossyrob,"figured out why the merge into `catalog-refactor` has been such a crazy pain (for those of you who accidently tried to update a catalog-refactor branch to master, you_Ñéll know the pain) - I had accidentally merged the catalog-refactor changes into master at some point, and then reverted the changes through github. That has really affected the merge, where the merge sometimes takes changes of the revert over the changes of the commit, since it was a later commit. ",5/7/15,1
lossyrob,"so, know not to ever do that again.",5/7/15,1
lossyrob,and now I know that `git merge -s ours upstream/master` would have saved me many an hour if I thought of it back then,5/7/15,1
hectcastro,I merged the Accumulo branch on the cluster project.,5/7/15,1
lossyrob,woot,5/7/15,1
lossyrob,"also in case anyone didn_Ñét catch it from my late night journaling, I merged `catalog-refactor` into `master`",5/7/15,1
lossyrob,"if you_Ñéve been using `master`, and update, the changes will be breaking since there are API updates. Please let us know if you run into troubles and we_Ñéll work them out together",5/7/15,1
pomadchin,so master branch compiles?,5/7/15,1
lossyrob,I hope so! you can always rely on that being the case,5/7/15,1
lossyrob,sometimes the Travis build says it fails. but also sometimes Travis is a liar :),5/7/15,1
lossyrob,"but nothing goes into master that breaks the build, that_Ñés the rule anyway",5/7/15,1
lossyrob,we set the bar pretty high: _ÑÒmaster should compile_Ñù :shipit: ,5/7/15,1
alimavrakis,"Working in gt-admin in ubuntu, using vagrant to run the server, would I need to port forward 2181 to talk with zookeeper?",5/7/15,1
alimavrakis,I'm running into issues where vagrant is starting zookeeper but ingest will say zookeeper isn't running. Not positive as to whether than interaction is direct or indirect.,5/7/15,1
lossyrob,have you stared zookeeper with `zkServer start`?,5/7/15,1
alimavrakis,Inside the vbox? ,5/7/15,1
alimavrakis,I believe the provisioner starts it,5/7/15,1
alimavrakis,I may be wrong ,5/7/15,1
lossyrob,it should. but perhaps it fell off for some reason? something to try,5/7/15,1
alimavrakis,okay cool i'll look at it,5/7/15,1
alimavrakis,I'm having nfs issues so that may also be part of it...,5/7/15,1
pomadchin,"hm, i see accumulo version upgrade task #1088 o: all versions would be (tried to be) supported? what version would be primary?",5/7/15,1
lossyrob,"we_Ñére moving support to 1.6, so we would require that users use 1.6 (which is the most up to date version)",5/7/15,1
alimavrakis,Turns out it was running,5/7/15,1
alimavrakis,the ports needed to be forwarded,5/7/15,1
alimavrakis,I updated my vagrant file and it's connecting now,5/7/15,1
notthatbreezy,oh great -- did you figure out the NFS issues too?,5/7/15,1
alimavrakis,"I have it syncing now. I'm not even entirely sure if there was an issue though.  It wouldn't mount, but then after it did, it didn't actaully show up. So I did a bunch of debugging--turns out I hadn't noticed the mount dir had been moved in the Vagrant file from old versions of the repo",5/7/15,1
alimavrakis,no longer mounting in ~/Vagrant. Now it mounts in /Vagrant.,5/7/15,1
alimavrakis,"Now when I ingest, it can't find gis on zookeper. I'm in the middle of running build-all right now, but I don't know if that could be the problem-- if anyone has a clue I wouldn't mind :)",5/7/15,1
alimavrakis,fixed.,5/7/15,1
josephwinston,Two questions:,5/8/15,1
lossyrob,@josephwinston shoot!,5/8/15,1
josephwinston,"Well -- It seems something happened, Trying again:1. I wasn't able to find out if there are any concerns on using GeoTrellis with specific spark versions.  Is there a document that describes what will and will not work?  2. I data normally viewed as a raster that has more than 2 dimensions.  An example of this is seen in the GeoTrellis web site is the Weighted Overlay Demo.  I want to quickly bring up a proof of concept with my data.  Is the Web Service Demo would be the best place to start?",5/8/15,1
lossyrob,Currently the master branch works with Spark 1.2.0. I also think we_Ñéve been working with Spark 1.3.0 as well,5/8/15,1
lossyrob,"I_Ñéve also run it with 1.1.1, however that was a bit ago. So I don_Ñét know exactly which work and which have API breaks, but I haven_Ñét hit up against any yet from 1.1.1 on",5/8/15,1
lossyrob,"As for 2, how do you mean by having more than 2 dimensions? Do you mean having different raster layers over the same area?",5/8/15,1
lossyrob,A good place to start might be our chattanooga demo: https://github.com/geotrellis/geotrellis-chatta-demo,5/8/15,1
lossyrob,you can see a running example of that here: http://demo.geotrellis.com/chatta/,5/8/15,1
josephwinston,"@lossyrob  -- Thanks for the details.  On my development system, I'm using trunk so that I can have access to some of the 1.4 features as soon as possible bit I certainly can use 1.3.x to see how well this works with my data.The extra dimension in this specific case is depth.  So, raster slices can be made ""parallel"" a normal GIS map.",5/8/15,1
lossyrob,"I would be curious to see how it worked with 1.4, I don_Ñét think anything *should* break, but it would take someone trying to figure it out",5/8/15,1
lossyrob,"and also, the spark work exists in the not-yet-released 0.10, so would have to pull from master.",5/8/15,1
lossyrob,"The chatta demo works with GeoTrellis 0.9, which doesn_Ñét use Spark, but uses an Akka actor system that parallelizes operation execution over tiles.",5/8/15,1
lossyrob,"We_Ñére currently pushing forward on the 0.10 release, and hope to have it out this month.",5/8/15,1
josephwinston,@lossyrob -- I don't have a problem using an older version of GeoTrellis if you feel that it would give me an idea of the performance of the system.  I need display a small amount of geo-referenced rasters from large data sets.,5/8/15,1
lossyrob,how large?,5/8/15,1
josephwinston,"For proof of concept, less than 30 gigabytes.",5/8/15,1
lossyrob,That about gets to the limit of where I_Ñéd say you should be using the 0.10 stuff and Spark.,5/8/15,1
lossyrob,Unfortunately it_Ñés still being built so documentation is rare. Though we_Ñére very willing to talk through how to set up examples,5/8/15,1
josephwinston,Ok.  I'll start on GeoTrellis head then.,5/8/15,1
lossyrob,what are you trying to run this on? Do you have any cluster capabilities? Something set up with HDFS?,5/8/15,1
lossyrob,"Accumulo is what we use to get the most speed out of computations, because it acts as an index on top of HDFS that we can control how to store the tiles.",5/8/15,1
josephwinston,I can run on a cluster or a single machine with enough physical RAM to hold the entire 30 GB data set.,5/8/15,1
lossyrob,But we also can read directly from S3 if you_Ñére working on EC2 and don_Ñét mind paying the cost of pulling data off S3,5/8/15,1
josephwinston,I'll be happy to use Accumulo too,5/8/15,1
lossyrob,"Ok, great. A good proof of concept could be devised with a hefty machine running HDFS/Accumulo/Spark locally",5/8/15,1
lossyrob,"First step would be to setup HDFS, ZooKeeper and Accumulo",5/8/15,1
lossyrob,next would be to ingest your data,5/8/15,1
lossyrob,what format is it in? GeoTIFF would be easiest,5/8/15,1
josephwinston,"I've done everything there before except Accumulo and the only real work is converting the data into GeoTiff, which I believe I can do in Python using GDAL.",5/8/15,1
lossyrob,surely. What_Ñés the resolution?,5/8/15,1
lossyrob,per raster. is it tiled already or stored in pretty large files?,5/8/15,1
josephwinston,I've got Python code to read the large file and spit out numpy arrays.,5/8/15,1
lossyrob,"great. if you could take 512x512 or 1024x1024  chunks and spit out GeoTiffs into a directory, that would be prime for ingest",5/8/15,1
lossyrob,you could put those into local filesystem or HDFS if you_Ñéd like,5/8/15,1
lossyrob,"you_Ñéd want to separate them out into directories per layer that you_Ñéll want to ingest. Each layer is a single-band raster layer. The ingest will ingest it at the TMS zoom level that most closely matches the native resolution, and if you want (per an option) it will pyramid up the zoom levels",5/8/15,1
josephwinston,The resolution can easily be interpolated to be whatever is needed. It is pretty coarse in the x-y plane.,5/8/15,1
lossyrob,"there_Ñés some logic in the ingest to get close, and resample based on the tiling scheme",5/8/15,1
josephwinston,Understand about the zoom.  That's why I want to try it out to see what the performance looks like.,5/8/15,1
lossyrob,"once you ingest, you_Ñéll have a set of tiles per layer, with each tile being 512x512 or 256x256 for the zoom level, and indexed according to a Space Filling Curve. From then on it_Ñés pretty easy to work with the layer.",5/8/15,1
lossyrob,what sort of output are you looking for?,5/8/15,1
josephwinston,Any magic behind building GeoTrillis head that I need to know?,5/8/15,1
lossyrob,just have `SPARK_HOME` defined in the env,5/8/15,1
lossyrob,"other then that, `./sbt ""project spark_Ñù assembly` should give you a fat jar of all the spark work",5/8/15,1
lossyrob,that jar is what you_Ñéll call the ingest on through `spark-submit`,5/8/15,1
josephwinston,Good to know.  What is the right way to use HDFS/Accumolo stack?  Is it building the table (image) in Accumulo?  I don't see how that would be correct since the result isn't a GeoTiff.,5/8/15,1
lossyrob,sorry not sure what you mean_Ñ_building which table?,5/8/15,1
josephwinston,"I've never used Accmulo so I don't know what I need to do.  Hopefully, the task is nothing more than placing the GeoTiffs in the right HDFS directory.",5/8/15,1
lossyrob,"So, you_Ñéd place the GeoTiffs into HDFS",5/8/15,1
josephwinston,Much easier.  I like that ;-),5/8/15,1
lossyrob,then you_Ñéd use `spark-submit` to run an `AccumuloIngestCommand` with some parameters (a script to do so can be found here https://gist.github.com/lossyrob/fda457994a4be6db598c),5/8/15,1
lossyrob,"That runs the ingest process, which takes the GeoTiff, does the appropriate reprojecting, tiling, indexing, and saving of the data into accumulo",5/8/15,1
lossyrob,"Accumulo stores the indexed tiles in an Accumulo table, along with metadata",5/8/15,1
josephwinston,Ok.  I thought that I needed to make the indexed tiles directly.  Your approach is so much better than mine.,5/8/15,1
lossyrob,"then in code, to start working with that layer as a `RasterRDD`, you_Ñéll use an `AccumuloRasterCatalog` to retreive that layer by it_Ñés LayerId",5/8/15,1
lossyrob,nope the GeoTrellis code takes care of the (pretty complicated) task of indexing the tiles,5/8/15,1
josephwinston,Any rules of thumb on the number of spark workers?,5/8/15,1
lossyrob,"As many as cores that you have is a rule of thumb, but if you find that you_Ñére IO bound and not pegging CPUs while processing, you can push that number up",5/8/15,1
josephwinston,@lossyrob -- Thank you very much for your help.  It is time for me to build code and see what I can do before I ask another question.,5/8/15,1
lossyrob,"great! no problem, looking forward to hearing the next batch :)",5/8/15,1
pomadchin,"@lossyrob again, about ingesting tiles without pyramid: just ingested tiles, without pyramid, nothing got here `catalog.metaDataCatalog.fetchAll`mb I understand it (ingesting without pyramid) incorrect?",5/8/15,1
lossyrob,"@pomadchin there was bug in an older version of the codebase that made it so when you didn_Ñét pyramid it didn_Ñét actually save. That bug is fixed. If you update to master, that should be solved. Also the API has changed, so you_Ñéll have to update your code to the new catalog API",5/8/15,1
echeipesh,Merging in Accumulo 1.6 change. The clustter repo already supports it. This is the official switch to 1.6.2 as the supported version.,5/8/15,1
lossyrob,"cool, thanks for the heads up",5/8/15,1
pomadchin,"yep, updating now; api changed a lot ._.' Took me time to fix my code",5/8/15,1
lossyrob,@echeipesh does this mean I_Ñéll have to upgrade my Accumulo? Do we have a path for that?,5/8/15,1
echeipesh,Oh your machine ?,5/8/15,1
lossyrob,yeah,5/8/15,1
echeipesh,"Yeah, it_Ñés actually super simple. I was able to download the 1.6 tarbal, copy config from 1.5 and launch without issues.",5/8/15,1
pomadchin,"accumulo 1.6.2 would be in master soon, or waiting for next pack of updates?",5/8/15,1
echeipesh,You just need to make sure you change your $ACCUMULO_HOME and you_Ñére gtg.,5/8/15,1
lossyrob,ok. so nothing to migrate the data,5/8/15,1
lossyrob,?,5/8/15,1
echeipesh,Nope,5/8/15,1
lossyrob,cool,5/8/15,1
pomadchin,:0,5/8/15,1
lossyrob,"@pomadchin Accumulo 1.6 was just merged into master, so if you update to master you_Ñéll have to use 1.6",5/8/15,1
echeipesh,I don_Ñét see anything about changes to the record structure in the release notes either.,5/8/15,1
pomadchin,"Yep, cool",5/8/15,1
echeipesh,geotrellis-ec2-cluster was upgraded yesterday with accumulo 1.6.2 support. So it should be a smooth transition.,5/8/15,1
lossyrob,we need to get `vagrant.geotrellis` up to date,5/8/15,1
echeipesh,What was the reasoning to keep that sepratate from the vagrant stuff on `geotrellic-ec2-cluster`?,5/8/15,1
lossyrob,nice. Only down to 2 PR_Ñés open. that_Ñés a breath of fresh air,5/8/15,1
lossyrob,for having a single VM development environment,5/8/15,1
echeipesh,"ah, right",5/8/15,1
lossyrob,"if we_Ñére using `geotrellis-ec2-cluster` for a dev environment just to run geotrellis, then that name might be too ec2 centric",5/8/15,1
echeipesh,We really need to make a geotrellis-spark demo project that has small enough dataset to work on that machine.,5/8/15,1
lossyrob,but I_Ñéve always had problems with running the multiple VMs,5/8/15,1
lossyrob,yeah. that would be a good thing to have for the release,5/8/15,1
lossyrob,publishing local is being real tricky with master for me and a facebook open academy student. anyone else have that issue?,5/8/15,1
echeipesh,@moradology had issues yestorday. He ended up having to blow away target directory in geotrellis project and .ivy2/local to fix it.,5/8/15,1
lossyrob,fun w/ the simple build tool,5/8/15,1
echeipesh,I thought it stood for Solid Build Tool ?,5/8/15,1
lossyrob,it_Ñés actually staccato build tool,5/8/15,1
echeipesh,"ah, that makes sense why they have the flamenco dancer in their logo",5/8/15,1
echeipesh,[![Stacatto SBT.png](https://files.gitter.im/geotrellis/geotrellis/dHUe/thumb/Stacatto-SBT.png)](https://files.gitter.im/geotrellis/geotrellis/dHUe/Stacatto-SBT.png),5/8/15,1
lossyrob,@echeipesh what did `val zooms = catalog.metaDataCatalog.zoomLevelsFor(layer)` turn into again?,5/8/15,1
echeipesh,That didn_Ñét turn into anything. You could make a function that gives you from `readAll[LayerMetaData]`. Do you think that belongs in the attributeStore ?,5/8/15,1
lossyrob,not sure,5/8/15,1
lossyrob,we should probably have some functionality on the RasterCatalog to deal with metadata,5/8/15,1
lossyrob,I_Ñém writing stuff in the gt-admin that doesn_Ñét feel like client code,5/8/15,1
echeipesh,Ok. I was wondering if we were going to be able to avoid that.,5/8/15,1
lossyrob,having this in client code feels weird,5/8/15,1
lossyrob,"```  val metaDatas = attributeStore.readAll[AccumuloLayerMetaData](""metadata_Ñù)  def zoomLevelsFor(layer: String) =    metaDatas.keys.filter(_._1.name == layerName).map(_._1.zoom).toSeq```",5/8/15,1
ZachSand,@lossyrob /offtopic/ Sorry I couldn't make it to the meeting this morning. Let me know if there are any outstanding questions you have for me. I'm available to respond on here/Kato/email,5/8/15,1
lossyrob,ok thanks Zach,5/8/15,1
echeipesh,"yeah, `(_._1.name == layerName)` kills it",5/8/15,1
echeipesh,I had a thought a while ago that we could use zoom level 0 as a special place to keep attributes that apply across all layers.,5/8/15,1
lossyrob,"Another thing, `  def readTile[K: TileReader: JsonFormat: ClassTag](layerId: LayerId): Reader[K, Tile] =` in AccumuloRasterCatalog seems like a misnomer. returns a reader",5/8/15,1
echeipesh,"So if you wanted to have a pyramid wide histogram or a a list of layers you had, you could stosh them under there.",5/8/15,1
lossyrob,"`catalog.readTile[SpaceTimeKey](layerId).read(SpaceTimeKey(x, y, time))`",5/8/15,1
echeipesh,"yeah, it_Ñés a .tileReader, didn_Ñét catch that.",5/8/15,1
lossyrob,@echeipesh I_Ñém getting this error when trying to run with the 1.6 stuff,5/8/15,1
lossyrob,"```Exception in thread ""main"" org.apache.accumulo.core.client.AccumuloException: Invalid method name: 'beginFateOperation'	at org.apache.accumulo.core.client.impl.TableOperationsImpl.doFateOperation(TableOperationsImpl.java:338)```",5/8/15,1
lossyrob,I just upgraded to 1.6.2,5/8/15,1
lossyrob,hm perhaps my environment is out of date,5/8/15,1
echeipesh,"Have not seen that, make sure $ACCUMULO_HOME points to the right place, that_Ñés likely the problem.",5/8/15,1
lossyrob,seemsl ike htat was the problem,5/8/15,1
echeipesh,cool,5/8/15,1
ZachSand,@lossyrob Are there any tasks you need/want done for gt-admin? I was thinking about making a top navbar like the one in the admin project but wasn't sure if there were any more pertinent things to work on. ,5/8/15,1
lossyrob,"@ZachSand no.  I need to get your PR merged in today, and then that should wrap up the course",5/8/15,1
ZachSand,What about for contributing after the course is complete? ,5/8/15,1
lossyrob,"that_Ñés always, of course, welcome :)",5/8/15,1
lossyrob,"for now we_Ñéll be working on getting everything merged into gt-admin and see where it_Ñés at. I think after things get sured up to where they are now a little bit, we_Ñéll know where we want to take it next, what UI features we want to add, etc",5/8/15,1
lossyrob,I_Ñéll keep you in the loop via Gitter for sure,5/8/15,1
ZachSand,Sounds good. I'll make sure to watch for notifications -- I set it to notify me when my gitter handle is mentioned,5/8/15,1
lossyrob,"cool, thanks Zach",5/8/15,1
pomadchin,"guyz, thx a lot for accumulo 1.6.2",5/8/15,1
pomadchin,:),5/8/15,1
lossyrob,did the upgrade go well for you,5/8/15,1
lossyrob,?,5/8/15,1
pomadchin,"yep, everything seems to be ok; but new geotrellis api is not very obvious ",5/8/15,1
lossyrob,we_Ñéd appreciate feedback on the new API,5/8/15,1
lossyrob,"so if there_Ñés anything you find that_Ñés wonky, let us know!",5/8/15,1
lossyrob,"The idea is that a catalog provides readers and writers, and those are the objects taht actually do reading and writing of RasterRDD_Ñés and Tiles",5/8/15,1
lossyrob,"That architecture makes it easier to be consistent across the different backends we now support: Hadoop, Accumuo, S3, and soon Cassanrda",5/8/15,1
pomadchin,Hm I noticed some generic approach,5/8/15,1
pomadchin,"but there were different useful function to get useful metadata like layer zooming, etc",5/8/15,1
pomadchin,functions**,5/8/15,1
lossyrob,"yes, that_Ñés now missing. We have to work on that API",5/8/15,1
lossyrob,I ran into that as well,5/8/15,1
lossyrob,"some thoughts around that, we had a MetaDataCatalog concept and an AttributeStore concept",5/8/15,1
lossyrob,"and it seemed like MetaData was just a particular Attribute, so we merged it into AttributeStore",5/8/15,1
lossyrob,though there are some things missing now. We_Ñéll have to review that,5/8/15,1
pomadchin,"heh, noticed your code In a gt admin, very useful, but it was too late, Guessed how to refactor before ",5/8/15,1
pomadchin,"btw, seems to me there is some semantics issues, like `readTile`",5/8/15,1
lossyrob,"yup. I caught that, we_Ñére going to change that to `tileReader`",5/8/15,1
lossyrob,`catalog.tileReader[SpatialKey](layerId).read(key)`,5/8/15,1
lossyrob,Perhaps there_Ñéll be some delegation,5/8/15,1
lossyrob,"to make it simpler, but that is what is accurate with the architecture",5/8/15,1
pomadchin,"in conclusion it seems to me new api is better, but little harder (mb seems harder)anyway it was possible for me to refactor old code without sorts of _migration guides_, but not very quicknice work :)",5/8/15,1
lossyrob,"ok, thank you for being a first round test of new changes! that_Ñés invaluable",5/8/15,1
notthatbreezy,"re: `vagrant.geotrellis` - I don't think it will be much work to update, I can give it a shot now -- it just uses the `azavea.accumulo` role so should just be able to update the role version",5/8/15,1
pomadchin,"got a very strage ingesting with pyramid: ```5/05/09 20:53:33 INFO ClientCnxn: Session establishment complete on server master1.owm/172.17.2.175:2181, sessionid = 0x14d3a629ae20005, negotiated timeout = 3000015/05/09 20:54:31 INFO ClientCnxn: Client session timed out, have not heard from server in 22612ms for sessionid 0x14d3a629ae20005, closing socket connection and attempting reconnect15/05/09 20:54:55 INFO ClientCnxn: Opening socket connection to server master1.owm/172.17.2.175:2181. Will not attempt to authenticate using SASL (unknown error)15/05/09 20:54:55 INFO ClientCnxn: Socket connection established to master1.owm/172.17.2.175:2181, initiating session15/05/09 20:54:55 INFO ClientCnxn: Unable to reconnect to ZooKeeper service, session 0x14d3a629ae20005 has expired, closing socket connection```",5/9/15,1
pomadchin,"checking accumulo 1.6.2 conf; it is the only reason to find whats upand increasing zookeeper connections number",5/9/15,1
pomadchin,"btw, there were problems with loading tiles without pyramiding: gt-adming couldnot get catalogp.s. thing mb this thing connected with the error above",5/9/15,1
pomadchin,using hadoop 2.6.0 + spark 1.3.1 + accumulo 1.6.2,5/9/15,1
pomadchin,tested: full log: https://gist.github.com/pomadchin/e5178d477e3f93cef6db,5/9/15,1
pomadchin,"as you can see, timeouting; and this happens during 10 minutes; and happens after ~5 minutes of workseems to me need some accumulo settings or smth else; though using old geotrellis with accumulo 1.5.2 with the same settings finished job in ~9 minutes",5/9/15,1
pomadchin,mb there is a bug,5/9/15,1
pomadchin,every time this happens on `[time] INFO ContextCleaner: Cleaned broadcast 4`,5/9/15,1
pomadchin,"my job was ```scalaclass AccumuloIngestTimeArgs extends IngestArgs with AccumuloArgs {  @Required var table: String = _  var time: String = _}object AccumuloIngestTimeCommand extends ArgMain[AccumuloIngestTimeArgs] with Logging {  def main(args: AccumuloIngestTimeArgs): Unit = {    System.setProperty(""com.sun.media.jai.disableMediaLib"", ""true"")    implicit val sparkContext = SparkUtils.createSparkContext(""Ingest"")    val conf = sparkContext.hadoopConfiguration    conf.set(""io.map.index.interval"", ""1"")    implicit val tiler: Tiler[ProjectedExtent, SpaceTimeKey] = {      val getExtent = (inKey: ProjectedExtent) => inKey.extent      val createKey = (inKey: ProjectedExtent, spatialComponent: SpatialKey) =>        SpaceTimeKey(spatialComponent, DateTime.parse(args.time))      Tiler(getExtent, createKey)    }    val accumulo = AccumuloInstance(args.instance, args.zookeeper, args.user, new PasswordToken(args.password))    val source = sparkContext.hadoopGeoTiffRDD(args.inPath).repartition(args.partitions)    val layoutScheme = ZoomedLayoutScheme(256)    Ingest[ProjectedExtent, SpaceTimeKey](source, args.destCrs, layoutScheme, args.pyramid){ (rdd, level) =>      accumulo.catalog.save(LayerId(args.layerName, level.zoom), args.table, rdd, args.clobber)    }  }}```",5/9/15,1
pomadchin,"now it is ```scalaclass AccumuloIngestTimeArgs extends IngestArgs with AccumuloArgs {  @Required var table: String = _  var time: String = _}object AccumuloIngestTimeCommand extends ArgMain[AccumuloIngestTimeArgs] with Logging {  def main(args: AccumuloIngestTimeArgs): Unit = {    System.setProperty(""com.sun.media.jai.disableMediaLib"", ""true"")    implicit val sparkContext = SparkUtils.createSparkContext(""Ingest"")    val conf = sparkContext.hadoopConfiguration    conf.set(""io.map.index.interval"", ""1"")    implicit val tiler: Tiler[ProjectedExtent, SpaceTimeKey] = {      val getExtent = (inKey: ProjectedExtent) => inKey.extent      val createKey = (inKey: ProjectedExtent, spatialComponent: SpatialKey) =>        SpaceTimeKey(spatialComponent, DateTime.parse(args.time))      Tiler(getExtent, createKey)    }    implicit val accumulo = AccumuloInstance(args.instance, args.zookeeper, args.user, new PasswordToken(args.password))    val source = sparkContext.hadoopGeoTiffRDD(args.inPath)    val layoutScheme = ZoomedLayoutScheme()   // ZCurveKeyIndexMethod.byYear?    val writer = AccumuloRasterCatalog().writer[SpaceTimeKey](      ZCurveKeyIndexMethod.by { dt => s""${dt.getYear}${dt.getDayOfYear}"".toInt },  args.table    )    Ingest[ProjectedExtent, SpaceTimeKey](source, args.destCrs, layoutScheme, args.pyramid) { (rdd, level) =>      writer.write(LayerId(args.layerName, level.zoom), rdd)    }  }}```",5/9/15,1
pomadchin,"** after loading in a such way without pyramid option, there were problems with histo finding ): no such attribute error: `java.lang.RuntimeException: Attribute histogram not found for layer Layer(name = ""MOD13Q1"", zoom = 8)`",5/9/15,1
pomadchin,** it is gt-admin testing,5/9/15,1
pomadchin,"and got a lil bit strange err, on asking pixel route: `com.esotericsoftware.kryo.KryoException: Buffer underflow.`",5/9/15,1
pomadchin,"but, the most interesting bug is with pyramid ingesting",5/9/15,1
pomadchin,sry for night bugs ):,5/9/15,1
lossyrob,@pomadchin interesting. Not sure about the timeout bug.,5/11/15,1
lossyrob,gt-admin master should have a fix to the histogram issue,5/11/15,1
lossyrob,@pomadchin this is a local ingest correct?,5/11/15,1
pomadchin,"@lossyrob on a test cluster http://104.155.79.182:8080 (opened ports for all monitoring uis: hdfs, accumulo, spark) in a client mode",5/11/15,1
pomadchin,There was nothing changed in settings except of accumulo version,5/11/15,1
pomadchin,wonder if only I have this bug ):,5/11/15,1
pomadchin,"@lossyrob there would be more information tomorrow, possible variants: my hands, some bugs, planets position... ):",5/11/15,1
pomadchin,"  got to maintain two versions of geotrellis-based libs ): and ingesting via old into old accumulo, before the problem investigation",5/11/15,1
pomadchin,: D problems of not stable versions,5/11/15,1
lossyrob,@pomadchin how many nodes in the test cluster? How did you distribute the accumulo upgrade across the cluster?,5/11/15,1
pomadchin,"3 nodes, like this : 104.155.79.182:50095 but, 1.6.2 (now it is disabled); no upgrade, just new db, on its own hdfs with its own zookeepers ",5/11/15,1
lossyrob,and each worker is running accumulo 1.6 now?,5/11/15,1
pomadchin,"now there's 1.5.2 and old geotrellis, yep, but, when there was 1.6.2 it was everywhere; same settings as 1.5.2 ",5/11/15,1
lossyrob,"are you using our deployment scripts? `geotrellis-ec2-cluster`, or are you doing installs manually?",5/11/15,1
pomadchin,The results of jobs here 104.155.79.182:8080 ; have an idea that it is really my Accumulo cfgs problem; will try to use new api but with not updated Accumulo dep,5/11/15,1
pomadchin,"oh, it is in docker containers",5/11/15,1
echeipesh,,5/11/15,1
echeipesh,"@pomadchin unfortunatly we have to use internal Accumulo API, so it_Ñés not possible to use the new code base with 1.5.2 dependency.",5/11/15,1
pomadchin,"Mb can provide them for public usage (it is very easy to run em on single fat machine, and it would wake up 3 worker containers with hdfs, zookeepers, accumulo, spark, ready to test spark Accumulo jars in a dev mode)",5/11/15,1
lossyrob,"we aren_Ñét running docker containers, but that_Ñés something we_Ñéd like to do in the future",5/11/15,1
lossyrob,"right now we have scripts that can run vagrant VM_Ñés, and then use packer to cut amazon AMI_Ñés, and then cloud formation to spin up the cluster",5/11/15,1
pomadchin,"have an idea to publish containers, to make easy development for geotrellis, if you'll find it useful I'll be glad",5/11/15,1
lossyrob,that would definitely be awesome,5/11/15,1
pomadchin,"btw, there's a spark-notebook by @andypetrella, mb it would be a nice addition to run smth in it ",5/11/15,1
lossyrob,"yup. we have plans on doing a spark-notebook integration. Once this release is out it_Ñés on our plate to focus some attention on an integration. But if you wanted to spearhead that, that would really help",5/11/15,1
pomadchin,"heh, ok, sure, want to help you if I can ): but can be a little passive soon; making a grade work ): ",5/11/15,1
pomadchin,ok so will provide info about new Accumulo and new api tomorrow; thing there is a problem in settings or smth like,5/11/15,1
lossyrob,"that seems like it would be a problem, we are deploying the new 1.6 code against clusters for a bit so we know it works in some settings, have to figure out what_Ñés different about your deployment",5/11/15,1
pomadchin,ty for help,5/11/15,1
lossyrob,ty!,5/11/15,1
echeipesh,Note: You can set accumulo properties (from accumulo-site.xml) and they will be persisted in ZooKeeper.,5/11/15,1
lossyrob,you can set accumulo properties_Ñ_meaning from the shell?,5/11/15,1
echeipesh,yes from `accumulo shell -u root -p secret`,5/11/15,1
pomadchin,O: ,5/11/15,1
echeipesh,right now I_Ñém testing `config -set tserver.wal.sync.method=hflush` vs `hsync`,5/11/15,1
andypetrella,@pomadchin @lossyrob ... men you made my day(s) :-). Poke me if you think I might be of any help!,5/11/15,1
lossyrob,"Nice, thanks @andypetrella!",5/11/15,1
philippeback,@andypetrelle small world.,5/11/15,1
philippeback,@andypetrella small world. ,5/11/15,1
lossyrob,"@pomadchin we_Ñére having problems with the Accumulo 1.6 unfortunately. So your not the only one having problems. We_Ñére working through it, worst case scenario is that we roll back to 1.5",5/11/15,1
pomadchin,@lossyrob heh ): ,5/11/15,1
pomadchin,@lossyrob this commit with the new api will work correct with accumulo 1.5.2 ? https://github.com/geotrellis/geotrellis/commit/8266b96d9b59cb9106a6fd5146040afd3962bfea,5/11/15,1
lossyrob,"Yes, the catalog refactor should work pre-accumulo 1.6 PR",5/11/15,1
lossyrob,so master before the 1.6 upgrade should be working,5/11/15,1
pomadchin,"yes, have an idea to try it; think too all would be ok",5/11/15,1
pomadchin,"there is smth in the new accumulo ._._Ñé mb cfgs / or special code to make long heavy operations; (mean accumulo 1.6.2 core lib and api)got the same err, or smth else?",5/11/15,1
lossyrob,"haven_Ñét run into that error yet, but running into other problems",5/11/15,1
pomadchin,wow ): bad news,5/11/15,1
lossyrob,"```#!/bin/shspark-shell \--jars /tmp/benchmark.jar--master mesos://zk://zookeeper.service.geotrellis-spark.internal:2181/mesos \--conf spark.mesos.coarse=false \--conf spark.executor.memory=5G \--conf spark.kryo.registrator=geotrellis.spark.io.hadoop.KryoRegistrator \--conf spark.executorEnv.SPARK_LOCAL_DIRS=""$LOCAL_DIRS""```",5/11/15,1
echeipesh,http://boundingbox.klokantech.com,5/12/15,1
lossyrob,Benchmarking jar: https://drive.google.com/file/d/0B_qGBN8rOJmONTNFSzNGaHZLUGs/view?usp=sharing,5/12/15,1
lossyrob,"@pomadchin we solved our Accumulo 1.6 problem, and there_Ñés a fix in master. Not sure that will fix your problem, but FYI",5/12/15,1
pomadchin,"Think will solve, going to try now",5/12/15,1
pomadchin,Ty!,5/12/15,1
lossyrob,"@echeipesh @notthatbreezy @moradology I tried out the `repartition(800)` on my 100 node m3.2xlarge cluster (with 800 cores), and did not get better results. The USA benchmark failed out because of a `com.esotericsoftware.kryo.KryoException: java.io.IOException: No space left on device` exception",5/12/15,1
lossyrob,here_Ñés the other result comparisons though:,5/12/15,1
lossyrob,"```# Regular{type: SingleModel-MinMax, name: philadelphia } in 15492 ms{type: SingleModel-Average, name: philadelphia } in 13445 ms{type: MultiModel-localSubtract-Average, name: philadelphia, layers: List(Layer(name = ""tasmax-rcp26-ccsm4"", zoom = 8), Layer(name = ""tasmax-rcp45-ccsm4"", zoom = 8))} in 15137 ms{type: MultiModel-localMean-MinMax, name: philadelphia, layers: List(Layer(name = ""tasmax-rcp26-ccsm4"", zoom = 8), Layer(name = ""tasmax-rcp45-ccsm4"", zoom = 8))} in 2164 ms{type: MultiModel-localSubtract-count, name: philadelphia, layers: List(Layer(name = ""tasmax-rcp26-ccsm4"", zoom = 8), Layer(name = ""tasmax-rcp45-ccsm4"", zoom = 8))} in 1457 ms{type: MultiModel-localSubtract-ZonalSummary, name: philadelphia, layers: List(Layer(name = ""tasmax-rcp26-ccsm4"", zoom = 8), Layer(name = ""tasmax-rcp45-ccsm4"", zoom = 8))} in 2246 ms# repartition(800){type: SingleModel-MinMax, name: philadelphia } in 85345 ms{type: SingleModel-Average, name: philadelphia } in 17787 ms{type: MultiModel-localSubtract-Average, name: philadelphia, layers: List(Layer(name = ""tasmax-rcp26-ccsm4"", zoom = 8), Layer(name = ""tasmax-rcp45-ccsm4"", zoom = 8))} in 14107 ms{type: MultiModel-localMean-MinMax, name: philadelphia, layers: List(Layer(name = ""tasmax-rcp26-ccsm4"", zoom = 8), Layer(name = ""tasmax-rcp45-ccsm4"", zoom = 8))} in 5665 ms{type: MultiModel-localSubtract-count, name: philadelphia, layers: List(Layer(name = ""tasmax-rcp26-ccsm4"", zoom = 8), Layer(name = ""tasmax-rcp45-ccsm4"", zoom = 8))} in 4147 ms{type: MultiModel-localSubtract-ZonalSummary, name: philadelphia, layers: List(Layer(name = ""tasmax-rcp26-ccsm4"", zoom = 8), Layer(name = ""tasmax-rcp45-ccsm4"", zoom = 8))} in 5250 ms# Regular{type: SingleModel-MinMax, name: eastKansas } in 17523 ms{type: SingleModel-Average, name: eastKansas } in 18811 ms{type: MultiModel-localSubtract-Average, name: eastKansas, layers: List(Layer(name = ""tasmax-rcp26-ccsm4"", zoom = 8), Layer(name = ""tasmax-rcp45-ccsm4"", zoom = 8))} in 16729 ms{type: MultiModel-localMean-MinMax, name: eastKansas, layers: List(Layer(name = ""tasmax-rcp26-ccsm4"", zoom = 8), Layer(name = ""tasmax-rcp45-ccsm4"", zoom = 8))} in 5513 ms{type: MultiModel-localSubtract-count, name: eastKansas, layers: List(Layer(name = ""tasmax-rcp26-ccsm4"", zoom = 8), Layer(name = ""tasmax-rcp45-ccsm4"", zoom = 8))} in 6617 ms{type: MultiModel-localSubtract-ZonalSummary, name: eastKansas, layers: List(Layer(name = ""tasmax-rcp26-ccsm4"", zoom = 8), Layer(name = ""tasmax-rcp45-ccsm4"", zoom = 8))} in 8392 ms# repartition(800){type: SingleModel-MinMax, name: eastKansas } in 9450 ms{type: SingleModel-Average, name: eastKansas } in 12448 ms{type: MultiModel-localSubtract-Average, name: eastKansas, layers: List(Layer(name = ""tasmax-rcp26-ccsm4"", zoom = 8), Layer(name = ""tasmax-rcp45-ccsm4"", zoom = 8))} in 13729 ms{type: MultiModel-localMean-MinMax, name: eastKansas, layers: List(Layer(name = ""tasmax-rcp26-ccsm4"", zoom = 8), Layer(name = ""tasmax-rcp45-ccsm4"", zoom = 8))} in 9538 ms{type: MultiModel-localSubtract-count, name: eastKansas, layers: List(Layer(name = ""tasmax-rcp26-ccsm4"", zoom = 8), Layer(name = ""tasmax-rcp45-ccsm4"", zoom = 8))} in 8719 ms{type: MultiModel-localSubtract-ZonalSummary, name: eastKansas, layers: List(Layer(name = ""tasmax-rcp26-ccsm4"", zoom = 8), Layer(name = ""tasmax-rcp45-ccsm4"", zoom = 8))} in 8963 ms# Regular{type: SingleModel-MinMax, name: Rockies } in 31543 ms{type: SingleModel-Average, name: Rockies } in 29057 ms{type: MultiModel-localSubtract-Average, name: Rockies, layers: List(Layer(name = ""tasmax-rcp26-ccsm4"", zoom = 8), Layer(name = ""tasmax-rcp45-ccsm4"", zoom = 8))} in 33797 ms{type: MultiModel-localMean-MinMax, name: Rockies, layers: List(Layer(name = ""tasmax-rcp26-ccsm4"", zoom = 8), Layer(name = ""tasmax-rcp45-ccsm4"", zoom = 8))} in 28805 ms{type: MultiModel-localSubtract-count, name: Rockies, layers: List(Layer(name = ""tasmax-rcp26-ccsm4"", zoom = 8), Layer(name = ""tasmax-rcp45-ccsm4"", zoom = 8))} in 25841 ms{type: MultiModel-localSubtract-ZonalSummary, name: Rockies, layers: List(Layer(name = ""tasmax-rcp26-ccsm4"", zoom = 8), Layer(name = ""tasmax-rcp45-ccsm4"", zoom = 8))} in 31035 ms# repartition(800){type: SingleModel-MinMax, name: Rockies } in 36156 ms{type: SingleModel-Average, name: Rockies } in 41861 ms{type: MultiModel-localSubtract-Average, name: Rockies, layers: List(Layer(name = ""tasmax-rcp26-ccsm4"", zoom = 8), Layer(name = ""tasmax-rcp45-ccsm4"", zoom = 8))} in 76629 ms{type: MultiModel-localMean-MinMax, name: Rockies, layers: List(Layer(name = ""tasmax-rcp26-ccsm4"", zoom = 8), Layer(name = ""tasmax-rcp45-ccsm4"", zoom = 8))} in 62574 ms{type: MultiModel-localSubtract-count, name: Rockies, layers: List(Layer(name = ""tasmax-rcp26-ccsm4"", zoom = 8), Layer(name = ""tasmax-rcp45-ccsm4"", zoom = 8))} in 64514 ms{type: MultiModel-localSubtract-ZonalSummary, name: Rockies, layers: List(Layer(name = ""tasmax-rcp26-ccsm4"", zoom = 8), Layer(name = ""tasmax-rcp45-ccsm4"", zoom = 8))} in 71269 ms```",5/12/15,1
pomadchin,/hehe not solved my problem; trying to figure out wtf,5/12/15,1
notthatbreezy,@lossyrob & @echeipesh -- when reading/writing from s3 -- have either of you taken a look at the Spark API for reading binary records/files? https://github.com/apache/spark/pull/1658 -- the example here is actually for reading tiffs and makes some sense given we're just reading/writing byte arrays possibly,5/12/15,1
echeipesh,"That looks pretty good. I_Ñéll have to test it, it can replace `S3InputFormat`.",5/12/15,1
notthatbreezy,"cool, will it allow us to defer, ignore page size at all so we can just pass in something like `s3a:/path/to/datasets/*.tif`?",5/12/15,1
echeipesh,"it has some default setting, you would have to either live with it or set it.",5/12/15,1
echeipesh,"actually, I take it back. s3a `FileSystem` has settings for that. I am not sure how it would pass from `binaryFiles`",5/12/15,1
notthatbreezy,"got it, yeah I could see wanting to tweak that still if the default isn't working for whatever reason",5/12/15,1
notthatbreezy,nice thing about s3a is it obviates the need to worry about credentials,5/12/15,1
notthatbreezy,as long as you assign an IAM role to your ec2 instance,5/12/15,1
echeipesh,"Yeah, we_Ñére using that mechanism in the s3 catalog, haven_Ñét put it into S3InputFormat yet. But if we can save ourself 4 files, that would be great. I did not see that s3a existed when I wrote that.",5/12/15,1
lossyrob,@sandan @bretts61 here_Ñés the 3D hilbert code: https://github.com/geotrellis/geotrellis/blob/master/spark/src/main/scala/geotrellis/spark/io/index/hilbert/HilbertSpaceTimeKeyIndex.scala,5/12/15,1
lossyrob,getting this on S3 raster catalog save:,5/12/15,1
lossyrob,"```ted	at com.amazonaws.http.AmazonHttpClient.executeHelper(AmazonHttpClient.java:454)	at com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:232)	at com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:3528)	at com.amazonaws.services.s3.AmazonS3Client.putObject(AmazonS3Client.java:1393)	at geotrellis.spark.io.s3.AmazonS3Client.putObject(S3Client.scala:92)	at geotrellis.spark.io.s3.RasterRDDWriter$$anonfun$write$1$$anonfun$2$$anonfun$apply$1.apply(RasterRDDWriter.scala:82)	at geotrellis.spark.io.s3.RasterRDDWriter$$anonfun$write$1$$anonfun$2$$anonfun$apply$1.apply(RasterRDDWriter.scala:80)	at scalaz.concurrent.Task$.Try(Task.scala:379)	at scalaz.concurrent.Task$$anonfun$apply$10.apply(Task.scala:281)	at scalaz.concurrent.Task$$anonfun$apply$10.apply(Task.scala:281)	at scalaz.concurrent.Future$$anonfun$apply$15$$anon$3.call(Future.scala:367)	at scalaz.concurrent.Future$$anonfun$apply$15$$anon$3.call(Future.scala:367)	at java.util.concurrent.FutureTask.run(FutureTask.java:262)	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)	at java.lang.Thread.run(Thread.java:744)Caused by: javax.net.ssl.SSLPeerUnverifiedException: peer not authenticated	at sun.security.ssl.SSLSessionImpl.getPeerCertificates(SSLSessionImpl.java:397)	at org.apache.http.conn.ssl.AbstractVerifier.verify(AbstractVerifier.java:126)	at org.apache.http.conn.ssl.SSLSocketFactory.connectSocket(SSLSocketFactory.java:437)	at org.apache.http.impl.conn.DefaultClientConnectionOperator.openConnection(DefaultClientConnectionOperator.java:180)	at org.apache.http.impl.conn.ManagedClientConnectionImpl.open(ManagedClientConnectionImpl.java:294)	at org.apache.http.impl.client.DefaultRequestDirector.tryConnect(DefaultRequestDirector.java:643)	at org.apache.http.impl.client.DefaultRequestDirector.execute(DefaultRequestDirector.java:479)	at org.apache.http.impl.client.AbstractHttpClient.execute(AbstractHttpClient.java:906)	at org.apache.http.impl.client.AbstractHttpClient.execute(AbstractHttpClient.java:805)	at com.amazonaws.http.AmazonHttpClient.executeHelper(AmazonHttpClient.java:384)```",5/12/15,1
lossyrob,https://github.com/mcohen01/amazonica/issues/10,5/12/15,1
lossyrob,trying `--conf com.amazonaws.sdk.disableCertChecking=true` in the spar-submit script,5/12/15,1
lossyrob,"S3 ingest is pretty slow, it_Ñés been 30 min for NEX model (~79 GB), 879/1106 (55 failed) for tasks on save",5/12/15,1
echeipesh,What upload are you getting per core ?,5/12/15,1
lossyrob,![s3upload](http://i.imgur.com/xqTUZtC.png),5/12/15,1
lossyrob,"@notthatbreezy what_Ñés the limit on Lmabda you were talking about? reading in the FAQ, ""Each Lambda function receives 500MB of non-persistent disk space in its own /tmp directory._Ñù doesn_Ñét seem that bad",5/12/15,1
notthatbreezy,http://docs.aws.amazon.com/lambda/latest/dg/limits.html,5/12/15,1
notthatbreezy,I was talking about the maximum duration of a task limit,5/12/15,1
lossyrob,ahhh gotcha,5/12/15,1
lossyrob,@mojodna would these limits effect the lambda tiler you were mentioning in the OAM Server doc?,5/12/15,1
mojodna,not for sync requests,5/12/15,1
mojodna,since those need to finish asap,5/12/15,1
mojodna,it was a blocker for using lambda to do upload processing though,5/12/15,1
mojodna,(both size + time),5/12/15,1
lossyrob,right. so the sync requests_Ñ_when does that happen?,5/12/15,1
lossyrob,I_Ñém a bit fuzzy on the possible lambda tiler architecture,5/12/15,1
allixender,Hi @kyeah @lossyrob maybe this is also a more general simple mistake. I get a stack trace with a spark-submit job for a GeoTIFF ingest:,5/13/15,1
allixender,"Exception in thread ""main"" java.lang.NoSuchMethodError: org.apache.hadoop.mapreduce.Job.getInstance(Lorg/apache/hadoop/conf/Configuration;)Lorg/apache/hadoop/mapreduce/Job;        at geotrellis.spark.io.hadoop.HdfsUtils$.putFilesInConf(HdfsUtils.scala:57)        at geotrellis.spark.io.hadoop.package$HadoopConfigurationWrapper.withInputDirectory(package.scala:55)        at geotrellis.spark.io.hadoop.HadoopSparkContextMethods$class.hadoopGeoTiffRDD(HadoopSparkContextMethods.scala:29)        at geotrellis.spark.io.hadoop.package$HadoopSparkContextMethodsWrapper.hadoopGeoTiffRDD(package.scala:43)",5/13/15,1
allixender,It seems a generic basic Hadoop/JobConf thing?,5/13/15,1
lossyrob,Can you describe your setup a bit more? Are you just submitting to a local spark master?,5/13/15,1
allixender,"Yes, sry, was on the AWS Summit in Auckland over the day. So I have the stand-alone local spark-1.2.0 distribution plain ""installed""/extracted",5/13/15,1
allixender,I can for example use a local spark-shell and add the spark-cassandra-connector jar and then use CassandraTableRDD within spark,5/13/15,1
allixender,"when running the tests  in the geotrellis build, I export SPARK_HOME but must not have a master (or slave?) running, and then the spark tests run",5/13/15,1
allixender,otherwise they complain about port already being in use.,5/13/15,1
mfsny,"Hi all, is there anybody who has already migrated the spray renderPng from 0.9.0 to 0.10.0-SNAPSHOT? The ""respondWithMediaType(MediaTypes.'image/png')"" returns a compilation error `Error:(63, 91) could not find implicit value for parameter marshaller: spray.httpx.marshalling.ToResponseMarshaller[geotrellis.raster.render.Png]              Complete(img, h) => respondWithMediaType(MediaTypes.'image/png') { complete(img) }                                                                                          ^`.",5/13/15,1
echeipesh,That is spray not finding an implicit type class to Marshall the response. I wonder if something was forcing it to byte array before. Could you thy explicitly pulling out bytes with img.bytes?,5/13/15,1
mfsny,"Yes, ""img.bytes"" did the trick. Thanks!",5/13/15,1
lossyrob,perhaps we should write a spray marsheller for Png. Could go in `geotrellis.service`,5/13/15,1
echeipesh,"Yes, might make sense. We really need to take a look at what would make user expirience better when we make spark-local tutorial for the release.",5/13/15,1
pomadchin,mb it is possible also to add smth like toPng but for tiff? now it is possible to write tiff to file,5/13/15,1
pomadchin,very useful to give output as a tiff for a tile server,5/13/15,1
echeipesh,For rendering it in a service ?,5/13/15,1
pomadchin,yep mb,5/13/15,1
pomadchin,"had plans to store different bands, combine them, and give a colored output for example",5/13/15,1
lossyrob,@pomadchin I_Ñém actually in the midst of rewriting the geotiff writer to do just that,5/13/15,1
lossyrob,"There_Ñés an issue where we can_Ñét handle many projections, because we were porting some of the GeoTIFF CRS writing code from GDAL, which actually parses from WKT where we are parsing from a proj4 string, so it only handles some cases",5/13/15,1
lossyrob,but what I want to end up with is,5/13/15,1
lossyrob,`rasterRdd.toGeoTiff.write(path)`,5/13/15,1
lossyrob,"and that would write out a GeoTIFF file, with underlying tiles as the same tiles as the RDD (or something like that)",5/13/15,1
lossyrob,`rasterRdd.toGeoTiff(DeflateCompression).write(path)` etc,5/13/15,1
lossyrob,that work is here: https://github.com/geotrellis/geotrellis/pull/1066,5/13/15,1
lossyrob,"which redoes the GeoTiff writer, handles multiband geotiffs and also a lot of improvements, redoes the writer, starts on the multiband feature",5/13/15,1
pomadchin,Ty! Nice news,5/13/15,1
pomadchin,"guyz, had smb such exception: ",5/14/15,1
pomadchin,"```Exception in thread ""main"" org.apache.accumulo.core.client.AccumuloException: file:/geotrellis-ingest/MOD13Q1-11-BAEqd1VU6X-failures does not exist    at org.apache.accumulo.core.client.impl.TableOperationsImpl.doFateOperation(TableOperationsImpl.java:335)    at org.apache.accumulo.core.client.impl.TableOperationsImpl.doFateOperation(TableOperationsImpl.java:295)    at org.apache.accumulo.core.client.impl.TableOperationsImpl.doTableFateOperation(TableOperationsImpl.java:1594)    at org.apache.accumulo.core.client.impl.TableOperationsImpl.importDirectory(TableOperationsImpl.java:1186)    at geotrellis.spark.io.accumulo.RasterRDDWriter$class.write(RasterRDDWriter.scala:90)    at geotrellis.spark.io.accumulo.spacetime.SpaceTimeRasterRDDWriter$.write(SpaceTimeRasterRDDWriter.scala:28)    at geotrellis.spark.io.accumulo.AccumuloRasterCatalog$$anon$2.write(AccumuloRasterCatalog.scala:77)    at geotrellis.spark.io.accumulo.AccumuloRasterCatalog$$anon$2.write(AccumuloRasterCatalog.scala:50)    at geotrellis.admin.ingest.AccumuloIngestTimeCommand$$anonfun$main$1.apply(AccumuloIngestTimeCommand.scala:53)    at geotrellis.admin.ingest.AccumuloIngestTimeCommand$$anonfun$main$1.apply(AccumuloIngestTimeCommand.scala:52)    at geotrellis.admin.ingest.Ingest$.sinkLevels$1(Ingest.scala:70)    at geotrellis.admin.ingest.Ingest$.apply(Ingest.scala:86)    at geotrellis.admin.ingest.AccumuloIngestTimeCommand$.main(AccumuloIngestTimeCommand.scala:52)    at geotrellis.admin.ingest.AccumuloIngestTimeCommand$.main(AccumuloIngestTimeCommand.scala:26)    at com.quantifind.sumac.ArgMain$class.mainHelper(ArgApp.scala:45)    at com.quantifind.sumac.ArgMain$class.main(ArgApp.scala:34)    at geotrellis.admin.ingest.AccumuloIngestTimeCommand$.main(AccumuloIngestTimeCommand.scala:26)    at geotrellis.admin.ingest.AccumuloIngestTimeCommand.main(AccumuloIngestTimeCommand.scala)    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)    at java.lang.reflect.Method.invoke(Method.java:606)    at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:569)    at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:166)    at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:189)    at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:110)    at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)Caused by: ThriftTableOperationException(tableId:3, tableName:null, op:BULK_IMPORT, type:BULK_BAD_ERROR_DIRECTORY, description:file:/geotrellis-ingest/MOD13Q1-11-BAEqd1VU6X-failures does not exist)    at org.apache.accumulo.core.master.thrift.FateService$waitForFateOperation_result$waitForFateOperation_resultStandardScheme.read(FateService.java:4243)    at org.apache.accumulo.core.master.thrift.FateService$waitForFateOperation_result$waitForFateOperation_resultStandardScheme.read(FateService.java:4212)    at org.apache.accumulo.core.master.thrift.FateService$waitForFateOperation_result.read(FateService.java:4146)    at org.apache.thrift.TServiceClient.receiveBase(TServiceClient.java:78)    at org.apache.accumulo.core.master.thrift.FateService$Client.recv_waitForFateOperation(FateService.java:174)    at org.apache.accumulo.core.master.thrift.FateService$Client.waitForFateOperation(FateService.java:159)    at org.apache.accumulo.core.client.impl.TableOperationsImpl.waitForFateOperation(TableOperationsImpl.java:267)    at org.apache.accumulo.core.client.impl.TableOperationsImpl.doFateOperation(TableOperationsImpl.java:309)    ... 26 more```",5/14/15,1
pomadchin,"what is the path `/geotrellis-ingest/` ?",5/14/15,1
pomadchin,in `writer`,5/14/15,1
echeipesh,"Yeah, HDFS has default fs setting. In our cluster it defaults to the HDFS file system",5/14/15,1
echeipesh,You can either set the setting or you can pass a fully qualified Path to the HdfsWriteStrategy.,5/14/15,1
echeipesh,"Give me a sec, I_Ñéll find the exact property.",5/14/15,1
echeipesh,"```<property>    <name>fs.defaultFS</name>    <value>hdfs://localhost:8020</value>  </property>```",5/14/15,1
echeipesh,in `core-site.xml`,5/14/15,1
echeipesh,Basically spark needs to dump the records there in sorted order for the accumulo to perform it_Ñés bulk ingest.,5/14/15,1
pomadchin,hm wonder what is with this task wrong,5/14/15,1
pomadchin,"this file is created, hm",5/14/15,1
pomadchin,ofcource it is obvious that smth wrong with the accumulo settings,5/14/15,1
echeipesh,"It_Ñés not really meant to use local file system, I_Ñéve tried it once and it kind of worked, but had some issues.",5/14/15,1
pomadchin,"heh, will publish containers with the accumulo spark env, to show antoher exception, which was missing on accumulo 1.5.2",5/14/15,1
lossyrob,"that config portion that eugene just mentioned is in the hadoop config, not the accumulo config",5/14/15,1
pomadchin,"yep, sure",5/14/15,1
pomadchin,got running cluster and tasks on it (easy tasks) completed correct,5/14/15,1
pomadchin,failures here comes with accumulo and geotrellis,5/14/15,1
lossyrob,"nice. well since it got to the stage of saving to the temp HDFS directory, it got pretty far in the ingest process",5/14/15,1
pomadchin,"sec, will try new ingest run",5/14/15,1
pomadchin,such moment: spark got to see hadoop config?,5/14/15,1
lossyrob,i think you_Ñéll have to restart hadoop,5/14/15,1
pomadchin,"omg, ty; dat was a problem ._.",5/14/15,1
pomadchin,TY a lot! uff ._.',5/14/15,1
pomadchin,"heh, got same exception on ` INFO ContextCleaner: Cleaned broadcast 4` step; zookeeper failure by timeout",5/14/15,1
pomadchin,"will publish containers with the same error, mb smth wrong with `zookeeper`, have no ideas ._.",5/14/15,1
lossyrob,can you look in hdfs to see if hte tiles are written there?,5/14/15,1
lossyrob,also dropping into the accumulo shell and seeing what tables exist in accumulo,5/14/15,1
pomadchin,"yep, tables exists",5/14/15,1
pomadchin,everything writes,5/14/15,1
pomadchin,spark sees hadoop config,5/14/15,1
lossyrob,`/geotrellis-ingest/` exists and is full of tiles?,5/14/15,1
pomadchin,yep,5/14/15,1
pomadchin,"this error was solvedthe error https://gist.github.com/pomadchin/e5178d477e3f93cef6db here again",5/14/15,1
pomadchin,"zookeeper has:```WARN  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxn@357] - caught end of stream exceptionEndOfStreamException: Unable to read additional data from client sessionid 0x0, likely client has closed socket    at org.apache.zookeeper.server.NIOServerCnxn.doIO(NIOServerCnxn.java:228)    at org.apache.zookeeper.server.NIOServerCnxnFactory.run(NIOServerCnxnFactory.java:208)    at java.lang.Thread.run(Thread.java:745)```",5/14/15,1
lossyrob,are there any tiles in the table?,5/14/15,1
lossyrob,you can use `scan -f 20` on the tile table to tell,5/14/15,1
lossyrob,"it almost looks like the job finished, and zookeeper is holding onto a dead connection: https://issues.apache.org/jira/browse/ZOOKEEPER-1382",5/14/15,1
lossyrob,"have you tried reading the RasterRDD? might be worth testing to see if it actually all went through, was a finish-up error",5/14/15,1
echeipesh,are you calling .stop on SparkContext after you are done with it ?,5/14/15,1
pomadchin,making task with a standart ingest job,5/14/15,1
echeipesh,"hmm, that_Ñés right. Looks a lot like an error you would see if that didn_Ñét happen.",5/14/15,1
lossyrob,yeah because it_Ñés spark-submit you hsouldn_Ñét have to call stop,5/14/15,1
lossyrob,does hte spark UI hang?,5/14/15,1
pomadchin,/catalog/ in gt-admin works though smth else crashes on an error,5/14/15,1
pomadchin,yep hangs,5/14/15,1
lossyrob,what_Ñés the last job it shows? does it say it completed all it_Ñés tasks?,5/14/15,1
lossyrob,/catalog/ only hits the metadata,5/14/15,1
lossyrob,you could fire up a spark shell with the geotrellis jar and see if you can access the RDD from a catalog you build up there,5/14/15,1
pomadchin,"not finished, just hanging with this error about broadcastif i ran `AccumuloIngestArgs` it hangs on a `broadcast 3` and then fails with a zookeeper error, though zookeeper is accesseble",5/14/15,1
pomadchin,"just published https://github.com/pomadchin/accumulo-spark to run in a local distributed mode all the environment",5/14/15,1
lossyrob,if fails on a broadcast var cleanup step,5/14/15,1
pomadchin,https://issues.apache.org/jira/browse/ZOOKEEPER-1382 seems to be a right link o:,5/14/15,1
lossyrob,"I_Ñém wondering if this is just that it_Ñés done, but hangs on the finish. Are there leftover uncompleted tasks in the UI? Did you check the accumulo tile table to see if there are tiles in there?",5/14/15,1
pomadchin,"```root@gt> table nextdayroot@gt nextday> scan -f 2003_00000176313861574 MOD13Q1:2015-01-01T00:00:00. []    Z\x01(\x01\x08\x04\x01\x80\xE0\xAA\x96\xAA)\x81\x81\x1A\x01\x81\x80\x0803_00000176313861574 MOD13Q1:2015-01-01T00:00:00. []    Z\x01(\x01\x0A\x04\x01\x80\xE0\xAA\x96\xAA)\x81\x81\x1A\x01\x81\x80\x0803_00000176313861574 MOD13Q1:2015-01-01T00:00:00. []    Z\x01(\x01\x08\x06\x01\x80\xE0\xAA\x96\xAA)\x81\x81\x1A\x01\x81\x80\x0803_00000176313861574 MOD13Q1:2015-01-01T00:00:00. []    Z\x01(\x01\x0A\x06\x01\x80\xE0\xAA\x96\xAA)\x81\x81\x1A\x01\x81\x80\x0803_00000176313861574 MOD13Q1:2015-01-01T00:00:00. []    Z\x01(\x01\x0C\x06\x01\x80\xE0\xAA\x96\xAA)\x81\x81\x1A\x01\x81\x80\x0803_00000176313861575 MOD13Q1:2015-01-01T00:00:00. []    Z\x01(\x01\x0A\x08\x01\x80\xE0\xAA\x96\xAA)\x81\x81\x1A\x01\x81\x80\x0803_00000176313861575 MOD13Q1:2015-01-01T00:00:00. []    Z\x01(\x01\x0C\x08\x01\x80\xE0\xAA\x96\xAA)\x81\x81\x1A\x01\x81\x80\x0804_00000176313861579 MOD13Q1:2015-01-01T00:00:00. []    Z\x01(\x01\x12\x08\x01\x80\xE0\xAA\x96\xAA)\x81\x81\x1A\x01\x81\x80\x0804_00000176313861579 MOD13Q1:2015-01-01T00:00:00. []    Z\x01(\x01\x12\x0A\x01\x80\xE0\xAA\x96\xAA)\x81\x81\x1A\x01\x81\x80\x0804_00000176313861579 MOD13Q1:2015-01-01T00:00:00. []    Z\x01(\x01\x14\x08\x01\x80\xE0\xAA\x96\xAA)\x81\x81\x1A\x01\x81\x80\x0804_00000176313861579 MOD13Q1:2015-01-01T00:00:00. []    Z\x01(\x01\x16\x08\x01\x80\xE0\xAA\x96\xAA)\x81\x81\x1A\x01\x81\x80\x0804_00000176313861579 MOD13Q1:2015-01-01T00:00:00. []    Z\x01(\x01\x14\x0A\x01\x80\xE0\xAA\x96\xAA)\x81\x81\x1A\x01\x81\x80\x08```",5/14/15,1
lossyrob,so you have tiles.,5/14/15,1
lossyrob,"I do think it finished, it_Ñés just something weird is happening on cleanup",5/14/15,1
lossyrob,so you should be able to get that RDD out of accumulo,5/14/15,1
hectcastro,pomadchin: Interesting approach to containerizing everything.,5/14/15,1
pomadchin,"heh sry g2go now, will be in the evening and tomorrow;but these conatiners I published worked perfect with accumulo 1.5.2 so if you got a lil bit time, you can run them and some job; (just mount a volume)in dockerfile it is possible to fix accumulo version (just load another `tar.gz`), and it would work; so seems to me smth wrong with the configuration",5/14/15,1
pomadchin,"and now, you can run any ingest job, or https://gist.github.com/pomadchin/e7f7719ac215856a24f5 this job, to see the result",5/14/15,1
pomadchin,ill try to get rdd through the shell,5/14/15,1
pomadchin,@hectcastro ty,5/14/15,1
lossyrob,"ok, I_Ñéll try to check that out when I have a chance. thanks @pomadchin!",5/14/15,1
pomadchin,@lossyrob if you interested in containers you can publish them istead of a vagrant machine :),5/14/15,1
lossyrob,that would be great,5/14/15,1
mojodna,"@lossyrob: i was reminded about Beanstalk Workers yesterday, which obviate a bunch of the CloudFormation nonsense (for vapor-clock workers).  haven_Ñét done anything beyond opening https://aws.amazon.com/blogs/aws/background-task-handling-for-aws-elastic-beanstalk/ and starting to wonder if it works similarly for SWF.",5/14/15,1
lossyrob,"I tried beanstalk workers, couldn_Ñét get it to run GDAL",5/14/15,1
lossyrob,"docker on beanstalk would make more sense, but never got there",5/14/15,1
lossyrob,For us Mac folk looking to use docker: http://viget.com/extend/how-to-use-docker-on-os-x-the-missing-guide,5/14/15,1
hectcastro,The new hotness appears to be Docker Machine vs. Boot2docker: https://docs.docker.com/machine/,5/14/15,1
pomadchin,"nice, or it is possible just to download package (very lazy mode): https://github.com/boot2docker/osx-installer/releases",5/14/15,1
lossyrob,"@pomadchin once I have the docker containers up, how do I interact with them?",5/14/15,1
pomadchin,"`docker exec -it master1 /bin/bash``su - hduser`",5/14/15,1
pomadchin,"but before, you need to load jar and files  to it;https://github.com/pomadchin/accumulo-spark/blob/master/start-cluster.sh#L5fix it: ```docker run -d -t --dns 127.0.0.1 -e NODE_TYPE=m -e ZOOKEEPER_ID=1 -e JOIN_IP=$FIRST_IP -p 4444:4444 -p 9000:9000 -p 50010:50010 -p 50020:50020 -p 50070:50070 -p 50075:50075 -p 50090:50090 -p 50475:50475 -p 8030:8030 -p 8031:8031 -p 8032:8032 -p 8033:8033 -p 8040:8040 -p 8042:8042 -p 8060:8060 -p 8088:8088 -p 50060:50060 -p 2181:2181 -p 2888:2888 -p 3888:3888 -p 9999:9999 -p 9997:9997 -p 50091:50091 -p 50095:50095 -p 4560:4560 -p 12234:12234 -p 8080:8080 -p 8081:8081 -p 7077:7077 -p 4040:4040 -p 4041:4041 -p 1808:1808 --name master1 -h master1.owm daunnc/hadoop-nn-dn:0.2```to ```docker run -d -t --dns 127.0.0.1 -v /localFolder:/dockerFolder  -e NODE_TYPE=m -e ZOOKEEPER_ID=1 -e JOIN_IP=$FIRST_IP -p 4444:4444 -p 9000:9000 -p 50010:50010 -p 50020:50020 -p 50070:50070 -p 50075:50075 -p 50090:50090 -p 50475:50475 -p 8030:8030 -p 8031:8031 -p 8032:8032 -p 8033:8033 -p 8040:8040 -p 8042:8042 -p 8060:8060 -p 8088:8088 -p 50060:50060 -p 2181:2181 -p 2888:2888 -p 3888:3888 -p 9999:9999 -p 9997:9997 -p 50091:50091 -p 50095:50095 -p 4560:4560 -p 12234:12234 -p 8080:8080 -p 8081:8081 -p 7077:7077 -p 4040:4040 -p 4041:4041 -p 1808:1808 --name master1 -h master1.owm daunnc/hadoop-nn-dn:0.2```",5/14/15,1
pomadchin,flag `-v` forwards volumes ,5/14/15,1
lossyrob,"awesome, I_Ñém going to try this out once  this stand up meeting is over :)",5/14/15,1
pomadchin,hope you'll help to solve this issue :),5/14/15,1
lossyrob,@pomadchin what was the ingest script you were using?,5/14/15,1
pomadchin,"```#!/usr/bin/env bash### INGEST SPATIAL GEOTIFF IN ACCUMULO #### geotrellis-spark JAR. Shouldn't have to change this one if running in the root folder (remember to run ./sbt ""project spark"" assembly)JAR=gt-admin-ingest-assembly-0.1.0-SNAPSHOT.jar# Amount of memory for the driverDRIVER_MEMORY=10G# Amount of memory per executor. If in local mode, change the DRIVER_MEMORY instead.EXECUTOR_MEMORY=8G# Directory with the input tiled GeoTIFF's # NEED TO FILL THIS OUTINPUT=file:/data/MOD13Q1/rgb/2015/001 # Table to store tilesTABLE=nextday# Name of the layer. This will be used in conjunction with the zoom level to reference the layer (see LayerId)LAYER_NAME=MOD13Q1# This defines the destination spatial reference system we want to use# (in this case, Web Mercator)CRS=EPSG:3857 # true means we want to pyramid the raster up to larger zoom levels,# so if our input rasters are at a resolution that maps to zoom level 11, pyramiding will also save# off levels 10, 9, ..., 1.PYRAMID=true # true will delete the HDFS data for the layer if it already exists.CLOBBER=true # We need to remove some bad signatures from the assembled JAR. We're working on excluding these# files as part of the build step, this is a workaround.zip -d $JAR META-INF/ECLIPSEF.RSAzip -d $JAR META-INF/ECLIPSEF.SF # Run the spark submit job spark-submit \--class geotrellis.admin.ingest.AccumuloIngestTimeCommand \--driver-memory $DRIVER_MEMORY \--executor-memory $EXECUTOR_MEMORY \$JAR \--instance gis --user root --password secret --zookeeper localhost \--crs $CRS \--pyramid $PYRAMID --clobber $CLOBBER \--input $INPUT \--layerName $LAYER_NAME \--table $TABLE \--time 2015-001```",5/14/15,1
pomadchin,"but, same error happens if you run `AccumuloIngestCommand` too",5/14/15,1
pomadchin,"so you can use ```park-submit \--class geotrellis.admin.ingest.AccumuloIngestCommand \--driver-memory $DRIVER_MEMORY \--executor-memory $EXECUTOR_MEMORY \$JAR \--instance gis --user root --password secret --zookeeper localhost \--crs $CRS \--pyramid $PYRAMID --clobber $CLOBBER \--input $INPUT \--layerName $LAYER_NAME \--table $TABLE```",5/14/15,1
lossyrob,I_Ñém getting memory errors,5/14/15,1
lossyrob,"```hduser@master1:~/hostFolder$ ./ingest.sh	zip warning: benchmark-spark-assembly-0.1.0-SNAPSHOT.jar not found or empty	zip warning: name not matched: META-INF/ECLIPSEF.RSAzip error: Nothing to do! (benchmark-spark-assembly-0.1.0-SNAPSHOT.jar)	zip warning: benchmark-spark-assembly-0.1.0-SNAPSHOT.jar not found or empty	zip warning: name not matched: META-INF/ECLIPSEF.SFzip error: Nothing to do! (benchmark-spark-assembly-0.1.0-SNAPSHOT.jar)Java HotSpot(TM) 64-Bit Server VM warning: INFO: os::commit_memory(0x000000072aa80000, 3579314176, 0) failed; error='Cannot allocate memory' (errno=12)## There is insufficient memory for the Java Runtime Environment to continue.# Native memory allocation (malloc) failed to allocate 3579314176 bytes for committing reserved memory.# An error report file with more information is saved as:# /home/hduser/hostFolder/hs_err_pid2685.log```",5/14/15,1
pomadchin,"ouch seems you have not enough memory to start a job : D you can run only master container to run everything in a singlenode cluster modesec will make a branch with standalone settings",5/14/15,1
lossyrob,"I_Ñém on my laptop, has 16GB mem, do you run against a much larger machine?",5/14/15,1
lossyrob,is there a setting for docker memory allocaiton?,5/14/15,1
pomadchin,yep; on a google cloud machine ,5/14/15,1
pomadchin,"no, but, you can see memory allowed for your virtual machine ",5/14/15,1
pomadchin,in a virtual box settings,5/14/15,1
lossyrob,2048,5/14/15,1
pomadchin,this spark submit wants 8gb for each node: 8 * 3,5/14/15,1
pomadchin,sec,5/14/15,1
pomadchin,branch singlenode cluster,5/14/15,1
pomadchin,https://github.com/pomadchin/accumulo-spark/tree/singlenode-cluster,5/14/15,1
pomadchin,though not tried it ,5/14/15,1
pomadchin,trying now,5/14/15,1
pomadchin,you have to re-build hadoop-base and hadoop-nn-dn images,5/14/15,1
lossyrob,"ok, same commands as in README?",5/14/15,1
pomadchin,yep,5/14/15,1
pomadchin,everything the same,5/14/15,1
lossyrob,great,5/14/15,1
lossyrob,I_Ñéll try it now,5/14/15,1
pomadchin,ok,5/14/15,1
pomadchin,updated start script there was a typo,5/14/15,1
pomadchin,"not tried this error in a single node cluster mode (: but provide more than 8 gbs, to have memory for hdfs and accumulo; i gave it all mem",5/14/15,1
pomadchin,"not tried less memory, but seems to me it possible to have less, need then to fix accumulo config, to use less memory ",5/14/15,1
lossyrob,"getting further now, still memory issues",5/14/15,1
hectcastro,May have to reload the Boot2docker VM to have the memory settings changes take effect.,5/14/15,1
lossyrob,"I havn_Ñéet modified the boot2docker vm memory setting, would I do that directly through vbox?",5/14/15,1
pomadchin,yep,5/14/15,1
hectcastro,https://github.com/boot2docker/boot2docker-cli#configuration,5/14/15,1
pomadchin,you will see the virutal machine `boot2docker`,5/14/15,1
lossyrob,doesn_Ñét look like I can change it after the fact,5/14/15,1
lossyrob,"I_Ñéll have to mess with boot2docker and try to give it 8 GB, and try. I have a lot of stuff up right now so need to clean up my machine_Ñés mem before I do that, so will try it out later",5/14/15,1
pomadchin,"you can change executor / driver memory parameters, if you think spark would not have heap error",5/14/15,1
lossyrob,I set them to 1 GB,5/14/15,1
pomadchin,on your tiles,5/14/15,1
lossyrob,which might end up giving me heap memory exceptions,5/14/15,1
lossyrob,"but not even getting there, can_Ñét allocate JVM",5/14/15,1
pomadchin,accumulo takes all memory hm; will fix cfgs to provide it only 512mb,5/14/15,1
lossyrob,"not sure how that_Ñéll all work, but if we can get a working example of the error running local, that would be much easier to debug",5/14/15,1
lossyrob,"also if we can get a dev environment working with containers, I think that would be cool to try to merge into `geotrellis-ec2-cluster`",5/14/15,1
pomadchin,"sure, btw, you can user docker files to install locally everything the same, and to use provided cfgs",5/14/15,1
pomadchin,and just to use them as a sample to make prod containers :),5/14/15,1
lossyrob,I_Ñéd need a lot more experience with docker containers to feel comfortable using them in prod :) small steps,5/14/15,1
pomadchin,:D ,5/14/15,1
pomadchin,"so, on my way to make accumulo eat lessso the single node branch would be as much economic as it can be",5/14/15,1
lossyrob,or perhaps be able to have one master one slave locally in one VM that takes only a couple of gigs?,5/14/15,1
lossyrob,that way we can test master-slave network stuff,5/14/15,1
pomadchin,"sure, it possible to make only master slave but then you submit a job and provide it 8 Gbs of memory, it eats 8 gbs on each node, moreover there is an accumulo which wants to eat memory too, and zookeeper ",5/14/15,1
echeipesh,"There is a mini-accumulo cluster that doesn_Ñét need HDFS, only local FS. Could be worth a look for mini container.",5/14/15,1
pomadchin,it would work without hdfs?,5/14/15,1
pomadchin,o:,5/14/15,1
echeipesh,http://accumulo.apache.org/1.4/user_manual/Development_Clients.html,5/14/15,1
lossyrob,why would you have to provide the job 8 gigs of mem?,5/14/15,1
pomadchin,:D just an example)),5/14/15,1
lossyrob,if we could run jobs that only require say 512 MB,5/14/15,1
lossyrob,and have test data sets that are small,5/14/15,1
lossyrob,and run integration tests on docker containers,5/14/15,1
lossyrob,that way we can troubleshoot things locally,5/14/15,1
pomadchin,hm nice,5/14/15,1
lossyrob,and then maybe develop a way to run the containers on ec2 or google cloud,5/14/15,1
pomadchin,"all in all hope this small sample would work on your local machine : D ** testing standalone cluster ingesting now",5/14/15,1
pomadchin,reduced memory for accumulo,5/14/15,1
pomadchin,and provided twonode cluster branch,5/14/15,1
pomadchin,with 512mb accumulo settings,5/14/15,1
pomadchin,hope it would be a successfull run next time :)),5/14/15,1
lossyrob,"ok, will try now",5/14/15,1
pomadchin,how it looks like ingesting now (http://104.155.79.182:50095),5/14/15,1
pomadchin,http://104.155.79.182:4040/environment/,5/14/15,1
pomadchin,http://104.155.79.182:4040/jobs/,5/14/15,1
pomadchin,"```15/05/14 21:32:23 INFO BlockManagerMaster: Updated info of block broadcast_4_piece015/05/14 21:32:23 INFO BlockManagerInfo: Removed broadcast_4_piece0 on master1.owm:56076 in memory (size: 4.0 KB, free: 4.1 GB)15/05/14 21:32:23 INFO ContextCleaner: Cleaned broadcast 4```",5/14/15,1
lossyrob,@pomadchin this is waht I_Ñém getting: http://pastebin.com/n3KLEnqG,5/14/15,1
pomadchin,"yep, same issue",5/14/15,1
pomadchin,this what i wanted to show you :D,5/14/15,1
pomadchin,"btw, same happens on a prod cluster not in a docker container",5/14/15,1
lossyrob,it seems different then the timeout exception,5/14/15,1
lossyrob,doesn_Ñét do any work_Ñ_i was pointing it at bogus files too,5/14/15,1
pomadchin,https://gist.github.com/pomadchin/e5178d477e3f93cef6db,5/14/15,1
pomadchin,same issue,5/14/15,1
pomadchin,(in the end of the gist document),5/14/15,1
lossyrob,they are slightly different,5/14/15,1
lossyrob,let me point it at actual files and see what happens,5/14/15,1
pomadchin,provide you tiles I ingesting? kk,5/14/15,1
lossyrob,no I have osme times,5/14/15,1
lossyrob,*tiles,5/14/15,1
lossyrob,it doesn_Ñét even start ingesting,5/14/15,1
pomadchin,wow,5/14/15,1
lossyrob,here_Ñés the full output http://pastebin.com/pDfbs7kX,5/14/15,1
lossyrob,ends with,5/14/15,1
pomadchin,it is a single node ?,5/14/15,1
lossyrob,"```15/05/14 22:09:16 INFO ClientCnxn: Socket connection established to localhost/127.0.0.1:2181, initiating session15/05/14 22:09:16 INFO ClientCnxn: Unable to read additional data from server sessionid 0x0, likely server has closed socket, closing socket connection and attempting reconnect15/05/14 22:09:18 INFO ClientCnxn: EventThread shut down15/05/14 22:09:18 INFO ZooKeeper: Session: 0x0 closedException in thread ""main"" java.lang.RuntimeException: Failed to connect to zookeeper (localhost) within 2x zookeeper timeout period 30000	at org.apache.accumulo.fate.zookeeper.ZooSession.connect(ZooSession.java:123)	at org.apache.accumulo.fate.zookeeper.ZooSession.getSession(ZooSession.java:167)	at org.apache.accumulo.fate.zookeeper.ZooReader.getSession(ZooReader.java:39)	at org.apache.accumulo.fate.zookeeper.ZooReader.getZooKeeper(ZooReader.java:43)	at org.apache.accumulo.fate.zookeeper.ZooCache.getZooKeeper(ZooCache.java:58)	at org.apache.accumulo.fate.zookeeper.ZooCache.retry(ZooCache.java:158)	at org.apache.accumulo.fate.zookeeper.ZooCache.get(ZooCache.java:291)	at org.apache.accumulo.fate.zookeeper.ZooCache.get(ZooCache.java:237)	at org.apache.accumulo.core.client.ZooKeeperInstance.getInstanceID(ZooKeeperInstance.java:168)	at org.apache.accumulo.core.client.ZooKeeperInstance.<init>(ZooKeeperInstance.java:158)	at org.apache.accumulo.core.client.ZooKeeperInstance.<init>(ZooKeeperInstance.java:139)	at org.apache.accumulo.core.client.ZooKeeperInstance.<init>(ZooKeeperInstance.java:86)	at geotrellis.spark.io.accumulo.BaseAccumuloInstance.<init>(AccumuloInstance.scala:36)	at geotrellis.spark.io.accumulo.AccumuloInstance$.apply(AccumuloInstance.scala:27)	at geotrellis.spark.ingest.AccumuloIngestCommand$.main(AccumuloIngestCommand.scala:35)	at geotrellis.spark.ingest.AccumuloIngestCommand$.main(AccumuloIngestCommand.scala:27)	at com.quantifind.sumac.ArgMain$class.mainHelper(ArgApp.scala:45)	at com.quantifind.sumac.ArgMain$class.main(ArgApp.scala:34)	at geotrellis.spark.ingest.AccumuloIngestCommand$.main(AccumuloIngestCommand.scala:27)	at geotrellis.spark.ingest.AccumuloIngestCommand.main(AccumuloIngestCommand.scala)	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)	at java.lang.reflect.Method.invoke(Method.java:606)	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:569)	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:166)	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:189)	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:110)	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)```",5/14/15,1
lossyrob,no twonodes,5/14/15,1
pomadchin,oh,5/14/15,1
pomadchin,sec,5/14/15,1
pomadchin,will try it,5/14/15,1
pomadchin,fixed,5/14/15,1
pomadchin,zookeepers didnot started,5/14/15,1
lossyrob,what do I have to rebiuld?,5/14/15,1
pomadchin,hadoop-nn-dn,5/14/15,1
pomadchin,"just wait before accumulo web ui starts, then start jobs",5/14/15,1
lossyrob,it_Ñés running,5/14/15,1
pomadchin,nice,5/14/15,1
pomadchin,how much you gave to the virtual machine?,5/14/15,1
lossyrob,seems like it_Ñés hung on 15/05/14 22:30:27 INFO ContextCleaner: Cleaned broadcast 2,5/14/15,1
lossyrob,I gave 8 GB,5/14/15,1
pomadchin,"nice, so yes, accumulo was stealing memory ._.'",5/14/15,1
lossyrob,still hung on that spot,5/14/15,1
lossyrob,"and that_Ñés where you find the timeout logs, after a ContextCleaner: Cleaned broadcast message, riht?",5/14/15,1
pomadchin,yes,5/14/15,1
pomadchin,just wait about half a minute,5/14/15,1
pomadchin,or smth like,5/14/15,1
pomadchin,"then there would be `INFO ClientCnxn: Client session timed out, have not heard from server in Nms for sessionid ...` ",5/14/15,1
lossyrob,"that makes sense, in that there_Ñés something hanging after the ingest is done",5/14/15,1
lossyrob,"so the question is, why doesn_Ñét it finish up and stop when it_Ñés done",5/14/15,1
pomadchin,):,5/14/15,1
lossyrob,"I_Ñém stuck on broadcast 2, you log says broadcast 4 (after cleaning over 40), not sure if it_Ñés hanging on the last couple of cleanups",5/14/15,1
lossyrob,(still hasn_Ñét timed out for me),5/14/15,1
pomadchin,"yep; so different tasks hanging on different cleanups",5/14/15,1
lossyrob,"wondering if there are actually some to clean up, or if it_Ñés hanging on something after the cleanup",5/14/15,1
pomadchin,"also, didnot tried yet catalog refactor with accumulo 1.5.2 commit, to find out is it caused only by accumulo",5/14/15,1
lossyrob,you said you were running something similar before catalog refactor without this problem?,5/14/15,1
pomadchin,"absolutely everything the same; but geotrellis was https://github.com/geotrellis/geotrellis/tree/1be6c7f73f067e7dc2fdc9bac87c46a13651fad8and accumulo 1.5.2even docker containers were the same, and hdfs , spark , zookeepers ...",5/14/15,1
lossyrob,very strange. I_Ñéll have to look more into it,5/14/15,1
pomadchin,yep,5/14/15,1
pomadchin,tommorow I can provide you accumulo 1.5.2 containers version,5/14/15,1
pomadchin,"btw according to web ui, to hdfs, zookeepers, accumulo, spark logs everything is okfails only on geotrellis jobs ",5/14/15,1
pomadchin,):,5/14/15,1
lossyrob,"yeah, weird we_Ñére not seeing anything similiar on our deployments",5/14/15,1
pomadchin,"yep, seems to me cfgs differ or smth like ~~",5/14/15,1
lossyrob,but we_Ñéll figure it out. thanks for the help @pomadchin ,5/14/15,1
pomadchin,"kk, thank you too @lossyrob ",5/14/15,1
pomadchin,"reduced memory on master branch too, so it is possible to try three nodes now",5/15/15,1
mfsny,"Just a short question: Does the HadoopIngestCommand support EPSG 3857 only? Because I have a GeoTIFF file in two version, one with EPSG:3857 and one with EPSG:27700 (British). And while the ingest works fine with 3857, it fails on 27700 with ``Exception in thread ""main"" geotrellis.vector.ExtentRangeError: x: 2079796.0181885576 to -377867.36589647504``",5/15/15,1
pomadchin,"@mfsny souds like `geotrellis` bug, `ESPG: 3857` is a target projection.",5/15/15,1
pomadchin,"tested with a sinusoidal projection, didn_Ñét check the quality of ingesting, but it didn_Ñét failed with an exception",5/15/15,1
mfsny,"Thanks for the info. In the meantime I tried another version of the same file, this time w/o explicit AUTHORITY EPSG:27700 in WKT and got: ``Exception in thread ""main"" org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0, localhost): java.lang.IllegalStateException: Latitude is out of range: 2687.7197816086677        at org.osgeo.proj4j.datum.GeocentricConverter.convertGeodeticToGeocentric(GeocentricConverter.java:100)``",5/15/15,1
mfsny,"So it looks like we'd need to convert all out files to EPSG:3857 before ingesting, at least until this is fixed.",5/15/15,1
pomadchin,yep,5/15/15,1
pomadchin,"btw there were some issues with reprojecting, don_Ñét remember exactly",5/15/15,1
mfsny,"good to know, thanks. at least I do not have to search for the issue on my side.",5/15/15,1
pomadchin,"just tested https://github.com/geotrellis/geotrellis/commit/8266b96d9b59cb9106a6fd5146040afd3962bfeawith accumulo 1.5.2 and it works perfect :D ```15/05/15 11:09:58 INFO ContextCleaner: Cleaned broadcast 3```",5/15/15,1
pomadchin,so the bug is not in migrating to accumulo 1.6.2 ,5/15/15,1
pomadchin,"can publish accumulo 1.5.2 containers, but it seems to me it is unnecessary; so yes, seems problem in catalog refactor ):",5/15/15,1
pomadchin,*happend after catalog refactor,5/15/15,1
pomadchin,"we noticed in zookeeper logs on this laggy step (`cleaned broadcast lag`):```[myid:2] - INFO  [ProcessThread(sid:2 cport:-1)::PrepRequestProcessor@645] - Got user-level KeeperException when processing sessionid:0x34d57cbe7f10002 type:create cxid:0x3da7 zxid:0x300000171 txntype:-1 reqpath:n/a Error Path:/accumulo/9882ea84-d86c-4d52-8789-ac9cffb2425c/table_locks/!0 Error:KeeperErrorCode = NoNode for /accumulo/9882ea84-d86c-4d52-8789-ac9cffb2425c/table_locks/!0```is it an ordinary behaviour for zookeeper ?",5/15/15,1
lossyrob,"@mfsny currently we only support ingests of rasters into a tile layout of the world. for some projections that doesn_Ñét work, since the world extent is invalid in that projection. there_Ñés an issue to fix that. but you shouldn_Ñét have to convert the files beforehand, you just need to ingest to a projection that supports world extent",5/15/15,1
lossyrob,https://github.com/geotrellis/geotrellis/blob/master/spark/src/main/scala/geotrellis/spark/tiling/package.scala#L39,5/15/15,1
lossyrob,here_Ñés the issue,5/15/15,1
lossyrob,https://github.com/geotrellis/geotrellis/issues/1030,5/15/15,1
lossyrob,"@pomadchin that_Ñés good to know. We can scour the Accumulo 1.6 PR to see if it_Ñés something from the GeoTrellis side, or perhaps it_Ñés actually Accumulo 1.6_Ñ_to be determined ",5/15/15,1
lossyrob,"for now, I_Ñém headed to the woods for a long weekend and am going to disconnect. I_Ñéll be back on on Sunday! have a good weekend everyone",5/15/15,1
pomadchin,"@lossyrob oops, geotrellis/geotrellis@8266b96 not works too, so think the problem not in accumulo pr, but in a catalog refactor pr",5/15/15,1
notthatbreezy,later @lossyrob! have a good weekend!,5/15/15,1
pomadchin,"@lossyrob yep, good weekend :)",5/15/15,1
mfsny,"Has anybody got a code snippet for reading a shapefile and returning it as PNG? The labs1-demo  at http://www.azavea.com/blogs/labs/2013/03/developing-a-kernel-density-service-with-geotrellis/ uses 0.8.0 and the API is not compatible with 0.10.0-SNAPSHOT, I guess.",5/18/15,1
lossyrob,I can point you to the relevant classes,5/18/15,1
lossyrob,"https://github.com/geotrellis/geotrellis/blob/master/geotools/src/main/scala/geotrellis/geotools/ShapeFileReader.scala - This will read the shapefile. It_Ñés in the `geotrellis-geotools` subproject, so you_Ñéll have to `publish-local` on that subproject and declare it as a dependency to use it",5/18/15,1
lossyrob,"At that _ÑÉpoint_Ñé you should have a set of points. Since this is reading in a whole shapefile, it_Ñés probably not that many points, right? So you probably don_Ñét need the spark stuff",5/18/15,1
lossyrob,"And if you did, we wouldn_Ñét be quite up to date_Ñ_the Kernel Density operation is missing on RDD currently",5/18/15,1
lossyrob,but we do have it in `geotrellis.raster`,5/18/15,1
lossyrob,https://github.com/geotrellis/geotrellis/blob/master/raster/src/main/scala/geotrellis/raster/VectorToRaster.scala#L26,5/18/15,1
lossyrob,"`geotrellis.raster.VectorToRaster(pointsFromShapeFile, kernel, rasterExtent)`",5/18/15,1
lossyrob,"what you read from the shapefile should be `PointFeature[Int]`, so you would use `ShapeFile.readPointFeatures[Int](path, dataField)`",5/18/15,1
lossyrob,"then to render to PNG, `import geotrellis.raster.render._` and use something like `kernelDensityResult.renderPng(ColorRamps.GreenToRedOrange)`. That gives you a PNG instance, with a `bytes` member that is the bytes of the PNG. You can write it with `png.write(path)`",5/18/15,1
lossyrob,@mfsny let me know if that helps,5/18/15,1
lossyrob,"@allixender https://github.com/geotrellis/geotrellis/issues/1099 is particularly interesting because if it_Ñés a similar error to what @pomadchin is runnign into with the docker containers, we have two platforms that exhibit that bug (while our `geotrellis-ec2-cluster` code does not), and would provide some good filters on what to look for while trying to debug that.",5/18/15,1
mfsny,"@lossyrob Thanks a lot, that gives me some idea where to start. Though I am having trouble with the first step already. After adding ```geottrellis-geotools`` to the ``build.sbt`` file, I get: ``sbt.ResolveException: unresolved dependency: javax.media#jai_core;1.1.3: configuration not found in javax.media#jai_core;1.1.3: 'master(compile)'. Missing configuration: 'compile'. It was required from org.geotools#gt-epsg-hsql;11.0 compile``. The relevant lines from build.sbt : ``libraryDependencies ++= Seq( ... ""javax.media"" %% ""jai_core"" % ""1.1.3"" from ""http://download.osgeo.org/webdav/geotools/javax/media/jai_core/1.1.3/jai_core-1.1.3.jar"", ...)`` and ``resolvers ++= Seq(""Geotools"" at ""http://download.osgeo.org/webdav/geotools/"")``.",5/19/15,1
mfsny,Problem solved. had to delete ``javax.media....jar`` from ivy cache.,5/19/15,1
allixender,@lossyrob @pomadchin geotrellis/geotrellis#1099 seems to be solved for me in fact based on the provided libs/spark distro,5/19/15,1
allixender,"it is also odd that the file lists filter in HdfsUtils would only filter on `ne*.tif` files, not sure where this actually originates. Maybe this was only  added by @kyeah in the feature/cassandra branch?",5/19/15,1
mfsny,"@lossyrob so I was able to draw a PNG based on the description you provided (with a small addition required due to the fact, that kernelDensity returns a Tile and renderPng does not work with Tiles, so I had to create a RasterSource based on Tile and Extent and then I could use renderPng on this rasterSource). Thanks again. Now my colleague gave me  a different shape file which contains MultiPolygons. I can read it, no problems there. But what I do not really understand is how to use rasterize on multiPolygons. The first parameter is Geometry and I do not really get it how to convert from multiPolygons to Geometry. Sorry but thanks to my poor knowledge of scala and almost no documentation , especially for 0.10, I am forced to use this channel to get any further in evaluation of geotrellis for our project.",5/19/15,1
lossyrob,@mfsny renderPng does work with Tiles https://github.com/geotrellis/geotrellis/blob/master/raster/src/main/scala/geotrellis/raster/render/RenderMethods.scala#L34,5/19/15,1
lossyrob,That method gets implicitly added when you `import geotrellis.raster.render._`,5/19/15,1
lossyrob,Depends on what you want to do with the multipolygons?,5/19/15,1
lossyrob,"If you want to run a _ÑÒkernel density_Ñù i.e. a convolution on the multipolygons, then you can do that, but it_Ñés trickier and there_Ñés no nice single function call to do it",5/19/15,1
lossyrob,if you let me know what you_Ñére trying to do I_Ñéll try to help.,5/19/15,1
lossyrob,"p.s. MultiPolygons are Geometries, so no conversion necessary (https://github.com/geotrellis/geotrellis/blob/master/vector/src/main/scala/geotrellis/vector/MultiPolygon.scala#L38, MultiPolygon <: MultiGeometry <: Geometry, similar to JTS)",5/19/15,1
lossyrob,"@mfsny  And it_Ñés fine to use this channel for help. We don_Ñét have documentation for 0.10 because we_Ñére a small team and it_Ñés still unreleased. We had some documentation added to README_Ñés in the various packages in the codebase, I_Ñém wondering if that is helpful to you. Otherwise, we plan on focusing on documentation, and perhaps we should be doing a better job of it as we go, but just getting the features coded enough to get 0.10 release-ready is a feat for the couple of people we have dedicated to this project. If you find something undocumented and want to solve that, you can make an issue for it on GitHub, and also PR_Ñés are welcome.",5/19/15,1
mfsny,"Thanks, Rob! As for Geometry I must have been blind, but now I get it and it's clear to me. And as for renderPng for Tile, it was the missing ""import"". ",5/19/15,1
mfsny,"Regarding MultiPolygons, I have a shapefile with many MultiPolygons and want to use spray to display these over the map, very much like you did with raster data in Chattanooga demo. I am using ``VectorToRaster.rasterize(multiPolygons(0).geom,rasterExtent,10)`` but it works for one Polygon only. As you said, I must write my own code, there is no single API call for that at the moment. But thanks for the hint anyways.",5/19/15,1
mfsny,"What we are trying to figure out is the feasibility of the following architecture with Geotrellis: * ingest GeoTIFF files into Hadoop (hdfs at the moment, in future we should consider Accumulo I guess). * spray as webserver can access the ingested geodata in HDFS and serve different HTTP requests, e.g. draw the raster/vector in a map * optionally, some calculation is done by spark on ingested geodata and the results are used by spray to serve the requests.",5/19/15,1
lossyrob,"For drawing polygons on a leaflet map: we just use vector stuff to do that, and skip the raster",5/19/15,1
lossyrob,"if you look at the chatta demo you can see how that_Ñés done, I_Ñéll look up the code in a sec",5/19/15,1
lossyrob,"but also, how many vectors are you trying to draw? If it_Ñés a lot on a single map, then you might want to do vector tiling, which GeoTrellis does not do (and is not at least on the immediate roadmap, unless we get a contribution)",5/19/15,1
lossyrob,"but if it_Ñés not that many, you can just use Leaflet GeoJSON to display the geometries on the mpa",5/19/15,1
lossyrob,"hmm we actually just use LeafletDraw for displaying the polygons on the map, since they are user input and not from the server (they are sent to the server as part of the request)",5/19/15,1
mfsny,in the current file it is 781 polygons.,5/19/15,1
lossyrob,"Here_Ñés somehting similar that_Ñés in an old commit of gt-admin, just to display some geojson on disk https://github.com/geotrellis/gt-admin/blob/920427f0ee228312e6e84227e959ecbd5a6b546f/server/src/main/scala/geotrellis/admin/server/CatalogService.scala#L259",5/19/15,1
lossyrob,"once you have the MultiPolygons,```import geotrellis.vector.io.json._import spray.json._val mp: MultiPolygon = ???mp.toJson```",5/19/15,1
lossyrob,something like that,5/19/15,1
lossyrob,`geotrellis.vector.io.json` has all of the implicits that make `spray-json` be able to convert Geometries or features to/from GeoJSON,5/19/15,1
lossyrob,"`mp.toJson` gives the actual Spray JsonValue. So to send it back through a spray route, you would need to do",5/19/15,1
lossyrob,"```mp.toJson.asJsObject```",5/19/15,1
lossyrob,and then that would get mashelled through.,5/19/15,1
lossyrob,"if you want the actual GeoJSON string, you would just do `mp.toGeoJson`, which is defined here: https://github.com/geotrellis/geotrellis/blob/master/vector/src/main/scala/geotrellis/vector/io/json/package.scala#L27",5/19/15,1
lossyrob,there_Ñés also this README https://github.com/geotrellis/geotrellis/blob/master/vector/src/main/scala/geotrellis/vector/io/json/README.md,5/19/15,1
mfsny,Thanks Rob! I'll give it a try,5/19/15,1
pomadchin,"@lossyrob just pulled latest commit, seems to me now it is all working in containers published o: though, I didn_Ñét changed anything in its cfg, now testing ingest jobs: http://104.155.79.182:8080/ ",5/20/15,1
pomadchin,"btw, got a `com.esotericsoftware.kryo.KryoException: Buffer underflow.` error in `gt-admin` while trying to get tms image or get pixel",5/20/15,1
pomadchin,https://gist.github.com/pomadchin/fc23d2658a75e4215e69,5/20/15,1
pomadchin,"After ingest, it is not possible to get historgram info, mb these errors could be connected. (I can provide prepeared tiff to load)`val histogram = attributeStore.read[Histogram](layer, ""histogram"")`",5/20/15,1
mfsny,"Hi, can anybody help with CRS reprojection, pls? I have my data in the British EPSG:27700 and my leaflet is using WSG84 for passing bbox. I tried following first: ```val src = CRS.fromName(""EPSG:3857"")val dst = CRS.fromName(""EPSG:27700"")val oldx = Extent.fromString(""-39135.75848201024,6692214.700423751,0,6731350.458905762"")val newx = oldx.reproject(src,dst)``` but all I get is ``geotrellis.vector.ExtentRangeError: y: -1.3747887674479786E7 to -1.374961923978705E7``. ",5/20/15,1
mfsny,"So I checked the proj4 string in /nad/epsg and it looks like the value (``<27700> +proj=tmerc +lat_0=49 +lon_0=-2 +k=0.9996012717 +x_0=400000 +y_0=-100000 +datum=OSGB36 +units=m +no_defs  <>``) is not correct. I found some other defs online and this one worked the best: ``+proj=tmerc +lat_0=49 +lon_0=-2 +k=0.9996012717 +x_0=400000 +y_0=-100000 +ellps=airy +towgs84=446.448,-125.157,542.06,0.15,0.247,0.842,-20.489 +units=m +no_defs``. But it has still some southwards.",5/20/15,1
mfsny,"I get ``514302.4577355564,188783.730894182,537991.4618278733,213686.72817825677`` when I reproject using geotrellis.  http://cs2cs.mygeodata.eu/ says it should be : ``514771.247816,167974.826075,538560.963803,192915.540317`` . ",5/20/15,1
lossyrob,can you give me some geojson of the original?,5/20/15,1
lossyrob,"@pomadchin that_Ñés awesome news that the containers are working. @echeipesh do you have any insight on what might have happened? It seems like it would be your PR that would fix it. Also Eugene is looking into gt-admin in the near future, so that_Ñés good info about the buffer underflow",5/20/15,1
echeipesh,double checked this with `cs2cs` and it_Ñés not giving the same results as proj4j even using `CRS.fromString`,5/20/15,1
echeipesh,"hmm, not sure what would have changed with the PR",5/20/15,1
lossyrob,ah right the Extents are right there,5/20/15,1
lossyrob,might be a Proj4j bug,5/20/15,1
lossyrob,but are we sure the extent is valid in the new projection,5/20/15,1
echeipesh,You mean the source projection?,5/20/15,1
pomadchin,(:,5/20/15,1
pomadchin,and no info about buffer underflow? ,5/20/15,1
pomadchin,"tried to increase memory for kryo serializer, didnt helped ):",5/20/15,1
pomadchin,"oh sry, internet connection problems, see this post about it ._., Ty :d",5/20/15,1
lossyrob,@pomadchin no that seems like gt-admin is using the new stuff wrong,5/20/15,1
lossyrob,the catalog refactor code,5/20/15,1
lossyrob,let me check it out,5/20/15,1
lossyrob,@pomadchin are you up to date on gt-admin?,5/20/15,1
lossyrob,taht stack trace points to a non-existant line in CatalogService: https://github.com/geotrellis/gt-admin/blob/master/server/src/main/scala/geotrellis/admin/server/CatalogService.scala,5/20/15,1
pomadchin,just used some modified code ,5/20/15,1
lossyrob,what_Ñés the line say that it_Ñés pointing to?,5/20/15,1
lossyrob,geotrellis.admin.server.CatalogService$$anonfun$pixelRoute$1.apply(CatalogService.scala:332),5/20/15,1
pomadchin,"`          val rdd = catalog.reader[SpaceTimeKey].read(layer, FilterSet(SpaceFilter[SpaceTimeKey](key.col, key.row)))`",5/20/15,1
lossyrob,k thanks,5/20/15,1
pomadchin,"and the 324 just route defi: `        parameters('name, 'zoom.as[Int], 'x.as[Double], 'y.as[Double]) { (name, zoom, x, y) =>`",5/20/15,1
lossyrob,it fails on reading in the index. Were the layers ingested with the new code/,5/20/15,1
lossyrob,?,5/20/15,1
lossyrob,"`attributeStore.read[KeyIndex[K]](layerId, ""keyIndex_Ñù)`",5/20/15,1
pomadchin, ,5/20/15,1
pomadchin,yes,5/20/15,1
lossyrob,with what ingest command?,5/20/15,1
lossyrob,"one of the ingests in gt-admin (if so which one), or in geotrellis.spark.ingest (if so which one)?",5/20/15,1
pomadchin,Own ingest: ,5/20/15,1
pomadchin,https://gist.github.com/pomadchin/e7f7719ac215856a24f5,5/20/15,1
lossyrob,"ok, so this seems to be the issue",5/20/15,1
lossyrob,"ZCurveKeyIndexMethod.by { dt => s""${dt.getYear}${dt.getDayOfYear}"".toInt }",5/20/15,1
lossyrob,"I had tested serializing and deserailizing anonymous functions like that, and it worked (to my surprise)",5/20/15,1
lossyrob,"it seems like I either my tests might have been faulty, or they work in one situation and not the other",5/20/15,1
lossyrob,I_Ñém wondering if code like this will narrow the bug down,5/20/15,1
pomadchin,"hm, and is this approach to make ingest correct? Wanted layers by days",5/20/15,1
lossyrob,"```accumuloAttributeStore.write(layerId, _ÑÒkeyIndexTest_Ñù, ZCurveKeyIndexMethod.by { dt => s""${dt.getYear}${dt.getDayOfYear}"".toInt })val index = accumuloAttributeStore.read[KeyIndex[SpaceTimeKey]](layerId, _ÑÒkeyIndexTest_Ñù)```",5/20/15,1
lossyrob,layers by days,5/20/15,1
lossyrob,can you explain what you mean by that more? I think there_Ñés a misunderstanding of the index ,5/20/15,1
pomadchin,"Hmm yes mb, before it was just space time ingest",5/20/15,1
pomadchin,after api changing there was such thing: byYear,5/20/15,1
lossyrob,the index is actually an internal storage mechanism. We exposed the indexing through the API because the performance of how the tiles are stored and retrieved is very dependant on the type of data being stored (and the index range),5/20/15,1
lossyrob,"but this doesn_Ñét effect anything outside of the internal storage mechanisms, and isn_Ñét anything past a knob to turn to tune performance.",5/20/15,1
pomadchin,"Yep, the idea is that I need to store the whole map every 18 days",5/20/15,1
lossyrob,"we weren_Ñét sure how to make this clear in the API, so for now we put it as a necessary param to write",5/20/15,1
lossyrob,but are you trying to break up your ingested raster into different layers? or just query by time ranges?,5/20/15,1
pomadchin,just query by time ranges,5/20/15,1
lossyrob,ok gotcha,5/20/15,1
lossyrob,and whats the startTime-endTime of the whole range?,5/20/15,1
lossyrob,we use byYear since the data we_Ñére working with currently is 2005-2099,5/20/15,1
lossyrob,also we are doing benchmarking tests to see if there is a better indexing scheme (like HilbertKeyIndexMethod),5/20/15,1
pomadchin,smth like 2005-2015,5/20/15,1
pomadchin,and what exactly means this time part? It is incorrect to make such 'day' index?,5/20/15,1
lossyrob,you could def make a day part,5/20/15,1
lossyrob,maybe something like says since minDate,5/20/15,1
lossyrob,i_Ñém testing the serializination issue now,5/20/15,1
pomadchin,"Kk will try to ingest by year now,",5/20/15,1
lossyrob,you could also try the HilbertKeyIndexMethod,5/20/15,1
pomadchin,kk (: ,5/20/15,1
lossyrob,HilbertKeyIndexMethod(5) // with temporalResolution of 5 (2^5),5/20/15,1
lossyrob,This method passes in `AccumuloAttributeStoreSpec`,5/20/15,1
lossyrob,"```      it(""should save and load a KeyIndexMethod with an anonymous function"") {        val indexMethod = ZCurveKeyIndexMethod.by { dt => s""${dt.getYear}${dt.getDayOfYear}"".toInt }        val minKey = SpaceTimeKey(200, 400, new DateTime(2000, 1, 1, 0, 0, 0, DateTimeZone.UTC))        val maxKey = SpaceTimeKey(200, 400, new DateTime(2020, 1, 1, 0, 0, 0, DateTimeZone.UTC))        val index = indexMethod.createIndex(KeyBounds(minKey, maxKey))        attribStore.write(layerId, ""keyIndex"", index)        val actual = attribStore.read[KeyIndex[SpaceTimeKey]](layerId, ""keyIndex"")        val key = SpaceTimeKey(200, 400, new DateTime(2010, 1, 1, 0, 0, 0, DateTimeZone.UTC))        actual.toIndex(key) should be (index.toIndex(key))      }```",5/20/15,1
lossyrob,so I_Ñém pretty confused,5/20/15,1
lossyrob,maybe it_Ñés something to do with the in-memory accumulo,5/20/15,1
pomadchin,loading by year,5/20/15,1
lossyrob,K. that won_Ñét be good for indexing rasters with day-resolution,5/20/15,1
pomadchin,yep,5/20/15,1
pomadchin,"im trying to load one day, only some tiles",5/20/15,1
pomadchin,this would take about 5 minutes,5/20/15,1
lossyrob,ok,5/20/15,1
pomadchin,same error ._.,5/20/15,1
pomadchin,"loaded ```val writer = AccumuloRasterCatalog().writer[SpaceTimeKey](      ZCurveKeyIndexMethod.byYear, args.table    )```",5/20/15,1
lossyrob,"ok. we_Ñére using ZCurveKeyindexMethod.byYear in our cluster tests, so that_Ñés not the problem",5/20/15,1
pomadchin,yep,5/20/15,1
lossyrob,"can you try a test similar to the unit test I made, or the one I mentioned, against your accumulo? Just writing and reading a KeyIndex, to see if it_Ñés something with the setup",5/20/15,1
lossyrob,you could just plop that code into the pixel route and see if the exception is thrown there,5/20/15,1
pomadchin,"ok sure, in ten minutes ): g to go for a few minutes, will be back soon",5/20/15,1
pomadchin,:D toilet_Ñ_.,5/20/15,1
pomadchin,"got error here: `val actual = attributeStore.read[KeyIndex[SpaceTimeKey]](layer, ""keyIndex"")`",5/20/15,1
pomadchin,same err,5/20/15,1
pomadchin,"btw, before, in the afternoon tried to make spark settings: ```spark.serializer                   spark.KryoSerializer        spark.kryoserializer.buffer.mb     256spark.akka.frameSize               500spark.akka.askTimeout              30```so it was not successful as you can guess ):",5/20/15,1
lossyrob,hmm,5/20/15,1
pomadchin,tried to perform it as a single job (had a thought mb there is smth with spray deps),5/20/15,1
pomadchin,got same error,5/20/15,1
pomadchin,"you can use containers : D, will publish them to the hub soon, so it will be possible just to run them via single (two) line script ",5/20/15,1
lossyrob,https://www.youtube.com/watch?v=7J61pPG9j90&hd=1,5/20/15,1
allixender,ag - the silver searcher :+1: ,5/21/15,1
lossyrob,:feelsgood:,5/21/15,1
notthatbreezy,"yeah, ag is sweet -- meant to mention this (for emacs users) -- projectile is a great project that makes searching for files and searching in all files in a project great",5/21/15,1
notthatbreezy,https://github.com/bbatsov/projectile,5/21/15,1
notthatbreezy,has support for using ag too,5/21/15,1
lossyrob,"@notthatbreezy in R studio```geom_smooth: method=""auto"" and size of largest group is <1000, so using loess. Use 'method = x' to change the smoothing method.Error in grid.Call(L_textBounds, as.graphicsAnnot(x$label), x$x, x$y,  :   polygon edge not foundIn addition: Warning messages:1: In grid.Call(L_textBounds, as.graphicsAnnot(x$label), x$x, x$y,  :  no font could be found for family ""Liberation Serif""2: In grid.Call(L_textBounds, as.graphicsAnnot(x$label), x$x, x$y,  :  no font could be found for family ""Liberation Serif_Ñù```",5/21/15,1
lossyrob,am I missing something? I installed `extrafont`,5/21/15,1
notthatbreezy,"oh -- yeah, eugene ran into that too",5/21/15,1
echeipesh,I picked a different font,5/21/15,1
notthatbreezy,"yeah, that's the solution :smile: ",5/21/15,1
lossyrob,hahaa,5/21/15,1
lossyrob,nice,5/21/15,1
echeipesh,I think that_Ñés some linux font that is supposed to look Courie New,5/21/15,1
lossyrob,do you know what the name was? how did you find a list?,5/21/15,1
echeipesh,according to wiki it_Ñés Times New Roman: http://en.wikipedia.org/wiki/Liberation_fonts,5/21/15,1
lossyrob,thanka,5/21/15,1
echeipesh,"got the query interface worked out, it really makes the writer interface stand out like a sore thumb.",5/21/15,1
mfsny,"@lossyrob ... sure, here is the GeoJson for the ARG file I am trying to process: ````",5/21/15,1
mfsny,"```````{    ""cellwidth"": 5.0,     ""layer"": ""River09Q100B_London"",     ""projection"": ""+proj=tmerc +lat_0=49 +lon_0=-2 +k=0.9996012717 +x_0=400000 +y_0=-100000 +datum=OSGB36 +units=m +no_defs "",     ""cols"": 9819,     ""xskew"": 0.0,     ""yskew"": 0.0,     ""xmin"": 505335.0,     ""ymin"": 153995.0,     ""cellheight"": 5.0,     ""rows"": 9561,     ""ymax"": 201800.0,     ""datatype"": ""int8"",     ""epsg"": 27700,     ""xmax"": 554430.0,     ""type"": ""arg""}``````",5/21/15,1
mfsny,"From my point of view it looks like the ``/nad/epsg`` contains a n invalid entry for EPSG:27700, the ``+ellps=airy `` is missing. And instead of ``+datum=OSGB36`` some people recommend to use ``+towgs84=446.448,-125.157,542.06,0.15,0.247,0.842,-20.489``.  But it still does not reproject correctly. As I said, the result is about  20km southwards. Proj4 bug?",5/21/15,1
mfsny,I have just got the ARG file from my colleague in EPSG:3857 which renders fine. It's geoJSON:,5/21/15,1
mfsny,,5/21/15,1
mfsny,"``````{    ""cellwidth"": 8.217552935837206,     ""layer"": ""River09Q100B_London_3857"",     ""projection"": ""+proj=merc +a=6378137 +b=6378137 +lat_ts=0.0 +lon_0=0.0 +x_0=0.0 +y_0=0 +k=1.0 +units=m +nadgrids=@null +wktext  +no_defs"",     ""cols"": 9811,     ""xskew"": 0.0,     ""yskew"": 0.0,     ""xmin"": -54689.64217137951,     ""ymin"": 6668160.857047593,     ""cellheight"": 7.826298525596598,     ""rows"": 10070,     ""ymax"": 6746971.683200351,     ""datatype"": ""int8"",     ""epsg"": 3857,     ""xmax"": 25932.769682119324,     ""type"": ""arg""}``````",5/21/15,1
lossyrob,@mfsny does sound like a Proj4j bug.,5/21/15,1
lossyrob,"Those are ARG json files, not GeoJSON..",5/21/15,1
mfsny,"@lossyrob thanks for opening an issue for this. I was not sure if this is really an issue or if I was doing something wrong. As you might have noticed, I am fairly new to the topic and currently in the process of ""learning by doing"", more or less. You asked for the geojson. All I've got is the ARG file with the JSON for both EPSG 27700 and 3857. And I have the same files in GeoTIFF format as well. Which format and which EPSG do you need? Though I can't guarantee that I can provide the files. I#m not sure if there isn't any license on them. Would need to clarify with my colleagues.",5/22/15,1
lossyrob,"No it seems like a legitimate issue, and unfortunately with proj4j.",5/22/15,1
lossyrob,"The GeoJSON I asked for was for the footprint you were reprojecting, but you had given me the Extents, so I have all I need",5/22/15,1
mfsny,"Cool. Why do you mean ""unfortunately"" ?",5/22/15,1
lossyrob,because it_Ñés harder to fix if it_Ñés not a GeoTrellis bug,5/22/15,1
lossyrob,"I understand a lot about proj4j, but it seems like a very specific bug if it_Ñés doing an uncommon projection incorrectly. So to fix it it will probably take reading through the java code a lot",5/22/15,1
mfsny,"well, it is just a coincidence, that the sample data for our evaluation of Geotrellis is in the British EPSG. so no high priority from our point of view.",5/22/15,1
lossyrob,oh. that is good to know.,5/22/15,1
mfsny,The colleagues can provide us with the same data in WSG84 and we can go ahead.,5/22/15,1
lossyrob,"we_Ñéve mainly used WSG84, EPSG:3857, and UTM projections, all which have worked without a problem (so far)",5/22/15,1
pomadchin,"@lossyrob have you tried modis sin projection?p.s. nothing heard about kryo err? ):",5/22/15,1
lossyrob,@pomadchin no we haven_Ñét.,5/22/15,1
lossyrob,We have assigned work to go over the gt-admin project; but that_Ñés one of many many things we_Ñére trying to do now :),5/22/15,1
jamesmcclain,"Hello, may I ask a question about setting up geotrellis?  I have cloned the code found here https://github.com/geotrellis/vagrant.geotrellis, setup the VM, and been able to successfully run `buildall.sh` in in the `/vagrant/geotrellis` directory.  My goal is to just get a ""hello world"" geotrellis app going, maybe similar to the one found here https://github.com/jamesmcclain/geotrellis.  What is my next step?",5/26/15,1
echeipesh,You should be able to start making an app that depends on `geotrellis-spark` and launching it with `spark-submit`.,5/26/15,1
echeipesh,"Probably that part of README could use some elebaration, going to mess around with it.",5/26/15,1
jamesmcclain,"Thank you for your response.  I see the sample code in the readme https://github.com/jamesmcclain/geotrellis, how do I actually run this?  My questions are very basic: what should be the name of the file that I paste this into, what command do I use to run it, et cetera.",5/26/15,1
lossyrob,@jamesmcclain I would recommend you start working with the web service tutorial: http://geotrellis.io/tutorials/webservice/spray/,5/26/15,1
jamesmcclain,"Ah, okay.  I will get started on that this afternoon.  Thank you for the pointer",5/26/15,1
vishalanand,"I am working on Kriging Interpolation in geotrellis and I have practiced coding Scala assignments in the Scala Eclipse IDE in Ubuntu 14.04.However I am facing problems setting up the geotrellis project in the Eclipse Scala IDE. Post adding a .project file (for importing into Eclipse) with the meta tags ```<projectDescription>  <name>geotrellis</name></projectDescription>```and after appending configurations in the `plugins.sbt` file in `geotrellis/projects/` (for scalatest Junit resolution ) with :```addSbtPlugin(""com.typesafe.sbteclipse"" % ""sbteclipse-plugin"" % ""3.0.0"")```the Scala IDE does not recognize the project properly.It does not prompt with red underscores on writing *incorrect code* nor does it import the junit scalatest (does not show `Run as JUnit`) for running the tests.![alt](http://i.imgur.com/Mva7HM6.png)However, I would also mention that I can add *""code, compile and test""* them via terminal (sbt and vim). But as compared to vim, I found working on Scala IDE faster (due to autofills and type popups for variables). Could someone give some pointers on how to make the project work with Scala IDE? Or if it is not often used, which IDE is generally used for geotrellis development?",5/26/15,1
echeipesh,"I use IntelliJ IDEAL time to time, it has quite good support, version 14 imported geotrellis project without any hassle. You will need to get the scala plugin, not sure if it's included by default.",5/26/15,1
vishalanand,"@echeipesh  is that the IntelliJ IDEA (L?)licensed version you are referring to, or is it the free version?",5/26/15,1
echeipesh,"They included scala support in community edition ""recently"". Not sure when exactly.",5/26/15,1
vishalanand,"Okay, I will hack into the IntelliJ community edition setup and get it up and running. Thanks!",5/26/15,1
pomadchin,@vishalanand you generated project after including eclipse plugin?,5/26/15,1
pomadchin,"the way intelij idea works -- just generating `idea` project using sbt files. year or two ago there was intelij idea plugin and it worked same way as an eclipse plugin, now it simply integrated in idea as an idea plugin",5/26/15,1
notthatbreezy,there are some notes on getting set up with intellij on a post from the listserv from a code sprint last fall https://groups.google.com/forum/#!msg/geotrellis-user/4R_QSEUZZ9A/i1ueshYynYMJ that may help,5/27/15,1
notthatbreezy,@allixender has also been trying to get eclipse set up -- he has some notes on an issue on github that might be of use https://github.com/geotrellis/geotrellis/issues/1100,5/27/15,1
notthatbreezy,I'm not super familiar with either eclipse or intellij though,5/27/15,1
vishalanand,"@pomadchin The Eclipse IDE was pre-built with the plugin (it is meant for Scala only development).@echeipesh @notthatbreezy  There were some hiccups, but I have now got the IntelliJ set up! Thanks! :)I was going through tutorials on importing projects in IntelliJ, and they refer to a build.sbt file. However `importing` via `external tools` (as SBT) in the IDE sorted it out.",5/27/15,1
notthatbreezy,awesome,5/27/15,1
allixender,"Hey folks, @jamesmcclain @vishalanand I am compiling my experiences with Eclipse and IDEA here https://github.com/geotrellis/geotrellis/issues/1100 , skip the upper part of the Sphinx docs and scroll further down. Basically with Eclipse you would add the `sbt-eclipse` plugin and generate the project files with `./sbt eclipse`, it will understand that there are in fact many sub projects/modules. Some of those project inter-dependencies won't be resolved properly. Haven't worked out why, but found a work around. For IDEA it seems in the newest versions it is in fact discouraged to use the `sbt-idea` plugin, but to import natively via IDEA. Both sort of works. I'd like to write up on that a bit more in-depth for the docs, so please feel free to comment on the issue :+1:  ",5/27/15,1
allixender,"also `ag - the_silver_searcher` is an amazingly powerful code navigation/search tool, particularly if you don't know where implicits come crawling through or the likes :smile: ",5/27/15,1
allixender,"SCALA IDE is ""just"" a special Eclipse build, you can add SCALA IDE also into your Java Eclipse via update site.",5/27/15,1
allixender,"@vishalanand after all I'd still recommend execute unit tests, compiling etc on the shell :smile_cat:  I'm working on the cassandra topic, but unit testing is also important here, so I keep documenting on that too https://github.com/geotrellis/geotrellis/issues/1097#issuecomment-105345222",5/27/15,1
allixender,"Particularly interesting if you want to run only single unit tests: ```Bash./sbt ""project spark"" 'test-only geotrellis.spark.io.cassandra.CassandraRasterCatalogSpec'```",5/27/15,1
vishalanand,"@allixender  Yeah, `sbt test-only` works great. I was working solely on the terminal the past few days! Here are some more [gems](http://www.scala-sbt.org/0.13/docs/Testing.html) on using sbt with terminal. However, the auto-fill features of IDEs are too good to ignore.I am also planning to add the whole setting up guide in a blog (have collated links and snips)!",5/27/15,1
allixender,"Good stuff! Just running testing import with IntelliJ IDEA, apparently really works just out of the box really. Setting up Eclipse with GeoTrellis was a bit more tricky. Of course, code completion and navigation with IDE is something I also don't want to miss, haha. Hint for us students, Jetbrains (the company developing the IDEA, PyCharm etc tools) has a free one year student license, where you can use all the professional products, if you like:https://www.jetbrains.com/student/license.html",5/27/15,1
echeipesh,@jamesmcclain It sounds like you may need to test out a geotrellis spark application soon. I tried to use `vagrant.geotrellis` run one of the ingest commands and ran into some issues. I_Ñéve made a PR: https://github.com/geotrellis/vagrant.geotrellis/pull/10 that will be helpful if you_Ñére looking at this today.,5/27/15,1
pomadchin,"@echeipesh have you tried to run some custom jobs, to get tiles out of Accumulo (read), on this vm?(just an example)",5/27/15,1
echeipesh,"no not yet, but that_Ñés a good thing to try next. ",5/27/15,1
pomadchin,"just have no time now ): mb only next week I can return to the geotrellis research ): now we have to use old api in prod (before catalog refactor pr); btw don't you know, the old ingest is compatible with the new api? (I mean the old space time ingest to read via new api)",5/27/15,1
lossyrob,nope the value we stored changed :/,5/27/15,1
echeipesh,the metadata too actually,5/27/15,1
lossyrob,"sorry this is changing so fast_Ñ_and is going to change again. @echeipesh developed a better filter API so some code will have to be updated. We_Ñére doing a lot of testing and working out a lot of kinks, and trying to improve as much as possible before feeling comfortable cutting a release. So it goes with living on the bleeding edge...",5/27/15,1
pomadchin,Just enough would be to test write and read on a real (single node) cluster :D,5/27/15,1
pomadchin,:( that's my life on a master branch ,5/27/15,1
pomadchin,:D,5/27/15,1
lossyrob,we_Ñére doing that on 200 node clusters,5/27/15,1
lossyrob,"with `geotrellis-ec2-cluster`, so we have to figure out what_Ñés going on with the other environments",5/27/15,1
pomadchin,"Yep, I know :)) meant not ec2 env",5/27/15,1
jamesmcclain,"Thank you for your answers to my previous questions, those were very helpful.  I have another question: for model my watershed, the initial problem that we need to solve with geotrellis is to essentially take population counts inside of polygons.",5/27/15,1
jamesmcclain,"In other words, we have some discrete raster data, and we would like to know the number of occurrences of each type in a query polygon.  It looks like that can be done by masking or clipping the area of interest against the tile set then doing a histogram over what is left.",5/27/15,1
jamesmcclain,Does that sound like a reasonable/efficient method for geotrellis?,5/27/15,1
echeipesh,That sounds like exactly the way to do it.,5/27/15,1
jamesmcclain,"Okay, cool.  What is the recommended way to do this clipping.  Essentially I want to take one or more polygons and clip them against the tile set for the entire US.",5/27/15,1
jamesmcclain,I guess what I am asking is: is there an easy way to restrict attention to some bounding box (say the bounding box of the query polygon)?  Or do I as a user even need to worry about that?,5/27/15,1
notthatbreezy,I think there is kind of an example of that here: https://github.com/geotrellis/benchmark/blob/master/spark/src/main/scala/geotrellis/cmd/Benchmark.scala,5/27/15,1
notthatbreezy,Some of the benchmarks are assessing how quickly relevant tiles can be pulled out of accumulo,5/27/15,1
notthatbreezy,for a polygon,5/27/15,1
jamesmcclain,"Okay, I'll have a look.  Thanks again for the help!",5/27/15,1
notthatbreezy,Also as @lossyrob mentioned -- the filtering API is going to improve shortly so it's not so clunky hopefully,5/27/15,1
jamesmcclain,"Okay, sounds exciting!",5/27/15,1
lossyrob,@pomadchin what is the data type of the rasters you are ingesting? (float32? int32?) we_Ñéve been using float32,5/27/15,1
lossyrob,@echeipesh @notthatbreezy @moradology finished GeoTiff reading/writing and Multiband in `geotrellis.raster`. If you could give it a look it would be appreciated. Happy to answer questions about it. https://github.com/geotrellis/geotrellis/pull/1066,6/1/15,1
pomadchin,"@lossyrob sry for too late response, Int16",6/1/15,1
pomadchin,"`Block=26731x1 Type=Int16, ColorInterp=Gray`",6/1/15,1
jamesmcclain,"I am trying to get started working on the code that @lossyrob was generous enough to provide, https://github.com/WikiWatershed/model-my-watershed/pull/248, and I am having some trouble compiling it.  I type `sbt` at the command line then `compile` from within sbt.  I eventually see this line: `[error] (processing/*:update) sbt.ResolveException: unresolved dependency: com.azavea.geotrellis#geotrellis-spark_2.10;0.9.1: not found`",6/1/15,1
jamesmcclain,"Sorry, it is actually: `[error] (processing/*:update) sbt.ResolveException: unresolved dependency: com.azavea.geotrellis#geotrellis-spark_2.10;0.10.0-SNAPSHOT: not found`, I had changed the version to 0.9.1 because I saw discussion of that on a Google Group",6/1/15,1
jamesmcclain,"Incidentally, I am running Spark 1.3.1",6/1/15,1
notthatbreezy,have you run `./publish-local.sh` on your machine/vm? ,6/1/15,1
notthatbreezy,for geotrellis - that-is,6/1/15,1
notthatbreezy,geotrellis doesn't have snapshots on maven so if you'll need to do that so it's in your machine's repository,6/1/15,1
jamesmcclain,"No, I have not.  Just before your message came, I remembered that Rob had mentioned something about that",6/1/15,1
jamesmcclain,Can you please tell me a little bit more about that?  I pull down the latest geotrellis code,6/1/15,1
jamesmcclain,and run the publish_local command in there?,6/1/15,1
pomadchin,yep,6/1/15,1
jamesmcclain,"Okay, one more quick question",6/1/15,1
notthatbreezy,"yep, it's just a helper script around some sbt commands",6/1/15,1
pomadchin,it publishes all projects to a local `.ivy2` repo ,6/1/15,1
notthatbreezy,it might be quicker to simply do `./sbt` then in the console `project spark` then run the command `publish-local`,6/1/15,1
jamesmcclain,"Okay, I'll do that",6/1/15,1
jamesmcclain,"in the `Build.scala` file that Rob supplied, I see `  val geotrellis  = ""0.10.0-SNAPSHOT""`",6/1/15,1
notthatbreezy,yeah,6/1/15,1
jamesmcclain,is that the version string that I want to use for this?,6/1/15,1
notthatbreezy,yeah,6/1/15,1
notthatbreezy,that sounds correct -- you'll have to be on the current master when you do publish local,6/1/15,1
notthatbreezy,0.10.0 will be the first version with spark support,6/1/15,1
jamesmcclain,"Okay, that sounds great",6/1/15,1
jamesmcclain,"I'll give this a try, thank you so much",6/1/15,1
notthatbreezy,no problem,6/1/15,1
lossyrob,"@pomadchin still haven_Ñét gotten to working with gt-admin, my plate has been full :/ What I am suspecting is that perhaps the buffer underflow error occurs with Int16 but not Float32, which is what we_Ñéve been working with? It_Ñés a shot in the dark.",6/1/15,1
pomadchin,"@lossyrob heh same, no time, got state exam soon :( no time to work; I_Ñéll try your idea, hm didn_Ñét even thought about it, have you some tiles (or tile), prepeared to load?",6/1/15,1
lossyrob,Are these accessable to you? https://drive.google.com/open?id=0B_qGBN8rOJmOfnliVjAyb0gwQzJFRlJCaEtLbTJHWDNDVWl6MnVIRlN6UlJzVC1KX2NnaHc&authuser=1,6/1/15,1
pomadchin,"yep, ty!",6/1/15,1
lossyrob,no problem!,6/1/15,1
pomadchin,thats the exact reason man,6/1/15,1
pomadchin,"my steps: * ran local clean env* build and dropped indexes: ```val indexMethod = ZCurveKeyIndexMethod.by { dt => s""${dt.getYear}${dt.getDayOfYear}"".toInt }    val minKey = SpaceTimeKey(200, 400, new DateTime(2000, 1, 1, 0, 0, 0, DateTimeZone.UTC))    val maxKey = SpaceTimeKey(200, 400, new DateTime(2020, 1, 1, 0, 0, 0, DateTimeZone.UTC))    val index = indexMethod.createIndex(KeyBounds(minKey, maxKey))    attributeStore.write(layer, ""keyIndex"", index)    val actual = attributeStore.read[KeyIndex[SpaceTimeKey]](layer, ""keyIndex"")    val keyIndex = SpaceTimeKey(200, 400, new DateTime(2010, 1, 1, 0, 0, 0, DateTimeZone.UTC))```",6/1/15,1
pomadchin,this worked perfect,6/1/15,1
pomadchin,"than downloaded tile: `test-200506000000_0_0.tif`and ran simple spatial ingest",6/1/15,1
pomadchin,ingest was successfull,6/1/15,1
pomadchin,and server works perfect,6/1/15,1
pomadchin,o:,6/1/15,1
lossyrob,hmmmmm,6/1/15,1
lossyrob,ok so my intuition was at least in the right direction :),6/1/15,1
lossyrob,"can you give me one of the geotiffs your working with, the Int16?",6/1/15,1
lossyrob,"I wonder if this is something that is maybe with the GeoTiffReader, which I just completely rewrote and have a PR for. I can look at both of them.",6/1/15,1
pomadchin,"sure, sec (ingesting it after ingesting yours, want to break the system :D)",6/1/15,1
pomadchin,10 minutes left o: a big test file just have locally,6/1/15,1
pomadchin,https://dl.dropboxusercontent.com/u/40424983/NMD20140812004.tif,6/1/15,1
pomadchin,is it accessible for u?,6/1/15,1
lossyrob,yup dl_Ñéing now,6/1/15,1
pomadchin,lil bit heavy o: sry for a big file ):,6/1/15,1
lossyrob,no worries!,6/1/15,1
lossyrob,such is life when working with raster data :),6/1/15,1
pomadchin,:D,6/1/15,1
pomadchin,got plans to test modis sinusoidal projections ingest,6/1/15,1
pomadchin,it will help to reduce file sizes,6/1/15,1
lossyrob,"we have some outstanding issues around ingests with various projections_Ñ_there_Ñés some work we need to do to support ingesting layers against a tile layout that does not span the whole world, for instance",6/1/15,1
lossyrob,it_Ñéll actually error on projections that don_Ñét support a world extent,6/1/15,1
lossyrob,something to keep in mind,6/1/15,1
lossyrob,that issue is high on the queue,6/1/15,1
pomadchin,"hm, smth weird happend",6/1/15,1
pomadchin,o:,6/1/15,1
pomadchin,seems to me kryo bug dissapeared o_O,6/1/15,1
pomadchin,wtf ._. didn_Ñét change a line / geotrellis version,6/1/15,1
lossyrob,hmmmm,6/1/15,1
pomadchin,o_O,6/1/15,1
pomadchin,try you mb :D ,6/1/15,1
pomadchin,mb smth weird ._.,6/1/15,1
lossyrob,very strange,6/1/15,1
lossyrob,ok I can try in a bit,6/1/15,1
pomadchin,heh o_O now getting `Unable to find class: geotrellis.admin.ingest.AccumuloIngestTimeCommand$$anonfun$4`,6/1/15,1
pomadchin,"it is in `write`, where i pass lambda function to build ZCurveIndex",6/1/15,1
pomadchin,oO,6/1/15,1
pomadchin,seems to me it works now o_O,6/1/15,1
pomadchin,(ëâ ó_ñ_ ó_«_Ñ«__,6/1/15,1
pomadchin,will test now not locally (not soon :\ ),6/1/15,1
lossyrob,"hmm if it can_Ñét find classes, it_Ñés probably a build issue",6/1/15,1
lossyrob,"and actually Kryo deserialize buffer underflow sounds like a build issue: It probably wrote the bytes as one version of a class, and then tried to deserialize it under another version of the class, which had more bytes, and ran out of bytes as it was trying to read it",6/1/15,1
lossyrob,"so I_Ñéd make sure that everything is cleaned/rebuilt/packaged/published correctly, and maybe wiping out some `target` directories so that sbt doens_Ñét think it has something built already that it needs to rebuild",6/1/15,1
pomadchin,"as you remember, I got such bug even on creating index reading",6/1/15,1
pomadchin,kryo bug**,6/1/15,1
lossyrob,that was inside the same projects correct,6/1/15,1
lossyrob,"so it_Ñés more about, what does this subproject think this class is, and what was written on ingest",6/1/15,1
lossyrob,"I got a similar error when I was trying to run some spark tests, but realized I needed to rebuild my test-catalog, because the KeyIndex changed",6/1/15,1
lossyrob,"so it found KeyIndex, but was trying to read a serialized old version of the class into a new version of the class",6/1/15,1
pomadchin,"heh, not sure but it can be the reason of the bug; though it seems to me a even cleaned ivy2 cache",6/1/15,1
lossyrob,"From my side, it seems like, if nothing has really changed and now it_Ñés working, and it has to do with serialization/deserialization, that could be pretty suspect",6/1/15,1
pomadchin,"btw, found that if you have some custom job in a gt-admin project (in and ingest subproject), the lambda function (for example to build zcurve index) have to be accesable to server project",6/1/15,1
lossyrob,this is running on your docker containers yes? did you get the kryo error running local?,6/1/15,1
lossyrob,oh really,6/1/15,1
pomadchin,"kk yeah, hope so; yes i got it in containers and locally, so it sounds like magic",6/1/15,1
lossyrob,that makes sense,6/1/15,1
lossyrob,but also is inconvenient,6/1/15,1
pomadchin,"and got even on standard jobs same kryo bug, and not on one cluster ): ",6/1/15,1
pomadchin,magic ._.',6/1/15,1
lossyrob,"hmmm so your saying that, if you use a lambda on the ingest, then that ingest assembly needs to be on the classpath of the workers for the server?",6/1/15,1
pomadchin,yep,6/1/15,1
lossyrob,not so anonymous haha,6/1/15,1
pomadchin,:D not anonymous at all,6/1/15,1
pomadchin,it is possible to make a subproject with anon functions :D ,6/1/15,1
pomadchin,with names,6/1/15,1
pomadchin,._.,6/1/15,1
pomadchin,::DDDDD ,6/1/15,1
lossyrob,We should probably make a number of those _ÑÒbyYear_Ñù like functions in the main library then,6/1/15,1
pomadchin,mb to add smth like `orderByString(str: String) = DateTime.parse(str)`,6/1/15,1
lossyrob,"also, I have a strong suspicion that we_Ñéll be mainly using Hilbert in the future  ",6/1/15,1
lossyrob,hmm that_Ñés a good idae,6/1/15,1
lossyrob,want to add that? ;),6/1/15,1
pomadchin,:D sure,6/1/15,1
pomadchin,will make a pull request ._. :D huge pr,6/1/15,1
lossyrob,awesome!,6/1/15,1
pomadchin,kk,6/1/15,1
pomadchin,"can you have a look at this tile (dropbox link), what about histogram ?",6/1/15,1
pomadchin,seems it is not creating it ._.,6/1/15,1
pomadchin,":D not critical at all, dont need it for these sort of tiles",6/1/15,1
lossyrob,you have to write that manually I believe. so if your ingest isn_Ñét writing it that_Ñés the problem,6/1/15,1
lossyrob,"this is where gt-admin needs some work, hasn_Ñét been fully updated. though I think I did update one of the ingetss",6/1/15,1
lossyrob,hmm I cant_Ñé find it,6/1/15,1
lossyrob,but in ingest,6/1/15,1
lossyrob,you_Ñéll have to save off the histogram,6/1/15,1
lossyrob,"which is specific to gt-admin, because it later tries to pull it out for it_Ñés purpose_Ñ_we don_Ñét necessarily want to store the histogram for every application, and there_Ñés  a cost to computing it on save, so that_Ñés pushed to the client",6/1/15,1
lossyrob,it_Ñés a matter of doing something like https://github.com/geotrellis/geotrellis/blob/master/spark/src/main/scala/geotrellis/spark/io/accumulo/AccumuloRasterCatalog.scala#L72,6/1/15,1
pomadchin,"wow, nice",6/1/15,1
lossyrob,"```scalaval histogram = rdd.histogramattributeStore.write(layerId, _ÑÒhistogram"", histogram)```",6/1/15,1
pomadchin,friendly thing,6/1/15,1
lossyrob,and now that histogram will be available to anyone trying to read the _ÑÒhistogram_Ñù out of that layer Id,6/1/15,1
lossyrob,"they will have to know the type, and read it like how it_Ñés being read here https://github.com/geotrellis/gt-admin/blob/master/server/src/main/scala/geotrellis/admin/server/CatalogService.scala#L149",6/1/15,1
lossyrob,in fact you can store anything that is JSON serializable in the attributeStore,6/1/15,1
lossyrob,"so if you had something you wanted to pre-compute, and store along with the layer, you can give it a unique string name and then do an ```attributeStore.write(layerId, _ÑÒmything_Ñù, mything)```",6/1/15,1
pomadchin,o: super,6/1/15,1
pomadchin,"A theoretical question: can be zcurve and hilbert indexes used for the same aim ? what exactly means spatialResolution and temporalResolution?",6/1/15,1
lossyrob,"Zcurve and hilbert curves are two instances of space filling curves. Space filling curves are what we use to map chunks of N-dimensional space into a 1 dimensional index. They are _ÑÒinterchangeable_Ñù, with the interchange affecting performance only (assuming that it all works right ;))",6/1/15,1
lossyrob,"Our hilbert implementation is based on the `uzaygezen` library, which is used by other projects and well tested. However, our implementation of it is the least tested, but it_Ñés something that I_Ñém excited to use.",6/1/15,1
lossyrob,"So what is resolution, and how does it play into space filling curves? Let me see how to talk about it...",6/1/15,1
lossyrob,Let me use this image to help,6/1/15,1
lossyrob,![curve](https://cloud.githubusercontent.com/assets/2320142/6543539/449db6e2-c4ed-11e4-865a-584e056b5469.png),6/1/15,1
lossyrob,"ok. So space `S` is the space we start with_Ñ_so for `SpatialKey` rasters, it_Ñés 2D and so would be a square, but for `SpaceTimeKey` rasters is 3D, and so would look like the cube we see above",6/1/15,1
lossyrob,"In order to index the space using a space filling curve, we first need to partition the space into chunks_Ñ_we are basically creating a grid of points that fill the space, and drawing lines that connect those points",6/1/15,1
lossyrob,"so we cut up the space evenly, and that is what `P(S)` is",6/1/15,1
lossyrob,"if we take the center point of each of those chunks, we get the points in `G`",6/1/15,1
lossyrob,"if we draw a Z curve to connect those points, we get `C(G)`",6/1/15,1
lossyrob,which is a 1 dimensional index,6/1/15,1
lossyrob,"great, so where does the resolution play into it? Well, when we make `P(S)`, that is, cut up the original space into chunks, we can either cut them up into large chunks or small chunks",6/1/15,1
lossyrob,the number of chunks is the resolution,6/1/15,1
lossyrob,sort of,6/1/15,1
lossyrob,because a requirement of SFCs is that the number of partitions in a dimension is `2^k` for some `k`,6/1/15,1
lossyrob,"In our Z curve, we have the added restriction that `k` is the same for each dimension",6/1/15,1
lossyrob,"so if we have 3 dimensions, our `P(S)` is going to be cut up into `2^k * 2^k * 2^k` chunks",6/1/15,1
lossyrob,"in Hilbert, we actually don_Ñét have that restriction.",6/1/15,1
lossyrob,We impose the restiction that the spatial resolution is the same. This fits with TMS resolutions,6/1/15,1
lossyrob,which have cols and rows be `2^k` and `2^k` for some zoom level `k`,6/1/15,1
lossyrob,but the temporal resolution doesn_Ñét have to be the same,6/1/15,1
lossyrob,"so for zoom level 8, but for a lot of time, we can have the number of cols and rows of our grid be `2^8`, but we might want our time to be chunked into `2^15` parts",6/1/15,1
lossyrob,"in which case, `spatialResolution = 8` and `temporalResolution = 15`",6/1/15,1
lossyrob,does that make sense?,6/1/15,1
pomadchin,O: everything is clear,6/1/15,1
lossyrob,awesome,6/1/15,1
pomadchin,"yep, you may have lectures on a coursera or smth like o_o ",6/1/15,1
pomadchin,i mean lead,6/1/15,1
pomadchin,teach,6/1/15,1
pomadchin,._.,6/1/15,1
lossyrob,I should definitely write a blog post explaining that stuff_Ñ_if only I could find time!,6/1/15,1
lossyrob,"it_Ñés good to explain it though, sures up my understanding, so thanks.",6/1/15,1
pomadchin,awesome! ,6/1/15,1
lossyrob,more info can be found here: https://github.com/geotrellis/curve,6/1/15,1
lossyrob,which is a project that we are collaborating with other community members to create an all purpose space filling curve library,6/1/15,1
lossyrob,and the most information can be found here: https://github.com/geotrellis/curve/issues/3,6/1/15,1
lossyrob,which is an epic discussion of SFCs,6/1/15,1
pomadchin,@lossyrob what poc to provide in agreement? you? :D,6/1/15,1
lossyrob,"sure, or info@azavea.com",6/1/15,1
lossyrob,"@pomadchin azavea is holding all of the CLA_Ñés, at least until we move over to LocationTech.",6/1/15,1
pomadchin,"kk, done",6/1/15,1
lossyrob,Awesome!,6/1/15,1
lossyrob,You_Ñére now a contributer :),6/1/15,1
pomadchin,:D yeeee,6/1/15,1
vishalanand,"I had a query regarding raster input for interpolation. In Kriging, can the raster input consist of the same geometrical coordinates with different associated values? (When I visualize it, this seems improbable; viz. the porosity of a coordinate can not take two different values in a given raster). I happened to come across some `non-Bucketed Semivariogram tests` comprising of such raster inputs (with multiple values for the same point). I was wondering if I should throw an error in such instances, or should I take the average of such raster points (considering multiple readings for porosity measured for the same coordinate)?",6/2/15,1
lossyrob,@vishalanand good question,6/2/15,1
lossyrob,we could do something like take the average of overlapping points,6/2/15,1
lossyrob,but that seems weird,6/2/15,1
lossyrob,"really the user should pass in distinct points, and preprocess them in any way that they want to get it to that point (no pun intended)",6/2/15,1
lossyrob,"are you talking about points that are the same space, or points that map to the same raste cells?",6/2/15,1
lossyrob,i.e.,6/2/15,1
pomadchin,"@lossyrob @vishalanand it is possible mb to define behavior via some function (pass it as an argument), if we need to define behavior to merge points ",6/2/15,1
lossyrob,"```scalaval twoPointsOverlapping =   Seq(PointFeature(1.0, 2.0, 5), PointFeature(1.0, 2.0, 7))val twoPointsSameRasterCell = {  val p1 = PointFeature(1.0, 2.0, 5)  val p2 = PointFeature(1.1, 2.0, 7)  // not the same point, but_Ñ_  val rasterExtent: RasterExtent = ??? // The target raster extent  val (col1, row1) = rasterExtent.mapToGrid(p1.x, p1.y)  val (col2, row2) = rasterExtent.mapToGrod(p2.x, p2.y)  if( col1 == col2 && row1 == row2) {    // These two different points have mapped to the same raster cell.    // What to do here?  }}```",6/2/15,1
lossyrob,"@pomadchin that might work, esp in the case of needing to merge points that take up the same raster cell (because the client code would have to do that computation of mapping to raster cells beforehand to deduplicate, which is non-trivial)",6/2/15,1
vishalanand,"@lossyrob I was talking about points that map to the same raster cells.In case the input corresponds to the one in [here](https://github.com/geotrellis/geotrellis/blob/master/vector-test/src/test/scala/spec/geotrellis/vector/SemivariogramSpec.scala#L105)```val points = Seq[PointFeature[Int]](        PointFeature(Point(0.0,0.0),10),        PointFeature(Point(0.0,0.0),16),        PointFeature(Point(1.0,0.0),20),        PointFeature(Point(0.0,1.0),24),        PointFeature(Point(2.0,2.0),50)        )```@pomadchin In case of merging, I think that could be done.",6/2/15,1
lossyrob,"it looks like that_Ñés the case of duplicate points ( [0.0, 0.0] )",6/2/15,1
vishalanand,"@lossyrob  Oh, yes! Sorry, I was speaking of points that are in the same space (and not points pointing to the same raster space).(I had assumed before that same raster space refers to the same PointFeatures with the exact feature value as well, in your previous comment).",6/2/15,1
lossyrob,"Points are in a (Double, Double) space, often I_Ñéll use the term _ÑÒMap Space_Ñù, since they are coordinates in the space of the projection (like lat long, web mercator)",6/2/15,1
lossyrob,"raster space is grid cols/rows, (Int, Int), like col = 5 row = 3",6/2/15,1
lossyrob,"also I_Ñéll call that _ÑÒGrid Space""",6/2/15,1
pomadchin,"/offtop/ this mapToGrid function is the most amazing thing: find (r,c) of a certain pixel o: awesome",6/2/15,1
lossyrob,:) yup it does all the math from converting to/from Map and Grid spaces,6/2/15,1
pomadchin,"@lossyrob thought a little about geotiff reader commit: mb to hink how to get rid of arity and types in some functions? smth like:```def combine[T](b: T*)(f: (T*) => T)(implicit num: Numeric[T]): T = f(b: _*)combine(1, 2, 3)({case Seq(a, b, c) => a + b + c})```",6/2/15,1
lossyrob,those will box and be slow,6/2/15,1
pomadchin,and to generate with macors?,6/2/15,1
pomadchin,"similar to `shapeless` `HMap` constructor, and other shapeless functions",6/2/15,1
lossyrob,not sure how to generate the seq call with macros_Ñ_but I am doing something similar with the `def combine(f: Array[Int] => Int)`,6/2/15,1
pomadchin,o: wow noticed,6/2/15,1
lossyrob,"If we want to do something like that, we could",6/2/15,1
lossyrob,"like, have a varargs of the bands",6/2/15,1
lossyrob,but it would end up being like the Array function,6/2/15,1
pomadchin,"yep, really",6/2/15,1
lossyrob,`def combine(b: Int*)(f: Array[Int] => Int): Tile`,6/2/15,1
lossyrob,and,6/2/15,1
lossyrob,`def combineDouble(b: Int*)(f: Array[Int] => Int): Tile`,6/2/15,1
pomadchin,:D,6/2/15,1
lossyrob,do you think that would be useful?,6/2/15,1
lossyrob,I can add that for sure,6/2/15,1
lossyrob,if so can you comment on the PR on `MuliBandTile.scala` as such? I_Ñém keeping my TODO list there :),6/2/15,1
pomadchin,":D not critical I think, just 'generic' thoughts; really need only three arity function",6/2/15,1
lossyrob,"it has up to 4, or you can use the Array one to get all of them.",6/2/15,1
lossyrob,"might be useful in case someone wants to use 5 bands out of an 11 band raster, say. that way not all 11 values need to be read",6/2/15,1
pomadchin,hm,6/2/15,1
pomadchin,some modis rasters have more then 11 bands,6/2/15,1
pomadchin,but actually don't need them all,6/2/15,1
lossyrob,would you need more than 4 bands for an operation?,6/2/15,1
lossyrob,also I want to write an operation on MultiBandTile which drops it to a subset of bands,6/2/15,1
lossyrob,"like `multiBandTile.bands(1, 3, 5)` would produce a 3 band tile out of X > 5 bands",6/2/15,1
lossyrob,so you can do things like drop the alpha band if you don_Ñét want it,6/2/15,1
pomadchin,"hm, mb it would be useful to specify what bands to ingest of multiband tiles, ot it is too heavy operation?",6/2/15,1
lossyrob,yeah that does make sense. and will be part of the MulitBand spark work_Ñ_still trying to nail the geotrellis.raster.MultiBand single threaded stuff,6/2/15,1
lossyrob,"but right, it oculd use that `multiBandTiles.bands(1,2,3)` function inside of the ingest, to specify what bands we want to carry over",6/2/15,1
lossyrob,@pomadchin do you have any experience with working with Byte type rasters that have NoData values?,6/2/15,1
pomadchin,"@lossyrob dont remember about `Byte`, but really had some problems with `NoData` values with landsat rasters",6/2/15,1
lossyrob,there_Ñés weirdness around the fact that java doesn_Ñét support unsigned types. serious weirdness,6/2/15,1
pomadchin,"): was really long ago, mb next week will be working with it again, there was a problem, that we needed to change raster type / to change some pixels values to load tiles without overlapping some areas, mb you remember it",6/2/15,1
lossyrob,that I_Ñém going to have to code a lot to get around,6/2/15,1
lossyrob,hmmm_Ñ_.I_Ñém having trouble remembering that,6/2/15,1
pomadchin,got `UInt16` `NoData` troubles,6/2/15,1
pomadchin,`isNoData(-2147483648) //> true`,6/2/15,1
pomadchin,so it is `-2^31`,6/2/15,1
pomadchin,"and as a variant workaround was to make snth like `gdal_calc.py -A LC81770282014081LGN00_B4.tif --outfile=result.tif --calc=""(A>0)*A + (A==0)*(-2147483648)""`",6/2/15,1
lossyrob,yup_Ñ_.same problems as Byte,6/2/15,1
lossyrob,well that looks like Int.MinValue,6/2/15,1
lossyrob,-2147483648,6/2/15,1
pomadchin,yes,6/2/15,1
lossyrob,"not Short.MinValue, which is the NoData of Short, so not exactly the same problem",6/2/15,1
lossyrob,"perhaps though, if it_Ñés being read in",6/2/15,1
lossyrob,"the problem is, for UInt16, if you had a value that in the signed version represented Short.MinValue, but in unsigned was an actual value, that gets converted to NODATA",6/2/15,1
lossyrob,which destroys info,6/2/15,1
pomadchin,** the line above was a NoData solution for UInt16 converted to Int32,6/2/15,1
pomadchin,for Int16 -32768,6/2/15,1
lossyrob,"```scala> Short.MinValue & 0xFFFFres0: Int = 32768```",6/2/15,1
pomadchin,not sure that it is a good idea to convert uint16 to int16,6/2/15,1
lossyrob,so if any values in the raster were 32768 (which is half of the max unsigned short value),6/2/15,1
lossyrob,haha no it_Ñés not,6/2/15,1
lossyrob,but in java,6/2/15,1
pomadchin,:D,6/2/15,1
lossyrob,we don_Ñét have uint,6/2/15,1
pomadchin,yes ):,6/2/15,1
lossyrob,we only have signed types,6/2/15,1
lossyrob,so_Ñ_it_Ñés going to take some trickery to get around that,6/2/15,1
lossyrob,and hopefully I can pull that off,6/2/15,1
pomadchin,hope this thing gonna help `>>`,6/2/15,1
lossyrob,haha yup it_Ñéll have to,6/2/15,1
lossyrob,`& 0xFF` and `& 0xFFFF`,6/2/15,1
lossyrob,are my friends,6/2/15,1
pomadchin,":D ",6/2/15,1
pomadchin,"quick google: https://github.com/nystrom/scala-unsigned/tree/master/src/main/scala/passera/unsigned",6/2/15,1
lossyrob,seems silly not to use Value types there,6/2/15,1
lossyrob,oh he did. haha,6/2/15,1
lossyrob,nvm,6/2/15,1
pomadchin,::DD,6/2/15,1
lossyrob,"another problem I_Ñém running into: having a required NoData value for byte (Byte.MinValue) takes a value out of the 0 - 255 space. Which when converting to Int, reads as Int.MinValue. But if we don_Ñét want any of our precious 256 values to be turned into Int.MinValue, we currently don_Ñét have a way to do that...",6/2/15,1
lossyrob,"so I need to make NoData optional. which breaks a really long standing (before my time) design decision, and requires some pretty deep changes.",6/2/15,1
lossyrob,"so, add that to the pile",6/2/15,1
lossyrob,re: that library_Ñ_.I don_Ñét know how he_Ñés extending AnyVal and also implementing a Generic trait. I don_Ñét think you can do that without boxing,6/2/15,1
lossyrob,AnyVals have pretty delicate rules about when the compiler is going to instantiate the wrapper vs allow the primative to stay primative,6/2/15,1
pomadchin,"mb it would be usefull let to define own NoData and own types? but it sounds too generic and mb there would be problems with the scala type system",6/2/15,1
lossyrob,the scala type system could deal with it. It_Ñés just going to kill speed in a very serious way,6/2/15,1
lossyrob,"and I want to avoid that completely. One of the biggest wins of GeoTrellis is that the core stuff is fast_Ñ_if you iterate over 1000000 cells, checking against Int.MinValue is going to be seriously faster than checking agianst some tile.noDataValue or calling `tile.isNoData(v)` or something like that",6/2/15,1
lossyrob,"we have the `isNoData(z)` implemented as a macro that inlines the conditional, it_Ñés that important to be fast",6/2/15,1
pomadchin,): have to use head alot before using type system features,6/2/15,1
lossyrob,"it_Ñés one of the most frustrating things when you_Ñére on one side of it, but once you get used to it and know how to weild it, it gets really really fun :)",6/2/15,1
lossyrob,"and I_Ñém not even at the level of type wizards, like the TypeLevel crew",6/2/15,1
pomadchin,like Miles Sabin :D ?,6/2/15,1
lossyrob,"yup. And Erik, who_Ñés one of the original two authors of GeoTrellis, and the guy who started/maintains Spire",6/2/15,1
pomadchin,"heh yep, real wizards; this stack: shapeless, scalaz, monocle, spire (and geotrellis ofc!) really mind blowing",6/2/15,1
pomadchin,"hm, think you are not a lot behind them (ftw, the same wizard :D) O: i thought you connected to typesafe very close",6/2/15,1
lossyrob,thanks :smile: ,6/2/15,1
lossyrob,had you used scala a lot before working with GeoTrellis?,6/2/15,1
pomadchin,not a lot :D but for a 3 years o:,6/2/15,1
lossyrob,"are you working on raster stuff as part of a job, or academia?",6/2/15,1
pomadchin,"we developed (with a very cool lead) and integration platform (between internet services), it was as a university grade work + we won startup seed money and got clients now, the idea was that it is typesafe, functional and formal. some integration part was statically precompiled (shapeless, scalaz, monocle, macros) and was a dynamic part (based on a pi calculus and lambda calculus formalism, created to describe processes)",6/2/15,1
pomadchin,it is about my Scala experience o:,6/2/15,1
pomadchin,"and raster stuff yes, it is connected with my current work; very interesting to work with geospatial data :p",6/2/15,1
lossyrob,sounds awesome!,6/2/15,1
pomadchin,:D not as awesome as geotrellis,6/2/15,1
lossyrob,:D,6/2/15,1
jamesmcclain,Really silly question: Once I have an object of type PolygonResult (as for example from intersecting a polygon with an extent) how do I work with it?  I have been grepping through the sources and I cannot see how to use this or convert it to something that I know how to use.,6/2/15,1
lossyrob,"```scalaextent & polygon match {  case PolygonResult(p) => // work with p: Polygon  case MultiPolygonResult(mp) => // work with mp: MulitPolygon  case NoResult => // the intersection is empty}```",6/2/15,1
jamesmcclain,"Okay, I got that part",6/2/15,1
lossyrob,"the compiler will complain if you don_Ñét account for a Result case that is possible, and will error if you try to account for an impossible result (e.g. PolygonResult for a point to line intersection)",6/2/15,1
jamesmcclain,"But I don't know how to do the actual work, that is I cannot see what PolygonResult derives from or is similar to",6/2/15,1
lossyrob,p is a Polygon,6/2/15,1
jamesmcclain,Hahhh,6/2/15,1
lossyrob,it_Ñés an unapply,6/2/15,1
jamesmcclain,That is how I get a polygon back out,6/2/15,1
lossyrob,just like for case classes,6/2/15,1
jamesmcclain,with the matching syntax,6/2/15,1
lossyrob,right right,6/2/15,1
jamesmcclain,Gotcha,6/2/15,1
non,@lossyrob hey random question: has geotrellis ever been published with scala 2.11?,6/2/15,1
non,"it seems like it's set up so it could, but looking on maven central i don't see it.",6/2/15,1
lossyrob,no it hasn_Ñét. been a long time since we published to mavn central,6/2/15,1
lossyrob,"I need to push out a milestone release soon. we wanted to do a full release back a couple of months ago, but under heavy testing the list got too long",6/2/15,1
lossyrob,"but we cross build, so you can publish local to 2.11 and 2.10",6/2/15,1
lossyrob,though spark doesn_Ñét support 2.11,6/2/15,1
lossyrob,"@non also 0.9 needed some changes to work with 2.11, and I think was published before the 2.11 release :worried: ",6/2/15,1
non,gotcha. thanks for the information.,6/3/15,1
non,"i was using geotrellis for benchmarking archery, but recently i updated the project to use 2.11 by default and noticed that geotrellis was not resolvable.",6/3/15,1
non,"i've removed it for now, and will consider adding it back once a release for 2.11 is published",6/3/15,1
lossyrob,"ok, I_Ñéll make sure to give you a heads up when that happens.",6/3/15,1
lossyrob,p.s. where_Ñés that code? Interested to see how GT is used for benchmarking it.,6/3/15,1
non,@lossyrob it's a comparison of the `SpatialIndex[Entry[A]]` and `RTree[A]`,6/3/15,1
non,https://github.com/meetup/archery/blob/master/benchmark/src/main/scala/main.scala,6/3/15,1
lossyrob,How do they compare?,6/3/15,1
lossyrob,"hmmm_Ñ_also, what_Ñés this `Thyme` project? Can you speak to why you chose it over caliper?",6/3/15,1
non,"oh, i was just experimenting with it",6/3/15,1
non,it's a benchmarking library by rex kerr (i'm not sure he's worked on it recently though -- i had to publish a jar myself),6/3/15,1
non,"it seems nice for doing small, self-contained comparative benchmarks",6/3/15,1
non,and it is a bit less java-y/reflection-y than caliper,6/3/15,1
non,"that said, i think the ""modern"" thing to use is JMH",6/3/15,1
non,"(or maybe scalameter, but i have less experience with that)",6/3/15,1
lossyrob,i had pretty bad experiences with scalameter,6/3/15,1
lossyrob,very inconsistent,6/3/15,1
lossyrob,but that was about 1.5 years ago,6/3/15,1
non,mmm yeah,6/3/15,1
lossyrob,I actually did a lot of coding towards porting caliper and merging it into the scalameter interface,6/3/15,1
lossyrob,https://github.com/lossyrob/scalpel,6/3/15,1
lossyrob,but i fell by the wayside,6/3/15,1
non,interesting :),6/3/15,1
lossyrob,it_Ñés a pretty valid port of caliper. I_Ñéve always meant to pick it back up and do something with it_Ñ_really want to be able to run caliper through sbt test infra,6/3/15,1
non,yes. that's one nice thing about thyme,6/3/15,1
non,you can just run it directly via scala code,6/3/15,1
non,but i'm not sure i would use it in new projects these days,6/3/15,1
non,"JMH is really really solid, the big issue is that by default its benchmarks are *super* slow",6/3/15,1
non,so you have to run with custom arguments to get feedback reasonably-fast,6/3/15,1
lossyrob,that_Ñés rough,6/3/15,1
lossyrob,I do really like caliper. I really don_Ñét like that we_Ñére still pulling it off of your maven repo :),6/3/15,1
pomadchin,@non the current version ,6/3/15,1
non,@lossyrob haha yeah :),6/3/15,1
lossyrob,"I think there_Ñés an updated version on maven central, but that broke things and I wasn_Ñét into the way it was working",6/3/15,1
lossyrob,it seemed very Google App Engine based,6/3/15,1
lossyrob,where I just want text output,6/3/15,1
non,right. and we now know what is going on with that :P,6/3/15,1
lossyrob,while I sweat over tiny details,6/3/15,1
lossyrob,haha,6/3/15,1
pomadchin,sry guys ** morning msg left from the evening on a keyboard ._.,6/3/15,1
lossyrob,how do you mean?,6/3/15,1
lossyrob,"meaning, is there some caliper-app engine thing I_Ñém not privy to?",6/3/15,1
lossyrob,"or is it just, google trying to push their product",6/3/15,1
non,"well -- i thought i heard a recent announcement about google app engine that either was shutting it down, or making it more expensive, or removing a free option, or something",6/3/15,1
lossyrob,"oh right. yeah I think they_Ñére getting rid of it, in liue of all the google cloud stuff",6/3/15,1
lossyrob,"which actually I want to check out. they_Ñére cheaper than EC2 by pennies, but still. I just got done a huge benchmarking round of geotrellis/spark on EC2, so curious to see what the comparison is",6/3/15,1
jamesmcclain,"Thank you to everyone for answering so many questions.  I apologize for being obtuse, but I am afraid that I am having trouble using `foreachCellByPolygon`.  I have the following code snippet:case PolygonResult(p) =>  println(""XXX"")  println(p.intersects(re.extent))  Rasterizer.foreachCellByPolygon(p, re)(    new Callback {      def apply(col: Int, row: Int): Unit = {        println(""."")      }    }  )  println(""XXX"")",6/3/15,1
jamesmcclain,"```case PolygonResult(p) =>  println(""XXX"")  println(p.intersects(re.extent))  Rasterizer.foreachCellByPolygon(p, re)(    new Callback {      def apply(col: Int, row: Int): Unit = {        println(""."")      }    }  )  println(""XXX"")```Which produces the word ""true"" between two instances of ""XXX"".  It is my understanding that I should be seeing some periods in there, as well.  Clearly I am doing something incorrectly.",6/3/15,1
lossyrob,Probably projections,6/3/15,1
lossyrob,ye olde producer of gotchas,6/3/15,1
lossyrob,hmm..I see you are saying there is instersections though,6/3/15,1
jamesmcclain,:smile: ,6/3/15,1
lossyrob,"that is strange, if the re and polyong intersect, it should be producing results",6/3/15,1
lossyrob,can you pastebin the GeoJSON of the polygon and give me the printout of the rasterextent?,6/3/15,1
jamesmcclain,"There is much more going on in the `foreeachCellByPolygon` function, I figured that my clipped polygon had somehow failed to qualify based on one of those",6/3/15,1
lossyrob,"```scalaimport geotrellis.vector.io.json._println(p.toGeoJson)println(re)```",6/3/15,1
jamesmcclain,"I can't get the geoJSON easily, but I can just pase the polygon as printed out by Scala",6/3/15,1
jamesmcclain,"Oh, okay",6/3/15,1
jamesmcclain,Will-do,6/3/15,1
jamesmcclain,"Is it okay if I just paste them into the chatroom, or do you want a pastebin link?",6/3/15,1
lossyrob,depends on how big it is,6/3/15,1
lossyrob,"if it_Ñéll flood the room, then better to pastebin",6/3/15,1
jamesmcclain,"Okay, I'll use pastebin just to be safe",6/3/15,1
jamesmcclain,http://pastebin.com/tvRE3Le0,6/3/15,1
lossyrob,ok. I_Ñém going to play around in the console with it in a second,6/3/15,1
lossyrob,i_Ñéll let you know what I find,6/3/15,1
jamesmcclain,"Thank you, I appreciate your help very much!",6/3/15,1
lossyrob,No worries dude!,6/3/15,1
jamesmcclain,"Also (and I should have mentioned this before) when I intersect the polygon representing the extent of the dataset with each tile, I only get an intersection (a `PolygonResult`) for one of them and `NoResult` for the rest.",6/3/15,1
lossyrob,where are you gettin the extent of the dataset?,6/3/15,1
jamesmcclain,"From this line in the `TR55Spec.scala` file: `        val polygon: Polygon = nlcd.metaData.extent.toPolygon`",6/3/15,1
lossyrob,can you map over the tiles and print the extent of the other tiles?,6/3/15,1
jamesmcclain,Sure,6/3/15,1
lossyrob,"One thing I_Ñém noticing: the raster extent is huge, the polygon is very small",6/3/15,1
lossyrob,comparitively,6/3/15,1
lossyrob,it seems like the test extent is off,6/3/15,1
lossyrob,how is the extent for the layer generated?,6/3/15,1
lossyrob,looking at the code now,6/3/15,1
jamesmcclain,"Okay, I'm sorry, I may have given you the wrong data",6/3/15,1
jamesmcclain,the polygon you are seeing is the clipped polygon,6/3/15,1
jamesmcclain,Did you want the original polygon?,6/3/15,1
jamesmcclain,There are the raster extents associated with all of the tiles: http://pastebin.com/Ae8j5cbu,6/3/15,1
lossyrob,so if I compute the clipped area,6/3/15,1
lossyrob,and then the cellwidth * cellheight,6/3/15,1
lossyrob,"0.0095627771259994970.25919996400000084",6/3/15,1
lossyrob,the polygon is tiny,6/3/15,1
lossyrob,"and so is not being picked up by the rasterizer, because the polyogn does not actually encapsulate the center of any cell",6/3/15,1
lossyrob,which is a requirement for the rasterizer_Ñ_it goes by what single points are contained in the cells,6/3/15,1
lossyrob,where is that polygon coming from?,6/3/15,1
lossyrob,ugh. so it_Ñés not the polygon. it_Ñés the tile,6/3/15,1
lossyrob,"```json{""type"":""Polygon"",""coordinates"":[[[-79.2000028,39.5999956],[-79.2000028,46.7999948],[-64.80000319999999,46.7999948],[-64.80000319999999,39.5999956],[-79.2000028,39.5999956]]]}```",6/3/15,1
lossyrob,that_Ñés the tile,6/3/15,1
lossyrob,_Ñés extent,6/3/15,1
lossyrob,plug that into geojsonlint.com,6/3/15,1
lossyrob,"if it_Ñés a 20 by 20 raster, that_Ñés some really big cells",6/3/15,1
jamesmcclain,"Okay, sorry, was in a conversation.",6/3/15,1
jamesmcclain,"Yes, agreed",6/3/15,1
jamesmcclain,That is quite large if it is a single 20x20 raster,6/3/15,1
jamesmcclain,Could it be because the extent is being derived from the spacial key?,6/3/15,1
jamesmcclain,"If that is a kd-tree box or similar, it might not match up with the size of a tile",6/3/15,1
lossyrob,"I would print out the extent of the nlcd raster read in, when it_Ñés reprojected from test files",6/3/15,1
lossyrob,and see how large taht is,6/3/15,1
lossyrob,"no it_Ñés in a grid format, and I wouldn_Ñét suspect the mapTransform logic too much, it_Ñés pretty simple",6/3/15,1
jamesmcclain,"Looking at the geotiff files in the test directory, they are only about 2 degrees by 2 degrees wide, so you are saying that I should be looking at the re projection going on in the actual code ",6/3/15,1
jamesmcclain,"I thank you very much for your help, again.  I think that I have enough to go on.  I have to do some debugging on a different project, but hopefully I will be able to get back to this in the afternoon, and maybe complete it.  Thanks again!",6/3/15,1
jamesmcclain,"I think that I may  have found something which is either a bug in geotrellis or is inconsistent with my understanding of how things should work (the latter is immanently possible, btw).",6/3/15,1
jamesmcclain,I have these two lines:,6/3/15,1
jamesmcclain,`val mapTransform = nlcd.metaData.mapTransform`,6/3/15,1
jamesmcclain,"`val extent = mapTransform(key)`, where key is a spatial key",6/3/15,1
jamesmcclain,"Then this line `println(s""[$key $extent]"")` produces (among other lines), these two lines: `[SpatialKey(16,0) Extent(50.399993600000016,82.7999908,64.79999320000002,89.99999)]` and `[SpatialKey(0,12) Extent(-180.0,-3.599999600000004,-165.6000004,3.599999600000004)]`",6/3/15,1
jamesmcclain,In light of lines 77 through 87 here: https://github.com/geotrellis/geotrellis/blob/8266b96d9b59cb9106a6fd5146040afd3962bfea/spark/src/main/scala/geotrellis/spark/tiling/MapKeyTransform.scala,6/3/15,1
jamesmcclain,"This implies to me that when I call `mapTransform(key)`, the entire world is being used as the extent, instead of the extent of the nlcd geotiff.",6/3/15,1
jamesmcclain,"I say that because when row is = 0, you get an x coordinate of -180 in the (upper-left?) corner of the resulting extent",6/3/15,1
jamesmcclain,"and when col is = 0, you get 90 for the (lower-right?) corner of the resulting extent",6/3/15,1
echeipesh,"That_Ñés currently the expected behavior, if you look at: https://github.com/geotrellis/geotrellis/blob/8266b96d9b59cb9106a6fd5146040afd3962bfea/spark/src/main/scala/geotrellis/spark/RasterMetaData.scala#L18",6/3/15,1
echeipesh,https://github.com/geotrellis/geotrellis/blob/8266b96d9b59cb9106a6fd5146040afd3962bfea/spark/src/main/scala/geotrellis/spark/tiling/MapKeyTransform.scala#L11,6/3/15,1
echeipesh,"MetaData is actually producing the MapTransform based on the CRS only, not counting the extent of the layer.",6/3/15,1
jamesmcclain,"Okay, good to know.  Is there a way for me to get the (sub)extent of the spatial key from my NLCD geotiff",6/3/15,1
jamesmcclain,Or should I just write that by hand?,6/3/15,1
echeipesh,what do you mean sub-extent ?,6/3/15,1
jamesmcclain,"The math is not difficult, I was just wondering there was an automatic way to do this",6/3/15,1
jamesmcclain,I just mean that each spatial key represents a sub-part of the overall geotiff.  I just want to get the extent corresponding to the spatial key,6/3/15,1
echeipesh,"Unless I_Ñém misunderstanding the question, that_Ñés it. NLCD is getting tiled into that particular layout that spans the world. Not all the tiles will be present in the layer from the layout, but that_Ñés them.",6/3/15,1
lossyrob,@jamesmcclain ok this is a problem with our test rasters then,6/3/15,1
lossyrob,"we are tiling them to the extent of the raster, where the mpa transform is using the entire world",6/3/15,1
jamesmcclain,"That makes sense, Eugene just came over and helped me understand",6/3/15,1
jamesmcclain,"I am going home for the day, but I appreciate you guys very much",6/3/15,1
echeipesh,"yeah, the createRDD function needs to use some form of the Ingest process, I_Ñém giving that some thought right now.",6/3/15,1
lossyrob,"well, once the feature is complete where we don_Ñét have to do that and can cut tiles however we want",6/3/15,1
echeipesh,the feature ?,6/3/15,1
echeipesh,"The feature where we are able to use layotus snapped to layer extent rather than world extent? Yeah, that would have been helpful here.",6/3/15,1
echeipesh,"Anyway, heading out for the day too, I_Ñém going to hack on this on the train, think I have an easy solution for tying call to `Ingest` here.",6/3/15,1
lossyrob,"ok cool, see ya",6/3/15,1
allixender,#1102 hey does this mean the cassandra and accumulo RRD handling will need to be adjusted?,6/4/15,1
allixender,as you have accumulo in master this would be covered in the tests on master?,6/4/15,1
allixender,I would at some point have to incorporate those changes in the cassandra feature branch I guess?,6/4/15,1
jamesmcclain,I just had another thought regarding loading a geotiff without using the regular ingest process: would it be sufficient to simply change the metadata on the geotiff so that it appears to cover the globe (this would also require applying the same transformation to the points of the query polygons)?,6/4/15,1
echeipesh,"@jamesmcclain check this out, this seems to produce sensical results: https://github.com/lossyrob/model-my-watershed/pull/1",6/4/15,1
echeipesh,"looking at it some more `getRaster` resamples the raster to 500,500 from 1000,1000, not sure if thats what you want.",6/4/15,1
echeipesh,"the stitched raster is bigger than 1000,1000 because of the nodata introduced by the tiling on the border tiles.",6/4/15,1
jamesmcclain,"Okay, I will check it out!  Thanks, again!",6/4/15,1
lossyrob,"@allixender yup this is the API changes I was mentioning_Ñ_once it_Ñés merged, you_Ñéll need to pull from upstream/master and adjust the cassandra stuff to use the new API",6/4/15,1
jamesmcclain,"@echeipesh your changes are working out great, thank you!",6/4/15,1
lossyrob,nice!,6/4/15,1
lossyrob,"Hey everyone, if you_Ñére interested in the SFCurve project, please join the mailing list: https://locationtech.org/mailman/listinfo/sfcurve-dev @notthatbreezy @echeipesh @moradology @jamesmcclain",6/4/15,1
lossyrob,"```scala// lol val x: Try[Option[Either.type]] = null```",6/4/15,1
vishalanand,"I had a quick question : Is there a Raster source I can import (which might not be in GT currently) for checking the Kriging rasters in similar lines as checking the IDW rasters?```it(""matches a QGIS generated IDW raster"") {   val rs = RasterSource(""schoolidw"")   //Does a raster source exist for Kriging?   < Checking of the interpolation carried out>}```",6/4/15,1
pomadchin,@lossyrob add future as boxing for Try :d,6/4/15,1
echeipesh,that_Ñés good!,6/4/15,1
lossyrob,@vishalanand there_Ñés no test raster that we have for Kriging that we_Ñéd be trying to match. perhaps I can try to get a collegue to generate one from ArcGIS,6/4/15,1
vishalanand,"@lossyrob thanks a lot! Lest the data is not easily generated, I was planning to churn out the calculations for raster manually by pen and paper.",6/5/15,1
allixender,thanks for the API changes info @lossyrob ,6/5/15,1
allixender,"by the way, FYI I did draft a post for ""newbies"" about setting up your IDE to work with GeoTrellis, repectively contribute to GeoTrellis development http://allixender.github.io/articles/gsoc-geotrellis-ides/ (also why mucking around a little with Github pages and Jekyll)",6/5/15,1
lossyrob,nice! that looks really great.,6/5/15,1
allixender,thanks :smile: any comments welcome to refine,6/5/15,1
allixender,@lossyrob We might as well close https://github.com/geotrellis/geotrellis/issues/1100 now?  all info has been compiled either in the blog post or in the geotrellis-site pull requests?,6/5/15,1
pomadchin,@allixender  just to add more genetic title: these steps are correct for every sbt / mvn (lil differs) project,6/5/15,1
allixender,"True, to a degree. ut the post is specific to GeoTrellis, which has been more awkward to setup than other sbt / mvn projects",6/5/15,1
allixender,I could add this info as a subtitle,6/5/15,1
allixender,@lossyrob @notthatbreezy https://github.com/geotrellis/geotrellis/issues/1097#issuecomment-109169546 I can't figure out why the **tests** CassandraAttributeStore can work with the session and the CassandraRasterCatalog can't ...,6/5/15,1
allixender,"How could I create a new SparkContext with an updated cassandra SparkConf in the geotrellis-spark testing for  the Cassandra, because I think the provided SparkContext in the tests (because it has already the spark cassandra connector jar available)  uses a ""Default Configuration"", whereas for the cassandra integration tests we actually want to provide dedicated connection parameters (which are based on our EmbeddedCassandra daemon config)",6/5/15,1
allixender,@lossyrob @notthatbreezy Would it be problematic to initialise the cassandra connection details from the provided embedded cassandra config (under bla.../src/test/resources)  in the SparkConf for the SparkContext in [OnlyIfCanRunSpark.scala](https://github.com/allixender/geotrellis/blob/feature/cassandra-gsoc2015/spark/src/test/scala/geotrellis/spark/OnlyIfCanRunSpark.scala) ?,6/5/15,1
allixender,"It is as I suspected, if the cassandra parameters for the embedded test instance are provided later like it has been done in [EmbeddedCassandraConnector.scala](https://github.com/allixender/geotrellis/blob/feature/cassandra-gsoc2015/spark/src/test/scala/geotrellis/spark/io/cassandra/EmbeddedCassandraConnector.scala) then the CassandraRRD functionality from the spark cassandra connector can't ""see"" them in the SparkConf, because they are never propagared in ""The One"" SparkContext that runs in the geotrellis-spark tests (https://github.com/geotrellis/geotrellis/issues/1097)",6/5/15,1
lossyrob,@allixender yes this seems like a problem with managing different spark contexts across tests,6/5/15,1
lossyrob,"one option is, do the thing that we are going to have to do anyway, and stick the cassandra stuff in it_Ñés own subproject",6/5/15,1
lossyrob,"as we currently have it, it doesn_Ñét make sense to have Accumulo and Cassandra to be in included in the main project by default",6/5/15,1
lossyrob,because you_Ñére probably going to only be using one or the other,6/5/15,1
lossyrob,they should be different subprojects,6/5/15,1
lossyrob,"`geotrellis-acummulo` and `geotrellis-cassandra`, for instance",6/5/15,1
lossyrob,there_Ñés a way those test projects can depend on the `geotrellis-spark` test project,6/5/15,1
pomadchin,"very interesting to have bench comparing for the similar tasks on both dbs, or its for different tasks? o:",6/5/15,1
lossyrob,"but another thing that we_Ñéd want to do, and have run into, is wanting a `testkit`. Currently the `testkit` in GeoTrellis deasl with the Akka `engine` project. But we need one for doing stuff with Spark. For instance, an internal application that James is writing, he could have used some testkit functionality, but instead had to copy some code in `geotrellis-spark` tests.",6/5/15,1
lossyrob,"If Cassandra had it_Ñés own subproject, you could manage the spark context inside that test project however you wanted",6/5/15,1
lossyrob,"@pomadchin you could still include both dependencies, and have both funcitonality. This just gives you the option to have one or the other, or none even",6/5/15,1
lossyrob,in case you are using spark only through memory or to read off local disk for instance. So you combine `geotrellis-spark` with a mix-and-match approach to the various backends that you want to use.,6/5/15,1
pomadchin,"sure, my msg was just a sort of offtop: about having benchmark results of cassandra and accumulo :) sry for interrupting your thoughts",6/5/15,1
lossyrob,"no worries. We hactually have some pretty interesting benchmarking data from Accumulo, unfortunately Cassandra doesn_Ñét do range queires on spatial indexes like we want, but that_Ñés @allixender_Ñés summer project to figure that out :)",6/5/15,1
lossyrob,"we_Ñére going to publish results of those benchmarks at a later point. For a teaser, here_Ñés some results of doing a local subtract over two 439.5 GB raster layers, and then taking a zonal mean over the entire region, comparing m3.2xlarges and r3.2xlarges based on cluster size:",6/5/15,1
lossyrob,![results](http://i.imgur.com/tO9M2n9.png),6/5/15,1
lossyrob,This is using Accumulo,6/5/15,1
pomadchin,hm,6/5/15,1
pomadchin,realtime work o:,6/5/15,1
lossyrob,"that_Ñés doing the whole data set. when you scope the query down spatially it_Ñés a much faster (and varies less based on cluster size), which means that the indexing and query mechanisms are doing their job :)",6/5/15,1
lossyrob,"suprising result is that the m3.2xlarges were much better at this work than the r3.2xlarges. Doesn_Ñét make sense when you look at the specs. but that_Ñés what we found time and again (we ran the benchmarks many times, on different clusters of the same makeup)",6/5/15,1
lossyrob,"__note__ I just merged in the new GeoTiff reader API, as well as MultiBand rasters, and GeoTiff writing/reading multiband. The API has changed. See https://github.com/geotrellis/geotrellis/blob/master/raster-test/src/test/scala/geotrellis/raster/io/geotiff/reader/SingleBandGeoTiffReaderSpec.scala and https://github.com/geotrellis/geotrellis/blob/master/raster-test/src/test/scala/geotrellis/raster/io/geotiff/reader/MultiBandGeoTiffReaderSpec.scala for examples of the new API. All reading should be done through the `GeoTiff`, `SingleBandGeoTiff` and `MultiBandGeoTiff` objects, and instances of `GeoTiff` have a write method.",6/6/15,1
mpfaffenberger,Quick question from someone who's interested in the project -- A lot of raster data is unsigned. How does GeoTrellis deal with the fact that Java doesn't support unsigned arithmetic? Thanks all!,6/6/15,1
lossyrob,@mpfaffenberger what a greatly timed question. I_Ñém currently working on a feature to add UByte and UShort types,6/6/15,1
lossyrob,all of the access to values in geotrellis uses Ints or Doubles,6/6/15,1
lossyrob,so you get something like this,6/6/15,1
lossyrob,"```scalaval path: String = ???val geoTiff = SingleBandGeoTiff(path)val tile = geoTiff.tile//get value at col 10 row 20val z: Int = tile.get(10, 20)// map over valuestile.map { z: Int => z + 1 }// map over doubles (slower)tile.mapDouble { z: Double => z + 1.1 }```",6/6/15,1
lossyrob,currently the GeoTiff reader reads unsigned types as upcasting them to the next type. So it_Ñéll convert `unit16` into Int. It actually converts `uint32` into Float.,6/6/15,1
lossyrob,"`byte` is unsigned in GDAL, but our bytes are signed. I_Ñém fixing this now to include a `TypeByte` and `TypeUByte` so that the integer and double access functions will return the correct values",6/6/15,1
lossyrob,"Have you run into a lot of `uint32` data? We don_Ñét really have a good solution for this (besides converting to floats or doubles). But I think in most cases it_Ñés `uint32`, `uint16`, and the unsigned `byte`",6/6/15,1
mpfaffenberger,@lossyrob -- Yeah we've run into the same deal. We used some JNI + scala trait trickery to basically bypass the JVM altogether. At this point we essentially write C++ in Scala. We use OpenCV Matrices to do most of the arithmetic operations. Most of our data is unsigned byte or unsigned short. I can't say I've run into a lot of 32 bit unsigned data. But yeah... I think maybe Java 8 might have some options for unsigned primitives. ,6/6/15,1
mpfaffenberger,"So to sum up, we open with GDAL Java bindings, and pipe that input directly to OpenCV Java bindings. ",6/6/15,1
mpfaffenberger,"Probably not appropriate for all types of rasters, but it suited our needs for the time being.",6/6/15,1
lossyrob,"That sounds interesting. I_Ñéve been playing around with the OpenCV bindings a bit, but haven_Ñét added any library support for it yet",6/6/15,1
lossyrob,Is the code open so I can take a look?,6/6/15,1
mpfaffenberger,"It's not open at the moment, I'm trying to work with GeoMesa folks though. So maybe soon?",6/6/15,1
lossyrob,"On the roadmap is to create a translation between GeoTrellis rasters and OpenCV types, so you can do a lot of the processing in Scala and lean on OpenCV for their awesome algorithms",6/6/15,1
mpfaffenberger,That's a good plan I think.,6/6/15,1
mpfaffenberger,Curse designers of Java for not including unsigned :(,6/6/15,1
lossyrob,"How have you found the OpenCV bindings? I looked under the covers about how they were generated, it_Ñés really complicated",6/6/15,1
lossyrob,not using something like SWIG like GDAL,6/6/15,1
lossyrob,"I wanted to use the Stitcher but it_Ñés currently not in the Java API_Ñ_looked into writing the binding, and then turned away and ran :)",6/6/15,1
lossyrob,yeah that was an interesting design decision that we all have to live with now forever,6/6/15,1
mpfaffenberger,I've tried to keep myself as blind as possible to C++. I found it to be similar to GDAL's SWIG bindings though. I read on their site that they use a parser over the C++ code to generate the JNI stuff. Seems similar to SWIG. You can compile it with Python and Java support. If you run with Java it uses ANT to make the .jar...  then you can just toss it up on an artifactory or something.,6/6/15,1
lossyrob,"Swig has an interesting way of making cross language bindings_Ñ_and basically you write some C++ classes with some additional syntax. And then the framework generates it_Ñ_you have to write type mappings, but it_Ñés mostely compile safe.",6/6/15,1
lossyrob,OpenCV is just straight up generating text source code,6/6/15,1
lossyrob,using regex_Ñés and such,6/6/15,1
mpfaffenberger,Gotcha.,6/6/15,1
mpfaffenberger,So ... some stuff is missing in the Java API. That's interesting.,6/6/15,1
lossyrob,yeah would def be cool to see how your using OpenCV bindings once they let you open up the source :) I could probably learn a lot from it,6/6/15,1
mpfaffenberger,"Well, they also cut a release a few days ago.",6/6/15,1
lossyrob,"I was working off of master a couple of months ago, but let me check again...",6/6/15,1
mpfaffenberger,3.0.0 is out apparently.,6/6/15,1
mpfaffenberger,With new feature detectors :),6/6/15,1
mpfaffenberger,"Yeah, I'll talk to my company. At the very least I'd like to write a blog post or something.",6/6/15,1
lossyrob,hmm yeah as far as I can find there isn_Ñét java bindings for the stitcher,6/6/15,1
mpfaffenberger,That's rough... I wonder if we can submit a ticket or something.,6/6/15,1
lossyrob,That would make sense,6/6/15,1
lossyrob,what feature extraction are you using? That_Ñés a topic I don_Ñét know nearly as much as I want to about.,6/6/15,1
mpfaffenberger,Well... the only option that isn't patented is ORB.,6/6/15,1
mpfaffenberger,Oriented FAST and rotated BRIEF,6/6/15,1
mpfaffenberger,It's a detector that responds nicely to high frequency changes. ,6/6/15,1
mpfaffenberger,SIFT is also available (Scale Invariant Feature Transform I think?),6/6/15,1
mpfaffenberger,but costs an arm/leg.,6/6/15,1
mpfaffenberger,patented...,6/6/15,1
lossyrob,oof,6/6/15,1
lossyrob,what does ORB produce? edges?,6/6/15,1
mpfaffenberger,keypoints,6/6/15,1
mpfaffenberger,usually you'll see them fit around edges though... you can configure it,6/6/15,1
mpfaffenberger,You kinda have to engineer it around your goal.,6/6/15,1
lossyrob,hmmm. So a goal that I think a lot of us have is to extract vector data out of imagery,6/6/15,1
mpfaffenberger,Yeah definitely.,6/6/15,1
lossyrob,is that a good approach for that task?,6/6/15,1
mpfaffenberger,I'm not entirely sure.,6/6/15,1
mpfaffenberger,I think you definitely want to use an edge detector ,6/6/15,1
mpfaffenberger,"There's one built into OpenCV that's not so amazing, but blazing fast",6/6/15,1
mpfaffenberger,it's called Canny,6/6/15,1
mpfaffenberger,You should be able to get LineStrings out of Canny,6/6/15,1
mpfaffenberger,and you could stitch together those linestrings into polygons,6/6/15,1
lossyrob,"nice. out of ignorance, what_Ñés the difference between Edge Detection and Feature Extraction?",6/6/15,1
mpfaffenberger,"You'll get pixel coordinate values out of opencv, then you just apply the GeoT from GDAL on those and reproject... then make a well known text?",6/6/15,1
mpfaffenberger,Damn.... good question.,6/6/15,1
lossyrob,"what we_Ñéd want to do is transfer the geotrellis raster to OpenCV, get the coordinate values, then translate it into geotrellis.vector.Geometry objects (wrapper around JTS)",6/6/15,1
lossyrob,so you can one-liner some polygon extraction. that_Ñés the dream anyway,6/6/15,1
mpfaffenberger,"I think edge detection is a potential step of image segmentation. If you were doing some kind of object (cats or faces) I think you'd want to segment using an edge detector, and then use feature extraction to characterize each segment into a matrix.",6/6/15,1
mpfaffenberger,(smaller matrix),6/6/15,1
mpfaffenberger,Might not be the right person to ask though.,6/6/15,1
lossyrob,"So feature extraction is more in the traditional ML sense, as in preparing input vectors to models?",6/6/15,1
mpfaffenberger,Precisely,6/6/15,1
lossyrob,awesome. thanks for clearing that up,6/6/15,1
mpfaffenberger,But you have to slam the matrix down into a vector,6/6/15,1
mpfaffenberger,A lot of neural network papers suggest the pooling operation,6/6/15,1
mpfaffenberger,max/mean on columns basically,6/6/15,1
lossyrob,hmmm,6/6/15,1
lossyrob,"This is cool stuff, glad to know someone who knows a lot more than me on this subject :)",6/6/15,1
mpfaffenberger,"Dude, I feel like I'm the one in the dark. :)",6/6/15,1
mpfaffenberger,These are tough problems for sure.,6/6/15,1
mpfaffenberger,Been keeping my eye on GeoTrellis for a few weeks now.,6/6/15,1
mpfaffenberger,You guys support cassandra?,6/6/15,1
lossyrob,"I_Ñém going to ping you once GeoTrellis CV work starts. Also @alasin is doing an Google Summer of Code project on cloud detection/removal and has a CV background, might be interesting to talk to.",6/6/15,1
lossyrob,We have a cassandra feature branch,6/6/15,1
lossyrob,and a GSoC student working to get it fully supported,6/6/15,1
lossyrob,the problem with cassandra is range queries,6/6/15,1
lossyrob,"If we have our tiles stored in cassandra, we can pull them out one by one no problem",6/6/15,1
mpfaffenberger,Ah... so space filling curves are out of the picture?,6/6/15,1
mpfaffenberger,Or you just have to do one query per possible entry in the range?,6/6/15,1
lossyrob,"but if we want to take a 3D space and only pull those tiles, then we translate that into SFC ranges, which can be a whole lot of ranges",6/6/15,1
lossyrob,and so to be efficient it would have to handle the many SFC index ranges really fast,6/6/15,1
lossyrob,Accumulo kicks ass at it,6/6/15,1
lossyrob,Cassandra doesn_Ñét even support OR clauses,6/6/15,1
mpfaffenberger,Yeah... HBase and Accumulo are similar right? Does HBase support range queries?,6/6/15,1
lossyrob,"so I can_Ñét do `where (r1Min <= index AND index < r1Max) OR (r2Min <= AND index <r2Max)`, only the `AND`s ",6/6/15,1
mpfaffenberger,Ouch.,6/6/15,1
lossyrob,"yes, Ihaven_Ñét worked with HBase, but as far as I know",6/6/15,1
lossyrob,they are sort of drop in,6/6/15,1
mpfaffenberger,Gotcha.,6/6/15,1
lossyrob,"we want to support HBase, got some questions about htat at the Spark Summit",6/6/15,1
lossyrob,and our answer for why we went with Accumulo over HBase was basically_Ñ_well GeoMesa was using it so we checked it out lol,6/6/15,1
mpfaffenberger,GeoMesa's currently refactoring a little to support HBase as well.,6/6/15,1
lossyrob,nice,6/6/15,1
lossyrob,"We have abstracted over our backends, so it wouldn_Ñét be too tough to support it",6/6/15,1
mpfaffenberger,Yeah that's really good.,6/6/15,1
lossyrob,"it_Ñés how we_Ñére supporting HDFS, Accumulo, Cassandra, and S3_Ñ_would be adding it to the list",6/6/15,1
lossyrob,"that_Ñés assuming we find a way to do the range indexing on Cassandra. Even if not, it seems like it will be a good option for just serving out single tiles fast",6/6/15,1
mpfaffenberger,That's definitely a +. Solid design choices.,6/6/15,1
mpfaffenberger,Seems like cassandra is geared towards a different kind of data.,6/6/15,1
mpfaffenberger,I didn't know that it doesn't support ranges though...,6/6/15,1
mpfaffenberger,http://docs.opencv.org/doc/tutorials/imgproc/imgtrans/canny_detector/canny_detector.html,6/6/15,1
mpfaffenberger,That's an example of your edge detector,6/6/15,1
mpfaffenberger,"In that post, they use a slider to have the user choose an appropriate threshold... For your use case you will have to create an algorithm that chooses an adaptive threshold.",6/6/15,1
mpfaffenberger,OpenCV should have some builtin stuff for that too.,6/6/15,1
lossyrob,yeah that was a dissapointing find,6/6/15,1
lossyrob,but I think we can get around it.,6/6/15,1
lossyrob,we have some good people on it  (@allixender) ,6/6/15,1
mpfaffenberger,Good to know. It's been good chatting. Glad I decided to drop by.,6/6/15,1
lossyrob,"ok cool. I_Ñém excited to start to try out Canny. It_Ñés not too far down the task list, but there_Ñés a lot of stuff above it (as always)",6/6/15,1
lossyrob,yeah thanks for stopping by!,6/6/15,1
allixender,"@lossyrob @mpfaffenberger yes, the querying in cassandra certainly is a bit special. Range Queries are supported, but they heavily depend on the indexing scheme and I think the data type. You still can do a full table scan (with `ALLOW FILTERING`) and filter the result set (not quite ideal). The Cassandra data model for GeoTrellis use might need to get redesigned. I am sort of switching between getting the unit tests nice and learning the GeoTrellis filtering and thinking/playing with the Cassandra indexing options",6/8/15,1
allixender,"@lossyrob @notthatbreezy https://github.com/allixender/geotrellis/commit/9c4ef7f4cea786004035faf315a9e4319ab0cd0b fixes the embedded cassandra unit tests ( https://github.com/geotrellis/geotrellis/issues/1097 ). Shall I issue a pull request to upstream/feature/cassandra or just continue for now? I would now start to look into layer structure, indexing and filtering, I guess?",6/8/15,1
lossyrob,"If you have fixes to the unit tests, would be good to get that into feature/cassandra, so PR.",6/8/15,1
lossyrob,"@allixender and we can go back and forth on that if we need to (which is nice about branches/PRs, if you have to make fixes you can just switch back, make the change, push and switch back to whatever else you_Ñére doing)",6/8/15,1
lossyrob,"if you need to you can branch from your branch, so that you can have your changes in what you_Ñére working on. Most likely we will merge in the unit test PR before we merge in any of your other work.",6/8/15,1
lossyrob,"The next thing would be to update the feature/cassandra branch to master, with the API changes from https://github.com/geotrellis/geotrellis/pull/1102",6/8/15,1
lossyrob,@echeipesh is working on getting that PR merged,6/8/15,1
alasin,"Is there a way to map tile values on the basis of (x, y) coordinates rather than their values? Something like this `tile.map((col, row) => if(col > ....) ....)`? Or some way in which we can iterate over a tile like `for{ i <- 0 to cols; j <- 0 to rows} tile.get(i, j)`, check a condition and create a new tile out of this?",6/8/15,1
lossyrob,"Hey @alasin_Ñ_good question. Previous to the PR I merged in with the MultiBand/GeoTiff changes (https://github.com/geotrellis/geotrellis/pull/1066), we would always do something like this",6/8/15,1
lossyrob,"```scalacfor(0)(_ < tile.rows, _ + 1) { row =>  cfor(0)(_ < tile.cols, _ + 1) { col =>    val v = tile.get(col, row)    val z = someFunction(col, row, v)    newTile.set(col, row, z)  }}```",6/8/15,1
lossyrob,so we_Ñéd iterate over the tile and set things into a mutable tile we had created above that code,6/8/15,1
lossyrob,"but now, I made changes to have a map function do this:",6/8/15,1
lossyrob,"```scalatile.map { (col, row, z) => z + col + row }//ortile.map(someFunction) // where someFunction: (Int, Int, Int) => Int```",6/8/15,1
lossyrob,the reason why we never had that before is because of Boxing and specialization,6/8/15,1
lossyrob,"which are fun topics, that I won_Ñét get into too much here",6/8/15,1
lossyrob,"but suffice it to say that Function2 is specialized, but Function3 is not specialized",6/8/15,1
lossyrob,"so `f: (T, T) => T` will not box primitives, so you won_Ñét box `f: (Int, Int) => Int`",6/8/15,1
lossyrob,but Function3 will box integers. Which makes things very slow.,6/8/15,1
lossyrob,It took some macro magic to get around that: https://github.com/geotrellis/geotrellis/blob/master/macros/src/main/scala/geotrellis/macros/TileMacros.scala#L34,6/8/15,1
lossyrob,"so, to answer your question, use the map method that takes col and row (or mapDouble if you are using doubles)",6/8/15,1
alasin,Cool. That's exactly what I was looking for :),6/8/15,1
jamesmcclain,@lossyrob : I have created a pull request: https://github.com/WikiWatershed/model-my-watershed/pull/318,6/8/15,1
lossyrob,ok thanks @jamesmcclain ,6/8/15,1
jamesmcclain,Thank you :smile: ,6/8/15,1
lossyrob,"I just published snapshots, which you can use if you add",6/8/15,1
lossyrob,"```scalaresolvers += ""snapshots""     at ""https://oss.sonatype.org/content/repositories/snapshots""```",6/8/15,1
lossyrob,"to your build files. That means you can pull `""com.azavea.geotrellis"" %% ""geotrellis-vector"" % ""0.10.0-SNAPSHOT_Ñù` from sonatype instead of building it locally",6/8/15,1
lossyrob,make sure to use `https`,6/8/15,1
lossyrob,"if you do have snapshots published locally, they will take precedence, so if you want to test you_Ñéll have to clear your .ivy2/local and cache dirs of geotrellis",6/8/15,1
jamesmcclain,"Okay, I'll give it a try.  So this repository will essentially track master?",6/8/15,1
lossyrob,nope,6/8/15,1
lossyrob,it_Ñés just a snapshot I published now,6/8/15,1
lossyrob,"so it_Ñés the state of master right now, but moving forward I dont_Ñé want to continually update snapshots (but you shouldn_Ñét rely on them remaining the same)",6/8/15,1
pomadchin,How often there will be repo snapshots update?,6/8/15,1
lossyrob,I_Ñéll publish a milestone release which you_Ñéll be able to point to with stability,6/8/15,1
lossyrob,that_Ñés a good question. what do you think?,6/8/15,1
jamesmcclain,"Yeah, I was going to ask if the snapshot updates or if it increments to 0.11",6/8/15,1
lossyrob,so far I haven_Ñét really been using it much. but I should be. I could push out after every major PR or something,6/8/15,1
lossyrob,no it just overwrites,6/8/15,1
pomadchin,yep mb every time smth global completes,6/8/15,1
lossyrob,"the milestone will be something like `""com.azavea.geotrellis"" %% ""geotrellis-vector"" % _ÑÒ0.10.0-M1_Ñù` and will be available on maven central, and won_Ñét ever change",6/8/15,1
pomadchin,"So then, I guess, filter refactor pr will require republishing",6/8/15,1
lossyrob,yeah,6/8/15,1
lossyrob,I_Ñém hoping to incorporate that into the milestone release,6/8/15,1
lossyrob,I think I have to actually,6/8/15,1
pomadchin,"Nice idea with milestones, very cool",6/8/15,1
lossyrob,"so I_Ñéll publish a snapshot after that, and then after testing publish the milestone",6/8/15,1
lossyrob,"yeah it gets around ahving to push out a full release, which I don_Ñét htink we_Ñére ready for just yet",6/8/15,1
jamesmcclain,"Cool, sounds like things are really moving",6/8/15,1
lossyrob,but allows people to use what_Ñés there in a non-insane way,6/8/15,1
pomadchin,In gt things are always moving :d ,6/8/15,1
jamesmcclain,It seems so!,6/8/15,1
allixender,"ok, @lossyrob @notthatbreezy https://github.com/geotrellis/geotrellis/pull/1107 pull request for cassandra unit tests update",6/9/15,1
allixender,"I am building and running geotrellis and geotrellis-spark both with OpenJDK 1.7.80 and Oracle JDK 1.8.45, just FYI",6/9/15,1
lossyrob,"ok cool, thanks. It was discussed that Cassandra gives a warning against using the OpenJDK, so we might just say for Cassandra support that we recommend the oracle version",6/9/15,1
allixender,hey,6/9/15,1
allixender,"umm, actually, not sure what happended but I get now test erros in geotrellis-engine? Did the mentioning of ""Fixes #1097 "" in the pull request already cause some subsequent actions?",6/9/15,1
allixender,and OpenJDK 7 the tests will fail occasionally with PermGenSpace OutOfMemory,6/9/15,1
lossyrob,test fails where? locally or travis?,6/9/15,1
allixender,"``` scalaOperation Error. Trace: LargeWindow     _ÓÒ_ÓÑ_ÓÑ_ÓÑ_ÓÑ_ÓÑ_ÓÎ_ÓÑLiteral     _Ó_     _Ó_    _ÓÓ_ÓÑ_ÓÑ_ÓÑResult: BoxedUnit (in 0 ms)     _Ó_     _ÓÓ_ÓÑWindow     _Ó_          _ÓÒ_ÓÑ_ÓÑ_ÓÑ_ÓÑ_ÓÑDefenestrator     _Ó_          _Ó_           _ÓÓ_ÓÑ_ÓÑ_ÓÑ_ÓÑ_ÓÑ_ÓÑResult: ERROR: extremely high (in 0 ms): geotrellis.engine.Defenestrator._run(error.scala:26)geotrellis.engine.Defenestrator._run(error.scala:24)geotrellis.engine.Operation.run(Operation.scala:165)geotrellis.engine.actors.Worker$$anonfun$receive$1.applyOrElse(Worker.scala:49)akka.actor.Actor$class.aroundReceive(Actor.scala:465)geotrellis.engine.actors.Worker.aroundReceive(Worker.scala:35)akka.actor.ActorCell.receiveMessage(ActorCell.scala:516)akka.actor.ActorCell.invoke(ActorCell.scala:487)akka.dispatch.Mailbox.processMailbox(Mailbox.scala:254)akka.dispatch.Mailbox.run(Mailbox.scala:221)akka.dispatch.Mailbox.exec(Mailbox.scala:231)scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.pollAndExecAll(ForkJoinPool.java:1253)scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1346)scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)s```",6/9/15,1
allixender,both,6/9/15,1
allixender,locally and in the travis build https://travis-ci.org/geotrellis/geotrellis/jobs/65985452,6/9/15,1
allixender,"um, maybe I haven't seen the local ones for the last days, as I mostly used testOnly, or ""project spark"" test",6/9/15,1
lossyrob,travis is failing because of travis,6/9/15,1
lossyrob,there_Ñés no error in that log,6/9/15,1
lossyrob,sometimes travis just dies.,6/9/15,1
lossyrob,we_Ñéve had to split up the build-all.sh to fix that. I think that_Ñés part of eugene_Ñés PR,6/9/15,1
allixender,there is the same error ,6/9/15,1
allixender,in travis and locally,6/9/15,1
lossyrob,https://github.com/echeipesh/geotrellis/blob/feature/better-catalog-filtering/buildall.sh,6/9/15,1
lossyrob,I don_Ñét see where there is an error in the travis log you linked to,6/9/15,1
allixender,https://travis-ci.org/geotrellis/geotrellis/jobs/65985452,6/9/15,1
allixender,1601.3 failed,6/9/15,1
allixender,around lines 4100,6/9/15,1
lossyrob,those are tests that test failure,6/9/15,1
allixender,oh,6/9/15,1
lossyrob,and you_Ñére seeing the failure output,6/9/15,1
allixender,:smile_cat: that's good then?,6/9/15,1
lossyrob,"unless you see at the end of the logs that scalatest says tests are failing, then the tests passed, even though they are checking for errors",6/9/15,1
lossyrob,yup. I know I_Ñéve been tripped up on that in the past.,6/9/15,1
allixender,"ok,",6/9/15,1
allixender,"ok, as said, the unit tests run fine under oracle jdk",6/9/15,1
allixender,8,6/9/15,1
allixender,I could test oracle jdk 7 vs openjdk 7,6/9/15,1
lossyrob,so it fails with openjdk 8?,6/9/15,1
lossyrob,is this when you try the buildall script? or manually runing it just for the one project/,6/9/15,1
allixender,"however, a MaxPermGen=384m come from somewhere in the project / test config, and then one Cassandra Unit test stumbles at that point, and the cassandra instance is not destroyed properly",6/9/15,1
allixender,haven't tested openjdk 8 actually,6/9/15,1
allixender,"better do that, right",6/9/15,1
allixender,makes more sense for future ,6/9/15,1
allixender,... that future is now anyway as JDK 7 is end of life,6/9/15,1
allixender,"ok, I have openjdk 8 at hand too",6/9/15,1
lossyrob,"yeah. I just don_Ñét think it_Ñés a problem with the JDK_Ñ_there seems to be a memory leak in sbt, so when we run the tests without restarting sbt, it pops",6/9/15,1
allixender,give me 10 minutes ,6/9/15,1
lossyrob,ok cool,6/9/15,1
allixender,"yes, the memory builds up to 2-3 GB for sbt",6/9/15,1
allixender,another 2 gb for cassandra,6/9/15,1
allixender,another 1-2 GB for IDEa,6/9/15,1
allixender,haha,6/9/15,1
allixender,OpenJDK 64-Bit Server VM warning: ignoring option MaxPermSize=384m; support was removed in 8.0,6/9/15,1
allixender,FYI,6/9/15,1
allixender,same in Oracle 8 JDK,6/9/15,1
lossyrob,oh fun. wonder what the replacement is,6/9/15,1
pomadchin,"removed in 8 version, thought there is `MaxMetaspaceSize`",6/9/15,1
allixender,I wonder where that setting in the GeoTrellis build/tests is?,6/9/15,1
allixender,because I can't seem to increase it :tongue: ,6/9/15,1
lossyrob,do the spark tests fork?,6/9/15,1
lossyrob,then it shouldn_Ñét matter about your IDE,6/9/15,1
lossyrob,"also I though the embedded cassandra was started in a new JVM, no?",6/9/15,1
allixender,no doesn't matter with IDE,6/9/15,1
allixender,would also die if Ihave no IDE on and only run ./build-all.sh for example,6/9/15,1
allixender,"yes, cassandra is visible as a separate process",6/9/15,1
allixender,in ps,6/9/15,1
allixender,or top,6/9/15,1
lossyrob,ok. then the max size shouldn_Ñét help there,6/9/15,1
allixender,separate vm,6/9/15,1
lossyrob,"so just to be sure, this is happening only when you_Ñére doing buildall.sh, yeah?",6/9/15,1
allixender,"umm, good question...",6/9/15,1
allixender,certainly not with testOnly,6/9/15,1
allixender,"but I think already with `./sbt ""project spark"" test`",6/9/15,1
lossyrob,hmmm. that wouldn_Ñét be good,6/9/15,1
allixender,"I'll confirm, once my openjdk 8 build/test is thorugh",6/9/15,1
allixender,Openjdk 8 also fine,6/9/15,1
allixender,"not sure where that MaxPermSize=384m stems from though, not sure if should be increased for Java 7",6/9/15,1
allixender,I can finally test Oracle JDK 7,6/9/15,1
allixender,and try to overwrite MaxPermSize=384m with like -J-MaxPermSize=512m,6/9/15,1
allixender,"and see if it'll show in `./sbt ""project spark"" test`",6/9/15,1
allixender,"by the way @lossyrob @echeipesh in `build-all.sh` why `./sbt -J-Xmx2G ""project spark"" package ""project spark"" test ` ?",6/9/15,1
allixender,it'll be tested in the packaging cycle anyway?,6/9/15,1
allixender,isn'T the package life cycle step checking on compile test package,6/9/15,1
allixender,the spark tests run twice if I see that correctly,6/9/15,1
allixender,"and they are the most time consuming too, right, hahaha?",6/9/15,1
allixender,"concluding at first, JAVA 8 is fine",6/9/15,1
lossyrob,package doesn_Ñét run the tests,6/9/15,1
allixender,"`./sbt -J-Xmx2G -XX:MaxPermSize=512m ""project spark"" package` maybe ..",6/9/15,1
allixender,WTF?! `ps shows me it still pulls another MaxPermSize from somewhere,6/9/15,1
allixender,"`java -Xms1536m -Xmx1536m -XX:MaxPermSize=384m -XX:ReservedCodeCacheSize=192m -Xmx2G -Dsbt.global.base=/home/akmoch/.sbt/0.13.8 -jar ./.lib/0.13.8/sbt-launch.jar -XX:MaxPermSize=512m project spark package`",6/9/15,1
pomadchin,try `./sbt  -perm 2048`,6/9/15,1
lossyrob,or ./sbt -mem 2048,6/9/15,1
lossyrob,setting java options through the SBT,6/9/15,1
lossyrob,is tough. you have to put it in the build file,6/9/15,1
allixender,isn't mem for Xmx,6/9/15,1
pomadchin,and for Xms ,6/9/15,1
allixender,"```[akmoch@acer1 gt-main-alex]$ ./sbt -J-Xmx2G -XX:MaxPermSize=512m -perm 512 ""project spark"" testDetected sbt version 0.13.8Using /home/akmoch/.sbt/0.13.8 as sbt dir, -sbt-dir to override.[info] Loading project definition from /home/akmoch/dev/build/gt-main-alex/project[info] Set current project to geotrellis (in build file:/home/akmoch/dev/build/gt-main-alex/)[info] Set current project to geotrellis (in build file:/home/akmoch/dev/build/gt-main-alex/)[info] Set current project to geotrellis (in build file:/home/akmoch/dev/build/gt-main-alex/)[warn] The `-` command is deprecated in favor of `onFailure` and will be removed in 0.14.0[warn] The `-` command is deprecated in favor of `onFailure` and will be removed in 0.14.0[error] Expected letter[error] Expected symbol[error] Expected '!'[error] Expected '+'[error] Expected '++'[error] Expected 'debug'[error] Expected 'info'[error] Expected 'warn'[error] Expected 'error'[error] Expected ';'[error] Expected end of input.[error] Expected '--'[error] Expected 'show'[error] Expected 'all'[error] Expected '*'[error] Expected '{'[error] Expected project ID[error] Expected configuration[error] Expected key[error] Expected '-'[error] 512[error] ^[error] Not a valid command: perm[error] Not a valid project ID: perm (similar: demo, proj4, spark)[error] Expected ':' (if selecting a configuration)[error] Not a valid key: perm[error] perm[error]     ^``````",6/9/15,1
lossyrob,yeah that_Ñés not going to do it. java options aren_Ñét set in the sbt script like that. ,6/9/15,1
lossyrob,I have to jump off now. Talk to you all later,6/9/15,1
allixender,"Ok, OpenJDK 7 with ""JUST"" `./sbt -J-Xmx2G -XX:MaxPermSize=512m ""project spark"" test` will fail that in feature/cassabdra, `[error] Not a valid key: MaxPermSize`  :angry: ",6/9/15,1
allixender,"also `./sbt -J-Xmx4G ""project spark"" test` fails in my cassandra branch ... , testOnly would work, standard tests run thorugh would only work in JDK 8 (both Oracle and OpenJDK) ... if I/we don't find the right place to increase the MaxPermSpace for the very most initial jvm initialisation, weird",6/9/15,1
lossyrob,you should try setting the options into the build file like here https://github.com/geotrellis/geotrellis/blob/master/project/Build.scala#L204,6/9/15,1
lossyrob,"__Note__ I just merged in Eugene_Ñés catalog refactor changes. Updating will break catalogs. I am including this in the Milestone release, but I_Ñéll keep the snapshots where they are at right now.",6/9/15,1
pomadchin,"Nice, waited this merge",6/9/15,1
allixender,"@lossyrob @echeipesh thanks for info. So now I would start looking into to merging those changes in the cassandra branch, and maybe even getting cassandra branch rebased to more recent status of the geotrellis master branch? Would that make sense?",6/10/15,1
lossyrob,I_Ñéd say no to rebase_Ñ_we don_Ñét want to lose when the commits happened.,6/10/15,1
allixender,oh rebasing would discard those?,6/10/15,1
lossyrob,I_Ñém never a fan of rewriting history,6/10/15,1
lossyrob,"yeah, it would delete the old commmits and push them up to be against HEAD",6/10/15,1
allixender,"yes fair enough, agreed",6/10/15,1
allixender,I didn't know that it worked this way,6/10/15,1
allixender,so I have to chase up commit from master ideally merge them one by one or so,6/10/15,1
allixender,?,6/10/15,1
lossyrob,I feel like a lot of people disagree with me about my no-rebase approach. But I like to keep the history as it happpened,6/10/15,1
lossyrob,just merge,6/10/15,1
allixender,so you can <blame> :tongue: ,6/10/15,1
lossyrob,"merge is the simpler version, rebase works harder",6/10/15,1
allixender,ok will do so,6/10/15,1
lossyrob,git fetch upstream & git merge upstream/master. then deal with all the merge conflicts :-X,6/10/15,1
allixender,"haha, is there a smoother way? like I could merge one by one via commits via their  hashs?",6/10/15,1
allixender,I don't know the code well enough probably to deal with all issues at once,6/10/15,1
lossyrob,hmm,6/10/15,1
lossyrob,I don_Ñét think so. Like something that merges one by one?,6/10/15,1
lossyrob,I feel like that might be a thing,6/10/15,1
allixender,"ah, by the way, for the cassandra unit test pull request - shall I do the edits and re-submit a pull request? Or can those edits be added to the existing pull requets?",6/10/15,1
lossyrob,my google-fu is failing me though :punch:,6/10/15,1
lossyrob,"you can just commit to that branch and push that  commit, it_Ñéll appear on the PR",6/10/15,1
allixender,ok cool,6/10/15,1
allixender,black magic,6/10/15,1
notthatbreezy,"ha yeah, I'm a big fan of rebasing -- just not on the develop/master branch...",6/10/15,1
allixender,git is handy,6/10/15,1
notthatbreezy,but the merge thing works too,6/10/15,1
lossyrob,yeah github makes it handier,6/10/15,1
lossyrob,@notthatbreezy what about rebasing a branch where you are not the only committer?,6/10/15,1
notthatbreezy,"yeah, it does get tricky in that situation and require some coordination. I would save rebasing until the last possible moment (right before doing a merge into master), or until it's necessary to continue work on the branch, otherwise you can end up with some really funky history in master",6/10/15,1
lossyrob,"so you would recommend rebasing, which rewrites commit history for commits that aren_Ñét yours?",6/10/15,1
notthatbreezy,"I don't have a big problem with that no, as long as the other people working on that branch are aware of it and know to do a fresh checkout/force pull before continuing work",6/10/15,1
lossyrob,keep-yo-hands-off-my-commit-history :rage1:,6/10/15,1
notthatbreezy,"it's still your commit though, that doesn't change, just the ordering",6/10/15,1
notthatbreezy,haha,6/10/15,1
notthatbreezy,dealing with conflicts in rebasing can be harder than a merge though when something gets really out-of-date,6/10/15,1
lossyrob,"yeah. It_Ñés easier, it_Ñés trutheir. Merging :tm:",6/10/15,1
lossyrob,I_Ñém going to write a youtube commercial for merging vs rebasing.,6/10/15,1
lossyrob,And that_Ñés the tagline,6/10/15,1
allixender,+1 for the commercial,6/10/15,1
allixender,I'd share it on twitter,6/10/15,1
notthatbreezy,your emoji game is strong @lossyrob :muscle: ,6/10/15,1
allixender,:thumbsup: ,6/10/15,1
lossyrob,haha http://www.emoji-cheat-sheet.com/ is my friend,6/10/15,1
notthatbreezy,"we should do a crossfire-esque discussion, I'll volunteer to be pro-rebase",6/10/15,1
lossyrob,lol. That would be so funny,6/10/15,1
notthatbreezy,:hammer: ,6/10/15,1
lossyrob,Who do we get to host?,6/10/15,1
notthatbreezy,"oh, I was gonna say put it on youtube",6/10/15,1
lossyrob,"no I mean, don_Ñét crossfire things usually have a host that moderates",6/10/15,1
lossyrob,and then try to calm things down when the two sides are basically just screaming LET ME FINISH at each other,6/10/15,1
notthatbreezy,"ha yeah, who's the calmest person at Azavea",6/10/15,1
lossyrob,kenny,6/10/15,1
allixender,it'll end in the killed kenny,6/10/15,1
allixender,*they,6/10/15,1
lossyrob,hahaha,6/10/15,1
notthatbreezy,"fine by me, kenny taught me all I know about rebasing",6/10/15,1
lossyrob,with some actual cross fire,6/10/15,1
allixender,this pun is probably really old at azavea ,6/10/15,1
lossyrob,nope actually haven_Ñét even heard it,6/10/15,1
lossyrob,kenny is too calm and nice to want to kill off at the end of every episode,6/10/15,1
allixender,hahaha,6/10/15,1
notthatbreezy,oh this is a great opportunity to use our nerf weapons,6/10/15,1
lossyrob,I want it to devolve into us playing crossfire ![cf](http://38.media.tumblr.com/a0aa2070fb39c51d339c7024125b6158/tumblr_mwy396F3qe1ro8ysbo1_500.gif),6/10/15,1
lossyrob,whoa that is a lot to look at. where is that lightning coming from?,6/10/15,1
notthatbreezy,oh I used to have that ,6/10/15,1
notthatbreezy,"kind of wish I still did have it, things are expensive on ebay and amazon",6/10/15,1
lossyrob,whoa yeah,6/10/15,1
lossyrob,"I mean I wish I still had mine to play, not to sell",6/10/15,1
lossyrob,either that or hungry hungry hippos,6/10/15,1
lossyrob,I like how the geotrellis room has now devolved into board game nostalgia.,6/10/15,1
lossyrob,for completeness_Ñ_![hh](https://metrouk2.files.wordpress.com/2015/02/hungry-hippos.gif),6/10/15,1
notthatbreezy,ha yeah,6/10/15,1
lossyrob,"@zifeo regarding reading raster layers without bounds, see here: https://github.com/geotrellis/geotrellis/blob/master/spark/src/test/scala/geotrellis/spark/io/accumulo/AccumuloRasterCatalogSpec.scala#L74",6/11/15,1
lossyrob,Query can just take a layer RDD.,6/11/15,1
lossyrob,"@echeipesh it was confusing to not have a read function that only took layer ID and no query, I asked Teo to add that",6/11/15,1
lossyrob,"but come to think of that, I_Ñéd like to have that in the milestone release. @zifeo I_Ñém publishing that Friday, so if you don_Ñét think you_Ñéll have it in by then one of us can handle that, just let me know.",6/11/15,1
allixender,"FYI, https://github.com/geotrellis/geotrellis/blob/master/project/Build.scala#L375 Build.scala settings is the place to add `""-XX:MaxPermSize=512m"",` (increasing PermSize from that seemingly standard-ish 384M size to 512M)",6/11/15,1
allixender,And then it would run with JDK 7 (OpenJDK and ORacle JDK),6/11/15,1
allixender,@lossyrob @notthatbreezy I amended the the pull request for #1097 ,6/11/15,1
zifeo,"@lossyrob all right, this Friday ? I will work on it today then.",6/11/15,1
lossyrob,cool thanks!,6/11/15,1
zifeo,"I got a two random questions when going through the code of spark version:- what are the advantages of using a callback class instead of a function type ?- what are the advantages of using mapPairs instead of mapTiles when only mapping on Tile (i.e. in `op/local/LocalRasterRDDMethods`)?",6/11/15,1
lossyrob,who uses a callback? is it the rasterizer?,6/11/15,1
zifeo,yes,6/11/15,1
lossyrob,one sec,6/11/15,1
lossyrob,ok,6/11/15,1
lossyrob,"so, the rasterizer API sucks. feel free to rewrite",6/11/15,1
lossyrob,but there is a reason the Callback is there,6/11/15,1
lossyrob,"and it_Ñés because of specialization and boxing, which is a common theme",6/11/15,1
lossyrob,"but, it actually doesn_Ñét have to be there now. So, `Callback` used to take another parameter",6/11/15,1
lossyrob,lemme find the code,6/11/15,1
lossyrob,this is what it is now,6/11/15,1
lossyrob,2184,6/11/15,1
lossyrob,ugh,6/11/15,1
lossyrob,https://github.com/geotrellis/geotrellis/blob/master/raster/src/main/scala/geotrellis/raster/rasterize/Rasterizer.scala#L26,6/11/15,1
lossyrob,this is what it used to be,6/11/15,1
lossyrob,https://github.com/geotrellis/geotrellis/blob/0.9/src/main/scala/geotrellis/feature/rasterize/Rasterizer.scala#L8,6/11/15,1
lossyrob,it used to pass in the geometry for each callback call,6/11/15,1
lossyrob,"which was totally unneeded, so I removed it",6/11/15,1
lossyrob,"*but*, when it was there, that made Callback a thing that mimiced Function3",6/11/15,1
lossyrob,https://github.com/scala/scala/blob/2.11.x/src/library/scala/Function3.scala,6/11/15,1
lossyrob,if you look at the difference between Function3 and Function2:,6/11/15,1
lossyrob,https://github.com/scala/scala/blob/2.11.x/src/library/scala/Function2.scala,6/11/15,1
lossyrob,"you_Ñéll see what_Ñés going on, and why we would want a specificly typed thing that represented our function with 3 parameters.",6/11/15,1
lossyrob,"Function2 is specialized. that means, for primitives, it doesn_Ñét box. Boxing is evil, if performance is good (which it very much is)",6/11/15,1
lossyrob,so we had a Function3 passed in that was being called for each cell value under a polygon lets say. That_Ñés a lot of boxed calls,6/11/15,1
lossyrob,so @ahinz realized this and made a change to use a specific trait.,6/11/15,1
lossyrob,that sped up performance dramatically.,6/11/15,1
lossyrob,"then, I refactored out the geometry call. which dropped that down to Function2, which is specialized. So there_Ñés no need for it anymore.",6/11/15,1
lossyrob,"Also, I devised a way to have specific traits implemented for things that represent Function3 +, but using macros it doesn_Ñét box. This can be seen here: https://github.com/geotrellis/geotrellis/blob/master/macros/src/main/scala/geotrellis/macros/TileMacros.scala and here: https://github.com/geotrellis/geotrellis/blob/master/raster/src/main/scala/geotrellis/raster/MappableTile.scala",6/11/15,1
lossyrob,"so really, any place in the codebase where we are using a specific trait to get around using Function3+ because of boxing issues, we should implement macros that do that instead. Because that makes the API nice without any cost to performance, which is sort of the holy grail in library design, at least for this library.",6/11/15,1
lossyrob,"and the Rasterizer API is in desparate need of an overhaul. I really don_Ñét like using it, and we can do better. Feel free to pitch ideas or PRs :) but I haven_Ñét gotten around to revamping it yet_Ñ_it_Ñés on the list",6/11/15,1
zifeo,"Okay, much more clearer now, thank you ! And what about the mapPairs/mapTiles thing ?",6/12/15,1
lossyrob,"mapPairs for an RDD maps over both the key and the tile. mapTiles only maps over the tiles. So they do the same thing, it_Ñés more about what the function passed in gets/returns",6/12/15,1
zifeo,"so we could replace `rasterRDD.mapPairs { case (t, r) => (t, Sinh(r)) }` by `rasterRDD.mapTile(Sinh(_))`, there is no implicit or hidden trick ?",6/12/15,1
pomadchin,mb they gonna release bey version every day :o,6/12/15,1
pomadchin,https://spark.apache.org/news/spark-1-4-0-released.html,6/12/15,1
lossyrob,we could definitely,6/12/15,1
lossyrob,"@zifeo if there are instances of this where we are using mapPairs when we could use mapTiles, it_Ñés a matter of that mapTiles wasn_Ñét around when those were written :)",6/12/15,1
lossyrob,@pomadchin yeah aweomse,6/12/15,1
allixender,"@lossyrob @notthatbreezy based on `git fetch upstream & git merge upstream/master. then deal with all the merge conflicts` I updated my github clone/branch from geotrellis/master. Now with `git log feature/cassandra-gsoc2015..master`I can see the complete difference in commit history between the current geotrellis master and my feature/cassandra branch, the logs show the difference from the perspective from the left branch, eg what's missing/different on master as compared to feature/cassandra-gsoc2015. If the other way round `git log master..feature/cassandra-gsoc2015`then it would list all the commits for cassandra outside of master.",6/15/15,1
allixender,"so with `git log feature/cassandra-gsoc2015..master | egrep ""^commit""` I get basically this list of 127 commits with their hashes that I could merge one by one into the feature cassandra branch",6/15/15,1
allixender,`git merge <commit> `to merge based on the commit hash,6/15/15,1
allixender,the git log also states possible confilicting files,6/15/15,1
allixender,but I think the major problem would be that I'd have to figure out how the new filter API etc now have to be used by the cassandra features :-) ,6/15/15,1
allixender,"for anyone who might be interested, catching up commit by commit is possible, but if you give git the opportunity to reason through all subsequent commits in the history, many conflicts will be solved later along the timeline by themselves ..",6/15/15,1
allixender,"@lossyrob @notthatbreezy so git fetch upstream and merge master, even over a period of 3 months and 120+ commits, I was lucky enough to end up with 3 file conflicts that didn't seem to hard to resolv. However, I haven't tried compiling yet :worried: ",6/15/15,1
allixender,I have this cassandra branch catching up with master here https://github.com/allixender/geotrellis/tree/feature/cassandra-merge-master,6/15/15,1
allixender,"first 10 or so single merges went really nicely, but then it got messy and I tried the full fetch and merge",6/15/15,1
allixender,https://github.com/geotrellis/geotrellis/issues/1117,6/15/15,1
allixender,I'll now have to look how many breaking changes in the geotrellis spark api in the last 3 months ,6/15/15,1
allixender,"```bash#$> git status -vOn branch feature/cassandra-merge-masterYour branch is ahead of 'feature/cassandra' by 137 commits.```",6/15/15,1
allixender,"when Kevin merged `Merge branch 'feature/catalog-refactor' into feature/cassandra` he introduced a few updates that now have to be resolved manually by me, although unrelated to cassandra",6/15/15,1
allixender,https://github.com/allixender/geotrellis/tree/feature/cassandra-merge-master this brnach won't compile,6/15/15,1
allixender,unfortunately therefore also not resolve into a nicely visible project in IDEA or Eclipse,6/15/15,1
allixender,so I'll go through the files now to sort out what needs to be added again,6/15/15,1
notthatbreezy,"@allixender sounds like you're on the right track; I almost always merge branches instead of commit by commit, mostly for the reasons you stated -- git will be able to resolve many conflicts on its own.",6/15/15,1
pomadchin,"@echeipesh @lossyrob so what about accumulo versions? is it possible to maintain crossversions? (1.5.2, 1.6.2 and later 1.7.2)",6/15/15,1
pomadchin,btw just interested in 1.6.2 and 1.5.2 :D ) but really it is not a huge problem cause of docker,6/15/15,1
echeipesh,"Right now we_Ñére supporting 1.6.2. We will need to break out the packages into sub-project for cross version support. I think that is on the table for .10 release. I_Ñém not sure about going back to 1.5.2. Your mystery issue with 1.6.2 is still resolved, correct?",6/15/15,1
pomadchin,"everything is ok with 1.6.2 (was smth weird), just a question about crossversion support (just a generic question)",6/15/15,1
alasin,@lossyrob Were you able to read/write the Landsat 8 images that you shared with me? `SingleBandGeoTiff` reader doesn't seem to be working correctly with 32-bit data. ,6/15/15,1
allixender,"@lossyrob @notthatbreezy https://github.com/allixender/geotrellis/tree/feature/cassandra-merge-master branch will now compile, I'll have to get the tests right again and sort of reimplement the whole FilterSet/Spacefilter concept. The approach with Seq[queryKeyBounds] seems practical. ",6/16/15,1
allixender,"Ok, https://github.com/geotrellis/geotrellis/issues/1117 The filtering is working again, basically STATUS QUO for the cassandra branch being up to date with the geotrellis master (also the tests run again)",6/16/15,1
pomadchin,"Seems to me there would be some problems with compilation on some java versions (not sure what it caused by)? due to this overloading: https://github.com/geotrellis/geotrellis/blob/master/spark/src/main/scala/geotrellis/spark/io/s3/S3RasterCatalog.scala#L48-L62```[error] geotrellis/spark/src/main/scala/geotrellis/spark/io/s3/S3RasterCatalog.scala:40: in class S3RasterCatalog, multiple overloaded alternatives of method read define default arguments.[error] class S3RasterCatalog([error]       ^[error] one error found```",6/16/15,1
pomadchin,"more simple err example: ```scalaobject test {   def f[A, B, C](a: A, B: B, c: C, s: String = """") = s  def f[A, B, C](a: A, B: B, c: C, i: Int, s: String = """") = s""${s}${i.toString}""}```",6/16/15,1
pomadchin,"but, i got this on my machine; and here: http://ideone.com/WHmGFE",6/16/15,1
pomadchin,"but here everything is ok: http://www.tutorialspoint.com/compile_scala_online.phpmb it caused by some jvm / compiler settings?",6/16/15,1
pomadchin,more investigations,6/16/15,1
pomadchin,http://scastie.org/10351 http://scastie.org/10350,6/16/15,1
pomadchin,depends on scala version compiler,6/16/15,1
echeipesh,"@pomadchin, that_Ñés a thing. Really confused how travis is letting that one fly. But I_Ñém able to reproduce it.",6/16/15,1
echeipesh,it_Ñés a bug that was fixed from 2.10: https://issues.scala-lang.org/browse/SI-8157,6/16/15,1
pomadchin,):,6/16/15,1
echeipesh,"It_Ñés no problem, we_Ñéll just work around it in the usual manner",6/16/15,1
pomadchin,:D how ?,6/16/15,1
pomadchin,btw really funny situation with the travis,6/16/15,1
echeipesh,"yes, really wondering what the hell it_Ñés doing. Shaking my faith :)",6/16/15,1
echeipesh,adding overload like this:```,6/16/15,1
echeipesh,```,6/16/15,1
pomadchin,:D,6/16/15,1
echeipesh,dur...,6/16/15,1
echeipesh,"```  def read[K: RasterRDDReader: Boundable: JsonFormat: ClassTag](layerId: LayerId): RasterRDD[K] =    query[K](layerId, sc.defaultParallelism).toRDD```",6/16/15,1
pomadchin,"yep, feels beautiful",6/16/15,1
pomadchin,:D,6/16/15,1
echeipesh,that wasn_Ñét supposed to get commited to master_Ñ_ good thing it_Ñés a trivial change.,6/16/15,1
pomadchin,"nice that it was noticed and fixed so fast, ty a lot! There's no such overloading in a Hadoop catalog?",6/16/15,1
echeipesh,Thank you for finding it. Hadoop catalog doesn_Ñét have the same problem because the number of partitions is implied by the sequence files on fs. Plus the clompiler didn't barf in any other place : ),6/16/15,1
pomadchin,"Wow really, just don't remember and got no laptop here ):, only know that there were similar methods",6/16/15,1
vishalanand,I was looking into Apache Math3 library in GT and realized we are using the version 3.2 [here](https://github.com/geotrellis/geotrellis/blob/master/project/Dependencies.scala#L55). And I found there is a [3.5_version](http://commons.apache.org/proper/commons-math/changes-report.html) of the API which is backward compatible. It has some nice features added.  I was thinking if we should update the two year old dependency to the new one?,6/18/15,1
echeipesh,Sounds good!,6/18/15,1
vishalanand,Should I do a PR?,6/18/15,1
echeipesh,"I think it can be part of the Kriging PR, since you_Ñére going to be using it there directly. I don_Ñét think any other branch would benefit from upgrade right away?",6/18/15,1
vishalanand,"Yeah, sure! That's right!",6/18/15,1
allixender,"@lossyrob @notthatbreezy For the IngestCommand and most of Cassandra functionality, the cassandra.host config also needs to be registered in the SparkContext at the point of creation of the SparkContext, not just adding it later in the IngestCommand",6/19/15,1
allixender,How can I (we) sort of sutainably check if a cassandra host param is mentioned in the spark submit command?,6/19/15,1
allixender,"basically, the datastax spark cassandra connector provides the CassandraRRD abstractions, but It will need the `sparkConf.set(""spark.cassandra.connection.host"", args.host)` paramater being set before the actual SparkContext ist created from that conf",6/19/15,1
allixender,"I have tackled that in the tests, where I could add it statically in the config",6/19/15,1
allixender,"```scalasparkConf.set(""spark.cassandra.connection.host"", host)      .set(""spark.cassandra.connection.rpc.port"", rpcPort.toString)      .set(""spark.cassandra.connection.native.port"", nativePort.toString)    val sparkContext = SparkUtils.createLocalSparkContext(""local[8]"", ""Test Context"", sparkConf)```",6/19/15,1
allixender,but at runtime such a parameter would only be needed if there is actually cassandra functionality required,6/19/15,1
allixender,"so, a simple ingest command via spark submit would need to be able to register these parameters",6/19/15,1
allixender,"I am now looking into SparkArgs.scala, `SparkUtils.createLocalSparkContext(sparkMaster, appName, sparkConf)`",6/19/15,1
allixender,"if I am not mistaken, this is the place where `spark.cassandra.connection.host=127.0.0.1` could be picked up?",6/19/15,1
allixender,"seems one can provide those options directly with the spark-submit :thumbsup: ```shellbin/spark-submit --conf ""spark.cassandra.connection.host=127.0.0.1"" --class geotrellis.spark.ingest.CassandraIngestCommand ...```",6/19/15,1
allixender,"ok, not to worry, sorry for the spam",6/19/15,1
allixender,"brilliant. I will incorporate this into the CassandraIngestCmd,",6/19/15,1
Manuri,"Hi, Sorry for interrupting. Does anybody know if there are any resources about mapping between proj4 and WKT parameters ? :)",6/19/15,1
allixender,"Ok, on my Linux Core i3 laptop, 8GB RAM, 120 GB SSD HD, two SparkWorkers with 1G each, the spark driver/master with 1G RAM, the cassandra instance with about 2GB RAM, loading the 98  `test-200506000000_x_x.tif` one-month-tiles into cassandra takes about 2 minutes, is probably not representative at all  :thought_balloon:  ",6/19/15,1
allixender,no prob @Manuri . What paramters do you mean? Do you mean if Proj4 can read WKT?,6/19/15,1
allixender,or do you mean like crs/projection values ?,6/19/15,1
allixender,Has anyone measured the additional disk space inflation from putting tiles in databases?,6/19/15,1
Manuri,@allixender I have to convert proj4 string to WKT. So I need to map them,6/19/15,1
pomadchin,@Manuri have you tried proj4j?,6/19/15,1
Manuri,@pomadchin  I have to write the converter. Sorry for the confusion. This is one of the GSoC projects. ,6/19/15,1
Manuri,"If what I said is not clear, here is the example Rob gave me. ```scalaimport geotrellis.proj4._val crs = CRS.fromName(_ÑÒEPSG:4326_Ñù)val proj4String = crs.toProj4Stringval wkt = crs.toWKT // You write!val wkt: String = ??? //WKT text of a CRSval crs = CRS.fromWKT(wkt) // You write!// Could implement a WKT -> Proj4 Stringdef wktToProj4(wkt: String): String = ???val crs = CRS.fromString(wktToProj4 wkt)```To do this I need to map the keywords/parameters of the two types. I am reading specifications but I thought it would be good if there were any documentation about mapping between the keywords of the two types. ",6/19/15,1
allixender,:thumbsup: ,6/19/15,1
notthatbreezy,"@allixender -- yup, --conf is definitely the way to go for that type of stuff",6/19/15,1
notthatbreezy,"@manuri one resource that might be helpful to compare formats like proj4 and WKT is spatialreference.org. (e.g. http://spatialreference.org/ref/epsg/4326/ will show how to represent that CRS in WKT, proj4, etc.)",6/19/15,1
notthatbreezy,Also maybe the WKT specification if you haven't looked at that - http://docs.opengeospatial.org/is/12-063r5/12-063r5.html -- neither of these really help with detailing how to convert between the two though,6/19/15,1
Manuri,@notthatbreezy  yes I am already using spatialreference.org and I read the WKT specification. Thank you.,6/19/15,1
echeipesh,WKT looks like it_Ñés a lot more explicit in specification of a CRS,6/19/15,1
Manuri,Yes. I also thought so,6/19/15,1
alasin,Has anyone tried reading/writing Int32/UInt16 rasters using `SingleBandGeotiff` ?,6/22/15,1
alasin,@echeipesh  ?,6/22/15,1
echeipesh,"Nope, are you having issues?",6/22/15,1
alasin,"Yes. It reads just fine after the recent bug fix. But when I write it and again read it, it throws an error.",6/22/15,1
alasin,"So the only difference is that the original image is UInt16, and the new image is Int32.",6/22/15,1
alasin,And pixel values remain the same.,6/22/15,1
alasin,Which I checked using gdalcompare.py .,6/22/15,1
echeipesh,"Thinking about breaking out geotrellis-spark-testkit. To start probably should have the in memory test spark catalog, OnlyIfCanRunSpark trait, maybe a method to read RasterRDD from a geotiff, other stuff ?",6/24/15,1
allixender,Hey @echeipesh would it make sense to reconcile this idea with the further development for the benchmark project? To get an idea how well the cassandra stuff actually performs I'd like to look into getting the benchmark (which looks like a little bit hard-coded towards Accumolo though),6/24/15,1
allixender,... getting the benchmark working with a cassandra benchmark,6/24/15,1
echeipesh,"@allixender if you_Ñére talking about the bencharks we_Ñéve been doing that sounds like geotrellis internal testing. Testkit is really a tool that can be used by projects that use geotrellis to make testing around our data types easier. Having said that, if you you see a way to formulate spark benchmarking in a generic enough way so it would be useful in this context, that would be awesome.",6/25/15,1
allixender,@echeipesh I mean this one https://github.com/geotrellis/benchmark,6/25/15,1
allixender,But I can't get it to compile yet because of the dependencies,6/25/15,1
allixender,"I remember @lossyrob published some 0.10.0-SNAPSHOTs of geotrellis/geotrellis, but geotrellis-gdal_2.10-0.10.0-SNAPSHOT and geotrellis-geotools_2.10-0.10.0-SNAPSHOT don't resolve",6/25/15,1
allixender,I am not sure what I am doing wrong. I think I could publish my geotrellis compiled artifacts locally?,6/25/15,1
allixender,"I think I am getting there, the geotrellis-spark module just didn't want to be published, and the geotiff reader moved from geotools into rast",6/26/15,1
allixender,https://github.com/allixender/benchmark/tree/feature/cassandra,6/26/15,1
allixender,just incorporating the latest S3RasterCatalog updates etc into geotrellis/feature/cassandra ... => actually https://github.com/allixender/geotrellis/tree/feature/cassandra-merge-master ,6/26/15,1
allixender,"so then I could use my locally published cassandra enabled spark artefacts in geotrellis benchmark, that's the idea anyway",6/26/15,1
allixender,"hey @echeipesh @lossyrob @notthatbreezy I created a pull request https://github.com/geotrellis/benchmark/pull/5 for https://github.com/geotrellis/benchmark so it would compile against current geotrellis master brnach artefacts. As discussed with Chris, I might add a Cassandra ingest to it and run it on a bigger cluster (EC2 if you guys have time and resources for it, or GCE if I have time and skills :-p )",6/26/15,1
echeipesh,"@allixender, Rob published 0.10-M1: http://search.maven.org/#artifactdetails%7Ccom.azavea.geotrellis%7Cgeotrellis-spark_2.10%7C0.10.0-M1%7Cjar",6/26/15,1
echeipesh,"But it sounds like you found you that you can do `publish-local` from sbt yourself. Be warned, it gets glitchy when you re-publish local so you may need to use `cleanCache` command beforehand, that_Ñés from https://github.com/sbt/sbt-dirty-money",6/26/15,1
pomadchin,"or just to clean cache with hands;really tricky thing this ivy2",6/26/15,1
pomadchin,smth like `rm -rf ~/.ivy2/local/com.azavea.geotrellis` just before publishing,6/26/15,1
pomadchin,"guyz, remember there was a projection issues, it solved?",6/26/15,1
pomadchin,with projections not covering the whole earth (like sin / others),6/26/15,1
pomadchin,or may i have a link to this issue?,6/26/15,1
pomadchin,"oh sry, here it is https://github.com/geotrellis/geotrellis/issues/1030",6/26/15,1
echeipesh,"yep, that_Ñés it.",6/26/15,1
allixender,"Thanks for info @echeipesh . In fact I was aware of that Rob published the SNAPSHOTS, but eventually I would need my ""own"" local SNAPSHOTS with Cassandra bindings before they are available in official place at some point",6/28/15,1
allixender,So I'll try stay clean cache on the local publish road for a while :-),6/28/15,1
vishalanand,"For testing the Kriging functionality and fitting of Semivariograms, I have been trying to use these [shape-files](https://github.com/xebia-studio/geographic-data/tree/master/src/main/resources/osm) to import into SAGA GIS and generate test-examples. (QGIS uses SAGA plugin internally to perform interpolation). However, these seem to be un-importable. (I have some working data files collected over different sites, but they are too small in size)I would be thankful if someone can point me to a krigable data/raster for testing.",6/29/15,1
allixender,"Hey @vishalanand , no one around these here, eh :smile:  ? Regarding your shape-file thing, why can't you import them, what'S the error message?",6/30/15,1
vishalanand,"Yeah,  :smile: I am getting these errors : `[2015-06-30/15:16:20] Load shapes: /home/va/qgis/sagaData/55-Meuse.shp...failed`",6/30/15,1
vishalanand,"After the import, I would be able to perform Kriging. However, the smaller datasets I have are the world country maps (not enough detail for kriging testing)",6/30/15,1
allixender,"oh, this is not really helpful. Do you have more stacktrace info?",6/30/15,1
allixender,"ahh, the good old meuse geosttistics dataset :thumbsup: ",6/30/15,1
allixender,"Hello @notthatbreezy @lossyrob , I compiled a few snippets regarding geotrellis benchmark cassandra on google compute engine / google cloud platform. http://allixender.github.io/articles/geotrellis-gce-experiment/ ",6/30/15,1
allixender,including a Gist which lists most of the trial cmds in more detail: https://gist.github.com/allixender/ccc5831e726f5fc7679d,6/30/15,1
allixender,"oops, Giiter pastes the whole gist ",6/30/15,1
vishalanand,"@allixender Meanwhile, I am looking for other source of such shapefilesYeah, in the error console, these get thrown up : ```dbf read: could not open fileDBase file could not be opened.```",6/30/15,1
allixender,"did you only provide the .shp file in the folder, or also the other supplementary files like .dbf :smile:  ?",6/30/15,1
vishalanand,I guess that should be it. The first time I am working with shapefiles.,6/30/15,1
vishalanand,Thanks trying it!,6/30/15,1
vishalanand,:),6/30/15,1
allixender,"yeah, should be at least like 4 or 5 supplementary files",6/30/15,1
allixender,"Hi dear @echeipesh @hectcastro , I had another look at the geotrellis-ec2-cluster project, and realised I must have missed the infos on cassandra, and now found there'S even a branch https://github.com/geotrellis/geotrellis-ec2-cluster/tree/feature/hmc/cassandra-support . Do you guys by chance have any info the status of cassandra in that ec2-cluster project?",6/30/15,1
vishalanand,"@allixender  Dude, that did the job. Along with .dbf, .prj, .xxx and .shx files, it got imported. Thanks!Had not realized the smaller data I had, contained these auxiliary files as well!",6/30/15,1
allixender,"hey man, you're welcome. I have been working with shape-files before and stumbled over the same issue",6/30/15,1
allixender,"I am really looking forward to use kriging on geotrellis, seriously :thumbsup: ",6/30/15,1
pomadchin,"guyz, just a quick question: you run ingest jobs in prod with fifo scheduling in a client mode? ",7/1/15,1
lossyrob,@pomadchin what do you mean by client mode?,7/1/15,1
pomadchin,`spark-submit _ÑÓdeploy-mode <client/cluster>`,7/1/15,1
lossyrob,right_Ñ_client mode. we have a spark master that is independent of worker nodes,7/1/15,1
pomadchin,hello guyz : o ,7/7/15,1
lossyrob,hey @pomadchin ,7/7/15,1
pomadchin,"the question not about geotrellis but mb about it too :D are there some ways to make tiles of a huge map more equal by color (brightness mb), cause all tiles are made on different angle so every tile has its unique color, so it is very important on tiles joints",7/7/15,1
lossyrob,your talking about color correction,7/7/15,1
lossyrob,"which I know there are techniques for. I think OSSIM might have some, but there are other projects.",7/7/15,1
lossyrob,"we haven_Ñét gotten there yet, but we_Ñére close :)",7/7/15,1
pomadchin,yeeee awesome news ,7/7/15,1
pomadchin,ty for a project link,7/7/15,1
lossyrob,"I might be wrong about OSSIM though, just a heads up",7/7/15,1
lossyrob,GRASS has some functionality for it I believe,7/7/15,1
lossyrob,oh and there is this,7/7/15,1
lossyrob,https://github.com/planetlabs/color_balance,7/7/15,1
pomadchin,o_o,7/7/15,1
pomadchin,"based on histograms correlation heh, tried to make smth like this",7/7/15,1
pomadchin,,7/7/15,1
jamesmcclain,"Hello, I am trying to create a simple web service using spray.  I am having trouble loading RDDs",7/7/15,1
jamesmcclain,"I am using the code the team provided to me for that, but it does not work outside of the context of a test",7/7/15,1
jamesmcclain,apparently due to a dependency on org.apache.spark.Logging,7/7/15,1
echeipesh,What do you mean? How is that dependency breaking things ?,7/7/15,1
jamesmcclain,I will create a link to a pastebin of the output,7/7/15,1
jamesmcclain,essentially what I did was to graft in the loading code from the `TestFiles.scala` file that you guys supplied,7/7/15,1
jamesmcclain,I can compile my code with that grafted in,7/7/15,1
lossyrob,hmmm. yeah I_Ñém going to have to see code in order to know what_Ñés going on,7/7/15,1
jamesmcclain,"But when I run the service, I get this: http://pastebin.com/QPSAdhEx",7/7/15,1
lossyrob,what_Ñés the Build.scala look like?,7/7/15,1
lossyrob,usually we_Ñéll run the services through `spark-submit`,7/7/15,1
lossyrob,which provides spark classes. there could be something going on there,7/7/15,1
jamesmcclain,"Okay, I will paste the `Build.scala`, but it is largely unchanged",7/7/15,1
lossyrob,what_Ñés the codebase you_Ñére working off  of again?,7/7/15,1
jamesmcclain,https://github.com/jamesmcclain/model-my-watershed/tree/WIP-2,7/7/15,1
jamesmcclain,There is a copy of what I have now,7/7/15,1
lossyrob,_ÑÉ?_Ñé as a commit message :rage1:,7/7/15,1
jamesmcclain,:smile: ,7/7/15,1
jamesmcclain,The WIP branches are just backups that I create,7/7/15,1
lossyrob,"```scala  ""org.apache.spark"" %% ""spark-core"" % Version.spark % ""provided"",        ""org.apache.hadoop"" % ""hadoop-client"" % Version.hadoop % ""provided"",```",7/7/15,1
lossyrob,in the build file,7/7/15,1
lossyrob,"see how they are marked as _ÑÒprovided""",7/7/15,1
lossyrob,that means the client will use their own,7/7/15,1
jamesmcclain,"Essentially, the `?` indicates me casting around, that the code isn't necissarily meaningful",7/7/15,1
jamesmcclain,"Yes, I see",7/7/15,1
lossyrob,so you have to reinclude those dependencies in the other subproject,7/7/15,1
lossyrob,or else don_Ñét mark them as provided,7/7/15,1
lossyrob,GeoTrellis marks them as provided because as a library we don_Ñét want to dictate what spark you are using,7/7/15,1
lossyrob,"but since this is an application, you shouldn_Ñét have to worry about that",7/7/15,1
jamesmcclain,So I should mark spark-core and hadoop-client as provided in the `services` project?  I have done that at various points,7/7/15,1
lossyrob,unless of course you want to use different versions for `spark-submit`,7/7/15,1
jamesmcclain,I have tried many permuatations,7/7/15,1
jamesmcclain,I know that you guys are working on putting the data on amazon,7/7/15,1
lossyrob,oh oops. I didn_Ñét see that you had those in the other dependencies,7/7/15,1
jamesmcclain,This test code will end up needing to be changed quite a bit when we go from the small local files to amazon -- should I just wait for that,7/7/15,1
lossyrob,how are you running the services?,7/7/15,1
jamesmcclain,I have spark running locally.  I run the service with `sbt 'project services' compile run`,7/7/15,1
lossyrob,you should be running assembly and then running a script like this https://github.com/geotrellis/gt-admin/blob/master/run-server.sh,7/7/15,1
lossyrob,"that gt-admin project has services like what you are trying to run, would be good to poke around there for clues",7/7/15,1
jamesmcclain,"Okay, I will give that a try",7/7/15,1
jamesmcclain,Thanks again,7/7/15,1
pomadchin,"@lossyrob good day! saw some multiband tiles traits / classes, but i guess multiband ingest not ready yet?",7/8/15,1
jamesmcclain,I would like to ask a follow-up question to my one from yesterday.  I have discovered what the problem is: the the spray system (via the underlying akka actor system) seems to be forking new JVMs to handle requests.  Those JVMs are not able to find some parts of the spark library which are needed to call into geotrellis.,7/8/15,1
jamesmcclain,"I have already tried such simple things as isolating the calls to geotrellis routines into a separate object then calling into that from the connection handler, but not no luck",7/8/15,1
jamesmcclain,is this a known issue?,7/8/15,1
jamesmcclain,"I also tried explicitly setting the classpath, but the new JVMs don't seem to pick up on that",7/8/15,1
jamesmcclain,"would trying to send a message to the original JVM from the handler,having the original JVM call into geotrellis, then passing the result back to the handler be a viable strategy?",7/8/15,1
echeipesh,@pomadchin haven_Ñét cooked up an ingest yet. Sounds like you have a multi-band dataset you want to play with ?,7/8/15,1
echeipesh,@jamesmcclain That doesn_Ñét sound right. sbt can fork a jvm to run a project or if you_Ñére using `re-start` sbt plugin that will do a similar thing. But the class paths should be identical between forked and non-forked jvms. What are the errors that you are getting ?,7/8/15,1
jamesmcclain,I was looking for my pastebin link from yesterday,7/8/15,1
jamesmcclain,http://pastebin.com/QPSAdhEx,7/8/15,1
jamesmcclain,"I get different errors when running with `spark-submit`, though",7/8/15,1
echeipesh,"the github link for the project is 404, is there another branch ?",7/8/15,1
jamesmcclain,"actually, may I save your time?  I am going to talk to justin walgren in a few minutes",7/8/15,1
jamesmcclain,he might be able to answer my questions,7/8/15,1
jamesmcclain,"if so, I will summarize in the chatroom",7/8/15,1
jamesmcclain,"if not, may I ask again?",7/8/15,1
echeipesh,"ok, sounds good.",7/8/15,1
jamesmcclain,"```I hit this errormodeling > run[info] Running org.opentreemap.modeling.Main[error] Uncaught error from thread [GeoTrellis-akka.actor.default-dispatcher-3] shutting down JVM since 'akka.jvm-exit-on-fatal-error' is enabled for ActorSystem[GeoTrellis][error] java.lang.NoClassDefFoundError: org/apache/spark/LoggingEugene explainedyes, spark and hadoop are marked as _ÑÒprovided_Ñù, which means they are not present in run context classPath.This happens because spark programs are usually expected to be run by `spark-submit` command.You can do three things: use spark-submit (pain), run your stuff from test, remove provided.I don_Ñét recommend removing provided, because that_Ñés not how your stuff will be run in production and it will make you jump through some hoops managing spark context.So you best best would be to run it from test context.maybe have some like `object LocalServer extends App {_Ñ_}`. And you can run it with `test:run`I haven_Ñét ever actually done that, but should work out.To get things working I TEMPORARILY IGNORED HIM and removed ""provided""```",7/8/15,1
jamesmcclain,it looks like justin already had this exact problem,7/8/15,1
jamesmcclain,I'll ask him what it means,7/8/15,1
pomadchin,"@echeipesh yep, modis (mod09a1, mod09ga, mod09gq) and landsat 4 band rasters o: would be very nice to play with it; the idea is in making tile server with colored tifs / pngs o: very important thing: tifs, so it is the main aim of playing with multiband rasters",7/8/15,1
echeipesh,"I guess we can make GeoTiffs, that sounds like it would fit the bill.",7/8/15,1
pomadchin,yep (:,7/8/15,1
echeipesh,soon:tm:,7/8/15,1
lossyrob,"@pomadchin a tile server using GeoTrellis that can serve tiffs out of spark backends sounds cool. Would that be something you_Ñéd have open on github, that we could all work on?",7/8/15,1
allixender,@lossyrob @pomadchin @echeipesh +1,7/8/15,1
allixender,I'd tile server with a WCS interface would open a huge opportunity :clap: ,7/8/15,1
allixender,"*a tile server, sry for bad engrish",7/8/15,1
pomadchin,@lossyrob @allixender just only ideas and experiments ): we are still making sutable version of ndvi api using _old_ geotrellis version (before accumulo 1.6 upgrade) ,7/9/15,1
pomadchin,"yep, have plans to make all (all possible) sat processing with geotrellis ",7/9/15,1
zifeo,"Hey guy, I just got this when running the test, any idea?`[info] - should read landsat 8 data correctly *** FAILED ***[info]   java.io.FileNotFoundException: raster-test/data/geotiff-test-files/ls8_uint16.tif (No such file or directory)`",7/9/15,1
lossyrob,"try taking off _ÑÒraster-test""",7/9/15,1
lossyrob,"depending on whether or not sbt forkes, the working directory is different",7/9/15,1
zifeo,"okay, no problem if it doesn_Ñét matter",7/9/15,1
lossyrob,@zifeo wait are you talking about a test that you didn_Ñét write?,7/9/15,1
lossyrob,it looks like travis is ok_Ñéing it,7/9/15,1
zifeo,yes!,7/9/15,1
lossyrob,that_Ñés pretty strange. and that file isn_Ñét listed as missing in your `git status`?,7/9/15,1
echeipesh,@lossyrob take a look at buildall.sh in test kit pr,7/9/15,1
echeipesh,That's why Travis is ok-ing it. Thought I would have merged by now or would have made a PR just for that.,7/9/15,1
lossyrob,hmm,7/9/15,1
lossyrob,so without the && it doesn_Ñét stop it,7/9/15,1
lossyrob,that_Ñés not good,7/9/15,1
echeipesh,"Travis just picks up exit code, ends up being the last sbt run.",7/9/15,1
echeipesh,On plus side putting things that change a lot up top now makes it fail out sooner if there is an error,7/9/15,1
lossyrob,eh I feel like it used to fail out quick,7/9/15,1
lossyrob,"like, if geotrellis-vector failed, it would ony get that far and fail",7/9/15,1
jwalgran,I have a spray web service that reads a cell values given a set of coordinates. I have it working with a local ``HadoopRasterCatalog`` and am now trying to switch it over to an ``S3RasterCatalog``,7/10/15,1
jwalgran,"I create my catalog like this```scala  def catalog(implicit sc: SparkContext): S3RasterCatalog = {    S3RasterCatalog(""bucketname"", ""catalog"")  }```",7/10/15,1
jwalgran,,7/10/15,1
jwalgran,"And I have some testing code like this```scalaval layerId = (layer, ModelingServiceSparkActor.DEFAULT_ZOOM)println(s""LAYERID:$layerId"")val rasterMetaData = ModelingServiceSparkActor.catalog.getLayerMetadata(layerId).rasterMetaDataprintln(s""RASTERMETADATA:$rasterMetaData"")val tileReader = ModelingServiceSparkActor.catalog.tileReader[SpatialKey](layerId)println(s""TILEREADER:$tileReader"")```",7/10/15,1
jwalgran,"I see the metadata is fetched and printed, but there is an exception becore the ``tileReader`` is created```[info] LAYERID:(rastername,13)[info] 17:31:40 AmazonHttpClient: x-amzn-RequestId: not available[info] 17:31:40 AmazonHttpClient: AWS Request ID: 08...[info] RASTERMETADATA:RasterMetaData(int8,Extent(-1.4497453913427E7,2480608.8780999966,-7087903.913427,6960358.878099998),geotrellis.proj4.CRS$$anon$2@a942e1e1,TileLayout(8192,8192,256,256))                                                                                                                                                                   [info] 17:31:41 AmazonHttpClient: x-amzn-RequestId: not available[info] 17:31:41 AmazonHttpClient: AWS Request ID: 10...[info] 17:31:41 AmazonHttpClient: x-amzn-RequestId: not available[info] 17:31:41 AmazonHttpClient: AWS Request ID: D3...[info] 17:31:41 AmazonHttpClient: x-amzn-RequestId: not available[info] 17:31:41 AmazonHttpClient: AWS Request ID: D2...[error] java.nio.charset.MalformedInputException: Input length = 1```",7/10/15,1
lossyrob,hmm,7/10/15,1
jwalgran,The full stack trace is here: https://gist.github.com/jwalgran/937b9e120ae64deac157,7/10/15,1
lossyrob,what version of geotrellis are you running? M1?,7/10/15,1
jwalgran,Yup ``0.10.0-M1``,7/10/15,1
lossyrob,looks like it_Ñés failing on getLayerKeyIndex,7/10/15,1
lossyrob,reading some code...,7/10/15,1
jwalgran,"Ah. I pulled the layerIndex file out of S3```json[{""name"":""layername"",""zoom"":13},{""serialized"":""\u0001\u0000geotrellis.spark.io.index.zcurve.ZSpatialKeyInde«èô\u0001""}]```",7/10/15,1
lossyrob,"sorry I_Ñém doing something I should have done before, cut the 0.10 branch so I can look at that code specifically. ",7/10/15,1
lossyrob,so it_Ñéll be a minute,7/10/15,1
lossyrob,"I_Ñém wondering if @echeipesh wrote it with master, and if things have changed that are causing a deserialization error",7/10/15,1
jwalgran,Looks like the code may be fine. I doubt those wacky unicode characters should be there.,7/10/15,1
lossyrob,hmm,7/10/15,1
lossyrob,"yeah your code seems fine. so it_Ñés either a bad write, or a version mismatch, or something with the codebase",7/10/15,1
jwalgran,"Is it a bad idea to try manually cleaning up the layerIndex file to look like```json[{""name"":""layername"",""zoom"":13},{""serialized"":""geotrellis.spark.io.index.zcurve.ZSpatialKeyIndex""}]```",7/10/15,1
lossyrob,it would be a good test I assume,7/10/15,1
jwalgran,Cool. I will give that a shot.,7/10/15,1
lossyrob,yeah. that is probably it. I wonder why it got written that way,7/10/15,1
jwalgran,"No dice. Looks like the deserializer was expecting more stuff.```java.lang.IndexOutOfBoundsException: Index: 99, Size: 0  java.util.ArrayList.rangeCheck(ArrayList.java:635)  java.util.ArrayList.get(ArrayList.java:411)  com.esotericsoftware.kryo.util.MapReferenceResolver.getReadObject(MapReferenceResolver.java:42)                                                               com.esotericsoftware.kryo.Kryo.readReferenceOrNull(Kryo.java:805)  com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:759)  org.apache.spark.serializer.KryoSerializerInstance.deserialize(KryoSerializer.scala:171)  ...```",7/10/15,1
jwalgran,I will put back the original keyIndex file for troubleshooting,7/10/15,1
allixender,"@lossyrob Haven't been overly successful yet with managing Cassandra through Mesos and Marathon, even on the bigger machines. It would deploy and start though (also on several nodes), but handling through that web interface and scaling didn't seem to be super robust ... Couldn't use cqlsh and nodetool to connect and enquire throughout the Cassandra ring (somehow only localhost). apparently Mesosphere folks provide cusomised versions of these tools which also Mesos aware.  I'll play around a little more. Maybe even bigger instances too and then together with Spark ... to see if the cassandra spark connector can use the cassandra instances from the spark cluster.",7/10/15,1
allixender,"And regarding the presumably slow union of RDDs, I found this code in Accumulo geotrellis.spark.io.accumulo.RasterRDDReader: ```scalaval tileRdd =       queryKeyBounds      .map{ subKeyBound =>         val job = Job.getInstance(sc.hadoopConfiguration)          instance.setAccumuloConfig(job)        InputFormatBase.setInputTableName(job, tileTable)                getCube(job, layerId, subKeyBound, index)              }      .reduce(_ union _)...```Doesn't that also cause some RDD unions ... ? As compared to the (presumably worse) cassandra implementation: geotrellis.spark.io.cassandra.spatial.SpatialRasterRDDReader```scalafor ( bounds <- queryKeyBounds ) {      val spaceMinKey = bounds.minKey      val spaceMaxKey = bounds.maxKey     for(row <- bounds.minKey._2 to bounds.maxKey._2) {        val min = index.toIndex(SpatialKey(bounds.minKey._1, row))        val max = index.toIndex(SpatialKey(bounds.maxKey._1, row))        rdds += rdd.where(""zoom = ? AND indexer >= ? AND indexer <= ?"", layerId.zoom, min, max)    }rdd.context.union(rdds.toSeq).asInstanceOf[RDD[(String, ByteBuffer)]]```",7/10/15,1
pomadchin,"guyz, think it is possible to update geotools lib (up to 13.1)usually I use `sbt-updates` plugin to find what to update",7/13/15,1
echeipesh,"@allixender re: accumulo code. Yeah, it_Ñés potentially slow, but there union happens for every disjoint n-dimensional region specified by the query. This is usually in order of of 1-8 (or something like that). With cassandra we_Ñére trying to union on SFC (1-dimensional) ranges and thats usually in the order of thousands to tens of thousands.",7/13/15,1
echeipesh,@pomadchin let me see if it_Ñés just a version bump.,7/13/15,1
jamesmcclain,"Hello, I am trying to update some code from an older version of geotrellis to the 0.10-SNAPSHOT version that is on sonatype",7/13/15,1
jamesmcclain,"So far I have the following diff```diff   val dataPath = either(""MMW_DATA"", ""src/test/data"")    def getRaster(p: String): Raster = {-    val geoTiff = GeoTiffReader.read(p)-    val crs = geoTiff.metaData.crs-    val extent = geoTiff.metaData.extent+    val geoTiff = GeoTiffReader.readMultiBand(p)+    val crs = geoTiff.crs+    val extent = geoTiff.extent     val reprojectedExtent = extent.reproject(crs, LatLng)-    val tile = geoTiff.bands.head.tile.reproject(extent, crs, LatLng).resample(512, 512)+    val tile = geoTiff.tile.reproject(extent, crs, LatLng).resample(512, 512)      Raster(tile, reprojectedExtent)   }```",7/13/15,1
jamesmcclain,"I am having trouble with the correct translation of the `val tile = ...`, it says that reproject is not a member of `geotrellis.raster.MultiBandTile`",7/13/15,1
jwalgran,"Do you need multiple bands? If not, ``GeoTiffReader.readSingleBand`` will get you a reprojectable tile.",7/13/15,1
jamesmcclain,I guess I will go with single-band,7/13/15,1
jamesmcclain,I don't really need multiple bands right now,7/13/15,1
jamesmcclain,"That seems to work, thanks!",7/13/15,1
jamesmcclain,"If you had trouble running my code before, it was because I had a stale version of geotrellis in my local repository",7/13/15,1
jamesmcclain,"I am fixing it to work with the snap shot, now",7/13/15,1
jamesmcclain,will update momentarily,7/13/15,1
lossyrob,"If you need to reproject multiband raster, you need to reproject the bands individually",7/13/15,1
lossyrob,that_Ñés some code I haven_Ñét written yet for the library,7/13/15,1
allixender,"@echeipesh thanks for clarifying the details :smile: Ok, I hope if I find a better solution for Cassandra during GSoC",7/13/15,1
pomadchin,"guyz, noticed some inssue in deps: https://github.com/geotrellis/geotrellis/blob/master/project/Build.scala#L469",7/14/15,1
pomadchin,very bad to try geotrellis gdal libs using scala 2.11.x,7/14/15,1
pomadchin,"there is a workaround, to exclude this lib, but think it should be fixed in a build file;were there any reasons this lib was included scala version independed?",7/14/15,1
echeipesh,"thanks for finding that @pomadchin. There is no reason for that, think that code was from a hackathon and we just haven_Ñét noticed bad dependency. Although it_Ñés strange that it_Ñés picking up 2.11, I don_Ñét see any unprefixed artifacts on maven. I would have expected for that to flat out fail. ",7/14/15,1
pomadchin,"(: nope it is all ok with 2.10I just use some geotrellis libs in a side project (Scala 2.11) and there appeared some dep caused errors",7/14/15,1
pomadchin,"(: nope it is all ok with 2.10I just use some geotrellis libs in a side project (project on Scala 2.11) and there appeared some dep caused errors",7/14/15,1
echeipesh,"Ah I see, so sbt always builds it with 2.10 even when doing cross compiliation.",7/14/15,1
pomadchin,yep,7/14/15,1
pomadchin,"it should be: `""com.github.scopt""  %% ""scopt""             % ""3.3.0""`",7/14/15,1
pomadchin,%% means scala version in a mvn styled repo,7/14/15,1
echeipesh,"yeah, there is a PR up for that and the travis snafu up right now.",7/14/15,1
pomadchin,"yep see it; ""com.github.scopt"" %% ""scopt_2.10"" % ""3.3.0""",7/14/15,1
echeipesh,"Just strange that it doesn_Ñét fail the dependency with only _ÑÒ%"", I recall it having that behavior before.",7/14/15,1
pomadchin,"hm not sure this would work %% means ""com.github.scopt"" % ""scopt_scalaversion"" % ""3.3.0""",7/14/15,1
pomadchin,._.,7/14/15,1
pomadchin,:D wooh,7/14/15,1
echeipesh,oh jesus _Ñ_ thats what I get for not reading source code I_Ñém editing :facepunch: ,7/14/15,1
pomadchin,":D",7/14/15,1
pomadchin,ty for updating (: waiting new version published on mvn central,7/14/15,1
jwalgran,@echeipesh  Is there a recommented version of spark to use with spark-etl? The project I have been working on uses 1.2. Ok to have a local install of 1.4?,7/15/15,1
echeipesh,"yes, 1.4 should work fine.",7/15/15,1
echeipesh,"I_Ñém using it with 1.3.1 at the moment, but spark iterates so fast that there is no choice but to make everything work for the latest version.",7/15/15,1
echeipesh,"Are you trying to use it? I can throw up a project that depends on it. It_Ñés going to be a template of some kind eventually, but no at the moment.",7/15/15,1
pomadchin,"Think all spark versions from 1.x should be compatible (1.0 to 1.3.1 were, with geotrellis)",7/15/15,1
echeipesh,"There was some feature that required at least 1.2, but it_Ñés a bit of a moot point now.",7/15/15,1
allixender,"Coool. Cassandra Spark Connector is also available throughout the latest versions 1.0 - 1.4, however, the corresponding spark-cassandra-connector version is linked to the targetted spark version :worried: https://github.com/datastax/spark-cassandra-connector ",7/15/15,1
pomadchin,"noticed today, that gdal `CoordinateTransformation.CreateCoordinateTransformation(from, to)` often works not correct for utm to merc proj4 string, though projected tile via gdalwarp seems like a working workaroundthis msg not connected with geotrllis, guyz have you heard smth about it?",7/16/15,1
pomadchin,with proj4j same thing; mb smb knows mth about it?,7/16/15,1
lossyrob,hmmm_Ñ_.gdal warp should use the same OGR reproject logic,7/16/15,1
lossyrob,"when you say doesn_Ñét work correct, you mean it_Ñéll repreject a geometry incorrectly? but gdalwarp reproejcts the raster extent correclty?",7/16/15,1
pomadchin,"the problem I guess mb in affine transformations mb: I wanted to check out how is it good, so took some pixels, got their coords, and found that some of them were out of extent bounds ._.",7/16/15,1
pomadchin,"later found that smth wrong was with srs strings, dunno even; and now wondering: these affine transformations (mapToGrid and gridToMap) work for all projections?",7/16/15,1
lossyrob,"your asking if RasterExtent.{mapToGrid, gridToMap} work with all projections?",7/16/15,1
pomadchin,Yes,7/16/15,1
lossyrob,"The logic of those is pretty dumb, it just converts grid coordinates to an extent, so whatever crs the extent is in, yeah it shouldn_Ñét matter",7/16/15,1
lossyrob,you can see them in geotrellis.raster.RasterExtent.scala,7/16/15,1
pomadchin,"heh, I wrote own such functions:d thought mb some specific projections are really bad and have own transformations",7/16/15,1
pomadchin,ty for help! ,7/16/15,1
pomadchin,having experiments with colorising ratsers by coords basing on the other raster ,7/16/15,1
pomadchin,:d rather dumb but fast ,7/16/15,1
lossyrob,hmm that_Ñés an interesting approach. can I see the code?,7/16/15,1
pomadchin,"sure :d not interesting (dumb ):) (now not in a laptop) w8 for ten minutes I 'll write it, but the idea is in taking each pixel of one raster, and getting the same from the other, having such histos we can change colors :0 now it works with rasters with 250m and 500m per pixel, so we need to interpolate (now it is just an average brightness for a raster, so we can get an average brightness for two rasters and to change brightness for tasters basing on default raters brightness) color (want at least to correct brightness)  ",7/16/15,1
lossyrob,"cool, yeah that sounds cool. perhaps seeing the code I_Ñéll have some suggestions. So you_Ñére trying to match the brightness between two images?",7/16/15,1
pomadchin,yep :),7/16/15,1
pomadchin,"smth like thishttps://gist.github.com/pomadchin/6960e5ad1bc01517b33b#file-geotiffservice-scala-L45-L85",7/16/15,1
pomadchin,"jsut an example of code (should get rid of unused vals, of vars, etc)",7/16/15,1
pomadchin,"the whole idea in these lines: ```scala      for (c <- 0 until secondColumns; r <- 0 until secondRows) {        val rgbaSecond = imgSecond.getRGB(c, r)        val brightnessSecond = ColorService.countBrightness(rgbaSecond)        if (brightnessSecond > 0) {          secondCounter += 1          secondAcc += brightnessSecond           val mapPointSecond = projector.reproject(gdalSecond.gridToMap(c, r))          val (fc, fr) = gdalFirst.mapToGrid(mapPointSecond)          val rgbaFirst = imgFirst.getRGB(fc, fr)           val brightnessFirst = ColorService.countBrightness(rgbaFirst)          if(brightnessFirst > 0) {            firstCounter += 1            firstAcc += brightnessFirst          }        }      }```",7/16/15,1
pomadchin,,7/16/15,1
pomadchin,): such dummy way :D ,7/16/15,1
pomadchin,"but works, it is possible to color landsat using modis (mod09a1) ",7/16/15,1
lossyrob,"cool got sucked into a meeting, but I_Ñém going to look over this in a bit",7/16/15,1
pomadchin,:),7/16/15,1
lossyrob,"FWIW I was talking to someone about image compositing for a similar service, and he had mentioned that using Java2D would be a good idea_Ñ_there_Ñés a lot of image functionality already built in",7/16/15,1
lossyrob,do you have any thoughts on that?,7/16/15,1
lossyrob,scala for loops over large collections are crazy slow,7/16/15,1
lossyrob,you_Ñére gonna wanna use cfor for stuff like this https://gist.github.com/pomadchin/6960e5ad1bc01517b33b#file-geotiffservice-scala-L51  (Line 51),7/16/15,1
lossyrob,we do a similar image -> image (different extent) row/col conversion here https://github.com/geotrellis/geotrellis/blob/master/raster/src/main/scala/geotrellis/raster/mosaic/package.scala#L38,7/16/15,1
lossyrob,"we have the extents of the images, which I don_Ñét if you have with the format that you have the images in",7/16/15,1
pomadchin,@lossyrob ty for advices! ,7/17/15,1
pomadchin,"yep thought about it too, but not sure it works well with all projections;moreover we have to use gdal always ): the fastest tool it seems to me",7/17/15,1
pomadchin,"hm really slow? ok, going to check it right now",7/17/15,1
pomadchin,"hm checked it, not really much faster o: mb not warmed jvm",7/17/15,1
pomadchin,"nice, speed up: ~50k-80k ms",7/17/15,1
pomadchin,ty for a nice link (mosaic package) O: useful! have not seen,7/17/15,1
pomadchin,"now tested this way of birghtness correction on the whole map;really works, but have some issues (near pole) due to modis weird sin projection :D so the brightness of tiles correcting right (or seems so); need to think about the whole color correction",7/17/15,1
pomadchin,"w0w guyz, spark 1.4.1 released; and (just noticed) hadoop 2.7.1",7/18/15,1
pomadchin,versions appear really fast,7/18/15,1
lossyrob,@pomadchin cool that it's is working for brightness correction. Thanks for sharing the approach!,7/20/15,1
lossyrob,"yup, spark and hadoop move fast_Ñ_I can_Ñét wait until we have the hundreds of contributors, maybe we_Ñéd be able to put out a release at a decent pace!",7/20/15,1
lossyrob,anyone care to help fix our GeoJson serializer? Good practice with spray-json and `geotrellis.vector`_Ñ_. https://github.com/geotrellis/geotrellis/issues/1142,7/20/15,1
allixender,"hey @lossyrob , I'd be keen, would it be appropriate to do that while having sort of different ""GSoC duties"" ?",7/21/15,1
lossyrob,"in an open source project, it_Ñés always appropriate to get coding done that needs to get done ;)",7/21/15,1
allixender,"yeah cool, I'd be keen to get more hands on in the geotrellis vector space in general :smile:  will be good practice",7/21/15,1
lossyrob,"I_Ñém always doing rastery things so I don_Ñét use it as often as I_Ñéd like to, and using it in anger is the only way to uncover bugs and missing features and such. If you see anything like that, let me know & make an issue!",7/21/15,1
allixender,1,7/21/15,1
pomadchin,"@lossyrob np (: btw, noticed, that better results we get not using rgb model, but using hsl, correcting luminance",7/21/15,1
pomadchin,"it is a bit slower (more floating point math), but more accurate result",7/21/15,1
pomadchin,"lso tried this with hsb (hsv), not best my idea :D",7/21/15,1
pomadchin,~ 100000ms longer for a landsat tile,7/21/15,1
pomadchin,):,7/21/15,1
pomadchin,want to see the result with ë‰badë_ (sea) tiles,7/21/15,1
allixender,@lossyrob I had a look at #1142 https://github.com/geotrellis/geotrellis/issues/1142#issuecomment-123559314  halfway between a bug and a feature. Catch up tomorrow how to proceed.,7/22/15,1
allixender,"Could it be that geotrellis-spark tests fail in the latest geotrellis master? Anyone else?```[error] Uncaught exception when running geotrellis.spark.op.local.XorSpec: com.esotericsoftware.kryo.KryoException: Unable to find class: geotrellis.spark.io.index.RowMajorSpatialKeyIndexsbt.ForkMain$ForkError: Unable to find class: geotrellis.spark.io.index.RowMajorSpatialKeyIndex...```",7/22/15,1
echeipesh,"@allixender checking now but I recall that class moved to `geotrellis/spark/io/index/rowmajor/RowMajorSpatialKeyIndex`. So it looks like the test catalog that test source the layers from  is stale in `geotrellis/spark/src/test/resources/test-catalog`. Try deleting it",7/22/15,1
echeipesh,It_Ñés a really annoying problem that happens as refactors go forward. I started this PR that introduces an in-memory only catalog to use for tests but it stalled out when very strange EOF error started happening on Travis for spark tests: https://github.com/geotrellis/geotrellis/pull/1122,7/22/15,1
allixender,"Cool, works again, thanks @lossyrob ",7/22/15,1
allixender,Spark testkit like vector-test and raster-test?,7/22/15,1
echeipesh,Np. Not exactly. Those projects are spun out to avoid circular dependencies. Spark test kit is supposed to be usable by geotrllis users in their tests as well,7/22/15,1
allixender,"ooohh, wow",7/22/15,1
allixender,"ahh, like spray or akka testkit",7/22/15,1
echeipesh,"Yep, exactly",7/22/15,1
lossyrob,"copying some stuff from a convo with @zifeo, @allixender you_Ñéll be interested:",7/23/15,1
lossyrob,"@lossyrobhey can you paste the polygon and extent that are throwing the TopologyExcepition?@lossyrobhttp://tsusiatsoftware.net/jts/jts-faq/jts-faq.html#D9@zifeo```scalaval figure = Polygon(Line((-257.1, -199.1), (26.900000000000006, -199.1), (26.900000000000006, 84.9), (-257.1, -199.1)), Line((-186.1, -128.1), (-44.099999999999994, -128.1), (-44.099999999999994, 13.899999999999999), (-186.1, -128.1)))val extent = Polygon(Line((-60.000003333333325, 0.0), (-60.000003333333325, 89.99999), (59.99999333333335, 89.99999), (59.99999333333335, 0.0), (-60.000003333333325, 0.0)))val figureJts = figure.jtsGeomval extentJts = extent.jtsGeom        println(figureJts)// POLYGON ((-257.1 -199.1, 26.900000000000006 -199.1, 26.900000000000006 84.9, -257.1 -199.1), (-186.1 -128.1, -44.099999999999994 -128.1, -44.099999999999994 13.899999999999999, -186.1 -128.1))println(extentJts)// POLYGON ((-60.000003333333325 0, -60.000003333333325 89.99999, 59.99999333333335 89.99999, 59.99999333333335 0, -60.000003333333325 0))figureJts.intersection(extentJts)// com.vividsolutions.jts.geom.TopologyException: found non-noded intersection between LINESTRING ( -44.099999999999994 13.899999999999999, -57.99999999999999 0.0 ) and LINESTRING ( -44.099999999999994 0.0, -58.00000000000001 0.0 ) [ (-57.99999999999999, 0.0, NaN) ]```@zifeocan be solved that way```scalaval simplifier = new GeometryPrecisionReducer(new PrecisionModel(12))val figureJts = simplifier.reduce(figure.jtsGeom)val extentJts = simplifier.reduce(extent.jtsGeom)```but I don_Ñét know if it is cost effective?12 seems to be the maximum as stated in the documentation@lossyrobmaximum what?@lossyrob```scalascala> import com.vividsolutions.jts.geom._import com.vividsolutions.jts.geom._scala> val g1 = new GeometryFactory(new PrecisionModel(12))g1: com.vividsolutions.jts.geom.GeometryFactory = com.vividsolutions.jts.geom.GeometryFactory@20501152scala> val g2 = new GeometryFactory()g2: com.vividsolutions.jts.geom.GeometryFactory = com.vividsolutions.jts.geom.GeometryFactory@7f5ce775scala> val x = new Coordinate(0.1234567890123456789, 0.9876543210987654321)x: com.vividsolutions.jts.geom.Coordinate = (0.12345678901234568, 0.9876543210987654, NaN)scala> g1.createPoint(x)res0: com.vividsolutions.jts.geom.Point = POINT (0.123 0.988)scala> g2.createPoint(x)res1: com.vividsolutions.jts.geom.Point = POINT (0.1234567890123457 0.9876543210987654)``````scalascala> val g3 = new GeometryFactory(new PrecisionModel(100))g3: com.vividsolutions.jts.geom.GeometryFactory = com.vividsolutions.jts.geom.GeometryFactory@596930a9scala> g3.createPoint(x)res2: com.vividsolutions.jts.geom.Point = POINT (0.123 0.988)``````scalascala> res2.getXres4: Double = 0.12345678901234568```",7/23/15,1
lossyrob,"if anyone here knows how to use the PrecisionModel in JTS effectively, especially when faced with TopologyErrors, we could use some help",7/23/15,1
lossyrob,@zifeo @allixender one thing that we need to do is make this configurable: https://github.com/geotrellis/geotrellis/blob/master/vector/src/main/scala/geotrellis/vector/GeomFactory.scala#L22,7/23/15,1
lossyrob,"so that we can have arbitrary precision models. So in your test Teo, you have some very precise values_Ñ_perhaps the user would just set the precision model on some object so that the geometries created would use thatprecision model",7/23/15,1
lossyrob,"I think we discussed using an implicit precision model that would be taken for each of the geometry construction methods, but didn_Ñét end up going that route. That would proliferate a lot of (implicit p: PrecisionModel) additions to the `geotrellis.vector` codebase",7/23/15,1
lossyrob,"which might be a bit ugly, but might also be the right way to go about it",7/23/15,1
pomadchin,"/offtop/@lossyrob geotrellis inside got functions (intersect, covers, contains) and for extent:`polygon1.envelope.contains(polygon2.envelope)`what differs?",7/23/15,1
lossyrob,"but I guess a question is..is there some default precision model that we can use that will always be precise enough, and also solve the TopologyErrors?",7/23/15,1
lossyrob,"@pomadchin not much. We just had an Extent implementation before we started using JTS wrappers. So all of that was already written, and doing the straight math, which is easy, for that sort of stuff is going to be faster than hitting up JTS envelopes_Ñ_at least that_Ñés my thought. It_Ñés definitely not going to be slower.",7/23/15,1
lossyrob,"So the Extent we use for envelopes is our own type, and not the JTS Envelope type",7/23/15,1
lossyrob,"yeah JTS does some casting and such to Geometries I was trying to avoid. I forget exactly what it was, but I remember find a step in using JTS envelops instead of our Extent that would produce some more bytecode/steps",7/23/15,1
pomadchin,"ty, just was searching intersections between polygons, not critical _ÑÉcause speed of both operations (on built polygons and on extent) rather fast (~3-5sec for all landsat tiles)",7/23/15,1
allixender,@lossyrob https://github.com/geotrellis/geotrellis/pull/1144 always a little puzzled when you say to change only one line in source and it fixes it really :smile: ,7/24/15,1
jamesmcclain,"Hello, can anyone share a pointer to the best place to learn about using S3 catalogues with Geotrellis?",7/24/15,1
echeipesh,There is not much in terms of documentation. I can say they will behave about the same as any other catalog. You can see some uses of it in the tests: https://github.com/geotrellis/geotrellis/blob/master/spark/src/test/scala/geotrellis/spark/io/s3/S3RasterCatalogSpec.scala,7/24/15,1
echeipesh,Other than that I can answer questions.,7/24/15,1
jamesmcclain,"Okay, thanks for the pointer.  I will give that a read.",7/24/15,1
jamesmcclain,"I have the following code:```Scala  implicit val sc: SparkContext = SparkUtils.createSparkContext(""Geoprocessing"", new SparkConf())  val catalogue = S3RasterCatalog(bucket, prefix)  val nlcd = catalogue.query[SpatialKey]((layerName, layerZoom)).toRDD```which gives me the error```GeopServiceActor.scala:39: value query is not a member of geotrellis.spark.io.s3.S3RasterCatalog[error]   val nlcd = catalogue.query[SpatialKey]((layerName, layerZoom)).toRDD[error]                        ^```",7/24/15,1
jamesmcclain,"This confusing, as this code https://github.com/OpenTreeMap/otm-modeling/commit/1156291d4e80319c948344d0638b936d12636bd7 seems to be doing a very similar thing",7/24/15,1
jamesmcclain,"There must be a subtlety that I am missing, but I am not seeing it",7/24/15,1
echeipesh,"yeah, that_Ñés not right. It_Ñés a pretty recent refactor, make sure you are pulling in latest GT version? I assume you_Ñére not able to do that on other catalogs (or is this the first)",7/24/15,1
jamesmcclain,This is my first attempt at working with a catalogue,7/24/15,1
jamesmcclain,I am using `0.10-SNAPSHOT`,7/24/15,1
jamesmcclain,"When you say it is not right, do you mean what I wrote is not  right, or that that error should not be appearing?",7/24/15,1
lossyrob,that the error shouldn_Ñét be appearing,7/24/15,1
lossyrob,"also, you_Ñére note constraining that query by any bounds, which will load the entire layer. You_Ñéll want to do something like ```catalogue.query[SpatialKey](LayerId(layerName, layerZoom)).where(Intersects(someBounds)).toRDD```",7/24/15,1
jamesmcclain,"I see.  I just destroyed my .ivy2 directory and did an sbt clean, and I am still getting the same behaviour.  My Build.scala is pointing to `0.10-SNAPSHOT`, so I'm fairly confident that I am not using a stale version of Geotrellis.",7/24/15,1
lossyrob,and you published local with an up-to-date master of geotrellis?,7/24/15,1
echeipesh,,7/24/15,1
echeipesh,btw: geotrellis project include a plugin for cleaning local cache selectivly: you can do `clean-cache` and it will do so for the sub-project you are in. Will save blowing away whole ivy cache.,7/24/15,1
jamesmcclain,I have not done a public local since I destroyed my .ivy2 directory (minutes ago),7/24/15,1
jamesmcclain,"I think Justin is using the same snapshot that I am, and I am looking at his code",7/24/15,1
jamesmcclain,"Thanks for the tip re: `clean-cache`, will write that down",7/24/15,1
jamesmcclain,"I meant to say ""publish local"" not ""public local"" above",7/24/15,1
jamesmcclain,I am essentially just using Justin's code as a model: https://github.com/OpenTreeMap/otm-modeling/blob/1156291d4e80319c948344d0638b936d12636bd7/src/main/scala/ModelingServiceSparkActor.scala,7/24/15,1
pomadchin,sometimes better to clean ivy2 cache by hands and project target dirs ,7/24/15,1
jamesmcclain,"Looking at his `build.scala`, he (seems to be) using the same version of Geotrellis that I am",7/24/15,1
jamesmcclain,feelsbadman.jpg,7/24/15,1
lossyrob,you should use publish local,7/24/15,1
lossyrob,to publish your own 0.10.0-SNAPSHOT,7/24/15,1
lossyrob,"if you inlcude the sonatype snapshot repo, you_Ñéll get a 0.10 snapshot out of it, but it will be old",7/24/15,1
echeipesh,"hehe. What is probably happening is there is geotrellis-spark-10-SNAPSHOT somewhere on maven. If you don_Ñét pre-empt it by publishing your own version to local cache, sbt will go looking and find something.",7/24/15,1
lossyrob,so just do a ./publish-local.sh in an up-to-date geotrellis checkout to fix that,7/24/15,1
jamesmcclain,"Okay, will try.  Will there be any fallout re: the geoprocessing VM in MMW?  In other words, when people provision that and try to use it, will it work?",7/24/15,1
jamesmcclain,Or will they need to build a fresh Geotrellis as part of the provisioning process?,7/24/15,1
lossyrob,"if you switch the version to 0.10.0-M1, does that work?",7/24/15,1
lossyrob,"because unless you need stuff that was added to GeoTrellis after the M1 release, you can use that",7/24/15,1
jamesmcclain,"I will try, but Justin seems to have switched to `0.10-SNAPSHOT` and it works for him",7/24/15,1
lossyrob,that_Ñés because he_Ñés publishing local,7/24/15,1
jamesmcclain,"I'll try, then report back ina few minutes",7/24/15,1
jamesmcclain,"Okay, I must not be understanding",7/24/15,1
lossyrob,"if you need more up to date geotrellis, yeah the publish-local will have to be part of the provisioning step",7/24/15,1
jamesmcclain,I thought that pulling from sonatype obviated the need to publish local?,7/24/15,1
jamesmcclain,"I don't know whether Justin is doing a publish local or not, but I know that his `build.scala` references `0.10-SNAPSHOT`",7/24/15,1
jamesmcclain,https://github.com/OpenTreeMap/otm-modeling/blob/spark/project/build.scala#L14,7/24/15,1
jamesmcclain,"Okay, but I'll try -M1 and report back",7/24/15,1
jamesmcclain,"Yeah, that had the same result.  I just pulled from master and am doing a `publish-local.sh`",7/24/15,1
jamesmcclain,Thank you for your help,7/24/15,1
echeipesh,no worries,7/24/15,1
jamesmcclain,"I have my program compiling and attempting to use data from S3.  If I see the following:```16:43:36 AmazonHttpClient: x-amzn-RequestId: not available16:43:36 AmazonHttpClient: AWS Request ID: xxxxxxxxxxxxxxxx16:43:36 AmazonHttpClient: x-amzn-RequestId: not available16:43:36 AmazonHttpClient: AWS Request ID: xxxxxxxxxxxxxxxx16:43:36 AmazonHttpClient: x-amzn-RequestId: not available16:43:36 AmazonHttpClient: AWS Request ID: xxxxxxxxxxxxxxxx16:43:37 SpatialRasterRDDReader$: Created 0 ranges, binned into 16 bins16:43:37 SpatialRasterRDDReader$: Created 0 ranges, binned into 16 bins```",7/24/15,1
jamesmcclain,I assume this means that the program is not properly connecting to S3?,7/24/15,1
jamesmcclain,My endpoint gives back a 400 when I hit it,7/24/15,1
qweqweq,"hi ,i want to create Raster RDD,and i import geotrellis.spark._ \geotrellis.spark.io._\geotrellis.vetctor._\geotrellis.raster._\geotrellis.spark.op._\geotrellis.spark.op.local._but i got some wrong.here it is:<console>48: error: type mismatch;found        : org.apahe.spark.SparkContenxtrequired «__org.apache.hadoop.fs.Pathval hc = =geotrellis.spark.io.hadoop.HadoopRasterCatalog(sc, new org.apache.hadoop.fs.Path(""hdfs://master:9000/localhost/catalog"") )",7/27/15,1
pomadchin,"@qweqweq`HadoopRasterCatalog` a little bit differs look at constructors (https://github.com/geotrellis/geotrellis/blob/master/spark/src/main/scala/geotrellis/spark/io/hadoop/HadoopRasterCatalog.scala).SparkContext is an `implicit` argument; so it can be smth like ```scalaimplicit val sc = SparkContext(_Ñ_)val hc = HadoopRasterCatalog(Path(""hdfs://master:9000/localhost/catalog""))```or explicitly:```scalaval hc = HadoopRasterCatalog(Path(""hdfs://master:9000/localhost/catalog""))(sc)```",7/27/15,1
qweqweq,"@pomadchin  Thanks for your explanation ,according to your answer.i have changed the command ,but i have got a error: not found: value Path.",7/27/15,1
pomadchin,"@qweqweq oh, sry, you have to make instance of `Path` object:```scalaimport org.apache.hadoop.fs.Pathval hc = HadoopRasterCatalog(new Path(""hdfs://master:9000/localhost/catalog""))```",7/27/15,1
qweqweq,"@pomadchin Thank you ,it's done. :smile: ",7/27/15,1
pomadchin,:) ,7/27/15,1
qweqweq,"@pomadchin hi pomadchin,i create a rasterRdd successfully,and i want to konw this function's  meaning:scala>val (min,max)= rdd.minMaxmin: Int = -108max: Int = 127",7/27/15,1
pomadchin,It is a pair of min and max values of raster data points (pixels),7/27/15,1
pomadchin,"as I remember, there were examples of this method usage (GlobalRasterSourceMethodsSpec or smth like that called spec file)",7/27/15,1
qweqweq,"@pomadchin  thank you very much ,learn a lot today with your help. :+1: ",7/27/15,1
qweqweq,thanks again @pomadchin :smile:  ,7/27/15,1
pomadchin,":D no worries, write questions here; btw there are megaminds: @lossyrob , @echeipesh and others",7/27/15,1
qweqweq,"haha,lossyrob is a good guy who helped me a lot and you too. I'm doing some research on this project ,it's very useful to deal with .tif file. And I hope to become a contributor in one day. :smile: ",7/27/15,1
jamesmcclain,"Hello, I would like to ask an S3 catalogue question.  I have the following code: https://github.com/jamesmcclain/model-my-watershed/blob/feature/use-s3-catalogue/src/geop/services/src/main/scala/com/azavea/mmw/GeopServiceActor.scala",7/27/15,1
jamesmcclain,"When I post this input https://gist.github.com/jamesmcclain/42ec932910b1014d7137 to my endpoint, I get a 400 back",7/27/15,1
jamesmcclain,But when I uncomment lines 53 and 58 in the file above and comment the appropriate parts,7/27/15,1
jamesmcclain,I get the expected results from my endpoint,7/27/15,1
jamesmcclain,I wonder if anyone can help me understand what is happening,7/27/15,1
jamesmcclain,"As near as I can tell, there is no rdd computation happening at all, because if I put a print statement at this line https://github.com/jamesmcclain/model-my-watershed/blob/feature/use-s3-catalogue/src/geop/processing/src/main/scala/com/azavea/mmw/Histogram.scala#L23 I see no output",7/27/15,1
jamesmcclain,But I can see the data being downloaded from S3 in the console window,7/27/15,1
jamesmcclain,And I am running a version of Geotrellis that was pulled and compiled on Friday afternoon,7/27/15,1
jamesmcclain,Never mind,7/27/15,1
jamesmcclain,Got it working.  The problem was that the extent that I was intersecting against (https://github.com/jamesmcclain/model-my-watershed/blob/feature/use-s3-catalogue/src/geop/services/src/main/scala/com/azavea/mmw/GeopServiceActor.scala#L55) was in the wrong coordinate system.,7/27/15,1
lossyrob,"@qweqweq glad you got that worked out! @pomadchin good to have good people to step up and answer questions when the _ÑÒmegaminds_Ñù are either afk or neck deep in other busyness, thanks! :)",7/27/15,1
lossyrob,@jamesmcclain I_Ñéve run into that about 1000 times. Glad you got it worked out,7/27/15,1
lossyrob,hard to keep coordinate systems straight.,7/27/15,1
lossyrob,"leaflet requires vector data to be LatLng, but we_Ñére storing tiles in WebMercator. Actually, if tiles are not going to ever be used for visual display, the could just be stored in LatLng and you wouldn_Ñét have to do conversions",7/27/15,1
jamesmcclain,Many thanks for all of your help.  Eugene pointed out a similar problem in another file and that allowed me to put 2 and 2 together,7/27/15,1
allixender,"Hi, could it be that some Travis build build jobs fail, because of some configuration issue? ",7/28/15,1
lossyrob,yeah,7/28/15,1
allixender,"```shellUpdating sbtgit.checkout8.62s$ git clone --depth=50 git://github.com/geotrellis/geotrellis.git geotrellis/geotrellisCloning into 'geotrellis/geotrellis'...remote: Counting objects: 9090, done.remote: Compressing objects: 100% (3745/3745), done.remote: Total 9090 (delta 4476), reused 7967 (delta 3924), pack-reused 0Receiving objects: 100% (9090/9090), 90.73 MiB | 24.17 MiB/s, done.Resolving deltas: 100% (4476/4476), done.Checking connectivity... done.$ cd geotrellis/geotrellis0.16s$ git fetch origin +refs/pull/1144/merge:remote: Counting objects: 56, done.remote: Compressing objects: 100% (20/20), done.remote: Total 56 (delta 20), reused 46 (delta 11), pack-reused 0Unpacking objects: 100% (56/56), done.From git://github.com/geotrellis/geotrellis * branch            refs/pull/1144/merge -> FETCH_HEAD$ git checkout -qf FETCH_HEAD$ jdk_switcher use oraclejdk7Switching to Oracle JDK7 (java-7-oracle), JAVA_HOME will be set to /usr/lib/jvm/java-7-oracle$ export JVM_OPTS=@/etc/sbt/jvmopts$ export SBT_OPTS=@/etc/sbt/sbtopts$ java -Xmx32m -versionjava version ""1.7.0_76""Java(TM) SE Runtime Environment (build 1.7.0_76-b13)Java HotSpot(TM) 64-Bit Server VM (build 24.76-b04, mixed mode)$ javac -J-Xmx32m -versionjavac 1.7.0_76Using Scala 2.10.40.16s$ ./buildall.shDetected sbt version 0.13.8Downloading sbt launcher 0.13.8:  From  http://typesafe.artifactoryonline.com/typesafe/ivy-releases/org.scala-sbt/sbt-launch/0.13.8/sbt-launch.jar    To  ./.lib/0.13.8/sbt-launch.jarDownload failed. Obtain the jar manually and place it at ./.lib/0.13.8/sbt-launch.jarThe command ""./buildall.sh"" exited with 1.Done. Your build exited with 1.```",7/28/15,1
lossyrob,I_Ñéd say often,7/28/15,1
lossyrob,someone posted a workaround for that,7/28/15,1
lossyrob,let me dig,7/28/15,1
allixender,but this donload of the launcher jar is actually 404,7/28/15,1
allixender,I also amended my pull request.,7/28/15,1
lossyrob,https://github.com/travis-ci/travis-ci/issues/4527,7/28/15,1
lossyrob,you could put that in as part of the PR,7/28/15,1
lossyrob,something about redirects not working in the sbt script,7/28/15,1
lossyrob,oh wait,7/28/15,1
lossyrob,I think the script actually got upgraded,7/28/15,1
lossyrob,i wonder if replacing the sbt script with https://github.com/paulp/sbt-extras/blob/master/sbt works,7/28/15,1
allixender,Why does it work locally? Because it is cached in the local ivy/sbt cache?,7/28/15,1
lossyrob,yeah you alreayd have it dl'ed,7/28/15,1
lossyrob,if you rm $sbt_launch_dir/$1/sbt-launch.jar,7/28/15,1
lossyrob,erm,7/28/15,1
lossyrob,in ~/.sbt,7/28/15,1
lossyrob,"and then try to run it, it might fail",7/28/15,1
lossyrob,what happens if you do that?,7/28/15,1
allixender,give me a sec,7/28/15,1
allixender,it is sitting in your project dir,7/28/15,1
allixender,./.lib/0.13.8/sbt-launch.jar,7/28/15,1
allixender,I don't have in ~/.sbt .. or ~/.ivy2,7/28/15,1
lossyrob,oh. ok,7/28/15,1
allixender,"```Downloading sbt launcher 0.13.8:  From  http://typesafe.artifactoryonline.com/typesafe/ivy-releases/org.scala-sbt/sbt-launch/0.13.8/sbt-launch.jar    To  ./.lib/0.13.8/sbt-launch.jar```",7/28/15,1
lossyrob,works fine?,7/28/15,1
allixender,"but will fail if deleted yes",7/28/15,1
lossyrob,oh ok cool,7/28/15,1
lossyrob,so if you try updating to the latest sbt script,7/28/15,1
allixender,"`Download failed. Obtain the jar manually and place it at ./.lib/0.13.8/sbt-launch.jar`",7/28/15,1
allixender,lol,7/28/15,1
allixender,well not funny,7/28/15,1
allixender,"so, shall I try that new script that your were pointing at?",7/28/15,1
lossyrob,yeah,7/28/15,1
lossyrob,seems like a good enough time as any to upgrade it :),7/28/15,1
allixender,"```Downloading sbt launcher for 0.13.8:  From  http://repo.typesafe.com/typesafe/ivy-releases/org.scala-sbt/sbt-launch/0.13.8/sbt-launch.jar    To  /home/akmoch/.sbt/launchers/0.13.8/sbt-launch.jar```",7/28/15,1
allixender,"loads it from a different place",7/28/15,1
allixender,I just updated from geotrellis master,7/28/15,1
allixender,I'll run a complete clean compile test thing over it to see,7/28/15,1
allixender,"@lossyrob regarding https://github.com/geotrellis/geotrellis/issues/1142#issuecomment-123885043 your comment for the extract and parse geojson functions, were would I put them, are the package.scala the right places?",7/28/15,1
lossyrob,yeah where parseGeoJson already exists,7/28/15,1
allixender,"yes, true",7/28/15,1
allixender,the new sbt script seems fine,7/28/15,1
lossyrob,"cool, if you could make a separate PR for that that_Ñéd be great",7/28/15,1
allixender,I'll try it with java 7 and 8 ,7/28/15,1
allixender,you guys still with java 7 or is java 8 becoming more de-facto standard,7/28/15,1
allixender,?,7/28/15,1
allixender,"yes, will do",7/28/15,1
lossyrob,"I_Ñém still on java 7, for no particular reason",7/28/15,1
allixender,"ok, I guess because of comfortable :smile: ",7/28/15,1
allixender,I'll test with 7 and 8 and then create a pull request only for the new sbt script,7/28/15,1
allixender,"when it's done compiling and testing spark on my computer, haha",7/28/15,1
allixender,"Hey, I sw your kryo feature/bug issue ... this is related to spark and vector, right?",7/28/15,1
jwalgran,"jamesmcclain and I are working on getting a Geotrellis zonal summary job pulling from an S3 catalog and launched via Spark Job Server. Our first attempt generated some Joda time exceptions```java.lang.IllegalStateException: Joda-time 2.2 or later version is required, but found version: null    at com.amazonaws.util.DateUtils.handleException(DateUtils.java:147)    at com.amazonaws.util.DateUtils.parseRFC822Date(DateUtils.java:195)    at com.amazonaws.services.s3.internal.ServiceUtils.parseRfc822Date(ServiceUtils.java:78)```A build of Spark Job Server with the latest Joda Time got us around that issue.https://github.com/azavea/spark-jobserver/commit/ce3e0dc3dcf9cb62123e3fca9b30549b5d9b665c",7/28/15,1
jwalgran,"We then ran into a serialization issue:```java.io.NotSerializableException: geotrellis.spark.io.s3.spatial.SpatialRasterRDDReader$Serialization stack:    - object not serializable (class: geotrellis.spark.io.s3.spatial.SpatialRasterRDDReader$, value: geotrellis.spark.io.s3.spatial.SpatialRasterRDDReader$@7918712b)    - field (class: geotrellis.spark.io.s3.RasterRDDReader$$anonfun$2, name: $outer, type: class geotrellis.spark.io.s3.RasterRDDReader)    - object (class geotrellis.spark.io.s3.RasterRDDReader$$anonfun$2, <function1>)    - field (class: scala.Tuple4, name: _3, type: class java.lang.Object)    - object (class scala.Tuple4, (<function0>,<function1>,<function1>,<function1>))    at org.apache.spark.serializer.SerializationDebugger$.improveException(SerializationDebugger.scala:38)    at org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:47)    at org.apache.spark.broadcast.TorrentBroadcast$.blockifyObject(TorrentBroadcast.scala:202)    at org.apache.spark.broadcast.TorrentBroadcast.writeBlocks(TorrentBroadcast.scala:101)    at org.apache.spark.broadcast.TorrentBroadcast.<init>(TorrentBroadcast.scala:84)    at org.apache.spark.broadcast.TorrentBroadcastFactory.newBroadcast(TorrentBroadcastFactory.scala:34)    at org.apache.spark.broadcast.TorrentBroadcastFactory.newBroadcast(TorrentBroadcastFactory.scala:29)    at org.apache.spark.broadcast.BroadcastManager.newBroadcast(BroadcastManager.scala:62)    at org.apache.spark.SparkContext.broadcast(SparkContext.scala:1051)    at geotrellis.spark.io.s3.RasterRDDReader.read(RasterRDDReader.scala:46)    at geotrellis.spark.io.s3.S3RasterCatalog.read(S3RasterCatalog.scala:58)    at geotrellis.spark.io.s3.S3RasterCatalog$$anonfun$query$1.apply(S3RasterCatalog.scala:71)    at geotrellis.spark.io.s3.S3RasterCatalog$$anonfun$query$1.apply(S3RasterCatalog.scala:71)    at geotrellis.spark.BoundRasterRDDQuery.toRDD(RasterRDDQuery.scala:44)    at com.azavea.datahub.SummaryJob$.queryAndCropLayer(SummaryJob.scala:136)    at com.azavea.datahub.SummaryJob$.runJob(SummaryJob.scala:44)    at com.azavea.datahub.SummaryJob$.runJob(SummaryJob.scala:29)    at spark.jobserver.JobManagerActor$$anonfun$spark$jobserver$JobManagerActor$$getJobFuture$4.apply(JobManagerActor.scala:235)    at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)    at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)    at java.lang.Thread.run(Thread.java:745)```",7/28/15,1
jwalgran,"lossyrob  and echeipesh got us around that by pointing out that the ``JavaSerializer`` was being used, rather than Kryo. ",7/28/15,1
jwalgran,"Adding a config key to the Spark Job Server .conf ensures that the Spark context uses the correct serializer.```spark {  ...  context-settings {    passthrough {      spark.serializer = ""org.apache.spark.serializer.KryoSerializer""    }  }}```",7/28/15,1
lossyrob,nice. glad that worked!,7/28/15,1
jwalgran,"Now we are running into a ``ClassCastException`````java.lang.ClassCastException: java.lang.Integer cannot be cast to geotrellis.spark.SpatialKey	at geotrellis.spark.SpatialKey$Boundable$.minBound(SpatialKey.scala:40)	at geotrellis.spark.Boundable$class.includes(Boundable.scala:23)	at geotrellis.spark.SpatialKey$Boundable$.includes(SpatialKey.scala:40)	at geotrellis.spark.KeyBounds$KeyBoundsSeqMethods$$anonfun$includeKey$1.apply(KeyBounds.scala:16)	at geotrellis.spark.KeyBounds$KeyBoundsSeqMethods$$anonfun$includeKey$1.apply(KeyBounds.scala:16)	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)	at scala.collection.immutable.List.foreach(List.scala:318)	at scala.collection.TraversableLike$class.map(TraversableLike.scala:244)	at scala.collection.AbstractTraversable.map(Traversable.scala:105)	at geotrellis.spark.KeyBounds$KeyBoundsSeqMethods.includeKey(KeyBounds.scala:16)	at geotrellis.spark.io.s3.RasterRDDReader$$anonfun$3.apply(RasterRDDReader.scala:48)	at geotrellis.spark.io.s3.RasterRDDReader$$anonfun$3.apply(RasterRDDReader.scala:48)	at geotrellis.spark.io.s3.RasterRDDReader$$anonfun$4$$anonfun$apply$2$$anonfun$apply$4.apply(RasterRDDReader.scala:66)	at geotrellis.spark.io.s3.RasterRDDReader$$anonfun$4$$anonfun$apply$2$$anonfun$apply$4.apply(RasterRDDReader.scala:66)	at scala.collection.TraversableLike$$anonfun$filter$1.apply(TraversableLike.scala:264)	at scala.collection.Iterator$class.foreach(Iterator.scala:727)	at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)	at scala.collection.TraversableLike$class.filter(TraversableLike.scala:263)	at scala.collection.AbstractTraversable.filter(Traversable.scala:105)	at geotrellis.spark.io.s3.RasterRDDReader$$anonfun$4$$anonfun$apply$2.apply(RasterRDDReader.scala:65)	at geotrellis.spark.io.s3.RasterRDDReader$$anonfun$4$$anonfun$apply$2.apply(RasterRDDReader.scala:59)	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)	at org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:210)	at org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:63)	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)	at org.apache.spark.scheduler.Task.run(Task.scala:64)	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:203)	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)	at java.lang.Thread.run(Thread.java:745)```",7/28/15,1
jwalgran,Here is the full output from Spark Job Server: https://gist.github.com/jwalgran/b39c9febed0a6bfd534b,7/28/15,1
jwalgran,"Our application code is no longer in the stack trace, just Java/Scala/Spark/Geotrellis.",7/28/15,1
lossyrob,that is a weird one,7/28/15,1
lossyrob,what_Ñés the query code?,7/28/15,1
lossyrob,why in the world is there casting going on,7/28/15,1
jwalgran,"I am not sure how deep into our job code this is, yet. This is likely where it is happening:```  def queryAndCropLayer(catalog: S3RasterCatalog, layerId: LayerId, extent: Extent): RasterRDD[SpatialKey] = {    catalog.query[SpatialKey](layerId)      .where(Intersects(extent))      .toRDD  }```",7/28/15,1
lossyrob,this might be a serialization problem again,7/28/15,1
jwalgran,The gist does have some Kryo code in some other traces,7/28/15,1
lossyrob,I_Ñém very confused about that stack trace,7/28/15,1
jwalgran,"```com.amazonaws.AbortedException:	at com.amazonaws.internal.SdkFilterInputStream.abortIfNeeded(SdkFilterInputStream.java:51)	at com.amazonaws.internal.SdkFilterInputStream.read(SdkFilterInputStream.java:71)	at com.esotericsoftware.kryo.io.Input.fill(Input.java:140)	at com.esotericsoftware.kryo.io.Input.require(Input.java:155)	at com.esotericsoftware.kryo.io.Input.readInt(Input.java:337)	at com.esotericsoftware.kryo.util.DefaultClassResolver.readClass(DefaultClassResolver.java:109)	at com.esotericsoftware.kryo.Kryo.readClass(Kryo.java:610)	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:721)	at org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:138)	at geotrellis.spark.utils.KryoSerializer$.deserializeStream(KryoSerializer.scala:55)	at geotrellis.spark.io.s3.RasterRDDReader$$anonfun$4$$anonfun$apply$2$$anonfun$apply$3.apply(RasterRDDReader.scala:63)	at geotrellis.spark.io.s3.RasterRDDReader$$anonfun$4$$anonfun$apply$2$$anonfun$apply$3.apply(RasterRDDReader.scala:61)	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)	at scala.collection.Iterator$class.foreach(Iterator.scala:727)	at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)	at scala.collection.TraversableLike$class.map(TraversableLike.scala:244)	at scala.collection.AbstractTraversable.map(Traversable.scala:105)	at geotrellis.spark.io.s3.RasterRDDReader$$anonfun$4$$anonfun$apply$2.apply(RasterRDDReader.scala:61)	at geotrellis.spark.io.s3.RasterRDDReader$$anonfun$4$$anonfun$apply$2.apply(RasterRDDReader.scala:59)	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)	at org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:210)	at org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:63)	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)	at org.apache.spark.scheduler.Task.run(Task.scala:64)	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:203)	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)	at java.lang.Thread.run(Thread.java:745)```",7/28/15,1
lossyrob,that_Ñés coming from a `import static com.amazonaws.util.SdkRuntime.shouldAbort`,7/28/15,1
lossyrob,call,7/28/15,1
lossyrob,so weird,7/28/15,1
lossyrob,"what happens if you  `set(""spark.kryo.registrator"", ""geotrellis.spark.io.hadoop.KryoRegistrator_Ñù)`",7/28/15,1
jwalgran,"Looks like that got us over the hump. I am hitting an exception, but it is a problem with the final output serialization to JSON.",7/28/15,1
lossyrob,woof.,7/28/15,1
lossyrob,kryo has been a pain in my ass.,7/28/15,1
jwalgran,I am a bit sore myself,7/28/15,1
jwalgran,Thanks so much for quickly dragging us out of that one,7/28/15,1
lossyrob,no worries,7/28/15,1
lossyrob,@lossyrob signs off for the night,7/28/15,1
allixender,"@lossyrob https://github.com/geotrellis/geotrellis/pull/1147 newer version of sbt runner script in geotrellis, travis build seems to pick it up and resolve and run the whole build process :relaxed:",7/28/15,1
allixender,https://travis-ci.org/geotrellis/geotrellis/builds/72947254 The Travis-CI build from this pull request :+1: ,7/28/15,1
allixender,"@lossyrob Sorry, I left some comments in again, would you please mind to review the bits around https://github.com/geotrellis/geotrellis/pull/1144/files#r35613614 in `geotrellis/vector/io/json/JsonFeatureCollection.scala`",7/28/15,1
allixender,"I think we discussed that lists/seq of pure Geometry types cannot be applied to a FeatureCollection, and therefore, this would need to be removed, right?",7/28/15,1
allixender,"ok, done.",7/28/15,1
pomadchin,how is the new script with java8?,7/28/15,1
pomadchin,the old one had some unsupported default flags,7/28/15,1
pomadchin,"p.s. :D guyz.. i am testing geotrellis on oracle jdk8 for nearly 5 months and everything is perfect (local builds, etc); though, in production we use oracle jdk7 ",7/28/15,1
pomadchin,seems to me it would be perfect to try jdk8 in ci,7/28/15,1
echeipesh,"I_Ñéve been using java8 in development for about a month now as well, no apparent issues. And its really nice not to have to worry about permgem.",7/28/15,1
lossyrob,Moving to java8 in CI is fine by me,7/28/15,1
allixender,Java8 (tested Oracle JDK 1.8.45 ) ran fine,7/29/15,1
allixender,"using the new script in your established working copy might cause dependencies not found, an `sbt clean` or fresh checkout should do",7/29/15,1
lossyrob,"@echeipesh here: http://blog.cloudera.com/blog/2015/03/how-to-tune-your-apache-spark-jobs-part-2/ in the quote, _ÑÒWhenever you have the power to make the decision about how data is stored on disk, use an extensible binary format like Avro, Parquet, Thrift, or Protobuf. _ÑÒ _Ñ_ We_Ñéve been considering all of the above there for storage serialization, except for Parquet, which I didn_Ñét know would be a format we could use. Could that be the answer to our serialization woes?",7/29/15,1
echeipesh,"Not sure, like you said I haven_Ñét considered it. Worth a look though. ",7/29/15,1
echeipesh,"Is there enough context to know what they mean by ""extensible binary format_Ñù?",7/29/15,1
echeipesh,"Reading it, doesn_Ñét look like it",7/29/15,1
lossyrob,womp womp,7/29/15,1
zifeo,"@echeipesh I just saw that spark-ETL replaced some ingest components, do you have a way to transpose gt-admin stuff that is using `cmg` into ETL ?",7/29/15,1
echeipesh,Let me take a look at that,7/29/15,1
echeipesh,"@zifeo just taking a quick look at it, the ingest, pyramid commands are all orphaned as of now. Their function should be performed by a project using spark-etl. The calculate command is outside of that usecase, but its really just a stub. Next time we come across a need to do something like that the spark-etl project can be augmented with ability to read from a catalog, not just a bunch of images.",7/30/15,1
echeipesh,"That leaves the arguments for the actual spray service. I think for the moment it would be safe to copy the AccumuloArgs from the github history and move it into gt-admin project. At some point some thought will need to be given to how to generalize gt-admin to work with sources other than accumulo. (it would be very helpful to run it on an S3 catalog for instance). Perhaps then sumac can be dumped for a more robust argument parser, like scallop or scopt. ",7/30/15,1
lossyrob,"@echeipesh @zifeo has already started the move to scopt, which is cool. check out this pr: https://github.com/geotrellis/gt-admin/pull/23",7/30/15,1
lossyrob,"@zifeo we should use the `etl` stuff in gt-admin, where needed. `gt-admin` ingest could be moved to an `etl` subproject, that uses the types in `geotrellis.etl` to write `objects` that perform various ingests.",7/30/15,1
lossyrob,Honestly I_Ñém not sure what that `Calculate.scala` file is doing. git says I committed it so I should know_Ñ_but yeah. The doc string doesn_Ñét make sense to what it_Ñés doing in the code. I_Ñéd say delete it.,7/30/15,1
lossyrob,"The idea is that `spark-etl` would be an easy way to write your own _ÑÒingest_Ñù (we_Ñére now using the ETL nomenclature) inside your own project. So for instance Chattanooga might have an etl project, or just a simpe object, that would do the necessary ingest for that project.",7/30/15,1
echeipesh,"Do you have any opnion on scopt vs scallop? We should pick one so we_Ñére - a dependency.",7/30/15,1
lossyrob,"I don_Ñét really. I know that we need scallop for the ETL stuff, but also we now have scopt in a couple of places",7/30/15,1
lossyrob,I think using scallop moving forward makes sense if there_Ñés a definite need for it.,7/30/15,1
echeipesh,"eh, we can brainstorm about it later. Maybe there is more ellegant way to do what ETL needs.",7/30/15,1
rahulpalamuttam,hi everyone - does geotrellis have a way to ingest netcdf files from hdfs?,7/30/15,1
lossyrob,"we have some gdal bindings that we_Ñéve used to read NEX netCDF data using a HDFS InputFormat, so yes.",7/30/15,1
lossyrob,"But we found that if we created a pre-processing step that exported each band of the netcdf out into a GeoTIFF using GDAL, Spark likes working over single band tiles that aren_Ñét too too large better than large netCDF files",7/30/15,1
rahulpalamuttam,sorry im a bit lost on the terminology - what do u mean by single band?,7/30/15,1
pomadchin,"Btw, soon there would be multiband tiff tiles support ",7/30/15,1
pomadchin,Hope so :D,7/30/15,1
lossyrob,"so, the netCDFs we were working with were tiles per month",7/30/15,1
lossyrob,@pomadchin it,7/30/15,1
lossyrob,_Ñés in the works for sure. The single threaded version is there. just the serialization issues need to be solved before we get a Catalog for them!,7/30/15,1
lossyrob,"right, so when I say bands, I mean the time dimension",7/30/15,1
lossyrob,if I do `gdalinfo` on those netCDFs,7/30/15,1
lossyrob,each month is represented by one band,7/30/15,1
lossyrob,"so it_Ñés a stack of rasters through time, but in one file",7/30/15,1
rahulpalamuttam,i see,7/30/15,1
rahulpalamuttam,okay i get it,7/30/15,1
lossyrob,which can get large and decrease the ability to parallelize over them,7/30/15,1
rahulpalamuttam,its the classic split by time,7/30/15,1
lossyrob,"so I wrote some python/rasterio code that used SQS and EC2 to chunk out the netCDF tiles, which meant just grabbing all the bands out and exporting them as single bands and saving them to s3",7/30/15,1
lossyrob,and writing the ISO_TIME into a GeoTIFF tag,7/30/15,1
lossyrob,"so that later I could read each month-tile as one banded GeoTiff, and get the metadata I needed",7/30/15,1
rahulpalamuttam,its all fairly new terminology to me - so you'll have to bear with me - but i've been trying to do something similar with spark and netcdf data,7/30/15,1
rahulpalamuttam,i've see you have a rasterRDD and a tileRDD,7/30/15,1
lossyrob,"but if your netCDFs aren_Ñét too large, and your ok with installing gdal java bindings on each worker, you could use this: https://github.com/geotrellis/geotrellis/blob/master/spark/src/main/scala/geotrellis/spark/io/hadoop/formats/NetCdfInputFormat.scala",7/30/15,1
lossyrob,Not really a tileRDD...,7/30/15,1
lossyrob,RasterRDD is a collection of tiles (and soon to also support MultiBandTile),7/30/15,1
lossyrob,,7/30/15,1
rahulpalamuttam,okay i see,7/30/15,1
rahulpalamuttam,tiles as in geospatial tiles,7/30/15,1
lossyrob,A _ÑÒTile_Ñù is really just a piece of a raster,7/30/15,1
lossyrob,like a chunk. A raster is something that can mean the whole layer,7/30/15,1
rahulpalamuttam,to over simplify tile = 1 box on a grid?,7/30/15,1
lossyrob,"Yeah kind of, but each tile has like 256x256 pixels say",7/30/15,1
rahulpalamuttam,okay awesome and you do some sort of smoothing across the pixels?,7/30/15,1
lossyrob,how do you mean?,7/30/15,1
rahulpalamuttam,well it's just some of the netcdf data are grids with varying resolution,7/30/15,1
rahulpalamuttam,and they're many files split across time,7/30/15,1
lossyrob,hmmm yeah. our netCDF_Ñés were all of the same resolution,7/30/15,1
rahulpalamuttam,okay gotcha,7/30/15,1
rahulpalamuttam,ur dealing with a specific netcdf source,7/30/15,1
rahulpalamuttam,"our netcdf's could have many variables, and varying resolutions",7/30/15,1
lossyrob,"dealing with multi-resolution layers is something we_Ñére working on, but for now we fit everything into the same resolution grid layer_Ñ_right now it_Ñés jsut according to the TMS-like `{z}/{x}/{y}` scheme, but soon it will be so that you can define a custom tile layout",7/30/15,1
lossyrob,but we_Ñéd still keep it one resolution per layer,7/30/15,1
rahulpalamuttam,i see,7/30/15,1
lossyrob,what do you have to do between resolutions?,7/30/15,1
lossyrob,"like, are there operations you have to do between the different resolution data?",7/30/15,1
lossyrob,the one thing we_Ñéve done is just resample down to the highest resolution,7/30/15,1
rahulpalamuttam,well its more of an experiment  - but my basic task right now,7/30/15,1
lossyrob,"which is a bit poor on disk space usage. dynamic resampling would be nice, not implemented yet though.",7/30/15,1
rahulpalamuttam,is reading in a bunch of netcdf files,7/30/15,1
rahulpalamuttam, and reducing the resolution or I guess u could say regridding,7/30/15,1
rahulpalamuttam,so here's the pipeline,7/30/15,1
rahulpalamuttam,netcdf file -> rdd (inside the rdd is a multidimensional array structure),7/30/15,1
rahulpalamuttam,each variable array is copied to the array structure and then we do operations on that array,7/30/15,1
rahulpalamuttam,it was really a simple introduction task,7/30/15,1
lossyrob,"and the _ÑÉarray structure_Ñé, how many dimensions is that per variable?",7/30/15,1
rahulpalamuttam,well anywhere from 1 - 5,7/30/15,1
rahulpalamuttam,most are lat/lon,7/30/15,1
rahulpalamuttam,but there's lat/lon/height etc.,7/30/15,1
lossyrob,"well, a raster tile is pretty much set to a gridded space of width and height",7/30/15,1
lossyrob,and then you can hold tiles in other dimensions by having the keys be multidimensional,7/30/15,1
lossyrob,"so for instance we have a `SpaceTimeKey`, which has a spatial and temporal component",7/30/15,1
lossyrob,"and a `RasterRDD[SpaceTimeKey]` is an `RDD[(SpaceTimeKey, Tile)]`",7/30/15,1
lossyrob,so the actual value of the Key-Value RDD is still a 2D tile,7/30/15,1
rahulpalamuttam,i see,7/30/15,1
rahulpalamuttam,that's interesting,7/30/15,1
rahulpalamuttam,so you have already pre-computed the dimension pairs for different grids you could have,7/30/15,1
rahulpalamuttam,i hope that made sense,7/30/15,1
lossyrob,hmm not sure what you mean by that,7/30/15,1
lossyrob,haha,7/30/15,1
rahulpalamuttam,so the different tile u have,7/30/15,1
rahulpalamuttam,are associated with the key,7/30/15,1
lossyrob,right.,7/30/15,1
lossyrob,and the keys represent points in some bounded space,7/30/15,1
rahulpalamuttam,yes,7/30/15,1
lossyrob,"which in our case are always at least in the spatial 2 dimensions, in a discreet coordinate space",7/30/15,1
rahulpalamuttam,but the keys themselves tell u what your dimensions are as well,7/30/15,1
lossyrob,"and for SpaceTimeKey, in 2D discreet coordinate space + continuous time",7/30/15,1
rahulpalamuttam,awesome,7/30/15,1
lossyrob,yeah. And the key is set for the type of RDD your working on,7/30/15,1
lossyrob,so all values are the same. So no dimension mixing...,7/30/15,1
rahulpalamuttam,so the pysical tile is the 2d spatial tile,7/30/15,1
lossyrob,"we do want to have a function that scopes down a RasterRDD[SpaceTimeKey] to a RasterRDD[SpatialKey], but taking a time slice. ",7/30/15,1
lossyrob,yeah,7/30/15,1
rahulpalamuttam,and u split it across time,7/30/15,1
rahulpalamuttam,the tiles are split across time,7/30/15,1
rahulpalamuttam,actually both time and space,7/30/15,1
lossyrob,"right. So I have like, 1000 tiles that reperesnt the temperature data for one month in 2020",7/30/15,1
lossyrob,"and another 1000 tiles that represent the next month, and so on",7/30/15,1
rahulpalamuttam,awesome,7/30/15,1
lossyrob,"all in the same RDD, with different SpaceTimeKeys",7/30/15,1
rahulpalamuttam,awesome,7/30/15,1
lossyrob,"where SpaceTimeKey is a case class of `case class SpaceTimeKey(col: Int, row: Int, time: DateTime)`",7/30/15,1
rahulpalamuttam,so here's my next question,7/30/15,1
rahulpalamuttam,each spacetime key is a tuple,7/30/15,1
rahulpalamuttam,what's the value? a matrix?,7/30/15,1
lossyrob,it_Ñés a `Tile` type,7/30/15,1
lossyrob,which is like a matrix yes,7/30/15,1
rahulpalamuttam,a tensor of rank 2 - just to be clear,7/30/15,1
lossyrob,"has it_Ñés own `cols x rows` structure, wich are _ÑÒpixel_Ñù values",7/30/15,1
rahulpalamuttam,great,7/30/15,1
lossyrob,"yeah. But because they represent spatial values, we can_Ñét really do matrix maths on them",7/30/15,1
rahulpalamuttam,so u lost me there,7/30/15,1
lossyrob,there_Ñés specific geospatial operations that you use with them,7/30/15,1
lossyrob,which is what geotrellis is all about,7/30/15,1
rahulpalamuttam,i thought they were temp values,7/30/15,1
lossyrob,"Right, but each cell represents a physical space",7/30/15,1
rahulpalamuttam,right,7/30/15,1
lossyrob,so multiplying two rasters doesn_Ñét make sense to do in the matrix way,7/30/15,1
rahulpalamuttam,so if u wanted to do a reduce over a particular space key over all time,7/30/15,1
lossyrob,"like get average temperature for an area, right",7/30/15,1
rahulpalamuttam,yea,7/30/15,1
lossyrob,"yeah, it_Ñés an aggregation operation",7/30/15,1
lossyrob,"map each of the keys to the same time, say.",7/30/15,1
lossyrob,or move it to SpatialKey,7/30/15,1
lossyrob,like,7/30/15,1
rahulpalamuttam,so u can go from spatialtimekey -> spatialkey,7/30/15,1
lossyrob,"```scalaval rdd: RasterRDD[SpaceTimeKey] = ???rdd.map { case (key, tile) => (key.spatialComponent, tile) }       .reduceByKey { (t1, t2) => t1 + t2 }       // some other stuff```",7/30/15,1
rahulpalamuttam,nice,7/30/15,1
lossyrob,well there_Ñés some trickery there,7/30/15,1
lossyrob,because `RasterRDD` holds on to metadata,7/30/15,1
lossyrob,and uses that metadata to do spatial operations and the like,7/30/15,1
lossyrob,so if you change the key type_Ñ_we don_Ñét have good support for that,7/30/15,1
rahulpalamuttam,okay that's fair,7/30/15,1
lossyrob,"but you can do `RasterRDD[K] => RDD[(T, V)]` just fine",7/30/15,1
lossyrob,if that makes sense,7/30/15,1
rahulpalamuttam,"Key -> Time, Value?",7/30/15,1
lossyrob,"so change the key type, but you then are working with the _ÑÒraw_Ñù key-value pair",7/30/15,1
rahulpalamuttam,hmm okay,7/30/15,1
lossyrob,"so `RasterRDD[SpaceTimeKey]` is a wrapped `RDD[(SpaceTimeKey, Tile)]`",7/30/15,1
rahulpalamuttam,oh gotcha,7/30/15,1
lossyrob,"so you can do `RasterRDD[SpaceTimeKey] -> RDD[(SpatialKey, Tile)]` just fine",7/30/15,1
lossyrob,but you lose metadta,7/30/15,1
rahulpalamuttam,i see,7/30/15,1
rahulpalamuttam,so here's my next question,7/30/15,1
rahulpalamuttam,if u keep coordinates in space and time,7/30/15,1
rahulpalamuttam,hmm actually i lost the question - it was a certain use case that dealt with data locality,7/30/15,1
rahulpalamuttam,do u have any issues with re-shuffling the data?,7/30/15,1
rahulpalamuttam,if that needs to ever be done in your pipeline of operations,7/30/15,1
lossyrob,how do you mean reshuffling?,7/30/15,1
lossyrob,"shuffling happens, yeah",7/30/15,1
rahulpalamuttam,so using apache spark,7/30/15,1
lossyrob,"and we_Ñére actually going to do some work that will help in shuffle steps, by assigning partitioners based on the index space",7/30/15,1
lossyrob,"but right now, we have our storage mechanisms lined up (well Accumulo and HDFS at least)",7/30/15,1
lossyrob,"where if you store the same layers in the same index space (which requires same resolution, same layout)",7/30/15,1
lossyrob,"then those tiles will live on the same machines, in most cases",7/30/15,1
lossyrob,"so partitions would be local to each other, and when you combine partitions, the data should be local",7/30/15,1
rahulpalamuttam,oh okay,7/30/15,1
rahulpalamuttam,nice,7/30/15,1
lossyrob,which really helps with shuffle,7/30/15,1
rahulpalamuttam,yes absolutley,7/30/15,1
lossyrob,"but the problem is, that_Ñés just the initial shuffle",7/30/15,1
lossyrob,if we don_Ñét have partitioners in place(which we currently dont),7/30/15,1
lossyrob,"then the next shuffle that happens, who knows. those new partitions after the shuffle are based on a HashPartitioner",7/30/15,1
rahulpalamuttam,the default spark partitioner,7/30/15,1
lossyrob,so it_Ñés a matter of making a key bounds-aware partitioner and threading that through operations,7/30/15,1
lossyrob,something that @echeipesh is currently working on_Ñ_well was until we had a series of bad serialization problems that he_Ñés fixing,7/30/15,1
rahulpalamuttam,okay so for operations on tiles,7/30/15,1
rahulpalamuttam,do u use a custom library?,7/30/15,1
lossyrob,we are the custom library :),7/30/15,1
lossyrob,GeoTrellis really started as a fast scala raster library,7/30/15,1
lossyrob,in the single threaded sense,7/30/15,1
rahulpalamuttam,are ur operations still in JVM?,7/30/15,1
lossyrob,"so our `geotrellis.raster` core library is all the core types and operations, IO, stuff like that",7/30/15,1
lossyrob,yup,7/30/15,1
rahulpalamuttam,like from a linear algebra standpoint - and the libraries scala has,7/30/15,1
rahulpalamuttam,most libraries wrap around existing BLAS packages,7/30/15,1
rahulpalamuttam,that drop down to C level code and are pretty fast,7/30/15,1
lossyrob,"we_Ñére very performance oriented scala project, so even though we keep it to scala, we try to make it as performant as possible",7/30/15,1
lossyrob,"now, that_Ñés true BLAS is always going to be faster",7/30/15,1
lossyrob,but there_Ñés also the overhead of shipping the memory around,7/30/15,1
lossyrob,and also the added complication of interfacing with the JNI,7/30/15,1
lossyrob,though I can image that would be a really cool thing to let GeoTrellis do one day,7/30/15,1
rahulpalamuttam,"shipping the memory from ""c-space"" to jvm heap?",7/30/15,1
lossyrob,yeah back and forth,7/30/15,1
rahulpalamuttam,okay so jni does use a copy,7/30/15,1
rahulpalamuttam,and ur tiles,7/30/15,1
rahulpalamuttam,they're essentially 2d arrays,7/30/15,1
lossyrob,"yeah, backed by 1d primitave arrays",7/30/15,1
rahulpalamuttam,nice,7/30/15,1
rahulpalamuttam,data locality for the win,7/30/15,1
lossyrob,"based on how wide your data is, beacuse Array[Byte] is way faster than Array[Double]",7/30/15,1
lossyrob,ha yup,7/30/15,1
rahulpalamuttam,oh nice,7/30/15,1
rahulpalamuttam,so u don't really need precision?,7/30/15,1
lossyrob,"well, if you don_Ñét need it, you don_Ñét want to have it in the way of performance",7/30/15,1
rahulpalamuttam,gotcha it makes sense,7/30/15,1
lossyrob,"so an image that_Ñés a multiband tile of values 0-255, 3 bands",7/30/15,1
lossyrob,I don_Ñét want to have to convert that to Doubles to work with hit,7/30/15,1
lossyrob,because it_Ñés almost twice as slow,7/30/15,1
lossyrob,so we jump through hoops to try to keep the size of values down and performance up,7/30/15,1
rahulpalamuttam,right and im assuming its a pretty high resolution grid,7/30/15,1
lossyrob,"yeah, exactly. every operation is being done millions of times",7/30/15,1
lossyrob,"so a lot of thought goes into, how do we actually create loops that allow for anonymous functions to be called over tile values, while still allowing for JVM hotspotting and the like?",7/30/15,1
rahulpalamuttam,so what exactly do you mean by multiband tile and 3 bands?,7/30/15,1
lossyrob,and how do we never ever box primitives. because that is the death of performance,7/30/15,1
rahulpalamuttam,^^,7/30/15,1
lossyrob,"it just means that, there_Ñés one _ÑÒraster_Ñù, but that raster consists of 3 bands, which are _ÑÒseparate_Ñù tiles (but in some file formats, pixels are interleaved between bands in storage)",7/30/15,1
lossyrob,"so if I have a red channel, blue channel, and green channel",7/30/15,1
lossyrob,each takes a byte,7/30/15,1
rahulpalamuttam,oh okay ,7/30/15,1
lossyrob,"and can be thought of as it_Ñés own tile, or band",7/30/15,1
rahulpalamuttam,gotcha that makes sense,7/30/15,1
rahulpalamuttam,but the questions u asked - were exactly the questions I had in mind :),7/30/15,1
lossyrob,"and when I approached NetCDF_Ñés, I thought of them like rasters, like how GDAL thinks of rasters, and so I thought of the time dimension as bands. I know that in the scientific community they are used in a lot of different ways that I probably know nothing about",7/30/15,1
lossyrob,nice,7/30/15,1
lossyrob,"alright it_Ñés pretty late here on the east coast, I have to shut it down for the night. but great talking to you, I hope you stick around the channel and would like to chat more later!",7/30/15,1
rahulpalamuttam,yea absolutley - i really appreciate the time,7/30/15,1
rahulpalamuttam,my projects dealing with the whole scientific community behind netcdf - and you're doing alot of similar things I hope to do in the future :) ,7/30/15,1
rahulpalamuttam,I'll definitly be on to chat later,7/30/15,1
lossyrob,no problem at all.,7/30/15,1
lossyrob,that sounds awesome.,7/30/15,1
lossyrob,cheers!,7/30/15,1
zifeo,"@lossyrob @echeipesh Okay, thanks for the info. To be sure, gt-admin ingest is kind of outdated and should be replaced by etl stuff? I_Ñéll go through and see what I can update",7/30/15,1
rahulpalamuttam,@lossyrob so why again did you stick with your own tile operations over extending breeze or something else that leverages BLAS ? Were the primitive arrays really that much more effective?,7/30/15,1
lossyrob,@rahulpalamuttam it wasn_Ñét like we tried both approaches and picked one_Ñ_when the project started it was a decision to do all native scala. So I_Ñém not sure that they would be more effective.,7/30/15,1
lossyrob,"The `Tile` is a trait, and so someone could implement one that passed through to BLAS perhaps. But then again operations result in ArrayTiles_Ñ_so that wouldn_Ñét work.",7/30/15,1
lossyrob,I guess we side stepped the issue of dealing with native bindings all together. Was that the correct decision? Perhaps depends on the operations or the use case. But I imagine it_Ñés a lot simpler from a development perspective,7/30/15,1
rahulpalamuttam,awesome thanks - I was just making sure I didn't miss something :),7/30/15,1
rahulpalamuttam,so we've been implementing a very similar class heirarchy within a specialized RDD,7/30/15,1
rahulpalamuttam,rather than tiles we refer to them as tensors - and yea we hide the native BLAS ops behind a trait,7/30/15,1
rahulpalamuttam,*well native BLAS ops meaning breeze/nd4j/jblas - java wrappers that have already been written,7/30/15,1
lossyrob,"oh nice, that sounds like similar and cool work",7/30/15,1
lossyrob,is it on github?,7/30/15,1
rahulpalamuttam,yea its on my github account for now - and it's nowhere near as developed,7/30/15,1
rahulpalamuttam,one sec,7/30/15,1
rahulpalamuttam,https://github.com/rahulpalamuttam/SciSparkTestExperiments/tree/master/src/main/scala/org/dia/tensors,7/30/15,1
rahulpalamuttam,we even keep metadata for our tensors as well - but that's really a hashmap ,7/30/15,1
lossyrob,oh very cool,7/30/15,1
lossyrob,"so since you have some basic arithmentic operations between tensors implemented, it might be interesting to do a performance benchmark between the BLAS implementation and GeoTrellis",7/30/15,1
rahulpalamuttam,of course we aren't dealing with rgb values,7/30/15,1
lossyrob,how do you get data into this?,7/30/15,1
rahulpalamuttam,exactly what I was thinking :),7/30/15,1
rahulpalamuttam,so netcdf has a java library,7/30/15,1
rahulpalamuttam,u can get the shape of the variable and a 1 dimensional java array out of it,7/30/15,1
rahulpalamuttam,"most of the matrix libraries allow you to pass in a 1 dim array, with the dimensions",7/30/15,1
rahulpalamuttam,"and they construct their matrix representation - DenseMatrix, INDArray, etc.",7/30/15,1
lossyrob,are the netCDF_Ñés that your thinking about using open data?,7/30/15,1
rahulpalamuttam,yea they're all out in the wild,7/30/15,1
lossyrob,"because the test could just read it however each library reads it, and then we start the timer after a cached load RDD",7/30/15,1
lossyrob,so just test computation at first,7/30/15,1
rahulpalamuttam,so here's the loader file that uses the netcdf java calls,7/30/15,1
rahulpalamuttam,___,7/30/15,1
rahulpalamuttam,https://github.com/rahulpalamuttam/SciSparkTestExperiments/blob/master/src/main/scala/org/dia/loaders/NetCDFLoader.scala,7/30/15,1
lossyrob,"well if I can DL the netCDF and see if our netcdf reader can read it, then taht would be pretty simple",7/30/15,1
lossyrob,where_Ñés it at?,7/30/15,1
rahulpalamuttam,it's LoadNetcdfNDVar(),7/30/15,1
rahulpalamuttam,sorry i forget how to put linenumbers in git urls,7/30/15,1
lossyrob,you click on the line number and then copy that URL,7/30/15,1
rahulpalamuttam,https://github.com/rahulpalamuttam/SciSparkTestExperiments/blob/master/src/main/scala/org/dia/loaders/NetCDFLoader.scala#L52,7/30/15,1
lossyrob,what_Ñés the URL of the data your grabbing?,7/30/15,1
rahulpalamuttam,so it  doesn't have to be a url,7/30/15,1
rahulpalamuttam,it could be a .nc file on disk as well,7/30/15,1
rahulpalamuttam,but if u want an example url,7/30/15,1
lossyrob,"ok. but I mean, for the performance test",7/30/15,1
lossyrob,if we were to do it. what dataset should we use?,7/30/15,1
rahulpalamuttam,do you know a good netcdf source?,7/30/15,1
lossyrob,because then I can download it and see if it_Ñés the type of netcdf that our reader could handle,7/30/15,1
lossyrob,"ok, yeah",7/30/15,1
lossyrob,NEX data,7/30/15,1
lossyrob,I_Ñéll get a url for one,7/30/15,1
lossyrob,it_Ñéll be an S3 url actually.,7/30/15,1
rahulpalamuttam,it should work with any netcdf data source,7/30/15,1
lossyrob,in case your not familiar: http://aws.amazon.com/nasa/nex/,7/30/15,1
lossyrob,"you_Ñéd probably have to pull it down with the s3 API, like `s3cmd` first and work on the local file system",7/30/15,1
rahulpalamuttam,here's a particular url,7/30/15,1
rahulpalamuttam,http://disc2.nascom.nasa.gov:80/opendap/TRMM_L3/TRMM_3B42_daily/2014/365/3B42_daily.2015.01.01.7.bin,7/30/15,1
lossyrob,that_Ñés pretty tiny,7/30/15,1
rahulpalamuttam,oh yea these are very small,7/30/15,1
rahulpalamuttam,i'll be back - stepping out,7/30/15,1
lossyrob,ok cool. I_Ñéll fish out a s3 url for a good biggish netcdf file we could test against,7/30/15,1
rahulpalamuttam,So u load ur netcdf data into accu,7/30/15,1
lossyrob,"Eh if it_Ñés small enough (2G range), I might just read it into memory and parallelize from sc",7/30/15,1
lossyrob,because load isn_Ñét what we_Ñére necessarily testing,7/30/15,1
lossyrob,i have to check the file size tho_Ñ_downloading now,7/30/15,1
lossyrob,"@zifeo refresh my memory on the status of https://github.com/geotrellis/geotrellis/pull/1131 again?  seems like it_Ñés ready for merge, but the last commit says WIP (which is a shoddy commit msg btw ;) )",7/30/15,1
lossyrob,"and also if you saw the bump by @jwalgran, this feature is useful in another project using GeoTrellis, so looking to merge ASAp",7/30/15,1
lossyrob,@rahulpalamuttam here_Ñés the link to a potentially good test file: s3://nasanex/NEX-DCP30/BCSD/rcp60/mon/atmos/tasmax/r1i1p1/v1.0/CONUS/tasmax_amon_BCSD_rcp60_r1i1p1_CONUS_bcc-csm1-1_207601-208012.nc,7/30/15,1
lossyrob,which is 1.3 G on disk downloaded,7/30/15,1
lossyrob,converting to uncompressed GeoTIFF now to see how large it would be in memory. I_Ñém thinking pretty big based on how slow GDAL is doing it,7/30/15,1
lossyrob,4.9 gigs uncompressed GeoTIFF,7/30/15,1
lossyrob,so that_Ñés what it_Ñés going to look like in memory really. except it looks like you_Ñéll be converting to double,7/30/15,1
lossyrob,which will increase the byte width of the tile by 2,7/30/15,1
lossyrob,it_Ñés Float32 data,7/30/15,1
lossyrob,we could filter that data on load if that_Ñés too big,7/30/15,1
lossyrob,perhaps cut it down to a quarter of that size,7/30/15,1
lossyrob,"or take the top quarter and the bottom quarter, cache, and then start the timing from there. do a local add on it, and then see how that timing works. Perhaps commit the code to github so we can test it on eachothers machines so taht it_Ñés running in teh same dev environment",7/30/15,1
rahulpalamuttam,so this file is one variable right?,7/30/15,1
rahulpalamuttam,also how do u download it?,7/30/15,1
rahulpalamuttam,i don't use s3 that much,7/30/15,1
hectcastro,rahulpalamuttam: s3cmd or the AWS CLI should do it: http://docs.aws.amazon.com/cli/latest/reference/s3/cp.html,7/30/15,1
rahulpalamuttam,i get an unable to locate credentials,7/30/15,1
rahulpalamuttam,"aws s3 cp s3://nasanex/NEX-DCP30/BCSD/rcp60/mon/atmos/tasmax/r1i1p1/v1.0/CONUS/tasmax_amon_BCSD_rcp60_r1i1p1_CONUS_bcc-csm1-1_207601-208012.nc .Unable to locate credentialsCompleted 1 part(s) with ... file(s) remaining",7/30/15,1
hectcastro,You may need a set a valid AWS API credentials to pull it down. You can wire that up with an interactive `aws configure` once you have them.,7/30/15,1
zifeo,"@lossyrob almost done, I need to implement the safe method before. WIP is just to be able to show you the code as it is written, the commit will be cancelled at the end :)",7/30/15,1
rahulpalamuttam,aww no i thought s3 was a free service,7/30/15,1
rahulpalamuttam,as in I won't have to put in my card details,7/30/15,1
rahulpalamuttam,@lossyrob  i'll try to do the comparison test later today or tomorrow,7/30/15,1
hectcastro,Boom: http://nasanex.s3.amazonaws.com/NEX-DCP30/BCSD/rcp60/mon/atmos/tasmax/r1i1p1/v1.0/CONUS/tasmax_amon_BCSD_rcp60_r1i1p1_CONUS_bcc-csm1-1_207601-208012.nc,7/30/15,1
hectcastro,"It won't be fun to interact with more of these files without the AWS CLI installed, but since you're just testing one, this should work.",7/30/15,1
rahulpalamuttam,awesome thank you @hectcastro ,7/30/15,1
rahulpalamuttam,hey im running my program to read the tasmax variable into a breeze array,7/30/15,1
rahulpalamuttam,the java footprint ..... is far worse than i expected,7/30/15,1
rahulpalamuttam,the array is 7025 * 3015,7/30/15,1
rahulpalamuttam,so on my node,7/30/15,1
rahulpalamuttam,it takes 3 minutes,7/30/15,1
rahulpalamuttam,to extract the data from the file,7/30/15,1
rahulpalamuttam,and reformat it into a 2 dimensional breeze array,7/30/15,1
rahulpalamuttam,don't know what kind of speedups you would be seeing,7/30/15,1
rahulpalamuttam,the node has 258 gb mem - 32 cores etc. - but it only uses 1 core for the majority of the operation,7/30/15,1
rahulpalamuttam,here's the main file https://github.com/rahulpalamuttam/SciSparkTestExperiments/blob/1.3performance/src/main/scala/org/dia/Main.scala,7/30/15,1
rahulpalamuttam,there's something off,7/30/15,1
rahulpalamuttam,3 minutes is kind of ridiculous,7/30/15,1
lossyrob,@zifeo could the `safe` methods be implemented in a separate PR?,7/30/15,1
lossyrob,"@rahulpalamuttam  `7025 * 3015` is just one of the tiles. There_Ñés 60 _ÑÒbands_Ñù in that netCDF, so should be `7025 * 3015 * 6` values",7/30/15,1
lossyrob,looks like it took about 45 s to read it into an RDD and count the elements,7/30/15,1
echeipesh,Using gdal bindings ?,7/30/15,1
rahulpalamuttam,oh okay u refer to bands as the different variables,7/30/15,1
rahulpalamuttam,tasmax is one,7/30/15,1
rahulpalamuttam,then theirs lat lon,7/30/15,1
lossyrob,no their all tasmax,7/30/15,1
lossyrob,@echeipesh yes,7/30/15,1
lossyrob,"so if you look into the tasmax variable, that should have 60 different entries (bands) that are `7025 * 3015`",7/30/15,1
rahulpalamuttam,oh interesting,7/30/15,1
rahulpalamuttam,when i use the netcdf api to list the variables,7/30/15,1
rahulpalamuttam,this is all i see,7/30/15,1
rahulpalamuttam,"List(tasmax, time_bnds, lat_bnds, lon_bnds, time, lat, lon)",7/30/15,1
rahulpalamuttam,oh never mind,7/30/15,1
rahulpalamuttam,haha,7/30/15,1
rahulpalamuttam,i got it,7/30/15,1
rahulpalamuttam,when i took the dimensions,7/30/15,1
rahulpalamuttam,of tasmax,7/30/15,1
rahulpalamuttam,wow okay,7/30/15,1
rahulpalamuttam,7k * 3k * 60 is what im reading in,7/30/15,1
lossyrob,yeah,7/30/15,1
rahulpalamuttam,okay so the majority of the time is spent reading in the array on my end,7/30/15,1
rahulpalamuttam,using the netcdf api,7/30/15,1
rahulpalamuttam,you use gdals?,7/30/15,1
rahulpalamuttam,also are u on ssds?,7/30/15,1
rahulpalamuttam,and how do you manage the huge memory footprint - for some reason I need 30gb + to read it in,7/30/15,1
lossyrob,"hmm, not sure, I_Ñéve bumped up my heap space to 5G",7/30/15,1
lossyrob,but I_Ñém on a laptop,7/30/15,1
lossyrob,I think this here thing has an SSD,7/30/15,1
lossyrob,"working to get some code to show, probably ~ 10 min",7/30/15,1
rahulpalamuttam,5g is reasonable,7/30/15,1
rahulpalamuttam,i first get an array in netcdf's python format,7/30/15,1
rahulpalamuttam,then copy that to a 1d java array,7/30/15,1
rahulpalamuttam,and that's what i return,7/30/15,1
rahulpalamuttam,at most i copy the data three times,7/30/15,1
lossyrob,"```consoleMin and Max: (261.23492431640625,326.1426086425781)  TOOK: 83.5370 s```",7/30/15,1
lossyrob,https://github.com/lossyrob/netcdf-geotrellis-spark-read-test,7/30/15,1
lossyrob,"you_Ñéll have to muck with gdal java bindings if you want to run it. Which can either be painless or so, so painful",7/30/15,1
lossyrob,"and if you have a ton of cores, would muck with `    val sc = SparkUtils.createLocalSparkContext(""local[8]"", ""netcdf-test_Ñù)`, change it to `local[*]` probably",7/30/15,1
lossyrob,and mess with the partitions. though max is 60,7/30/15,1
lossyrob,well actually no. I_Ñém chunking it into 256x256,7/30/15,1
lossyrob,forgot I did that. so you could repartition to a lot,7/30/15,1
lossyrob,but I mean most of the time is the initial read,7/30/15,1
lossyrob,but you could certainly get the computation time down,7/30/15,1
lossyrob,"I gotta head out, so I_Ñéll be offline for a bit, back on later",7/30/15,1
rahulpalamuttam,okay cool - still is that min and max value of the entire 7k x 3k x 60?,7/30/15,1
rahulpalamuttam,hmm mine took twice as long,7/31/15,1
rahulpalamuttam,i guess it makes sense since im using doubles not ints,7/31/15,1
rahulpalamuttam,and the load probably makes sense too since im again copying doubles,7/31/15,1
lossyrob,yeah,7/31/15,1
lossyrob,that timing includes loading,7/31/15,1
lossyrob,your min max find took 160 seconds after loading or with loading/,7/31/15,1
lossyrob,?,7/31/15,1
lossyrob,"@rahulpalamuttam have you thought about using Space Filling Curves with your multidimensional approach? If so, this might be a useful conversation to read through: https://github.com/geotrellis/curve/issues/3",7/31/15,1
lossyrob,that project has moved to https://github.com/locationtech/sfcurve,7/31/15,1
lossyrob,https://locationtech.org/projects/technology.sfcurve,7/31/15,1
lossyrob,"which is being built as a general purpose space filling curve library in scala, that a couple geospatial libraries will use. Currently being built up_Ñ_there_Ñés more ideas in this project: https://github.com/cne1x/sfseize, who_Ñés functionality is hopefully going to be merged into sfcurve (the author is a committer to the sfcurve project)",7/31/15,1
rahulpalamuttam,My min max took 160 after loading ,7/31/15,1
lossyrob,"well you_Ñére also taking a single threaded approach it seems, right? I_Ñém using spark with 8 worker threads.",7/31/15,1
lossyrob,"it_Ñés a single threaded read from disk, and then after that I cut up the tile into 256x256 chunks and repartition to 32 partitions, so I_Ñém working over 32 partitions of about 20,000 tiles with 8 cores",7/31/15,1
rahulpalamuttam,41 billion points?,7/31/15,1
rahulpalamuttam,oh no did my math wrong,7/31/15,1
rahulpalamuttam,u chuked the 7k x 6k x 60 by 256 x 25,7/31/15,1
rahulpalamuttam,*256 x 256,7/31/15,1
rahulpalamuttam,I think they spacetimekey approach works beautifully - but I wonder if there's a way to do this with an n-key,7/31/15,1
lossyrob,we tried to make it generic enough to pave the way for new key types,7/31/15,1
lossyrob,"the key mechanism is a bit subtle, really relies on type classes",7/31/15,1
lossyrob,"but the thought was, one day we_Ñére going to want to do N-Dimensional work, and we tried to code towards that",7/31/15,1
lossyrob,"so most of our operations take a SpatialComponent[K] of a key, which is a typeclass that means the key type can expose a spatial component",7/31/15,1
lossyrob,"but there_Ñés no actual type heirarchy on the Key types, it_Ñés just generic",7/31/15,1
rahulpalamuttam,oh there's no heirarchy for the key types? that's very interesting,7/31/15,1
lossyrob,how that will actually play out and look once we try to lean on it for N-Dimensional support_Ñ_I guess we_Ñéd have to try to find out,7/31/15,1
lossyrob,"Yeah, so you can define custom key types",7/31/15,1
lossyrob,"but then you_Ñéll need all the type classes that go into supporting key types as far as writing them to storage backends, etc, via type classes",7/31/15,1
lossyrob,"so adding a new key type is a bit of work, and we only have two (well really three, but only two supported for IO) right now",7/31/15,1
lossyrob,are you thinking of a generic N-Dimensional key type? or would there be some concrete classes?,7/31/15,1
lossyrob,"We_Ñéve been thinking of them as case classes, sort of like TupleN",7/31/15,1
rahulpalamuttam,a generic N-dimensional key,7/31/15,1
lossyrob,I_Ñéd have to think that one out,7/31/15,1
rahulpalamuttam,which we can split the tensor by,7/31/15,1
rahulpalamuttam,and de-compose into subtensors,7/31/15,1
lossyrob,I_Ñém not familiar with the tensor language,7/31/15,1
lossyrob,which I feel like I should be,7/31/15,1
rahulpalamuttam,sorry im not a math person myself - i just use the word tensor instead of n-dimensional array,7/31/15,1
rahulpalamuttam,i mean im pretty sure im abusing the term :),7/31/15,1
lossyrob,"ok, gotcha. would the tensor represent points in N-D space, or the whole cell? That_Ñés one thing we_Ñéve been thinking about",7/31/15,1
lossyrob,"so like, our Tiles are 2D but can be in space time, and they represent areas in 2D, but points in time",7/31/15,1
rahulpalamuttam,points in space,7/31/15,1
lossyrob,ok,7/31/15,1
rahulpalamuttam,ur tiles are 256x256,7/31/15,1
rahulpalamuttam,like they're squares of values right?,7/31/15,1
lossyrob,"yeah, they cover an area",7/31/15,1
lossyrob,where as it seems like you want N continuous point spaces,7/31/15,1
rahulpalamuttam,yes,7/31/15,1
rahulpalamuttam,that is what I want,7/31/15,1
rahulpalamuttam,i want to be able to split on the individual points ,7/31/15,1
rahulpalamuttam,numpy does this - because well numpy is awesome,7/31/15,1
rahulpalamuttam,nd4j is a new library in java/scala that does this as well,7/31/15,1
lossyrob,"that_Ñés more vector-y, which we_Ñére getting into, not a lot of work is done there by us yet, to get stuff into spark, but we_Ñére in collaboration with projects like GeoWave and GeoMesa for doing vector data, which is like point data",7/31/15,1
lossyrob,"how do you mean, split on individual points?",7/31/15,1
rahulpalamuttam,umm,7/31/15,1
rahulpalamuttam,well in numpy u can take ranges from a particular dimension,7/31/15,1
rahulpalamuttam,[1:5][4:2][3:4],7/31/15,1
rahulpalamuttam,"lie that and you get a smaller ""frame"" of the entire grid",7/31/15,1
lossyrob,"ok, so the index space is a discrete space",7/31/15,1
lossyrob,"yeah, taking subspaces is something we do",7/31/15,1
lossyrob,"so if we have a spacetimekey raster layer, we can query Accumulo or other stores for a subspace, and then only work with that data. Which it_Ñés much better to do that on load",7/31/15,1
lossyrob,and that_Ñés where that spatial indexing stuff comes in,7/31/15,1
rahulpalamuttam,gotcha,7/31/15,1
rahulpalamuttam,right it makes sense that you're able to do it,7/31/15,1
rahulpalamuttam,but to extend that,7/31/15,1
lossyrob,"so, what does the index in the array mean if your dealing with point data?",7/31/15,1
rahulpalamuttam,so the dimensions have labels,7/31/15,1
rahulpalamuttam,lattitude,7/31/15,1
rahulpalamuttam,longitude,7/31/15,1
lossyrob,because an index in our sense means a specific point in 2D,7/31/15,1
rahulpalamuttam,height,7/31/15,1
rahulpalamuttam,yea,7/31/15,1
lossyrob,ok. yeah it_Ñés hard to switch my brain into that mode,7/31/15,1
rahulpalamuttam,exacly and u can extend it to a ND point is what we want to be able to use,7/31/15,1
lossyrob,"so the array index is really just _ÑÒPoint 1_Ñù, _ÑÒPoint 2""",7/31/15,1
lossyrob,"and that point can have a set of vars, like lat lng time etc",7/31/15,1
rahulpalamuttam,yea,7/31/15,1
lossyrob,"So how my brain wanst to represent that is the actual key is the lat, lng, time, etc_Ñ_and then the value is the actually interesting values, like the dependent vars",7/31/15,1
rahulpalamuttam,^^ yup that's what I wanted to do as well,7/31/15,1
lossyrob,gotcha,7/31/15,1
rahulpalamuttam,but then we get to the whole data locality and performance thing,7/31/15,1
lossyrob,"yeah. Right now we just work with Tiles as the _ÑÒvalue_Ñù portion, but we need to extend that for some vector (geometry) work",7/31/15,1
rahulpalamuttam,yup that's where we're at right now as well,7/31/15,1
rahulpalamuttam,well we've extended it to the n-d case,7/31/15,1
lossyrob,"if you index the keys right, you can achieve data locality on systems like accumulo",7/31/15,1
rahulpalamuttam,so right now we don't have a key per-say,7/31/15,1
rahulpalamuttam,basically we naively load in an arbitrary array,7/31/15,1
rahulpalamuttam,and use the netcdf library to figure out the dimensions,7/31/15,1
rahulpalamuttam,i guess u could say our key is our file-name - which differs by time,7/31/15,1
lossyrob,"hmm, right",7/31/15,1
lossyrob,is the code you_Ñére talking about all here? https://github.com/rahulpalamuttam/SciSparkTestExperiments,7/31/15,1
rahulpalamuttam,yea its all there in netcdfutils and netcdfloaders,7/31/15,1
lossyrob,cool. so the value is a N-D array,7/31/15,1
rahulpalamuttam,yea,7/31/15,1
rahulpalamuttam,breeze only supports 2d arrays,7/31/15,1
lossyrob,"I think, if you_Ñére loading an entire file into one value",7/31/15,1
rahulpalamuttam,theyr'e small files right now,7/31/15,1
lossyrob,"if it_Ñés a lot of dimensions, your values will be very large",7/31/15,1
rahulpalamuttam,as u saw by the link,7/31/15,1
lossyrob,gotcha,7/31/15,1
rahulpalamuttam,yea,7/31/15,1
rahulpalamuttam,they will be,7/31/15,1
rahulpalamuttam,so right now,7/31/15,1
rahulpalamuttam,we're handling the simple parallel case,7/31/15,1
rahulpalamuttam,of a large number of small files,7/31/15,1
lossyrob,are you doing any reduce actions?,7/31/15,1
lossyrob,or cross file operations?,7/31/15,1
rahulpalamuttam,so aggregating array values?,7/31/15,1
lossyrob,yeah,7/31/15,1
rahulpalamuttam,we do,7/31/15,1
rahulpalamuttam,but i haven't thoroughly tested that yet,7/31/15,1
rahulpalamuttam,infact i really need to get to testing performance of the computation in a distributed pipeline,7/31/15,1
rahulpalamuttam,like a heavy amount of matrices - not just a small batch,7/31/15,1
lossyrob,"what sort of aggregation? I_Ñém trying to think it through because all the vars are packed in array values, how do you match on them?",7/31/15,1
rahulpalamuttam,we have a dictionary,7/31/15,1
rahulpalamuttam,"so our version of the ""tile"" is really just a hashmap",7/31/15,1
lossyrob,is the aggregation code up?,7/31/15,1
rahulpalamuttam,code up?,7/31/15,1
lossyrob,like on github,7/31/15,1
lossyrob,so I can take a look,7/31/15,1
lossyrob,usually reading code answers questions pretty fast :),7/31/15,1
lossyrob,if you could point me at the aggregation operation I_Ñéd get a sense,7/31/15,1
rahulpalamuttam,i dont think i have the sum of all matrix ops up on there yet,7/31/15,1
rahulpalamuttam,theirs a simple one in my main,7/31/15,1
lossyrob,ok I see the hashmap in the sRDD,7/31/15,1
rahulpalamuttam,its not the sum,7/31/15,1
rahulpalamuttam,yea,7/31/15,1
rahulpalamuttam,sRDD is a bunch of these tiles,7/31/15,1
lossyrob,hmm yeah hashmaps are a bit of a performance kill...,7/31/15,1
rahulpalamuttam,the tile could be many variables,7/31/15,1
rahulpalamuttam,yea it is,7/31/15,1
rahulpalamuttam,the variables themselves have the array behind them,7/31/15,1
lossyrob,right. and the RDD converts them to a hashmap of values,7/31/15,1
rahulpalamuttam,and the array could have coordinates lat/lon/height etc,7/31/15,1
rahulpalamuttam,so remember we talked about keys,7/31/15,1
lossyrob,whats the hashmap keys?,7/31/15,1
lossyrob,the var name?,7/31/15,1
rahulpalamuttam,the varname,7/31/15,1
rahulpalamuttam,yes,7/31/15,1
rahulpalamuttam,"but all variables are different ""values"" in one tile",7/31/15,1
rahulpalamuttam,ugh,7/31/15,1
rahulpalamuttam,sorry im getting my terms mixed,7/31/15,1
rahulpalamuttam,sRDD is a collection of tiles,7/31/15,1
rahulpalamuttam,a tile is a bundle of variables - the variable could be rainfall/temp/etc.,7/31/15,1
lossyrob,I_Ñém having a bit of trouble reading the tensor -> hash map -> sciTensor -> T conversion,7/31/15,1
lossyrob,so is the sRDD supposed to be an RDD of sciTensors?,7/31/15,1
rahulpalamuttam,yes,7/31/15,1
lossyrob,`class sRDD[T: ClassTag <: sciTensor]`,7/31/15,1
rahulpalamuttam,ya,7/31/15,1
lossyrob,or is it `class sRDD[T <: sciTensor : ClassTag]`. I forget the order,7/31/15,1
lossyrob,"and a sciTensor is really just one of these Maps, with the keys being the var names like lat, lng, tmp, and the values are a set of values that belonged to that file",7/31/15,1
rahulpalamuttam,yea,7/31/15,1
rahulpalamuttam,awesome that's that,7/31/15,1
lossyrob,nice,7/31/15,1
lossyrob,"hmm yeah. So I_Ñéd almost htink you_Ñéd want an RDD[(String, Double)], with each var name being the key and Double being the value, no? what_Ñés the reason for keeping the hashmap grouping?",7/31/15,1
rahulpalamuttam,the variables themselves are the n-d array,7/31/15,1
rahulpalamuttam,the hasmap,7/31/15,1
rahulpalamuttam,is,7/31/15,1
rahulpalamuttam,"(string, ndarray)",7/31/15,1
rahulpalamuttam,the variables themselves are they arrays,7/31/15,1
lossyrob,but I thought each var was one dimensions of the N dimensions?,7/31/15,1
lossyrob,"so if _ÑÒlat_Ñù, _ÑÒlng_Ñù, _ÑÒtmp_Ñù are the vars, I was imagining that that was 3-D space, and each dimension was one of the vars",7/31/15,1
rahulpalamuttam,lat lng are not vars,7/31/15,1
rahulpalamuttam,tmp is,7/31/15,1
rahulpalamuttam,lat long height are usually coordinates,7/31/15,1
lossyrob,what would multiple dimensions of tmp mean?,7/31/15,1
rahulpalamuttam,tmp is a variable,7/31/15,1
rahulpalamuttam,like rainfall,7/31/15,1
lossyrob,right. but that_Ñés 1-D,7/31/15,1
rahulpalamuttam,or humidity,7/31/15,1
lossyrob,"if the element is a tensor, or to think of it as the underlying hashmap,  the wouldn_Ñét the hashmap be ( _ÑÒtmp_Ñù -> Array[Double(_Ñ_), _ÑÒlat_Ñù -> Array[Double(_Ñ_), etc)",7/31/15,1
lossyrob,like what does the shape of the Tensor mean?,7/31/15,1
rahulpalamuttam,one sec sorry gotta take,7/31/15,1
rahulpalamuttam,a call,7/31/15,1
lossyrob,no prob,7/31/15,1
lossyrob,"I mean, reading the code I can see that each var can represent a N-D space, where we_Ñéve been talking about _ÑÒlat_Ñù, _ÑÒlng_Ñù, and _ÑÒtmp_Ñù_Ñ_I guess an example of a var that has a dimension greater than 1 would help",7/31/15,1
rahulpalamuttam,take tasmax for example,7/31/15,1
lossyrob,"well I guess thinking about the NEX data, the tmp values were actually 3 dimensions, and those were the cell values",7/31/15,1
lossyrob,and time,7/31/15,1
rahulpalamuttam,yea,7/31/15,1
lossyrob,ok. I guess I_Ñém getting tripped up on _ÑÒlat_Ñù nad _ÑÒlng_Ñù being vars,7/31/15,1
rahulpalamuttam,yea,7/31/15,1
rahulpalamuttam,there u go,7/31/15,1
rahulpalamuttam,lat lon are usually dimensions,7/31/15,1
rahulpalamuttam,and height,7/31/15,1
lossyrob,"so inside of a var, there would be a 3 dimensional space, lat, lng, and height values",7/31/15,1
rahulpalamuttam,could be,7/31/15,1
lossyrob,"so really, our key refers to a tile layout col row, and it_Ñés not until you get into the pixel value of a tile that you get the point value. Though the time value is encoded in the key, whihc would be a dimension of the var",7/31/15,1
lossyrob,"So tiles can be thought of as chunks of values, with subspaces in a 2D plane, and the keys say what subspace that 2D plane represents, and also the location of the values in the rest of the n-D space (which for us is just 3D time)",7/31/15,1
lossyrob,"and our case, our points are always grouped on a plane in our 3D space, where we can represent a set of points always as fixed in one dimension (time) and covering an area in the other 2D (which is the area that the tile surface covers.",7/31/15,1
lossyrob,so really we_Ñére a pretty special case of the general N-D approach. for now at least :),7/31/15,1
lossyrob,"to go back to _ÑÒMultiBand_Ñù_Ñ_.you can think of MultiBand being extra dimensions on that 2D surface, so if we represent one dimension as height and another dimension as, temperature I guess_Ñ_.then those would be 2 values in the 2 bands that represent the cells of that 2D surface. So if we just had 2D space and those to values, it would be a 4D space, but where the points were distributed evenly in the 2D space, and the other 2 dimensions would be captured by 2 bands of a tile at that cell representing the 2D space. The cell of the one band_Ñés tile being height value, the cell of the other band_Ñés value being the temperature value",7/31/15,1
lossyrob,So we actually can represent N dimensional space with multi band tiles_Ñ_I never thought of it that way before.,7/31/15,1
lossyrob,but with the strict requirement that the 2D points be evenly covered in a grid,7/31/15,1
lossyrob,(which is just the raster use case),7/31/15,1
lossyrob,"with Point Features, which is something like Feature[Point, D], where D is generic, that could contain your other dimensions. So PointFeature[Data] with `case class Data(height: Double, temp: Dobule)` could cover the 4D space mentioned above, while having the points available for geospatial operations. Although the height there indicates you_Ñéd like to do 3D geometric operations perhaps, which is a whole other ballgame.",7/31/15,1
rahulpalamuttam,all right back,7/31/15,1
rahulpalamuttam,right so u still have indirection,7/31/15,1
rahulpalamuttam,to reference the 2D square,7/31/15,1
rahulpalamuttam,take for instance the tasmax variable,7/31/15,1
rahulpalamuttam,"it has 60 ""bands""",7/31/15,1
rahulpalamuttam,for me that's an extra dimension with 60 indices,7/31/15,1
rahulpalamuttam,for you having keys mapped to 2D spaces make sense - because your entire RDD represents a grid variable,7/31/15,1
rahulpalamuttam,*your entire RDD is the grid,7/31/15,1
rahulpalamuttam,what i have no tackles the many-small-values,7/31/15,1
rahulpalamuttam,what I want to do is extend to one-large-value (which is what you do right now ) :),7/31/15,1
rahulpalamuttam,"but i want to have the underlying ""tile"" be an n-darray",7/31/15,1
rahulpalamuttam,an n-cube if you will,7/31/15,1
zifeo,"@lossyrob Sure, I am solving one last bug (caused by the intersection) and its done",7/31/15,1
lossyrob,"@zifeo, looks like failing tests: https://travis-ci.org/geotrellis/geotrellis/jobs/73585966",7/31/15,1
zifeo,"hmm, weird, I have to relaunch a few time each time before getting the same",7/31/15,1
zifeo,"ha, it is caused by the change ```scalaif (rowMaxOrg > 0 && rowMinOrg < re.rows) {``````scalaif (rowMaxOrg >= 0 && rowMinOrg < re.rows) {```",7/31/15,1
zifeo,I guess either I am false or the old tests are...,7/31/15,1
lossyrob,Hmm. yeah that_Ñés going to take some thought and drawing out the examples.,7/31/15,1
zifeo,"I don_Ñét even see where the `PolygonRasterizer` is used in the tests and sub-functions, any idea ?",7/31/15,1
zifeo,Failing file is `ZonalSummaryRasterSourceMethodsSpec`,7/31/15,1
lossyrob,It_Ñés in the zonal summary methods,7/31/15,1
lossyrob,"the zonal summary takes the polygon, splits it up per tile, and does a rasterization on each tile",7/31/15,1
lossyrob,"so if the tile is partially intersecting the polygon, we do a rasterization call",7/31/15,1
echeipesh,"ok, so this is going to be a little tricky to intemperate",7/31/15,1
lossyrob,"inëˆtemëˆperëˆateinè_temp(Ûé)rÛét/adjectivehaving or showing a lack of self-control; immoderate.",7/31/15,1
lossyrob,?,7/31/15,1
echeipesh,"right, difficult",7/31/15,1
lossyrob,"lol. just so people arent confused, that conversation ended up continuing elsewhere",7/31/15,1
zifeo,Haha. Got it !,7/31/15,1
echeipesh,"ok, so I am re-running tests on S3 catalog here.",8/1/15,1
echeipesh,"using 256x256 tiles returns breaks in 25s from my home wifi, and using 512x512 tiles returns in 10s.",8/1/15,1
echeipesh,"since we are only doing analysis, increasing the tile size would have exactly the same effect as meta-tiles. Going to try a bigger tile next.",8/1/15,1
lossyrob,"yeah. def no reason to have 256x256 for non-visual tiles",8/1/15,1
echeipesh,"1024x1024 = 8s, 768x768 = 6s",8/1/15,1
lossyrob,interesting,8/1/15,1
echeipesh,"```mdsize  | zoom | tiles | time (s)--------------------------------256   | 13   | 560   | 24512   | 12   | 140   | 10768   | 11   | 40    | 61024  | 11   | 40    | 8```",8/1/15,1
echeipesh,"```bashcurl -X POST -d 'layers=nlcd-scodec-512&zoom=12&bbox=-121.81640624999999,43.91372326852401,-120.95947265624999,44.49650533109345&weights=1&numBreaks=10&srid=4326' 'http://localhost:8777/gt/spark/breaks'```",8/1/15,1
echeipesh,"pretty consistent timings too, from run to run.",8/1/15,1
echeipesh,seeing similar time improvements for ingest for these layers too.,8/1/15,1
echeipesh,"Also this makes true meta-tiles not as desirable unless there is a need to serve tiles out at specific small size, but IO would benefit from larger size. ",8/1/15,1
lossyrob,that makes sense. So really we should just chunk non-visual tiles into later tiles,8/1/15,1
lossyrob,as a rule of thumb,8/1/15,1
echeipesh,yeah. We_Ñéll need to repeat experiment when we have non-resampled layers. Some of those layers must be blown up to a larger size because of the snapping to pyramid.,8/1/15,1
echeipesh,"also, I forgot to compress the tiles. Need to zlib them.",8/1/15,1
echeipesh,,8/3/15,1
echeipesh,"Updated benchmarks with zlib compression from Azavea network:```mdsize  | zoom | tiles | time (s)--------------------------------256   | 13   | 560   | 8512   | 12   | 140   | 2768   | 11   | 40    | 1.361024  | 11   | 40    | 1.192```",8/3/15,1
echeipesh,"That was over ethernet, this is over WiFi, which should be a little more comparable for timings from last Friday:```mdsize  | zoom | tiles | time (s)--------------------------------256   | 13   | 560   | 7512   | 12   | 140   | 3.6768   | 11   | 40    | 2.31024  | 11   | 40    | 4.5```",8/3/15,1
jwalgran,"When doing a weighted overlay of ``Byte`` rasters, large values will start overflowing. Does GT support automatic promotion? If not, do you have a suggestion for when is the best time in the processing pipeline to convert to a larger integer? Any differences between the breaks calcualtion case vs. processing individual ``Tile``s?",8/4/15,1
lossyrob,You can convert to TypeShort,8/4/15,1
lossyrob,I would convert it before the possible overflow. It_Ñés a lazy operation that happens the next operation,8/4/15,1
lossyrob,If you could show me some code I can look over it to see when the best would be,8/4/15,1
lossyrob,wiki watershed is on public github yeah?,8/4/15,1
jwalgran,It is. The modeling code I am working on is in a private repo. I_Ñéll send you a link. Thanks.,8/4/15,1
echeipesh,"Updates:```md#Avro Decoding Byte Tiles with Deflate Compression size  | zoom | tiles | time (s)--------------------------------256   | 13   | 560   | 3.5, 2.9, 2.7512   | 12   | 140   | 1.4, 1.0, 1.0768   | 11   | 40    | 0.6, 0.9, 0.61024  | 11   | 40    | 0.7, 0.6, 0.7#Scodec Decoding Byte Tiles with lzip Compressionsize  | zoom | tiles | time (s)--------------------------------256   | 13   | 560   | 4.9, 5.1, 5.9512   | 12   | 140   | 1.6, 1.6, 1.9768   | 11   | 40    | 1.1, 1.3, 0.91024  | 11   | 40    | 0.9, 1.1, 1.1```This is a spetial case of Avro, since bytes go through ByteBuffer representation. Tiles ore ints/doubles may be much slower. But it looks like we can expect it to be performant eventually.",8/4/15,1
echeipesh,"@echeipesh That_Ñés interesting",8/4/15,1
echeipesh,"ok, apparently not much to worry about on int tiles: ",8/4/15,1
echeipesh,"```Avro 256x256 int tiles: 5.6, 3.6, 3.0Scodec 256x256 int tiles: 7.1, 5.0, 5.2```",8/4/15,1
lossyrob,@echeipesh does ETL let me save off histograms?,8/5/15,1
echeipesh,"There is no interface for that yet. You_Ñéd have to create S3 attribute store instance, which does detract from modularity a bit.",8/5/15,1
echeipesh,"Since AttributeStore is its own interface, no reason it can_Ñét be part of the plugin interface and be pulled off from the catalog. Just not there yet.",8/5/15,1
lossyrob,This looks tempting: https://github.com/travisbrown/jfc,8/5/15,1
echeipesh,Agreed,8/5/15,1
echeipesh,,8/5/15,1
qweqweq,@pomadchin ,8/6/15,1
qweqweq,"hi, i need some demo about  how to deal with the RasterRdd,when i ingest the tif  file into the HDFS.```val hc = HadoopRasterCatalog(new Path(""hdfs://master:9000/catalog""))(sc)val rdd  = hc.read[SpatialKey](""ncld"",0) //i found this method read turn red  ,i use IntelliJ IDEA```can you give me some guidance or demo ,i want to know the details about use geotrellis . ",8/6/15,1
pomadchin,"as I remember, `read` provides rdd for a layer, so it should be used like `hc.read[_Ñ_](_ÑÒlayerId_Ñù)`btw, how you_Ñéve ingested data? you used `geotrellis.spark.ingest.HadoopIngestCommand` ?",8/6/15,1
qweqweq,"yeah ,firstly i use this to ingest the tif file,and it's done:./spark-shell  --master spark://master:7077 --jars ~/geotrellis/spark/target/scala-2.10/geotrellis-spark-assembly-0.10.0-SNAPSHOT.jar  --input file:///home/zkpk/input/ --class geotrellis.spark.ingest.HadoopIngestCommand --driver-memory 2G --executor-memory 2G  --pyramid true --clobber true --catalog hdfs://master:9000/output --layerName yyh --crs EPSG:3857 next,i want to read it from HDFS,when i use IDEA ,it's got a error.",8/6/15,1
qweqweq,[![blob](https://files.gitter.im/geotrellis/geotrellis/sQbX/thumb/blob.png)](https://files.gitter.im/geotrellis/geotrellis/sQbX/blob),8/6/15,1
qweqweq,"now ,i'm doing some research  on how to extract  ""island or something else"" from the TIF file,just like this:http://4quant.com/geospatialdemo/,could you give me some advice?Does  geotrellis  do the same thing?thank you .",8/6/15,1
qweqweq,http://4quant.com/geospatialdemo/,8/6/15,1
pomadchin,"Err appears, that there is no `read` function application for such parameters.Try `hc.read[SpatialKey](""yyh"")`",8/6/15,1
pomadchin,the result would be an rdd for this layer,8/6/15,1
pomadchin,"yep it is possible to find neccecary objects analyzing tile information, by hands (didn_Ñét saw such functions in gt); mb our masters will give you a more concrete answer",8/6/15,1
qweqweq,thank you :smile: @pomadchin ,8/6/15,1
pomadchin,"oh typo found ** ```scalahc.read[SpatialKey](LayerId(_ÑÒyyh_Ñù, zoom))```",8/6/15,1
qweqweq,thank you ,8/6/15,1
qweqweq,[![blob](https://files.gitter.im/geotrellis/geotrellis/9Cn3/thumb/blob.png)](https://files.gitter.im/geotrellis/geotrellis/9Cn3/blob),8/6/15,1
lossyrob,@qweqweq we can_Ñét currently do the sort of feature extraction on that 4quant page,8/6/15,1
lossyrob,"It_Ñés something we_Ñére certainly headed toward and want to support, but we aren_Ñét there yet.",8/6/15,1
lossyrob,"thanks for pointing that project out. It seemsl like they use ImageJ, something I hadn_Ñét really looked into before, but seems like it might be a good route to go for doing some image processing",8/6/15,1
qweqweq,"yeah,thank you @lossyrobso i want to know what i can be done using  geotrellis ?Is not it can handle tif file and convert it into png.and next i can use these png to do something like  :https://github.com/geotrellis/geotrellis-chatta-demo",8/7/15,1
echeipesh,In order to have neat representation of the key indexies `ZSpaceTimeKeyIndex` needs to lose it_Ñés `DateTime => Int` signature. The motivation is to give enough information to reconsturct the index without using object/function serealization. All other index methods are already well behaved.,8/7/15,1
echeipesh,"One option that comes to mind is to construct a `Vector[(DateTime, Int)]` that would map date ranges to index space by specifying breaks. The index mappings don_Ñét need to be continous so this would encoud fairly abritrary schemes.",8/7/15,1
lossyrob,"@qweqweq yes you could ingest a tif file and convert to PNG. @zifeo is working with an example of the chatta demo that works with spark, so soon we_Ñéll have an example of that",8/7/15,1
lossyrob,"@echeipesh the one option was to have users define their `DateTime => Int` functions explicitly in a type, and then that type would be encoded. Is this not possible because of lack of auto-scheme generation?",8/7/15,1
echeipesh,"Actually yes, we can still do that and lean on java reflection. That_Ñés a better option",8/7/15,1
lossyrob,"Cool. I think if we supply a lot of default time indicies, the time when a user would have to make their own would be minimized",8/7/15,1
echeipesh,"That still makes it accessable only from java land, but at least its better than a baes64 blob. The class name will be some documentation.",8/7/15,1
lossyrob,Hmm yeah that_Ñés true,8/7/15,1
lossyrob,"Honestly, if we move to hilbert with a temporal resolution instead of a `DateTime => Int` mapping function, then that will be more readable outside of jvm",8/7/15,1
pomadchin,"@lossyrob as I remember, now it is possible make index by pattern for time, you want more specific?",8/7/15,1
lossyrob,"no, we_Ñére trying to support that for Z indexes, but we are changing our serialization mechanism",8/7/15,1
lossyrob,"since we were leaning on Kryo, which was a bad decision (by me) for doing storage serialization",8/7/15,1
lossyrob,"so now we_Ñére switching to Avro (thanks to Eugene), and so we_Ñére trying to think about how we can support the mapping of times to ints in the Avro way of serializing",8/7/15,1
echeipesh,"Right, so if you have a crazy function right now, you_Ñéd have to subclass some trait, like `DateTimeMapper` and provide that, rather than a closure. What do you think about that?",8/7/15,1
pomadchin,:d not a closure haha; but ok workaround ,8/7/15,1
lossyrob,"but also, with the hilbert curve, there_Ñés no concept of mapping date times to integers really, it_Ñés just a matter of having a temporal resolution (at `2^k` for some `k`) that divides up the temporal dimension just like there_Ñés some spatial resolution (`2^n` for some `n`). Interestingly, in Hilbert, `n` doesn_Ñét have to equal `k`, but in the Z index, `n == k`",8/7/15,1
pomadchin,"Hm; ok, remember your great explanation ",8/7/15,1
pomadchin,Seems have to move to hilbert,8/7/15,1
lossyrob,we_Ñéll have to do some benchmarking to confirm some suspicions I have about it. When we have the time of course :),8/7/15,1
pomadchin,): want to work more with gt,8/7/15,1
lossyrob,yeah it_Ñés nice to have help :) If you ever want to put some time into some low hanging fruit I can probably find some small stuff that would be really helpful to have someone in the community develop on,8/7/15,1
allixender,"Hi, after merging latest master I'm getting a few  of those in `""project spark"" test````scala[error] Uncaught exception when running geotrellis.spark.op.local.EqualSpec: spray.json.DeserializationException: { obj: [base64] } expected: {""serialized"":""5\u0001\u0010\u0000\u0000""}sbt.ForkMain$ForkError: { obj: [base64] } expected: {""serialized"":""5\u0001\u0010\u0000\u0000""}```",8/13/15,1
allixender,Should I just delete the test-data again?,8/13/15,1
allixender,I did `./sbt clean` before though,8/13/15,1
allixender,"spark/src/test/resources/test-catalogspark/src/test/scala/geotrellis/spark/testfiles",8/13/15,1
allixender,Delete latter one?,8/13/15,1
allixender,"Got it I guess:```shrm -rf spark/src/test/resources/catalog-specrm -rf spark/src/test/resources/test-catalog```",8/13/15,1
lossyrob,sorry,8/13/15,1
lossyrob,"if you do `test:run`, that should regenerate hte tes files",8/13/15,1
lossyrob,*the test files,8/13/15,1
lossyrob,should have mentioned that.,8/13/15,1
allixender,"ah ok, no prob, now I know :smile: ",8/13/15,1
pomadchin,"@lossyrob @echeipesh Are there any thoughts about accumulo crossversion support? Mb some generic wrapper for an accumulo adapter, which would be possible to extend?",8/13/15,1
pomadchin,is it possible mb to limit batches for an ingest job? mb would be a nice idea :D ,8/13/15,1
allixender,"Hi, how can I use the new Avro de/serializer if I have a a byteArray and know CellType, cols, and rows?Coming from here:```scala// val (_, tileBytes) = KryoSerializer.deserialize[(K, Array[Byte])](byteArray)```and then I assemble the actual Tile:```scalaArrayTile.fromBytes(      tileBytes,      rasterMetaData.cellType,      rasterMetaData.tileLayout.tileCols,      rasterMetaData.tileLayout.tileRows    )```I saw in `geotrellis.spark.io.s3.TileReaderTileReader[K: AvroRecordCodec]` ```scalaval recs = AvroEncoder.fromBinary(bytes)(KeyValueRecordCodec[K, Tile])```But I have to provide ```shell[error]     scala/geotrellis/spark/io/cassandra/TileReader.scala:52: could not find implicit value for evidence parameter of type geotrellis.spark.io.avro.AvroRecordCodec[K][error]     val (_, tileBytes) = AvroEncoder.fromBinary(byteArray)(KeyValueRecordCodec[K, Array[Byte]])```Anyone a quick hint how to I can provide the necessary evidence parameters or add the additional steps to build the schema or the likes?",8/14/15,1
pomadchin,@allixender you have to pass `AvroRecordCodec` implicit,8/14/15,1
pomadchin,"```scalatrait TileReader[Key: AvroRecordCodec] { _Ñ_val (_, tileBytes) = AvroEncoder.fromBinary(byteArray)(KeyValueRecordCodec[Key, Array[Byte]])... }```",8/14/15,1
echeipesh,"@pomadchin yes, we_Ñére planning to break out all accumulo dependant code into a subproject and crossversion that. That seems to be the only way to handle depending on non-public APIs. Your idea of having a single interface that can be implemented by crossversion support is good, kind of makes +1 project (and we already have so many!)",8/14/15,1
echeipesh,"@pomadchin what do you mean about limiting batches for ingest jobs? You mean how big source partitions are for _ÑÒraw_Ñù images? Something like that is possible with S3 using _ÑÒsplitSize_Ñù argument, are you looking at another source?",8/14/15,1
pomadchin,@echeipesh `splitSize` ? let me look,8/14/15,1
echeipesh,"@allixender you can import Avro codecs for keys from (at the call site): https://github.com/geotrellis/geotrellis/blob/master/spark/src/main/scala/geotrellis/spark/io/avro/KeyCodecs.scalaAlso, avro is storing the tile, including the row/col count as well as the array. https://github.com/geotrellis/geotrellis/blob/master/spark/src/main/scala/geotrellis/spark/io/avro/TileCodecs.scala#L99For the price of storing so many bytes adding the two ints is no big deal for clarity.",8/14/15,1
echeipesh,"@pomadchin maybe nominclature could be improved here: https://github.com/geotrellis/geotrellis/blob/master/spark/src/main/scala/geotrellis/spark/io/s3/S3InputFormat.scala#L106https://github.com/geotrellis/geotrellis/blob/master/spark-etl/src/main/scala/geotrellis/spark/etl/s3/S3Input.scala#L17",8/14/15,1
pomadchin,@echeipesh ty! would be great to have such function for accumulo & hadoop,8/14/15,1
echeipesh,"that_Ñés a good point, let me double check it_Ñés not availble in some form right now",8/14/15,1
echeipesh,"Yes, that area needs more control. It looks like if you_Ñére reading geotiffs from HDFS it_Ñéll make a split per file. rdd.coalesce is always cheap _Ñ_ but still not great interface.",8/14/15,1
echeipesh,also you can_Ñét ingest from accumulo :) the only possible source is an accumulo catalog which already gives RasterRDD,8/14/15,1
echeipesh,"Threw up an issue for it, pine in if you have thoughts on it :)",8/14/15,1
zifeo,"@echeipesh @lossyrob working on Avro for accumulo, results look good_Ñ_ but, I get lots of tests errors like those ones:",8/14/15,1
zifeo,"`[error] Uncaught exception when running geotrellis.spark.op.zonal.summary.SumDoubleSpec: spray.json.DeserializationException: { obj: [base64] } expected: {""serialized"":""\u0001\u0000geotrellis.spark.io.index.rowmajor.RowMajorSpatialKeyInde«èô\u0001\u0010\u0000\u0000ë_ë_}`",8/14/15,1
zifeo,how can I bypass them?,8/14/15,1
echeipesh,"Yes, that_Ñés stale test catalog, delete it and regenerate it.",8/14/15,1
zifeo,"oh, I believed it was recreated each time",8/14/15,1
echeipesh,"```bashrm -rf spark/src/test/resources/catalog-specrm -rf spark/src/test/resources/test-catalogsbt _ÑÒproject spark_Ñù test:run```",8/14/15,1
zifeo,and for those one:,8/14/15,1
echeipesh,"Nope, it is _Ñ_ not the best. It_Ñés actually around. I run into this all the time.",8/14/15,1
zifeo,"```bash[info] Accumulo Raster Catalog with Spatial Rasters - should load out saved tiles *** FAILED ***[info]   org.apache.spark.SparkException: Task not serializable   org.apache.spark.SparkException: Task not serializable[info]   at org.apache.spark.util.ClosureCleaner$.ensureSerializable(ClosureCleaner.scala:315)[info]   at org.apache.spark.util.ClosureCleaner$.org$apache$spark$util$ClosureCleaner$$clean(ClosureCleaner.scala:305)[info]   at org.apache.spark.util.ClosureCleaner$.clean(ClosureCleaner.scala:132)[info]   at org.apache.spark.SparkContext.clean(SparkContext.scala:1893)[info]   at org.apache.spark.rdd.RDD$$anonfun$map$1.apply(RDD.scala:294)[info]   at org.apache.spark.rdd.RDD$$anonfun$map$1.apply(RDD.scala:293)[info]   at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:147)[info]   at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:108)[info]   at org.apache.spark.rdd.RDD.withScope(RDD.scala:286)[info]   at org.apache.spark.rdd.RDD.map(RDD.scala:293)[info]   ...[info]   Cause: java.io.NotSerializableException: org.apache.avro.Schema$RecordSchema[info]  - field (class ""geotrellis.spark.io.avro.TileCodecs$ByteArrayTileCodec$"", name: ""schema"", type: ""class org.apache.avro.Schema"")[info]  - object (class ""geotrellis.spark.io.avro.TileCodecs$ByteArrayTileCodec$"", geotrellis.spark.io.avro.TileCodecs$ByteArrayTileCodec$@44997b37)[info]  - element of array (index: 0)[info]  - array (class ""[Lgeotrellis.spark.io.avro.AvroRecordCodec;"", size: 5)[info]  - field (class ""scala.collection.mutable.WrappedArray$ofRef"", name: ""array"", type: ""class [Ljava.lang.Object;"")[info]  - object (class ""scala.collection.mutable.WrappedArray$ofRef"", WrappedArray(geotrellis.spark.io.avro.TileCodecs$ByteArrayTileCodec$@44997b37, geotrellis.spark.io.avro.TileCodecs$FloatArrayTileCodec$@5269db34, geotrellis.spark.io.avro.TileCodecs$DoubleArrayTileCodec$@2459c0bd, geotrellis.spark.io.avro.TileCodecs$ShortArrayTileCodec$@49f33723, geotrellis.spark.io.avro.TileCodecs$IntArrayTileCodec$@463cade9))[info]  - field (class ""geotrellis.spark.io.avro.AvroUnionCodec"", name: ""formats"", type: ""interface scala.collection.Seq"")[info]  - object (class ""geotrellis.spark.io.avro.AvroUnionCodec"", geotrellis.spark.io.avro.AvroUnionCodec@1a2d5b8f)[info]  - field (class ""geotrellis.spark.io.avro.TupleCodec"", name: ""b"", type: ""class geotrellis.spark.io.avro.AvroRecordCodec"")[info]  - object (class ""geotrellis.spark.io.avro.TupleCodec"", geotrellis.spark.io.avro.TupleCodec@7f7563f6)[info]  - field (class ""geotrellis.spark.io.accumulo.RasterRDDReader$$anonfun$3"", name: ""readCodec$1"", type: ""class geotrellis.spark.io.avro.TupleCodec"")[info]  - root object (class ""geotrellis.spark.io.accumulo.RasterRDDReader$$anonfun$3"", <function1>)[info]   at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1182)```",8/14/15,1
zifeo,how to handle closure ?,8/14/15,1
echeipesh,S3 catalog avoids that by sending it through a broadcast variable (which uses kryo not java ser): https://github.com/geotrellis/geotrellis/blob/master/spark/src/main/scala/geotrellis/spark/io/s3/RasterRDDWriter.scala#L46,8/14/15,1
zifeo,"oh again, I believe spark what changed to avro too, and I used implicit broadcast...",8/14/15,1
echeipesh,lossyrob and I were talking about moving away from use/abuse of broadcasts since they_Ñére possibly not as efficient (Block manager keeps them around outside of task time): https://github.com/geotrellis/geotrellis/issues/1154,8/14/15,1
echeipesh,"Oh, no, spark uses Kryo for data broadcasts and always java for closures.",8/14/15,1
echeipesh,"You can give a shot to using `KryoWrapper`, to ship the codec in a closure to the workers. That_Ñés the only option that I can think of.",8/14/15,1
zifeo,Let_Ñés see...,8/14/15,1
zifeo,And could be the Hilbert key/curve be missing AvroCodec ?,8/14/15,1
echeipesh,"hmmm ? The KeyIndex is serialized with java ser, shouldn_Ñét touch Avro. Or do you mean something else ?",8/14/15,1
zifeo,I mean this resulting in empty vectors/arrays:,8/14/15,1
zifeo,"```bash[info] - HilbertKeyIndexMethod with min, max, and resolution *** FAILED ***[info]   java.io.NotSerializableException: com.google.uzaygezen.core.CompactHilbertCurve[info]  - field (class ""geotrellis.spark.io.index.hilbert.HilbertSpaceTimeKeyIndex"", name: ""chc"", type: ""class com.google.uzaygezen.core.CompactHilbertCurve"")[info]  - root object (class ""geotrellis.spark.io.index.hilbert.HilbertSpaceTimeKeyIndex"", geotrellis.spark.io.index.hilbert.HilbertSpaceTimeKeyIndex@24f28299)[info]   at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1182)```",8/14/15,1
echeipesh,"Right, that_Ñés spark not being able to use java seralization to wrap `com.google.uzaygezen.core.CompactHilbertCurve` when serializing a closure. Avro is only used, explicitly, to write records to the catalogs. Because we need that format to be super stable and somewhat future-proof.",8/14/15,1
echeipesh,The serilaization that_Ñés happening to communicate over the network during the job lifetime is still (and probably always) going to be kryo and java ser.,8/14/15,1
echeipesh,"Kryo handles RDD values (when shipped over network/disk) and Broadcast values, Java handles code closures (arguments to RDD.map/reduce/combineByKey)",8/14/15,1
zifeo,"Yeah, but which changes in the accumulo/avro things could lead to that ?",8/14/15,1
echeipesh,"Nothing really, it might have always been that way. I_Ñém not sure if we have test that use compact hilbert curve with RDD. (looking)",8/14/15,1
zifeo,"I don_Ñét know, but the build is failing now, so something is going on ^^",8/14/15,1
zifeo,"Failed tests are: `geotrellis.spark.io.accumulo.AccumuloRasterCatalogSpec` & `geotrellis.spark.io.hadoop.HadoopRasterCatalogSpec`",8/14/15,1
zifeo,I guess that hadoop is because I didn_Ñét change yet to avro (some errors being out of bounds..),8/14/15,1
zifeo,"Sorry, it is the inverse. Index out of bounds are coming from `AccumuloRasterCatalogSpec` and Hilberts from `HadoopRasterCatalogSpec`",8/14/15,1
echeipesh,"Yes, I see it.",8/14/15,1
echeipesh,Sorry I didn't follow you at first.,8/14/15,1
echeipesh,"Ok, this is an ugly hack:",8/14/15,1
echeipesh,"You can make chc Val in hipbertspacetimeindex a transient car set to null, then make a def that checks for null, news up an instance and returns it. ",8/14/15,1
echeipesh,Being transient will stop Java set from trying to encode it.,8/14/15,1
allixender,Thanks guys @pomadchin @echeipesh ,8/14/15,1
echeipesh,"@zifeo something like this fixes `HilbertSpaceTimeKeyIndex`:```scala  @transient  private var _chc: CompactHilbertCurve =  null  def chc = {    if (null == _chc) {      val dimensionSpec =        new MultiDimensionalSpec(          List(            math.pow(2, xResolution).toInt,            math.pow(2, yResolution).toInt,            math.pow(2, temporalResolution).toInt          ).map(new java.lang.Integer(_))        )      _chc = new CompactHilbertCurve(dimensionSpec)    }    _chc  }```",8/15/15,1
zifeo,thanks! I will finish tomorrow,8/15/15,1
pomadchin,"what problem can be while connecting to accumulo? (shell is operating normally, but as a client there occurs _ÑÒinfinte pending"")",8/15/15,1
pomadchin,~~`val accumulo = AccumuloInstance()`,8/15/15,1
pomadchin,oh guyz solved; buggy recovery after tablets fail,8/15/15,1
pomadchin,%),8/15/15,1
pomadchin,stiil using 1.5.2 :D,8/15/15,1
allixender,"hey @lossyrob, I have been orienting on  @zifeo's accumulo avro implementation as inspir the cassandra branch, however, there's a bit of code in the cassandra RDDWriters, that's use a KryoClosure over some code that used the KryoSerializer ëâëâëâscala",8/17/15,1
allixender,arg sry,8/17/15,1
allixender,"```scalaraster      .map(        KryoClosure {          case (key, tile) =>            val value = AvroEncoder.toBinary(key, tile)(writeCodec.value)            val indexer = closureKeyIndex.toIndex(key).toString            (indexer.reverse, layerId.zoom, indexer, layerId.name, value)        }      )      .saveToCassandra(session.keySpace, tileTable, SomeColumns(""reverse_index"", ""zoom"", ""indexer"", ""name"", ""value""))```",8/17/15,1
allixender,"That KryoClosure seems to support serializing the code to save the RDD to Cassandra I guess, and does not need to be replaced?",8/17/15,1
allixender,I'll see that I get a cassandra spark ETL demo working,8/17/15,1
zifeo,@echeipesh do you have any ETL example of accumulo ingesting (there is no AccumuloInput) ?,8/17/15,1
pomadchin,,8/17/15,1
pomadchin,guyz! `EPSG:3857`(`merc`) not supported by geotiff writer?,8/17/15,1
pomadchin,and what projection gives us a `renderPng` function? seems to me the ingest destination crs?,8/17/15,1
pomadchin,"and a fast qustion: for example i got tiles on three layers: (red, green, blue)how to combine them to get rgb?",8/17/15,1
lossyrob,"@pomadchin yes. that_Ñés one of the biggest holes we have in the library right now. The geotiff writer code uses some Proj4 -> GeoTiff code that was ported from GDAL. Apparently that code wasn_Ñét complete, and GDAL uses the WKT -> GeoTiff code in actuallity. That leaves us with some big holes. That_Ñés high priority, but not sure when I_Ñéll be able to get to it",8/17/15,1
lossyrob,current workaround is to write to LatLng and use GDAL to warp to webmercator. Horrible I know.,8/17/15,1
pomadchin,yep making definetly this way,8/17/15,1
lossyrob,"@pomadchin for your layers, what type are they?",8/17/15,1
lossyrob,we have some issues with TypeByte being unsigned and so workarounds for dealing with 0 - 255 values,8/17/15,1
lossyrob,are the layers RasterRDDs?,8/17/15,1
pomadchin,"`Band 1 Block=15941x1 Type=Byte, ColorInterp=Gray`",8/17/15,1
pomadchin,"seems to me it is bad that it is gray, should mark it as r / g / b",8/17/15,1
lossyrob,ok. and you want a packed integer raster? Like RGBA values?,8/17/15,1
pomadchin,"yep, layers are raster rdds",8/17/15,1
pomadchin,yes,8/17/15,1
lossyrob,ok,8/17/15,1
lossyrob,`SpatialKey`?,8/17/15,1
pomadchin,"yep, sure",8/17/15,1
lossyrob,one min,8/17/15,1
pomadchin,"tried to make magic with `Tile.combine(Tile)((Double, Double) => Double)`",8/17/15,1
pomadchin,":D got gray also (mb caused by bad `ColorInterp`, not sure)",8/17/15,1
lossyrob,it would be something like this (didn_Ñét compile):,8/17/15,1
lossyrob,"```scalaval red: RasterRDD[SpatialKey] = ???val green: RasterRDD[SpatialKey] = ???val blue: RasterRDD[SpatialKey] = ???val packed: RDD[(SpatialKey, Tile)] =  red.cogroup(green, blue) { case (key, (reds, greens, blues)) =>    // Maybe want to make sure that there's only one tile per key per color    val mb =       ArrayMultiBandTile(        reds.head,        greens.head,        blues.head      )      val tile =       // Need to convert to TypeInt, because TypeByte can only hold values -128 < x < 128      mb.convert(TypeInt).combine(0, 1, 2) { case (r, g, b) =>        // Correct values by converting unsigned byte values to byte values.        val cr = if(isNoData(r)) 128 else r.toByte & 0xFF        val cg = if(isNoData(g)) 128 else g.toByte & 0xFF        val cb = if(isNoData(b)) 128 else b.toByte & 0xFF        if(cr == 0 && cg == 0 && cb == 0) 0x00        else {          (cr << 24) | (cg << 16) | (cb << 8) | 0xFF        }      }    (key, tile)  }```",8/17/15,1
lossyrob,"That doesn_Ñét give you a `RasterRDD[K]` back, but a `RDD[(K, Tile)]`, so if you need a RasterRDD you_Ñéll have to feed it metadata",8/17/15,1
pomadchin,"Hmm, and geotiff writer support multi band write?",8/17/15,1
pomadchin,Wow the idea of combining is clear ,8/17/15,1
lossyrob,yeah GeoTiff can write multiband,8/17/15,1
lossyrob,just not in webmercator :(,8/17/15,1
pomadchin,:smile: No worries,8/17/15,1
pomadchin,Will give a look there (will try to),8/17/15,1
pomadchin,"Hm, it is so that im using an old gt, where only one banded tiles",8/17/15,1
lossyrob,"oh, darn",8/17/15,1
pomadchin,Have no chances to write three layers? :d,8/17/15,1
lossyrob,you can skip the multiband and just do some manual combine,8/17/15,1
lossyrob,"```scalacfor(0)(_ < cols, _ + 1) { col => ",8/17/15,1
pomadchin,clear,8/17/15,1
lossyrob,ugh,8/17/15,1
lossyrob,"ok cool",8/17/15,1
pomadchin,yep:) btw making two versions with old and new %) but happy to work with gt again,8/17/15,1
lossyrob,yeah I_Ñém glad!,8/17/15,1
pomadchin,"have I provide some information for writer, to write tiff meta data correct? (I mean ColorInterp and other info",8/17/15,1
lossyrob,not sure how to set the ColorInterp actually. I don_Ñét think we_Ñére writting that Tiff tag. ,8/17/15,1
lossyrob,Do you happen to know what Tiff tag that would be? Because we could write it if we knew the info,8/17/15,1
pomadchin,"Ok, will find info; anyway will work with it during this week :) ",8/17/15,1
pomadchin,Ty alot!,8/17/15,1
lossyrob,"np, thanks to you!",8/17/15,1
allixender,"Hi @echeipesh I have a question regarding `spark-etl` `OutputPlugin`I implemented a CassandraOutplut plugin like the ones for Accumulo, S3 or Hadoop, and the CassandraAttributeStore is much like the other AttributeStore implementations, too, including the type tags that were necessary, eghttps://github.com/allixender/geotrellis/blob/feature/cassandra-avro/spark/src/main/scala/geotrellis/spark/io/cassandra/CassandraAttributeStore.scala#L23```scalatype ReadableWritable[T] = JsonFormat[T]```https://github.com/allixender/geotrellis/blob/feature/cassandra-avro/spark-etl/src/main/scala/geotrellis/spark/etl/cassandra/CassandraOutput.scala However, I keep getting the following compile error:```scala[error] /home/akmoch/geotrellis/spark-etl/src/main/scala/geotrellis/spark/etl/cassandra/CassandraOutput.scala:25: type mismatch;[error]  found   : geotrellis.spark.io.cassandra.CassandraAttributeStore[error]  required: geotrellis.spark.io.AttributeStore.Aux[spray.json.JsonFormat][error]     (which expands to)  geotrellis.spark.io.AttributeStore{type ReadableWritable[T] = spray.json.JsonFormat[T]}[error]     CassandraAttributeStore(""attributes"")[error]                            ^[error] one error found```Anyone a suggestion?",8/18/15,1
allixender,"The only difference is that the Cassandra implementation has been using implicit sessions around, which possibly could be more practically changed to the getInstance approach in Accumulo? I had to create a workaround for the ETL Output plugin, which is not ideal, I know```scaladef attributes(props: Map[String, String]) = {    implicit val session = getSession(props)    CassandraAttributeStore(""attributes"")  }```",8/18/15,1
pomadchin,@lossyrob yep great tip; really works and work fast; (rgb merging),8/18/15,1
pomadchin,noticed some bad pixels though; do you know what they caused by?,8/18/15,1
pomadchin,http://s21.postimg.org/c80inxk1z/Screen_Shot_2015_08_18_at_15_50_51.png,8/18/15,1
pomadchin,oh that was my mistake %) used zero instead of 128 for nodata,8/18/15,1
pomadchin,"hm, more generic question: what about ingested tile quality? as I can see there is some loss",8/18/15,1
pomadchin,is there way to control it?,8/18/15,1
echeipesh,"@pomadchin not right now, those tiles are snapping to TMS grid so there is some resample. GT defaults to oversampling, but it still doesn_Ñét look great. I_Ñém making a PR to ingest those tiles into a layout matching native resolution. Even then its not clear how we_Ñére going to render them on leaflet (if at all), it would seem that would require some resampling along the way.",8/18/15,1
echeipesh,"@zifeo c/p from chat for posterity:  the ingest is supposed to be on unstructured data, it_Ñéll perform tiling first. But I can see how having an input from a catalog would be very useful for writing batch jobs. Originally I thought that it would have to be a new class, but I think you can formulate it right now as:```scalapackage geotrellis.spark.etl.accumuloimport geotrellis.proj4.CRSimport geotrellis.spark.io.index.KeyIndexMethodimport geotrellis.spark.{Intersects, RasterRDD, LayerId, SpatialKey}import geotrellis.spark.etl.InputPluginimport geotrellis.spark.io.accumulo.{AccumuloRasterCatalog, AccumuloInstance}import geotrellis.spark.tiling.LayoutSchemeimport geotrellis.vector.Extentimport org.apache.accumulo.core.client.security.tokens.PasswordTokenimport org.apache.spark.SparkContextimport org.apache.spark.storage.StorageLevelimport scala.reflect._class AccumuloInput extends InputPlugin {  def name = ""accumulo""  def format = ""catalog""  def key = classTag[SpatialKey]  val requiredKeys = Array(""instance"", ""zookeeper"", ""user"", ""password"", ""table"", ""layer"") //optional: bbox  def getInstance(props: Map[String, String]): AccumuloInstance =    AccumuloInstance(props(""instance""), props(""zookeeper""), props(""user""), new PasswordToken(props(""password"")))  def apply[K](lvl: StorageLevel, crs: CRS, scheme: LayoutScheme, props: Map[String, String])(implicit sc: SparkContext) = {    val bbox = props.get(""bbox"").map(Extent.fromString)    val chunks = props(""layer"").split("":"")    val id: LayerId = LayerId(chunks(0), chunks(1).toInt)    val rdd = if (bbox.isDefined)      AccumuloRasterCatalog()(getInstance(props), sc).query[SpatialKey](id).where(Intersects(bbox))    else      AccumuloRasterCatalog()(getInstance(props), sc).read(id)        (LayerId, rdd.tileLayout) -> rdd  }}```",8/18/15,1
echeipesh,"@allixender I think what you did is the only way to solve that problem. These plugins are all loaded at runtime and just filtered to find the _ÑÒcorrect_Ñù one. So anything that touches implicit scope has to be provided for explicitly. (Unless it can somehow be factored into a common Plugin interface, which is likely hard)",8/18/15,1
lossyrob,"@pomadchin we use nearest neighbor for ingest reprojection, but support bilinear and bicubic. Not sure how easy it is to add that to the ETL options",8/18/15,1
echeipesh,"shouldn_Ñét be too hard, `Tiler` would need be parameterized on it mostly, everything else would just flow through.",8/18/15,1
pomadchin,@lossyrob @echeipesh Thx guys!,8/18/15,1
jwalgran,"I am having some trouble with some GeoJson parsing code that worked previously. Below is a console session. The FeatureCollection is parsed, but when I try to ``getAllPolygons`` I get an empty list. ```> console[info] Starting scala interpreter...[info]Welcome to Scala version 2.10.5 (Java HotSpot(TM) 64-Bit Server VM, Java 1.7.0_55).Type in expressions to have them evaluated.Type :help for more information.scala> import geotrellis.vector.io.json._import geotrellis.vector.io.json._scala> val geoJson = ""{\""type\"":\""FeatureCollection\"",\""features\"":[{\""type\"":\""Feature\"",\""properties\"":{},\""geometry\"":{\""type\"":\""Polygon\"",\""crs\"":\""EPSG:3857\"",\""coordinates\"":[[[-13150073.472125152,4014380.7622378],[-13150073.472125152,4014380.7622378],[-13150073.472125152,4014380.7622378],[-13150073.472125152,4014380.7622378],[-13150073.472125152,4014380.7622378]]]}}]}""geoJson: String = {""type"":""FeatureCollection"",""features"":[{""type"":""Feature"",""properties"":{},""geometry"":{""type"":""Polygon"",""crs"":""EPSG:3857"",""coordinates"":[[[-13150073.472125152,4014380.7622378],[-13150073.472125152,4014380.7622378],[-13150073.472125152,4014380.7622378],[-13150073.472125152,4014380.7622378],[-13150073.472125152,4014380.7622378]]]}}]}scala> val fc = geoJson.parseGeoJson[JsonFeatureCollection]fc: geotrellis.vector.io.json.JsonFeatureCollection = geotrellis.vector.io.json.JsonFeatureCollection@6465f3a7scala> fc.getAllPolygons()res0: Vector[geotrellis.vector.Polygon] = Vector()scala> fc.toJsonres1: spray.json.JsValue = {""type"":""FeatureCollection"",""features"":[{""type"":""Feature"",""properties"":{},""geometry"":{""type"":""Polygon"",""crs"":""EPSG:3857"",""coordinates"":[[[-13150073.472125152,4014380.7622378],[-13150073.472125152,4014380.7622378],[-13150073.472125152,4014380.7622378],[-13150073.472125152,4014380.7622378],[-13150073.472125152,4014380.7622378]]]}}]}```",8/18/15,1
lossyrob,let me look into that,8/18/15,1
jwalgran,"Do I need to now use ``getAllPolygonFeatures``? If so, what is the proper way of calling it? I am running into ```scala> fc.getAllPolygonFeatures<console>:13: error: Cannot find JsonReader or JsonFormat type class for Nothing              fc.getAllPolygonFeatures```",8/18/15,1
lossyrob,you shouldn_Ñét have to,8/18/15,1
lossyrob,you could try calling,8/18/15,1
lossyrob,`geoJson.extractAll[Polygon]`,8/18/15,1
lossyrob,sorry,8/18/15,1
lossyrob,`geoJson.extractGeometries[Polygon]`,8/18/15,1
jwalgran,"```scala> geoJson.extractGeometries[Polygon]<console>:15: error: value extractGeometries is not a member of String              geoJson.extractGeometries[Polygon]```",8/18/15,1
lossyrob,https://github.com/geotrellis/geotrellis/blob/master/vector/src/main/scala/geotrellis/vector/io/json/package.scala#L57,8/18/15,1
lossyrob,is this working with master?,8/18/15,1
jwalgran,"Master from Aug 12, cb236a",8/18/15,1
lossyrob,"something changed around GeoJSON, but not till after then",8/18/15,1
lossyrob,what changed that made the thing that was working start to not work,8/18/15,1
jwalgran,Unfortunately I have not been exercising this JSON parsing code for a while. I am now working on getting all my various services fully working.,8/18/15,1
lossyrob,ok. sorry I figured this would have been a problem with a recent commit,8/18/15,1
jwalgran,This is being used to parse the polygon boundary used for the getting a histogram,8/18/15,1
jwalgran,Eugenes benchmarking code may have a similar operation. Let me see if I can dig that up,8/18/15,1
lossyrob,seems like that should work,8/18/15,1
jwalgran,"Ah, he is doing ``` val polygon: Polygon = config.getString(""polygon"").parseGeoJson[Polygon].reproject(LatLng, WebMercator)```",8/18/15,1
lossyrob,"that seems like the JSON has a polygon entry that is the GeoJson, so it_Ñés wrapped in an outer JSON object",8/18/15,1
lossyrob,"ok, I see what the problem is",8/18/15,1
lossyrob,@allixender this test will fail:,8/18/15,1
lossyrob,"```scala  it should ""extract polygons out of a valid FeatureCollection"" in {    val geoJson = """"""{""type"":""FeatureCollection"",""features"":[{""type"":""Feature"",""properties"":{},""geometry"":{""type"":""Polygon"",""crs"":""EPSG:3857"",""coordinates"":[[[-13150073.472125152,4014380.7622378],[-13150073.472125152,4014380.7622378],[-13150073.472125152,4014380.7622378],[-13150073.472125152,4014380.7622378],[-13150073.472125152,4014380.7622378]]]}}]}""""""    geoJson.extractGeometries[Polygon].size should be (1)  }```",8/18/15,1
lossyrob,"because of `JsonFeatureCollection`, it tries to convert the `Feature` into a `Polygon`, but really should unpack the feature to get to the polygon",8/18/15,1
lossyrob,we missed that one,8/18/15,1
jwalgran,Got it. I_Ñéll stick to plain polygons in the interim.,8/18/15,1
lossyrob,do you need the feature?,8/18/15,1
lossyrob,"if you need data, then you would be grabbing a `PolygonFeature[D]` with some D type that has a JsonReader",8/18/15,1
lossyrob,If you need multiple polygons you could send a `GeometryCollection`,8/18/15,1
lossyrob,or a `MultiPolygon`,8/18/15,1
lossyrob,both of which should work,8/18/15,1
lossyrob,"the thing was we were parsing invalid GeoJson by considering a FeatureCollection as if it were a collection of plain Geometries, which it_Ñés not",8/18/15,1
lossyrob,but the code we have shoul work with what you_Ñére asking to do,8/18/15,1
jwalgran,"We are not sending any feature attributes, so both of those other options would work. I suspect that the front end JS is just serializing the drawn polygon as a FeatureCollection by default.",8/18/15,1
lossyrob,Ah I see,8/18/15,1
lossyrob,hrmph,8/18/15,1
lossyrob,yeah you_Ñére taking it directly from leaflet draw,8/18/15,1
allixender,"Hey, I am just reading over the FeatureCollection thing...",8/18/15,1
allixender,"We could talk about it tomorrow morning @lossyrob ? And about the Avro in Cassandra and the spark-etl. I will see what I can find today for the FeatureCollection, as I have no clue how to proceed with the spark-etl thing.",8/18/15,1
lossyrob,ok cool. Yeah I don_Ñét know how immediate a need this is @jwalgran ,8/18/15,1
allixender,24 hours :smile: ,8/18/15,1
jwalgran,It_Ñés not a blocker to me getting anything working. Thanks for looking into it.,8/18/15,1
lossyrob,ok cool. thanks. @allixender we_Ñéll take a look at it tomorrow,8/18/15,1
jwalgran,"I don_Ñét think MMW is using feature collections, so they should be in the clear.",8/18/15,1
lossyrob,yeah. I mean what you_Ñére trying to do should work,8/18/15,1
lossyrob,so we_Ñéll make it work somehow. just not too immediately :),8/18/15,1
jwalgran,perfect,8/18/15,1
allixender,Hey @jwalgran @lossyrob could it be that there's something dodgy with the polygon geometry in geomety of the provided GeoJson?,8/19/15,1
allixender,"because we have tested polygon extraction from feature collection in the tests already and they work: See following:```scala// works!val geoJson2 = """"""{""type"":""FeatureCollection"",""features"":[{""type"":""Feature"",""geometry"":{""type"":""Polygon"",""coordinates"":[[[0.0,0.0],[0.0,5.0],[5.0,5.0],[5.0,0.0],[0.0,0.0]]]},""properties"":{""name"":""Rob"",""value"":31.2}}]}""""""geoJson2.extractGeometries[Polygon].size should be (1)// doesn't work!val geoJson3 = """"""{""type"":""FeatureCollection"",""features"":[{""type"":""Feature"",""properties"":{""name"":""Rob"",""value"":31.2},""geometry"":{""type"":""Polygon"",""crs"":""EPSG:3857"",""coordinates"":[[[-13150073.472125152,4014380.7622378],[-13150073.472125152,4014380.7622378],[-13150073.472125152,4014380.7622378],[-13150073.472125152,4014380.7622378],[-13150073.472125152,4014380.7622378]]]}}]}""""""geoJson3.extractGeometries[Polygon].size should be (1)```also the provided GeoJson from above (@lossyrob this test will fail) doesnt render on http://geojsonlint.com/",8/19/15,1
allixender,"And it doesn't seem to be related to either crs or feature properties ... no clue, I'll look a little more",8/19/15,1
allixender,"Hey, just realized, the provided polygon geometry is based on all identical points. If I try building a geometry out of identical points in GeoTrellis vector, it will fail too:```scala    val point1 = Point(5,5)    val line1 = Line(point1, point1, point1, point1, point1)    val poly1: Polygon = Polygon(line1)java.lang.AssertionError: assertion failed: Polygon Empty: POLYGON EMPTY[info]   at scala.Predef$.assert(Predef.scala:179)[info]   at geotrellis.vector.Polygon.<init>(Polygon.scala:74)[info]   at geotrellis.vector.Polygon$.jtsToPolygon(Polygon.scala:28)[info]   at geotrellis.vector.Polygon$.apply(Polygon.scala:65)[info]   at geotrellis.vector.Polygon$.apply(Polygon.scala:34)```I assume the GeoJson parse/extract to create the desired (jts) geometry will internally also fail, that's why the provided test fails.",8/19/15,1
allixender,"@lossyrob @jwalgran the referenced provided geometry for the test that fails,```json{""type"":""Polygon"",""crs"":""EPSG:3857"",""coordinates"":[[[-13150073.472125152,4014380.7622378],[-13150073.472125152,4014380.7622378],[-13150073.472125152,4014380.7622378],[-13150073.472125152,4014380.7622378],[-13150073.472125152,4014380.7622378]]]}```",8/19/15,1
allixender,"it's all the same points, it's sort of valid GeoJson (although the CRS is disputable http://geojson.org/geojson-spec.html#coordinate-reference-system-objects), and a GeoJson polygon built from identical points will still render on a map as the point, but jts internally it doen't want to build a polygon of identical points.",8/19/15,1
jwalgran,Yeah. Just realizing I gave you crap data.,8/19/15,1
allixender,:smirk: ,8/19/15,1
allixender,@jwalgran have a look here https://github.com/geotrellis/geotrellis/blob/master/vector-test/src/test/scala/spec/geotrellis/vector/io/json/GeoJsonSpec.scala#L164,8/19/15,1
jwalgran,"This totally worked. Sorry for the runaround.```scala> val geoJson = ""{\""type\"":\""FeatureCollection\"",\""features\"":[{\""type\"":\""Feature\"",\""properties\"":{},\""geometry\"":{\""type\"":\""Polygon\"",\""crs\"":\""EPSG:3857\"",\""coordinates\"":[[[-118.22906970977783,33.89275427793316],[-118.22906970977783,33.895621904536384],[-118.22522878646852,33.895621904536384],[-118.22522878646852,33.89275427793316],[-118.22906970977783,33.89275427793316]]]}}]}""geoJson: String = {""type"":""FeatureCollection"",""features"":[{""type"":""Feature"",""properties"":{},""geometry"":{""type"":""Polygon"",""crs"":""EPSG:3857"",""coordinates"":[[[-118.22906970977783,33.89275427793316],[-118.22906970977783,33.895621904536384],[-118.22522878646852,33.895621904536384],[-118.22522878646852,33.89275427793316],[-118.22906970977783,33.89275427793316]]]}}]}scala> val fc = geoJson.parseGeoJson[JsonFeatureCollection]fc: geotrellis.vector.io.json.JsonFeatureCollection = geotrellis.vector.io.json.JsonFeatureCollection@3ea6da02scala> fc.getAllPolygonsres3: Vector[geotrellis.vector.Polygon] = Vector(POLYGON ((-118.22906970977783 33.89275427793316, -118.22906970977783 33.895621904536384, -118.22522878646852 33.895621904536384, -118.22522878646852 33.89275427793316, -118.22906970977783 33.89275427793316)))```",8/19/15,1
allixender,glad it works for you :+1:  always good to re-iterate about stuff :smile: ,8/19/15,1
jwalgran,``throw new CheckYourDataYaGoof(_ÑÒAre you sure that is what you want_Ñù)``,8/19/15,1
allixender,lol :laughing: ,8/19/15,1
lossyrob,@allixender hmmmmm ok. thanks for finding that!,8/19/15,1
lossyrob,I think maybe our `Try`s should only `Catch` deserialization errors so that that exception would have bubbled through,8/19/15,1
allixender,seems reasonable,8/19/15,1
lossyrob,congrats to @vishalanand for a getting this mamoth PR merged! We now have some intense Kriging interpolation. https://github.com/geotrellis/geotrellis/pull/1103,8/20/15,1
allixender,congrats @vishalanand :+1:   Looks great :smiley_cat: ,8/21/15,1
vishalanand,"Thanks a lot @lossyrob , your guidance made it possible! :)@allixender thanks man! :smiley: ",8/21/15,1
pomadchin,@vishalanand gj o: nice refactor :D was waiting for this pr,8/21/15,1
lossyrob,"@allixender just merged the updates to the cassandra branch. I think that gets us pretty close to providing some if not full cassandra support for 0.10. Thanks for that, and all the other fixes and research you did this semester!",8/21/15,1
pomadchin,(: want to see cassandra vs. accumulo vs. hdfs bench,8/21/15,1
pomadchin,:d hope it won't be too private data,8/21/15,1
lossyrob,"that would be nice, we_Ñéd have to come up with a process they could all go head to head on",8/21/15,1
lossyrob,I can tell you right now that Accumulo blows HDFS out of the water,8/21/15,1
lossyrob,"Cassandra is going to be slow at range queires, but might be fast at full layer processing and single tile serving",8/21/15,1
pomadchin,wow ,8/21/15,1
lossyrob,"we would also want to get S3 in the mix there, a lot of work has gone into our S3 catalog",8/21/15,1
pomadchin,:dancer: nice nice,8/21/15,1
lossyrob,"Then there_Ñés HBase_Ñ_I haven_Ñét thought about it too much but I know that a lot of people want HBase support for GeoWave and GeoMesa, something we should try and benchmark against Accumulo",8/21/15,1
lossyrob,I think it_Ñés a pretty similar API to Accumulo so shouldn_Ñét be too hard to implement readers and writers,8/21/15,1
pomadchin,Seems to me benchmark will also be similar,8/21/15,1
pomadchin,"accumulo nicer then hbase :::ddd, we need to move it in the crowd",8/21/15,1
lossyrob,"They should be, but the argument is that HBase has more community supports. So worth investigating.",8/21/15,1
allixender,"Heya, thanks @lossyrob I had blast this GSoC with you guys. I can only concur with @vishalanand , your guidance was awesome.",8/22/15,1
allixender,*had a blast :smile: ,8/22/15,1
allixender,"Cassandra querying has still some potential to be improved, that said, overall consensus seems to be that Cassandra has a bit stronger performance focus in writing than in reading. ",8/22/15,1
allixender,"But one of the advantages of Cassandra would also be, you could do a pure spark/cassandra cluster, without zookeeper, hadoop/hdfs or mesos, and you can scale up/add cassandra nodes infinitely :sparkles: ",8/22/15,1
allixender,I'll be watchin' that space :fast_forward: ,8/22/15,1
allixender,"Hi @lossyrob , https://github.com/geotrellis/geotrellis/pull/1164 , the pull request for the additions for parse/extractGeometries/Features and JsonFeatureCollection/Map from #1162  ",8/23/15,1
allixender,"@vishalanand @lossyrob I had a test error in `vector-test````scalaNon-linear optimizations[info] Exception encountered when attempting to run a suite with class name: geotrellis.vector.interpolation.SemivariogramSpec *** ABORTED ***[info]   java.lang.NoClassDefFoundError: org/apache/commons/math3/fitting/leastsquares/LeastSquaresBuilder[info]   at geotrellis.vector.interpolation.LeastSquaresFittingProblem.optimum(LeastSquaresFittingProblem.scala:68)```I guess that came from some apache commons math3 dependency issue? ",8/23/15,1
vishalanand,"@allixender The `LeastSquaresBuilder` is a part of Apache Commons 3.5 package, the previously used package was version 3.2, try using `sbt clean` .",8/23/15,1
allixender,yep just doing :smile: Waiting for my computer haha,8/23/15,1
allixender,:+1: all good awesome,8/23/15,1
vishalanand,:),8/23/15,1
allixender,"Hey, I tested Spark ETL with Cassandra and it seemd to work :-D",8/23/15,1
allixender,"I ingested the one month tiles test files as Hadoop input and wrote them into Cassandra as Output, and then took an old ""benchmark"" project (for the tiling docu, which I still have to document/screencast) and did the ""normal"" extract from the raster catalog and it would write out the tiles again etc :+1:  ",8/23/15,1
allixender,"Hey @echeipesh @lossyrob A question regarding spark partitioning within GeoTrellis: Would it be the developers own responsibility to use spark methods like mapPartition(numPartitions: int) to force spreading out out on different spark workers? A nifty little detail in the Cassandra Spark Connector logic will on an intial loading only parallelize into the number of partitions as based on cassandra nodes/partitions. I came across that when I was doing some other spark processing with data out of a single cassandra node: The RDD would only occupy one worker even with the use of mapPartition ..., although I had more, and at first I couldn't figure why, it turns out the Cassandra Spark Connector only loaded the the data into one RDD with one partition. But with repartition(numPartitions: int) and then mapPartition(numPartitions: int) it was successfully spread onto the available workers. ",8/23/15,1
allixender,"I couldn't really verify reproduce this behaviour with GeoTrellis and Cassandra, but I often had the feeling most work was actually done in the driver program?",8/23/15,1
lossyrob,"@allixender the number of partitions is determined by the way the RDD is loaded. For example, in Accumulo, we have it so that there is one partition per tablet server, which there may be a few on each node in the cluster. So that takes care of initial partitioning. You can `repartition` or `partitionBy` in order to change that partitioning scheme later on. HDFS partitiones by, one per MapFile, which we set as max the size of one block. S3 is partitioned based on a even distribution algorithm against the filenames. I_Ñém not sure how the Cassandra connector does the partitioning",8/24/15,1
lossyrob,but `mapPartitions` is not a method that forces spreading out of data on different spark workers.,8/24/15,1
lossyrob,If you look at the code for `map`: https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/rdd/RDD.scala#L313,8/24/15,1
lossyrob,"You_Ñéll see that it falls through to a call to mapPartitions. mapPartitions is simple the way that a Map transformations works over one partition, which is how transformations always happen_Ñ_transformations are represented in Tasks, and each Task works over one partition. That work is done on the owrker node.",8/24/15,1
lossyrob,"If the Cassandra Spark Connector is only loading the data into an RDD with one partition, and that partition is large (as in your loading the whole layer), then that could certainly be a problem. I am curious about how the Cassandra connector does it_Ñés partitioning. But I was under the impression that it created a partition at least per node in the cluster, where the Task is shipped to a partition where all the data of that partition lives on the node that is processing the Task.",8/24/15,1
rahulpalamuttam,hi @lossyrob  - I was curious about how geotrellis indexes the tiles?,8/24/15,1
rahulpalamuttam,i.e. do you throw index the tile by key and throw the tuple into accumolo?,8/24/15,1
rahulpalamuttam,or do you do any sort of spatial partitioning?,8/24/15,1
lossyrob,"yes, index on the key, and then the key is used as the index in accumulo as part of the id",8/24/15,1
lossyrob,we use space filling curves to indedx,8/24/15,1
rahulpalamuttam,i remember talking with you about space filling curves,8/24/15,1
rahulpalamuttam,what exactly are they?,8/24/15,1
lossyrob,"I_Ñém deep in something right now, so I can_Ñét get into it too much, but some resources are, look into what a Z curve is, what a Hilbert curve is, check out this project https://github.com/geotrellis/curve, and then this issue: https://github.com/geotrellis/curve/issues/3",8/24/15,1
lossyrob,our indexing code lives here : https://github.com/geotrellis/geotrellis/tree/master/spark/src/main/scala/geotrellis/spark/io/index,8/24/15,1
lossyrob,"if anyone else in the room wants to take a crack at talking about SFC_Ñés, that would be cool too :)",8/24/15,1
rahulpalamuttam,thanks! I was just wondering why space curves vs kbd tree - would love to hear others opinions,8/24/15,1
moradology,it's my understanding that any space filling curve can be represented by a tree,8/24/15,1
lossyrob,"maintaining a tree isn_Ñét scalable, where SFC is just maths",8/24/15,1
rahulpalamuttam,i see,8/24/15,1
rahulpalamuttam,i see so a space filling curve can essentially be a function and indices can be quickly computed by the function,8/24/15,1
moradology,"yeah, and because it's just a function for describing arbitrarily large indexes",8/24/15,1
moradology,expanding the index size is not terribly difficult,8/24/15,1
rahulpalamuttam,interesting - so the search for a tile can effectively be O(1)?,8/24/15,1
rahulpalamuttam,"or is computing a key O(1), and finding that key a different operation all together",8/24/15,1
moradology,yeah,8/24/15,1
moradology,the latter,8/24/15,1
rahulpalamuttam,i see,8/24/15,1
moradology,of course the higher dimensional space must be bounded,8/24/15,1
moradology,so there's slight overhead,8/24/15,1
moradology,"but the actual key lookup, yes",8/24/15,1
lossyrob,"The thing it helps with is range queries_Ñ_the tough part is, say I have a subspace in 3 dimensions, how do I map that to a sequence of ranges in the 1 dimensional index space?",8/24/15,1
lossyrob,"SFCs can do that, that_Ñés what the code in the geotrellis.spark.io.index does",8/24/15,1
rahulpalamuttam,very cool,8/24/15,1
rahulpalamuttam,do you think this can be extended to the n-dimensional case. I'm talking about it from a purely generic usecase,8/24/15,1
rahulpalamuttam,anyway so here's what I see as the geo-trellis pipeline for a tile-get,8/24/15,1
lossyrob,yes SFCs are N-Dimensional,8/24/15,1
rahulpalamuttam,right - just is it feasible to extend them to the N-Dimensional case. I guess I'm not sure where the one dimensional array lives - is it an RDD?,8/24/15,1
rahulpalamuttam,"from the user ""querying"" for a (time, lat, lon). Your SFC computes the linear index. Now where does it get that tile from?",8/24/15,1
lossyrob,"it doesn_Ñét live anywhere, it_Ñés just the subdomain of the function that is the SFC",8/24/15,1
lossyrob,"index from something like (I, I, R) -> I",8/24/15,1
allixender,"@lossyrob thanks for the info the spark partitioning. AFAIK Spark Cassandra Connector creates a partition per Cassandra node, the connector is quite aware of the topology of the cassandra cluster. The situation I had, was, that I had one cassandra instance on my laptop and a local spark cluster with several cores assigned to my spark worker (but also tried with two spark workers to see it more obvious), and the computation would only occupy one CPU core. it would fire on 25%. Because the cassandra RDD was only one partition. After I explicitly 'repartioned' then I saw, that spark would occupy the available cores.",8/25/15,1
allixender,"So I did a 'repartion(4)' and then enclosed my computation in a 'mapPartition(4)' to be more explicit. I guess my point is, if you have more spark workers with more cores available than Cassandra nodes, a 'repartition' might be useful. As this will incur a shuffle, I am not sure how big the performance impact would be. So I think the behaviour is similar as with the Accumulo tablet servers, but there will be certainly only one cassandra instance per cluster node. But your idea is to have the spark workers on the same nodes as e.g. the Accumulo tablet servers, or alternatively the Cassandra nodes?",8/25/15,1
allixender,I think it could be necessary to add some logic into the GeoTrellis RDD handling for Cassandra to repartition? Or would you suggest doing so in the user-developed program?,8/25/15,1
pomadchin,"guyz, guess that there is some ingesting rendering, to make tms",8/25/15,1
pomadchin,but found very very very strange merging:,8/25/15,1
pomadchin,http://s22.postimg.org/hscyh2q81/Screen_Shot_2015_08_25_at_15_40_16.png,8/25/15,1
pomadchin,hm how to define interpolate method via tiler? and define the way tiles merging?,8/25/15,1
pomadchin,is it possible without fixing the code (this way I know where to fix),8/25/15,1
pomadchin,"```scaladef apply[T, K: SpatialComponent: ClassTag](    getExtent: T=> Extent,    createKey: (T, SpatialKey) => K,    rdd: RDD[(T, Tile)],    mapTransform: MapKeyTransform,    cellType: CellType,    tileLayout: TileLayout  ): RDD[(K, Tile)] =    cutTiles(getExtent, createKey, rdd, mapTransform, cellType, tileLayout)      .reduceByKey { case (tile1: Tile, tile2: Tile) =>        tile1.merge(tile2)      }```think here is the `merge` method, and in a `cutTiles` function",8/25/15,1
isaacsanders,what spark version is being targeted?,8/25/15,1
hectcastro,isaacsanders: 1.3.1 at a minimum. There are at least some branches with attempts to use 1.4.x features that I know of.,8/25/15,1
pomadchin,"just tested new gt with geotrellisseems to me there r some errors (with using tms route, and using spatial ingest); png_Ñés r _ÑÒbad_Ñù",8/27/15,1
pomadchin,"But! writing to file is ok!",8/27/15,1
pomadchin,): guyz! :D really strange bhaivor ,8/27/15,1
isaacsanders,Does geotrellis do radial querying?,8/27/15,1
isaacsanders,or is it all AABB,8/27/15,1
moradology,hey - let me see if i'm on the same page. you're wondering whether you could take a point and query for things within some radius of a circle with that point at center rather than just querying for things within some polygon?,8/27/15,1
moradology,"sorry i didn't respond to you when you were around, @isaacsanders, but the querying stuff within geotrellis is all ported from JTS",8/27/15,1
moradology,"worth noting, however, is that if you wanted to roll your own wrappers for JTS methods (a process that is slowly but surely being done by the GT team), there's an element of 'buyer beware' to consider: namely, that not all of these algorithms will work in the distributed, spark context",8/27/15,1
moradology,JTS stands for Java Topology Suite - i realize that wasn't 100% clear,8/27/15,1
moradology,"Here is some exemplary (hopefully relevant) JTS wrapping, if you were curious: https://github.com/geotrellis/geotrellis/blob/master/vector/src/main/scala/geotrellis/vector/Polygon.scala",8/27/15,1
isaacsanders,Thanks!,8/27/15,1
moradology,"absolutely, hope that helps",8/27/15,1
isaacsanders,"And yes, @moradology, I am looking for a point and radius query.",8/27/15,1
isaacsanders,like for determining neighborhoods in flocking behavior: http://cmuems.com/2012/c/files/2012/10/boids-rules2.png,8/27/15,1
isaacsanders,for example^,8/27/15,1
moradology,mmm,8/27/15,1
moradology,there isn't something *quite* like that implemented,8/27/15,1
moradology,it could currently be fudged with something like: https://github.com/geotrellis/geotrellis/blob/4c3c4d48540aff6bd8c7be4c8da53cc864ca76f3/testkit/src/main/scala/geotrellis/testkit/vector/GeometryBuilder.scala,8/27/15,1
moradology,to construct a circle,8/27/15,1
moradology,and then call a within query,8/27/15,1
moradology,"not ideal, i grant you, but workable perhaps",8/27/15,1
moradology,and thanks for reaching out,8/27/15,1
isaacsanders,"Happy to do so. Is there any way I can help out? I am fairly new to scala and spark, but I figure if I will be working in this for the next 6 months or so, I should find a way to learn more idiomatic ways of doing things.",8/27/15,1
moradology,"the silver lining on that little hack is that if you have a consistent radius that you're wanting to query within (as I assume is the case for analyzing flocking behavior), you can just call setCenter on that circle instead of creating one, ad hoc, when needed",8/27/15,1
moradology,fantastic question - i'm pretty certain your assistance would be appreciated but the two people most capable of talking to you about details on that aren't around today,8/27/15,1
moradology,"if there's anything else as you familiarize yourself, let me know",8/27/15,1
isaacsanders,thanks!,8/27/15,1
isaacsanders,"I have this tab open indefinitely, so no worries.",8/27/15,1
moradology,"as a tip, there is some documentation in the form of README.md files in some directories",8/27/15,1
moradology,in particular raster and vector,8/27/15,1
moradology,which attempt to elucidate the common patterns,8/27/15,1
moradology,"(probably best viewed on github, fwiw)",8/27/15,1
isaacsanders,mk,8/27/15,1
pomadchin,"@lossyrob @echeipesh and one more tip; for example got two chunks of files to load, as a map via ingest (10000 files & 10000 files _ÑÓ load them chunked); but as we can se, there would be an error, caused by rewriteing overlapping areas",8/27/15,1
pomadchin,"how it is possible to ingest them correctly as a map, chunked, not all tiles as one batch?",8/27/15,1
pomadchin,"I see it like a special ingest, which tries to retry tms square and to merge em (new with old)",8/27/15,1
echeipesh,"@pomadchin Yes, that_Ñés a hole in the ingest right now. I think most general way to handle this case would be to have a `RasterRDD.merge` similar to the way we have `Tile.merge` ( that is used by Tiler). Then you could load an RDD of existing tiles and merge it with freshly ingested tiles before saving the result. That makes it a mini-batch process in a sense. There may even be some prototype code for that.If you_Ñére super worried about the speed of ingest/merge it is possible to implement tile merge as an accumulo iterator (combiner). In that case writing to a table with the same layer name would be an _ÑÒupdate_Ñù done during accumulo compaction (before next read). Of course that is much more specific and fragile solution.",8/31/15,1
pomadchin,"@echeipesh I've implemented 'by tile' just to check out out, but found troubles at some zoom levels ",8/31/15,1
echeipesh,What do you mean by tile ?,8/31/15,1
pomadchin,"I am taking the keys of current input, and getting tiles (by key) for a certain zoom ",8/31/15,1
echeipesh,"Ah, you_Ñére pulling out tiles one by one for the the merge. I would think think that would be equivalent, what are the troubles ?",8/31/15,1
pomadchin,"Very very strange merge results on some zoom levels, seems like some issue with merge extents, but have no ideas why it happens only on some zoom lecelst",8/31/15,1
pomadchin,mailed you screens ): thought you will not be available here; ,8/31/15,1
pomadchin,Heh rdd merge would be the way to solve map partly update; ouff ): I should put more knowledges in my head %) ,8/31/15,1
echeipesh,Do you mean the the clipping on the most zoomed out screenshot? Other tiles look _ÑÒAbout right_Ñù without knowing what to expect : ),8/31/15,1
pomadchin,[![Screen Shot 2015-08-31 at 13.54.45.png](https://files.gitter.im/geotrellis/geotrellis/7t26/thumb/Screen-Shot-2015-08-31-at-13.54.45.png)](https://files.gitter.im/geotrellis/geotrellis/7t26/Screen-Shot-2015-08-31-at-13.54.45.png),8/31/15,1
pomadchin,[![Screen Shot 2015-08-31 at 13.56.11.png](https://files.gitter.im/geotrellis/geotrellis/2QnL/thumb/Screen-Shot-2015-08-31-at-13.56.11.png)](https://files.gitter.im/geotrellis/geotrellis/2QnL/Screen-Shot-2015-08-31-at-13.56.11.png),8/31/15,1
pomadchin,"same overlapping tiles, but diff zommed",8/31/15,1
pomadchin,:) other zoom lvls are correct :),8/31/15,1
pomadchin,"anyway, as u sad, it is not a very good idea to merge tiles by one; ",8/31/15,1
pomadchin,):,8/31/15,1
lossyrob,"@pomadchin Eugene was talking about having update functionality in GT, and that you_Ñéd need that. Would you need to update certain tiles directly, or need to read the tiles from the layer, use it to derive the new tile, and then save it off? Do you need to add new tiles to the layer? We haven_Ñét worked out the update story, so I_Ñém looking for ideas on what we_Ñéd have to cover for it.",9/1/15,1
lossyrob,"@isaacsanders your use case sounds very interesting, I_Ñéve been out but it would be great to discuss it some more.",9/1/15,1
pomadchin,@lossyrob seems to me that it would be nice to update by one tile and by some bulk insert,9/1/15,1
pomadchin,this would be great to update layers in a such way,9/1/15,1
pomadchin,"although, I thought about ingesting cloud tiles, but making them cloudless during the ingest, basing on an information layer (LANDSAT BQE, for example)",9/1/15,1
pomadchin,"so it doen_Ñét mean only cloudless, it is just a use case example)",9/1/15,1
lossyrob,"So as new imagery comes in, not to mosaic those images, but to replace imagery in a layer (overwrite) and possible add new tiles (and change the metadata)",9/1/15,1
pomadchin,yes,9/1/15,1
lossyrob,"Right now the thought is that we_Ñéd have independent writer classes for doing RDD writes, whether they be MultiBandTiles or Tiles. I_Ñém wondering if the update belongs there or in a separate process, where those classes can share funcitonality",9/1/15,1
lossyrob,"I think creating some spike code that would successfully update a layer and it_Ñés metadata for one of the backends, like accumulo or s3, would be a good first step.",9/1/15,1
lossyrob,@hectcastro @echeipesh so are we now able to pull down binaries from bin tray?,9/2/15,1
lossyrob,via sbt,9/2/15,1
hectcastro,lossyrob: https://github.com/WikiWatershed/mmw-geoprocessing/pull/4,9/2/15,1
lossyrob,very cool.https://github.com/geotrellis/geotrellis/pull/1167,9/2/15,1
lossyrob,"so we_Ñére going to modify the travis build to always push out a bintray binary for pushes to master, yeah?",9/2/15,1
pomadchin,"wow nice idea :D %) I had to publish on bintray some my fixes for an old gt, in order not to build gt every time on a new machine / docker",9/2/15,1
lossyrob,Nice. This should make it a lot easier for people to use new functionality.,9/2/15,1
hectcastro,"lossyrob: Yeah. Once that PR goes in, I'll work on setting up the automatic publishing.",9/2/15,1
lossyrob,@echeipesh maybe better to talk it out here. So what_Ñés with the move from `Build.scala` to `build.sbt`? Is that the new build file format you were mentioning?,9/2/15,1
echeipesh,"yup, that_Ñés the new _ÑÒrecommended standard_Ñù, it_Ñés less restrictive than old sbt and less scary than Build.scala.",9/2/15,1
echeipesh,http://www.scala-sbt.org/0.13/tutorial/Basic-Def.html,9/2/15,1
lossyrob,So there_Ñés some sort of import magic that imports things like `Version.scala` into the file automatically?,9/2/15,1
echeipesh,There is some of that going on. All objects in `project/` are available and I noticed less imports for plugins are required.,9/2/15,1
echeipesh,"Hector pointed out that a lot of the examples in the wild deal with `build.sbt` and not `Build.scala`, which makes it more familiar to people reading it.",9/2/15,1
lossyrob,Let me read up on it a bit,9/2/15,1
echeipesh,"It_Ñés a little bit of situation of when SBT says jump, you say how high moment. But I like this format.",9/2/15,1
echeipesh,"Yep, if you find a reason not to use it, we can just c/p the body back into a Build class.",9/2/15,1
lossyrob,"Whichever way we go, we should do all-or-nothing on subproject build.sbt_Ñés. We used to have those, and it lost me an hour or so trying to figure out why a build was happening in the way that it was.",9/2/15,1
lossyrob,"But yeah. If that_Ñés the recommended way of doing it, I just hope there_Ñés not a thing where it won_Ñét support it or SBT_Ñés magic won_Ñét be weird and we won_Ñét be able to scala our way out of it. But I see the appeal.",9/2/15,1
echeipesh,"Ok, let me throw out an alternative: main `build.sbt` contains the dependencies and project links, and subproject `build.sbt`s contain stuff like compiler flags and assembly settings. Do you think this is _ÑÒmodulerazation_Ñù or _ÑÒfragmentation_Ñù?",9/2/15,1
echeipesh,"The escape hatch seems to be if you need scala your way out, you just put it in `project/` class and do whatever the hell you want there, then just use it.",9/2/15,1
lossyrob,"I feel like it_Ñés fragmentation, but that_Ñés because I_Ñéve been burned by it before. It was messy though, subproject build.sbt_Ñés had random stuff and not in a coherent way. I think if we created rules it might be better, but also could easily slip out of coherence. @hectcastro or others have any input on this?",9/2/15,1
hectcastro,would be useful down the road as well.,9/2/15,1
hectcastro,"I'm not an expert on the new format, and don't have much experience with the old approaches either, but constraining open ended Scala in the build definition seems like a good way forward. Harnessing the new format appropriately to take advantage of separating out all of the details of each subproject build instead of having everything in one seems like it",9/2/15,1
lossyrob,"Subproject build.sbt_Ñés aren_Ñét a new thing, that was around before (and part of an old version of GeoTrellis). Is the `/project` scala files available inside the subproject `build.sbt` files? If not, then that_Ñés not good, because depending on whether or not some build setting needs e.g. a Version",9/2/15,1
lossyrob,it would go in the main `build.sbt` or the subproject `build.sbt`,9/2/15,1
lossyrob,looks like the subproject does get those,9/2/15,1
lossyrob,"if we are going to use subproject build.sbt_Ñés, can we just use them completely?",9/2/15,1
lossyrob,"I just don_Ñét want to get into a situation where someone would be looking at a perfectly complete build setup for a project, and be missing something because there migth be a subproject `build.sbt` containing some random piece of build info",9/2/15,1
lossyrob,"Are bintray credentials needed to pull down the JARs? No, right? I get 25 lines of output of `Missing bintray credentials /Users/rob/.bintray/.credentials. Some bintray features depend on this.` every time I reload SBT",9/2/15,1
lossyrob,I think I can calm that input down with a PR to the bintray plugin,9/2/15,1
echeipesh,"Just for publishing, you can set them from inside SBT in safe way.",9/2/15,1
lossyrob,"I_Ñéll try to work with the bintray peoples to calm down the warnings, don_Ñét want to berate new developers that don_Ñét have bintray creds with 25 lines when they first open sbt",9/2/15,1
echeipesh,"something like this?```scalaimport Dependencies._scalacOptions ++= Seq(""-optimize"", ""-language:experimental.macros"")parallelExecution := falsefork in test := falsejavaOptions in run += ""-Xmx2G""name := ""geotrellis-raster""libraryDependencies ++= Seq(  typesafeConfig,  jts,  spire,  monocleCore,  monocleMacro,  openCSV)addCompilerPlugin(""org.scalamacros"" % ""paradise"" % ""2.0.1"" cross CrossVersion.full)```",9/2/15,1
echeipesh,then the main file only has the `dependsOn` ?,9/2/15,1
echeipesh,"```scalalazy val raster = Project(""raster"", file(""raster"")).  dependsOn(macros, vector).  settings(commonSettings: _*).```",9/2/15,1
lossyrob,"If that_Ñés the most we can shuck out of the main one, and that works, then that would make sense.",9/2/15,1
echeipesh,That works. I_Ñém not sure how we could move out more. Dependencies would have to be at top level and I_Ñém not seeing magic syntax to change that.,9/2/15,1
pomadchin,so `gt` moves to `build.sbt` ?,9/2/15,1
echeipesh,It_Ñés looking like it. Do you have thoughts one way or the other?,9/2/15,1
pomadchin,"`Build.scala` makes convenient generation of different boilerplate (https://github.com/milessabin/shapeless/blob/master/project/Boilerplate.scala)Not sure it is possible with `build.sbt`  But seems that exactly, there is no difference; but for multi project projects i prefer `Build.scala`, mb as legacy from times when it was not possible to make it via `build.sbt`",9/2/15,1
pomadchin,"oh, it is possible; :D as `/project` objects are availble; hm dunno mb Build.scala is legacy",9/2/15,1
pomadchin,"`scalaz, shapeless, playframework` using `Build.scala`, but, `cats` using `build.sbt` with objects in `/project`",9/2/15,1
lossyrob,"looks like `spire` moved too. I_Ñéve been basing the GeoTrellis build on that build for a while, so yeah that makes sense. Let_Ñés do the move",9/2/15,1
lossyrob,"@echeipesh so do you think we should include the sub projects? Spire doesn_Ñét do that (https://github.com/non/spire/blob/master/build.sbt), but I think maybe of that_Ñés the convention we want to go with I_Ñéd be willing to see what it_Ñés like. Though the magic of the multiple definition places is a bit weird to me.",9/2/15,1
echeipesh,Let me push it up so you see what it looks like,9/2/15,1
echeipesh,"I like it broken up like that, makes it easier to browse the projects I think and easier to add/delete them.",9/2/15,1
echeipesh,also pushed down those hacks around benchmark projects,9/2/15,1
lossyrob,"alrighty. yeah that looks good to me, we_Ñéll see how that feels in the coming months",9/2/15,1
echeipesh,"woot, merging",9/2/15,1
echeipesh,Today is looking like _ÑÒmerge some PRs_Ñù day.,9/2/15,1
lossyrob,yeah it_Ñés about time. code reviews are hard,9/2/15,1
echeipesh,I wonder if it_Ñés just better to turn off those inline warnings on -> somehow ?,9/2/15,1
lossyrob,"yeah, not sure how to do that without turning off all warnings, or removeing the optimize compiler option",9/2/15,1
echeipesh,"@lossyrob what about `RasterMetaData. dataExtent`, do you still think that_Ñés too specific a name ?",9/2/15,1
lossyrob,yeah_Ñ_I think `metadata.extent` makes sense that it would be the data extent,9/2/15,1
lossyrob,"@echeipesh png stuff passed travis, can I merge?",9/2/15,1
echeipesh,"yep, gtg",9/2/15,1
echeipesh,pushed up changes to native layout branch,9/2/15,1
echeipesh,@pomadchin Are you using the `Ingest` object to get your layers in?,9/3/15,1
pomadchin,"@echeipesh Yes, you can check accumulo-legacy branch in my fork",9/3/15,1
pomadchin,https://github.com/pomadchin/geotrellis/tree/legacy-accumulo-1.5.2,9/3/15,1
echeipesh,the rdd caching trick wasn_Ñét working well ?,9/3/15,1
echeipesh,"How useful have you found it? I_Ñém thinking decomposing that into a bunch of extension methods that would look like:```scalaval rdd: RDD[(ProjectedExtent, Tile)] = ???val reprojected = rdd.reproject(LatLng)val metadata = reprojected.collectMetadata(ZoomedLayoutScheme, LatLng)val tiled = reprojected.tile(metadata)val rasterRdd = new RasterRDD(tiled, metdata)```",9/3/15,1
pomadchin,Rdd caching was ok (: problem was just in math; ,9/3/15,1
pomadchin,Hmm you want to replace ingest object?,9/3/15,1
pomadchin,"Nice idea, think to work with rdds is very useful",9/3/15,1
echeipesh,"Really just decompose it. Its not used internally, and I_Ñém not sure if it_Ñés useful as an API.",9/3/15,1
pomadchin,Let's look how it will be; and what you want to achieve by,9/3/15,1
pomadchin,decomposing*,9/3/15,1
pomadchin,"Oh I see, got it; nice, try it, will be great",9/3/15,1
echeipesh,"Cool, thanks for your thoughts.",9/3/15,1
echeipesh,"woot, not only can we ingest multiband geotiff but we can tile them at native resolution_Ñ_ apparently correctly.",9/3/15,1
lossyrob,:+1: ,9/5/15,1
pomadchin,"`Spark 1.5.0 released (Sep 09, 2015)`",9/11/15,1
echeipesh,"ever forward, never backwards",9/11/15,1
Elbehery,Geotrellis was supposed to be integrated with Spark .. any updates regarding this ?,9/11/15,1
echeipesh,"There is quite a lot of work that has been done with geotrellis and spark: https://github.com/geotrellis/geotrellis/tree/master/spark/src/main/scala/geotrellis/sparkWe have support for HDFS, Accumulo, and S3 for storage. We_Ñére using it internally to do multi-layer mosaicking (now with multi-band) and most/all local and focal operations are supported. The master is still moving quite fast as we_Ñére trying to get to a feature/API stable release at the moment.",9/11/15,1
lossyrob,"@Elbehery we_Ñére currently working mainly with Spark, and the 0.9 release which does not include the spark work has fallen out of date. We_Ñéd like to release a spark release soon, but there are a couple of features we are trying to get in before releasing our 0.10 release (which will have full spark support). There is a `0.10.0-M1` version on maven central that includes our spark work, but I_Ñéd recommend pulling the latest binaries published to bintray on each commit to master",9/11/15,1
lossyrob,"but to answer your question, with the current codebase we have integrated with Spark.",9/11/15,1
nicerobot,@lossyrob regarding #1175 i started from the 0.9 geotrellis-chatta-demo,9/11/15,1
lossyrob,"ok, gotcha",9/11/15,1
lossyrob,so you have some GeoTiffs that you_Ñéd like to convert into ARG files for use in that demo,9/11/15,1
lossyrob,you can use the `gtloader` command for that: http://geotrellis.io/geotrellis-docs/0.9.0/python-geotrellis/,9/11/15,1
nicerobot,"Initially, i'd like to keep using the GeoTIFFs. I only wanted to convert to ARG to verify that I can access the rasters using the same code but as ARG. If i can't access the ARG either, then i've broken something more in the code than i expected. If i can access the ARG, then i'd like to figure out how to use the GeoTIFFs",9/11/15,1
lossyrob,"The problem with that is, the catalogs don_Ñét really play nice with geotiffs in 0.9. It_Ñés required to convert your rasters to the GeoTrellis format",9/11/15,1
nicerobot,Great. That answers that :) How about 0.10? I'm not tied to 0.9.,9/11/15,1
lossyrob,"The reason for this is the speed of reading GeoTiffs is slow. Back then we were using a geotools GeoTiff reader, which was really slow. However, with our new GeoTiff reader (in the current codebase) that becomes less of an issue",9/11/15,1
lossyrob,but there_Ñés not really a _ÑÒcatalog_Ñù structure around reading GeoTiffs how you might want to,9/11/15,1
lossyrob,"we had someone update the chattanooga demo to 0.10 code, but that is a bit unverified, and also requires an ingest into a geotrellis-spark backend (like accumulo)",9/11/15,1
lossyrob,what_Ñés the end goal here?,9/11/15,1
nicerobot,Our GeoTIFFs are a one-time load and they'd be cached but we generate them externally and I just wanted to avoid the conversion. A conversion is certainly no a deal-breaker though. I just need to make sure i can integrate it easily.,9/11/15,1
nicerobot,"The end-goal is ramping up on GeoTrellis :) so i'm starting with content i know, our GeoTIFFs.",9/11/15,1
lossyrob,"Cool. Are you trying to use the Spark functionality (for large scale processing), or just work with smaller scale rasters in Scala (for doing weighted overlays like the chatta demo)?",9/11/15,1
nicerobot,"Yea, large. We have accumulo available (we're ramping up on GeoMesa) so I should try the 0.10 demo and go from there. Is the ingest documented with the demo?",9/11/15,1
lossyrob,"The demo subproject is another bunk set of code, sorry. We really need to do a cleanup",9/11/15,1
lossyrob,but that_Ñés good to know,9/11/15,1
lossyrob,"if you have accumulo on hand, then it_Ñés a matter of ingesting the geotiffs into Accumulo",9/11/15,1
nicerobot,:D bleeding edge,9/11/15,1
lossyrob,and then working with those rasters from there,9/11/15,1
lossyrob,haha yeah that_Ñés what we keep saying :),9/11/15,1
lossyrob,"to do an ingest, you should use the ETL subproject",9/11/15,1
lossyrob,https://github.com/geotrellis/geotrellis/tree/master/spark-etl,9/11/15,1
nicerobot,great. i'll start from there then. that'll be closer to our final desired stack anyway. thanks,9/11/15,1
lossyrob,"in order to use the spark-etl stuff, you could do this:",9/11/15,1
lossyrob,"add `resolvers += Resolver.bintrayRepo(_ÑÒazavea"", _ÑÒgeotrellis_Ñù)` to your resolvers",9/11/15,1
nicerobot,:+1: ,9/11/15,1
lossyrob,"then add the dependency _ÑÒcom.azavea.geotrellis_Ñù %% _ÑÒgeotrellis-spark-etl_Ñù % _ÑÒ0.10.0-5a1cc8c""",9/11/15,1
lossyrob,which is our latest published snapshot on bintray,9/11/15,1
nicerobot,:+1: thanks. You prefer that i just return here if i run into any issues or file them in github?,9/11/15,1
lossyrob,"Let us know when you run into any problems, the ETL stuff is a bit new and it_Ñéll be good to know about and work through problems",9/11/15,1
lossyrob,I think here would get the quickest response,9/11/15,1
lossyrob,"and if we determine that they are issues that deserve to be filed, we can do that as a next step",9/11/15,1
nicerobot,Ok. Great. Hopefully we'll get up to speed soon and maybe be able to contribute too.,9/11/15,1
lossyrob,for sure. Contributing by working through this stuff (and by giving a reminder that we have to clean up those scripts and demo subproject) is great :),9/11/15,1
nicerobot,Certainly :) We'll keep you posted.,9/11/15,1
echeipesh,"this is the way you need to construct the ETL object as of right now:```scalaval etl = new Etl[SpatialKey](args, s3.S3Module, hadoop.HadoopModule, accumulo.AccumuloModule)(ZoomedLayoutScheme.apply)```",9/11/15,1
timothymschier,"Hey folks, we're looking at something similar - GeoTrellis and GeoMesa. But I've heard that GeoWave is another option to GeoMesa. @nicerobot did you guys look at GeoWave? @lossyrob  or other GT folks do you have any experience with them either or both?",9/15/15,1
lossyrob,"Hey @timothymschier, I haven_Ñét worked with the projects per say, but have worked with the developers and know the two teams well. They are both great projects. They serve a very, very similar purpose",9/15/15,1
lossyrob,I_Ñéd take a look at the codebases between the two and perhaps make the choice based on an API standpoint,9/15/15,1
lossyrob,but I think it_Ñés worth evaluating the both of them if you have the time (and also I_Ñéd really love to hear what you find as far as the differences between them!),9/15/15,1
timothymschier,Thanks for the input @lossyrob. I'll have to fight for more resources to allow us to spend time on both as right now we've only got GeoMesa included in the scope. They apparently have some high level thoughts to merge/unify/combine/something but it seems like that is is still some ways off.,9/15/15,1
timothymschier,"and yeah, glad to share thoughts, if only to get a second opinion on them as we go... just hope I can make the case to add another candidate to the scope.",9/15/15,1
lossyrob,"I would push for it. GeoWave seems to have some really good ideas and developers, and a clean API_Ñ_although I might just be biased because when I dug into their DataStore API it looked a lot like what we_Ñére coming up with in GeoTrellis :)",9/15/15,1
lossyrob,Also the GeoTrellis Hilbert indexing code is all based on the GeoWave stuff,9/15/15,1
lossyrob,We_Ñéve also worked a bit on some integration of GeoTrellis into GeoWave_Ñ_they are doing some raster stuff as well,9/15/15,1
timothymschier,"ah, this is all good info... particularly the integration. GeoMesa on its own doesn't support raster based calcs and so it seems we may anyway be looking to integrate with GeoTrellis.",9/15/15,1
timothymschier,"GeoWave also have point cloud stuff, which is also of interest to us... cool I think this may be enough ammunition :)",9/15/15,1
lossyrob,"yeah, we have also been in talks with the GeoMesa team about integration. It_Ñés just slow rolling since teams get busy_Ñ_I feel like we made more progress with GeoWaveat least recently. Also there_Ñés a pretty clear path in my mind of having a GeoWave backed RDD that GeoTrellis can work with.",9/15/15,1
timothymschier,"oh nice, I like that idea (at least based on my current early level of understanding) - integration will be key for us I think",9/15/15,1
timothymschier,will be interesting to see how the SFCs perform,9/15/15,1
lossyrob,"We already have an Accumulo backend, with our own Avro datatypes and indexes, but integrating with GeoWave would be great and open up some possibilities. It_Ñés a matter of translating between the GeoTools types that GeoWave stores and the GeoTrellis types, creating a reader and writing in the `geotrellis.spark.io` package, and going from there. Just need the time to code it up :)",9/15/15,1
lossyrob,Definitely interested in that. We have Hilbert but haven_Ñét benchmarked it against our Z curve stuff to answer some burning questions of which should be the default,9/15/15,1
lossyrob,@echeipesh how is that RDDReader type problem working out?,9/16/15,1
echeipesh,"I_Ñéve spent some more time trying to get the original form with no solution, so I_Ñém sticking with what I showed you yesterday: `new S3RDDReader[SpatialKey, Tile, RasterRDD]` and `new S3RDDWriter[SpatialKey, Tile, RasterRDD]`. That has proven to work so I_Ñém just moving code to make that the solution now.",9/16/15,1
echeipesh,I think I_Ñéll have the PR updated for that today.,9/16/15,1
echeipesh,"It_Ñés going to require to changes to `S3LayerMetaData` which were not part of this PR, but that_Ñés no biggie.",9/16/15,1
lossyrob,So then we could type alias to `S3RasterRDDReader[K]` with `V` = `Tile` and `C` = `RasterRDD[_]`,9/16/15,1
lossyrob,and we_Ñére sort of back in the long name boat we were hoping to avoid,9/16/15,1
echeipesh,"well type alias wont give you a constructor, but yeah, we could extend. Maximise that sweet code reuse.",9/16/15,1
echeipesh,"We have a bit of nomencalture problem: We have `s3.RDDWriter` which will write `RDD[(K, V)]` to s3 given some codecs. And `s3.S3RDDWriter` which will handle the metadata and house keeping and call out the more `s3.RDDWriter`.",9/16/15,1
lossyrob,"Hmm, I think `s3.RDDWriter` should maybe be the `s3.S3RDDWriter`, and the `S3RDDWriter` should have a name that reflects that it does metadata stuff",9/16/15,1
lossyrob,"what_Ñés a name for a thing that is both an `RDD[(K, V)]` and also the metadata that describes it",9/16/15,1
lossyrob,I_Ñém wondering if the Layer nomenclature kind of comes in here,9/16/15,1
lossyrob,"and kind of plays into the idea that we_Ñéve been feeling like RDD_Ñés should just be RDD_Ñés, and we add capabilities to RDDs if certain information is available about that collection",9/16/15,1
lossyrob,I_Ñém just talking out loud here,9/16/15,1
lossyrob,*thinking,9/16/15,1
echeipesh,"I_Ñéve been having a crazy though that our life could be a lot easier if there was no such thing as `RasterRDD` or whatever, but only `Tuple2[RDD[(K, TileType)], MetaDataType]` and everything that was special about them was just implicit methods that propagated the tuple. eh ?",9/16/15,1
lossyrob,"Because RasterRDD is really just a RDD[(K, Tile)], and then some metadata that lets you do things with that composition. So something like `RasterLayer`, which?",9/16/15,1
lossyrob,hmmm yeah I_Ñém thinking that there_Ñés the composition which at is simplest is a tuple,9/16/15,1
lossyrob,"but having tried to do tuple stuff before without naming it, it turns ugly pretty quick",9/16/15,1
lossyrob,"But maybe just a case class, implicit conversions",9/16/15,1
lossyrob,"Here_Ñés a thought. If we had a MetaData type, and it was implicitly viewable as other metadata types. Like, RasterMetaData would have a TransformView",9/16/15,1
lossyrob,"and then methods were implicitly added to (RDD[(K, V)], TransformView)",9/16/15,1
echeipesh,"yeah, we already have beginnings of that",9/16/15,1
lossyrob,"and there was an implicit conversion to a RasterLayer -> (RDD[K, V], SomethingThatRasterMetaDataIsViewableAs)",9/16/15,1
echeipesh,you_Ñére talking about implicit methods on tuple there ?,9/16/15,1
lossyrob,then you could call methods on it,9/16/15,1
lossyrob,hmmm let me see if I can code a toy,9/16/15,1
echeipesh,"No,  that won_Ñét work, scalac won_Ñét chain two implicit conversion like that.",9/16/15,1
lossyrob,ugh,9/16/15,1
lossyrob,"hmm but if it_Ñés type classy, perhaps",9/16/15,1
lossyrob,do you have some implicit view type class (whatever that pattern is called) boilerplate hanging around?,9/16/15,1
echeipesh,"sure, it_Ñés not much boilerplate: https://github.com/geotrellis/geotrellis/pull/1174/files#diff-81c94201d0d50941d734bb9461ee07a5R11",9/16/15,1
lossyrob,ok. let me see if I can play around with something that compiles,9/16/15,1
echeipesh,"It doesn_Ñét have to be even a named trait you can totally do:``` scalatype MyMethodView[A] = A => { def myMethod(..) }implicit class withMyMethodView(target: T) {    def myMethod(_Ñ_)}```",9/16/15,1
lossyrob,weird,9/16/15,1
echeipesh,"yeah, when doing method extension it_Ñés actually using structural type when searching for a match.",9/16/15,1
lossyrob,where_Ñés an instance of ReprojectView being used?,9/16/15,1
echeipesh,`main/scala/geotrellis/spark/reproject/Reproject.scala`,9/16/15,1
echeipesh,"as an asside: what this is really crying out for is unorered tuple type. so (rdd, metadata) and (metadata, rdd) are the same thing. I know shapeless has ways to get that behavior but using dark magic always comes with a price.",9/16/15,1
lossyrob,Also using tuples as a method of composition is a really undiscoverable way to do things,9/16/15,1
echeipesh,good point,9/16/15,1
lossyrob,but a really generic way of coding against composed things. so wonder if there_Ñés a good compromise,9/16/15,1
echeipesh,case classes implementing product interface ?,9/16/15,1
lossyrob,Does this do the thing we were talking about?,9/16/15,1
lossyrob,"```scalapackage geotrellisobject Main {  type StringerView[T] = T => Stringer  trait Stringer {    def theString: String  }  object FooMeta {    implicit class FooMetaAsStringer(m: FooMeta) extends Stringer {      def theString = m.s    }  }  case class FooMeta(s: String, a: Double)  case class Foo(x: Int, meta: FooMeta)  object Foo {    implicit def foo2Tup(foo: Foo): (Int, FooMeta) = (foo.x, foo.meta)  }  def bam[T: StringerView](tup: (Int, T)) = {    println(s""Int: ${tup._1} String: ${tup._2.theString}"")  }  def main(args: Array[String]): Unit = {    val f = Foo(1, FooMeta(""asdf"", 4.3))    bam(f)  }}```",9/16/15,1
lossyrob,let me try to name things not insanely,9/16/15,1
lossyrob,"```scalaobject Main {  type TransformerView[T] = T => Transformer  trait Transformer {    def transform: String  }  object RasterLayerMetadata {    implicit class RasterLayerMetadataAsTrasformer(m: RasterLayerMetadata) extends Transformer {      def transform = m.s    }  }  case class RasterLayerMetadata(s: String, a: Double)  case class RasterLayer(rdd: Int, meta: RasterLayerMetadata)  object RasterLayer {    implicit def rasterLayer2Tup(layer: RasterLayer): (Int, RasterLayerMetadata) = (layer.rdd, layer.meta)  }  def bam[T: TransformerView](tup: (Int, T)) = {    println(s""Int: ${tup._1} String: ${tup._2.transform}"")  }  def main(args: Array[String]): Unit = {    val f = RasterLayer(1, RasterLayerMetadata(""asdf"", 4.3))    bam(f)  }}```",9/16/15,1
lossyrob,that runs,9/16/15,1
echeipesh,"yeap, the conversions are broken up into two parts. But then if you did `f.transform` it wouldn_Ñét find anything without more plumbing.",9/16/15,1
lossyrob,"ah, right",9/16/15,1
echeipesh,"the answer could just be more plumbing, you just need one more implicit class that_Ñés an adapter essentially.",9/16/15,1
lossyrob,"for reference, this doesn_Ñét compile:",9/16/15,1
lossyrob,"```scalaobject Main {  type TransformerView[T] = T => Transformer  trait Transformer {    def transform: String  }  object RasterLayerMetadata {    implicit class RasterLayerMetadataAsTrasformer(m: RasterLayerMetadata) extends Transformer {      def transform = m.s    }  }  case class RasterLayerMetadata(s: String, a: Double)  case class RasterLayer(rdd: Int, meta: RasterLayerMetadata)  object RasterLayer {    implicit def rasterLayer2Tup(layer: RasterLayer): (Int, RasterLayerMetadata) = (layer.rdd, layer.meta)  }  implicit class Bammer[T: TransformerView](tup: (Int, T)) {    def bam = {      println(s""Int: ${tup._1} String: ${tup._2.transform}"")    }  }  def main(args: Array[String]): Unit = {    val f = RasterLayer(1, RasterLayerMetadata(""asdf"", 4.3))    f.bam  }}```",9/16/15,1
lossyrob,doing something like:,9/16/15,1
lossyrob,"```scala  type TupleView[L,T1,T2] = L => (T1, T2)  implicit class Bammer[T: TransformerView, L](tup: L)(implicit tv: TupleView[L, Int, T]) {    def bam = {      println(s""Int: ${tup._1} String: ${tup._2.transform}"")    }  }```",9/16/15,1
lossyrob,"does. but also, woof",9/16/15,1
echeipesh,"I like that, views all the way down!",9/16/15,1
lossyrob,"how do you make that so you can use the TupleView like `[T: TransformerView, L: TupleView[Int, T]]`? I feel like there_Ñés a way with type lambdas",9/16/15,1
echeipesh,"anyway, I think that sidesteps the problem a lot. Two main points of contention is that programming vs naked tuples simplifies the API significantly but reduces descoverability. Having named classes requires abstracting out their construction (which is a pain) and making them viewable as tuples only complicates things.So in generall you have to pick one and write better documentation or deal with the construction type classes.",9/16/15,1
lossyrob,I do think we should name things,9/16/15,1
echeipesh,"You can document tuples with type aliases `type RasterRDD = (RDD[_Ñ_], RasterMetaData)` and that helps documentation but not much more. (Actually it helps documentation a lot because you can use it in argument names.",9/16/15,1
echeipesh,But they_Ñére not much good for anything else _Ñ_ to finish that thought.,9/16/15,1
lossyrob,I_Ñém not sure if that_Ñés true,9/16/15,1
echeipesh,_ÑÒnot good for anything else_Ñù part ?,9/16/15,1
lossyrob,yeah,9/16/15,1
lossyrob,"I feel like I_Ñéve been down the tuples programming path before and bailed out once I wanted to do a couple of things, but I forget the situations",9/16/15,1
echeipesh,Generally that happens when you want to hang methods on the objects.,9/16/15,1
echeipesh,"Also no named arguments, so if it_Ñés a long tuple making it becomes precarious.",9/16/15,1
lossyrob,yeah. and type aliases go only so far,9/16/15,1
lossyrob,but there_Ñés the thing about coding only against the composition that you need,9/16/15,1
lossyrob,that_Ñés what we_Ñére trying to solve here right?,9/16/15,1
echeipesh,exactly,9/16/15,1
echeipesh,"One thought is that type aliases for tuples make EVERYTHING a type parameter. One may see that as a bad thing, but we_Ñére steadily moving in that direction. Only metadata now is not a type parameter.",9/16/15,1
lossyrob,yeah. genericize-all-the-things leads to some crazy method signatures and potentially ugly code,9/16/15,1
lossyrob,sometimes you just want to work with concrete types dammit,9/16/15,1
echeipesh,"It_Ñés probably worth throwing a day at this kind of refactor and see the shape of things in code. It would be a fairly major change in API philosophy but again, at the extreme of the current direction.",9/16/15,1
lossyrob,Hmmm. I_Ñém not sure.,9/16/15,1
lossyrob,What would be the sketch of the refactor?,9/16/15,1
echeipesh,"delete rasterRDD, replace it with tuple type alias and fix readers/writers/ops.",9/16/15,1
lossyrob,Is there any other library that has that tuple design philosophy?,9/16/15,1
lossyrob,I_Ñém trying to picture it in many situations in my head but having trouble imagining the code,9/16/15,1
echeipesh,I do not believe it_Ñés a common pattern in scala. ,9/16/15,1
lossyrob,Is that day something we could put off until next sprint or would it need to happen now?,9/16/15,1
echeipesh,"`Programming DSLs in Scala` uses it all over the place, but that_Ñés just a book.",9/16/15,1
echeipesh,I would say it_Ñéd be better delayed until next sprint. You would have to answer questions of how you propagate partitioners with tuples etc. And it would help if all the backends were at least up to speed with level of generality in S3,9/16/15,1
echeipesh,re how common: I would *guess* that the pattern would be much more common in clojure. ,9/16/15,1
lossyrob,"Well, we_Ñéd have to check partitioner type on operations like Spark does",9/16/15,1
lossyrob,and just set it into the RDD as a regular RDD,9/16/15,1
lossyrob,"i.e. having a specific subclass saying RDD[(K,V)] _must_ have a specific partitioner type is a little weird",9/16/15,1
echeipesh,"Yeah, it sounds like that_Ñés the answer. But that would be something I_Ñéd want to see before being sold on a change like this.",9/16/15,1
lossyrob,truth,9/16/15,1
pomadchin,"guyz, how to count ram for ingest?",9/18/15,1
pomadchin,for example i got 600tiffs 1gb sized each,9/18/15,1
kgs,"Hi guys, can I use geotrellis to do spatial joins? For example I want to produce result set which consists only of records which are intersection of two sets? Or maybe yoyu know how to do this in Spark or similiar big-data framework?",9/18/15,1
pomadchin,+ anybody tested gt-admin? (:,9/18/15,1
echeipesh,@pomadchin you_Ñére trying to estimate how much to give to each executor ?,9/18/15,1
echeipesh,"@kgs We mostly deal in tiled rasters at the moment and we handle spatial join type of question by retiling our rasters into a common layout scheme which allows us to just use they key (tile layout col,row) to do the join. I haven_Ñét gone looking for the features but there is a good chance GeoMesa or GeoWave will have a spatial join on features that are stored in Accumulo.You_Ñéve probably found these guys who do spatial join on RDDs by calculating R-Trees http://simin.me/projects/spatialspark/. Their sample code seems suspect as it calls collect on one of the RDDs but maybe I_Ñém missing something.",9/18/15,1
kgs,"@echeipesh thanks, indeed SpatialSpark is best guess and I'm evaluating this now, thanks",9/18/15,1
echeipesh,A bit of a reversal but I would love to hear about your evaluation when you_Ñére done : ) I_Ñéve been meaning to play with that for at least a month now.,9/18/15,1
kgs,no problem!,9/18/15,1
kgs,from source code and technical paper looks promising,9/18/15,1
pomadchin,"@echeipesh yep, and to understand how much i need memory for every machine in cluster",9/18/15,1
echeipesh,"this is probably the most useful blog if you haven_Ñét seen it: http://blog.cloudera.com/blog/2015/03/how-to-tune-your-apache-spark-jobs-part-2/Speaking about geotrellis we_Ñéve had trouble re-tiling huge tiles at some point. That problem may have been fixed through refactors but we haven_Ñét verified. In general you need to make sure that each partition of your RDD is able to fit in executor memory (keeping in mind that spark will only use some fraction for working with the partition). That way if it can_Ñét fit all the partitions in memory at least it_Ñéll be able to swap them in and out without blowing the heap. If you run into problem with 1G tiles a good first step might be to flatMap the RDD to shard the tile. ",9/18/15,1
pomadchin,"and for example, having one 10G tile; in a cluster with 50 machines, 5gb mem and 32 cores each (just an example); it will be possible to process?",9/18/15,1
echeipesh,"Not in that case, you_Ñéll need to load that whole tile (since it_Ñés only one) in memory and that_Ñés going to overflow 5gb limit on that executor. Is that a hypothetical case or you actually have that situation ?",9/18/15,1
pomadchin,"just a hypothetical; now having tiles 1-10gbs but, soon, there can be a task with 10gbs tiff with different inforamtion (weather, wind, etc so its a weather map tiff, the whole tiff ~~~~ 1 map)",9/18/15,1
pomadchin,it would be possble to have machines with 30gbs to process such tile?,9/18/15,1
echeipesh,"yes, that should do the trick, you_Ñéll probably still need to split it once its in RDD before tiling it, but yeah, it should work.",9/18/15,1
pomadchin,the problem that i cant calculate sizes of ram that we need to ingest tile,9/18/15,1
pomadchin,hm to split in code you mean?,9/18/15,1
echeipesh,"yes, once it_Ñés a `Tile`",9/18/15,1
echeipesh,"It_Ñés a good question but we_Ñéve never profiled the memory requirements unfortunatly. But we have tiled layers that are > available RAM by keeping the partitions small enough, that_Ñés just general spark tuning.",9/18/15,1
pomadchin,yep i understood,9/18/15,1
pomadchin,ty!,9/18/15,1
pomadchin,btw,9/18/15,1
pomadchin,@echeipesh haven_Ñét you had such situation with gt-admin?,9/18/15,1
pomadchin,[![Screen Shot 2015-09-18 at 18.33.08.png](https://files.gitter.im/geotrellis/geotrellis/QQFe/thumb/Screen-Shot-2015-09-18-at-18.33.08.png)](https://files.gitter.im/geotrellis/geotrellis/QQFe/Screen-Shot-2015-09-18-at-18.33.08.png),9/18/15,1
pomadchin,it has serialised byte arr,9/18/15,1
pomadchin,"```[-119, 80, 78, 71, 13, 10, 26, 10, 0, 0, 0, 13, 73, 72, 68, 82, _Ñ_.. ]```",9/18/15,1
echeipesh,"ah yeah, this is really annoying part of API. What happens if you have `import DefaultJsonProtocol._` in scope of where it_Ñés handing over the byte array to spray it will use that Array `JsonFormat` that rather than sending bytes. As a result that import needs to be kept super local to avoid that. Is that happening in master gt-admin code or you have modified it?",9/18/15,1
pomadchin,"https://github.com/geotrellis/gt-admin/blob/master/server/src/main/scala/geotrellis/admin/server/CatalogService.scala#L26",9/18/15,1
echeipesh,"thank you, will patch that up.",9/18/15,1
pomadchin,"thank you, yep it works perfect now",9/18/15,1
pomadchin,oh while you here o: another batch of questions :D,9/18/15,1
pomadchin,is it possble to set custom NoData value? for a certain job,9/18/15,1
echeipesh,"No, it_Ñés a performance centric decision from a while ago. Each tile type has specific No Data value. I believe GeoTiff reader should read off no data value from the tags and replace that with our internal values. (looking now actually _Ñ_)",9/18/15,1
pomadchin,"ok, nice nice",9/18/15,1
pomadchin,ty! ,9/18/15,1
echeipesh,"yep, looks right: https://github.com/geotrellis/geotrellis/blob/master/raster/src/main/scala/geotrellis/raster/io/geotiff/Float32GeoTiffTile.scala#L43",9/18/15,1
pomadchin,"wow, so it is possible to set a nodata tag",9/18/15,1
pomadchin,perfect,9/18/15,1
lossyrob,"@pomadchin what I_Ñéve been doing is using GDAL_Ñés vsicurl capabilities through rasterio in a pyspark first step, to chunk out the large tiles into smaller tiles that are more fit for GeoTrellis/Spark to work with",9/18/15,1
lossyrob,you can see an example of that in some work I_Ñém doing for HOT in the OpenAerialMap project,9/18/15,1
lossyrob,https://github.com/hotosm/oam-server-tiler,9/18/15,1
lossyrob,"I run this on EMR, first a pyspark task to tile down to zoom max zoom levels, and then using geotrellis to mosaic",9/18/15,1
lossyrob,"I think this is the technique I_Ñéll be using; working with large tiles in GeoTrellis is tough because we don_Ñét have windowed reads, and no vsicurl-like abilities for reading byte ranges off of s3",9/18/15,1
lossyrob,this won_Ñét work of course if you_Ñére reading from say HDFS,9/18/15,1
lossyrob,"And yep, the GeoTiff reader respects the NoData tag, but it translates it into our internal NoData values",9/18/15,1
pomadchin,hm,9/18/15,1
pomadchin,if Ill set `gdal_translate -of GTiff -a_nodata 0 input.tif output.tif`,9/18/15,1
pomadchin,nodata value would be correct?,9/18/15,1
lossyrob,yeah,9/18/15,1
lossyrob,you could also use http://www.gdal.org/gdal_retile.html to chunk them into smaller tiles for processing,9/18/15,1
lossyrob,"we_Ñére also doing that for some rasters, before we put them up on s3",9/18/15,1
lossyrob,what source are you using for the large images? HDFS?,9/18/15,1
pomadchin,"yes, hdfs",9/18/15,1
pomadchin,but it is not a problem I think,9/18/15,1
lossyrob,"ok, cool. let me know if it becomes a problem, and if and how you resolve it, definitely a concern about how we handle ingesting large rasters. That initial step of reading it into memory in order to chunk it out can be problematic",9/18/15,1
pomadchin,"yes, sure! Im always here (:",9/18/15,1
lossyrob,"@echeipesh this PR is failing travis because of that ScalaCheck row transform undeterministic test that fails sometimes (gotta look into that, will write an issue). But we def need this change asap, geotiff reading of crs_Ñés is broken without it (we are reading from file paths instead of resource paths). If you +1 I_Ñéll merge: https://github.com/geotrellis/geotrellis/pull/1179",9/18/15,1
echeipesh,"+1, double checked the resource loading path, and using absolute path through `getClass` appears to be always correct thing to do (relevant link: http://www.thinkplexx.com/learn/howto/java/system/java-resource-loading-explained-absolute-and-relative-names-difference-between-classloader-and-class-resource-loading)",9/18/15,1
lossyrob,"cool, thx",9/18/15,1
nicerobot,"@timothymschier @lossyrob Sorry, i'm a little late to the GeoWave conversation but we did look into GeoWave and decided that they are a little behind GeoMesa for our use-case though I haven't gotten the details on that evaluation yet. That said, it also seems we've gotten an indication from GeoWave that they are in talks with GeoMesa to merge.",9/21/15,1
lossyrob,Thanks @nicerobot. I_Ñéd be curious to see what the features were in GeoMesa that were ahead of GeoWave on once those details are around,9/21/15,1
pomadchin,@lossyrob @echeipesh what about rdd merge method? got any ideas about it and how it would be working?,9/21/15,1
lossyrob,hey @pomadchin ,9/21/15,1
lossyrob,yeah we had discussed some strategies. Are you thinking of trying to work on it?,9/21/15,1
echeipesh,There is a trivial and case that I started some work to support (where you_Ñére updating two layers only) https://github.com/echeipesh/geotrellis/blob/feature/catalog-splitup-2/spark/src/main/scala/geotrellis/spark/mosaic/RddMergeMethods.scala,9/21/15,1
echeipesh,"There is a non-trivial case where you_Ñére updating more than two RDDs and you need to keep track of z-order per pixel, @lossyrob ran into that one.",9/21/15,1
nicerobot,"@lossyrob I got some feedback about the evaluation and it's less about technical capabilities. 1) We're focusing on Scala so like that GM is Scala. 2) We need to customize our keys and we didn't _notice_ a way to control the keys in GW. 3) The community was really quiet, making it seem GM is better supported. 4) the GW docs didn't seem as thorough.",9/21/15,1
lossyrob,@nicerobot that_Ñés good info. @rfecher would probably appreciate that feedback,9/21/15,1
pomadchin,@echeipesh nice tip will try top check it,9/21/15,1
echeipesh,What are you exactly trying to do? I_Ñém more than willing to talk it out with you and maybe help a PR form : ),9/21/15,1
pomadchin,"(: it anupdate ingest, which can be just mosaic or can update and overwrite already ingested data",9/21/15,1
echeipesh,Is a new pixel always overwriting old pixel?,9/21/15,1
pomadchin,"if it is valuable, yes;",9/21/15,1
echeipesh,"If it_Ñés availble, so your new tiles could contain _ÑÒNoData_Ñù values.",9/21/15,1
pomadchin,"yep, so the valuable data can have intersections with already ingested",9/21/15,1
pomadchin,"In terms of the current merge tiles function, just to add else branch (in the mosaic package)",9/21/15,1
echeipesh,"So the basic process that minizmes pain is probably:1. Get some data that_Ñés an update2. Query the layer for extent covering update tiles (you_Ñéll know this after mosaiking the update)3. Merge update into the existing tiles (the above merge method should work, but needs better testing)4. Write update tiles while adjusting the raster metadata in the AttributeStore",9/21/15,1
echeipesh,"Ah, that_Ñés right, we don_Ñét check if the source of pixels is no data, that_Ñés an oversight.",9/21/15,1
echeipesh,The `RasterMetaData` class will just need a `combine` method that can be called from the catalog on an update. That should cover it actually.,9/21/15,1
pomadchin,"hm, and how to query the old data? Need to pass the second rdd inside Pyramid.up function?",9/21/15,1
echeipesh,"No, I think it should live outside of Pyramid. Perhaps Pyramid.up should return `Seq[Int, RDD[(K, TileType)]` so you could just walk over it and query some catalog independently.",9/21/15,1
echeipesh,Maybe there can be an `UpdatingPyraimd` layer that automates the process given `FilteringRasterRDDReader` instance.,9/21/15,1
pomadchin,understood,9/21/15,1
echeipesh,Pyramiding is kind of strange that way. Is your base layer always a source of truth and you can merge at the base and pyramid up. Is every level a layer in its own right? I_Ñém not sure what is the _ÑÒcorrect_Ñù answer there.,9/21/15,1
echeipesh,"Ah yes, there is correct answer, you have to do it per level. The base level tiles will not cover all the tiles in the upper levels, you will always need more to pad as you zoom out.",9/21/15,1
pomadchin,Yes _ª¥ç ,9/21/15,1
echeipesh,"Also good news about Pyramid.up is that since it_Ñés a non-forking process it can safely return the sequence of RDDs without causing any collections. You would have to iterate it in the right direction though, to take advantage of the shuffle caching.",9/21/15,1
pomadchin,ps now upgraded up to master branch,9/21/15,1
pomadchin,"got some issues with ingesting, broadcast lost connection with zookeeper like couple of months ago, though rollback to 40b1866 is ok",9/21/15,1
echeipesh,40b1866 is really recent. Are you still having issues or is it working now?,9/21/15,1
pomadchin,had some problems on https://github.com/geotrellis/geotrellis/commit/337165ea8c426f293bcfcfdaf38b53700ad3a807,9/21/15,1
lossyrob,specifically that commit? or something in a range of commits between that and some start?,9/21/15,1
pomadchin,checkin it right now + checking mb some issues exactly in my local build,9/21/15,1
pomadchin,hm,9/21/15,1
pomadchin,[![Screen Shot 2015-09-22 at 00.57.51.png](https://files.gitter.im/geotrellis/geotrellis/n8XB/thumb/Screen-Shot-2015-09-22-at-00.57.51.png)](https://files.gitter.im/geotrellis/geotrellis/n8XB/Screen-Shot-2015-09-22-at-00.57.51.png),9/21/15,1
pomadchin,at this stage,9/21/15,1
pomadchin,it is 0.10.0-caf9e26,9/21/15,1
lossyrob,what_Ñés happening? erroring or just hanging?,9/21/15,1
pomadchin,"40b1866 is perfect, though master",9/21/15,1
pomadchin,after writes about connection (timeout) issues with zookeeper / accumulo,9/21/15,1
pomadchin,tried to clean manually .ivy2 cache,9/21/15,1
pomadchin,same,9/21/15,1
pomadchin,doesn_Ñét matter build local / load from bintray repo,9/21/15,1
pomadchin,ingest like gt-admin has (just replaced level.zoom to zoom),9/21/15,1
pomadchin,And scheme proj set to LatLng,9/21/15,1
lossyrob,hm. yeah zookeeper/accumulo timouts don_Ñét seem like they would be caused by any of the recent changes since 40b1866,9/21/15,1
lossyrob,are you losing nodes or something? does it always hang at that point in the spark job?,9/21/15,1
pomadchin,sec I will provide the log in console,9/21/15,1
pomadchin,yes always at this step ._. cleaning broadcast 2 is the last not err msg,9/21/15,1
pomadchin,"no, just locally: 1 node hdfs 1 node accumulo 1 node zookeeper and local mode spark-submit",9/21/15,1
lossyrob,this is with the containers?,9/21/15,1
pomadchin,nope on host machine,9/21/15,1
pomadchin,on mac ._.,9/21/15,1
lossyrob,that sounds a lot like what you had run into before. how did that get resolved last time?,9/21/15,1
pomadchin,._. just another commit and cleaned .ivy2 cache,9/21/15,1
pomadchin,cleaning cache now not helped ): and you ve tried to run master branch?,9/21/15,1
lossyrob,yeah I_Ñéve run it a bunch,9/21/15,1
lossyrob,I haven_Ñét really run into the problem of things hangning like that ever,9/21/15,1
pomadchin,oh; mb java version?,9/21/15,1
lossyrob,which version are you on?,9/21/15,1
pomadchin,1.8_045 oracle,9/21/15,1
lossyrob,I_Ñém running on 1.7,9/21/15,1
lossyrob,so maybe?,9/21/15,1
pomadchin,trying,9/21/15,1
pomadchin,): feel myself like a unique happy owner of a such bug,9/21/15,1
lossyrob,it_Ñés just really strange that it doesn_Ñét error out,9/21/15,1
lossyrob,if there was a stack trace somewhere it would be way easier to debug,9/21/15,1
pomadchin,https://gist.github.com/pomadchin/a8d4c43fab5c6d3e364d,9/21/15,1
lossyrob,oh. so it_Ñés a memory thing,9/21/15,1
lossyrob,how big are the images that you are reading in?,9/21/15,1
lossyrob,are these the big images you were talking about last weekish?,9/21/15,1
pomadchin,"no, just 245.78 MB",9/21/15,1
lossyrob,how many?,9/21/15,1
pomadchin,one,9/21/15,1
lossyrob,oh. hm,9/21/15,1
pomadchin,and it is one banded ,9/21/15,1
pomadchin,grayscaled ,9/21/15,1
pomadchin,byte,9/21/15,1
lossyrob,"well, what kind of mem are you giving to spark-submit",9/21/15,1
pomadchin,"and yes, checked all the commits, the err appeares only after 40b1866",9/21/15,1
pomadchin,8g o:,9/21/15,1
lossyrob,driver or executor?,9/21/15,1
pomadchin,"local mode but provide 8 and to driver and to executor",9/21/15,1
pomadchin,I can provide you a raster,9/21/15,1
pomadchin,"): but sounds like _ÑÒonly my bug""",9/21/15,1
lossyrob,sure. can you give me your ingest code as well,9/21/15,1
pomadchin,yep,9/21/15,1
pomadchin,https://gist.github.com/pomadchin/4c708a56681646aae816,9/21/15,1
pomadchin,https://www.dropbox.com/s/4aekf3u7akezaz2/LC81750272014227LGN00.TIF?dl=1,9/21/15,1
pomadchin,ive tested; since next commit fails: https://github.com/geotrellis/geotrellis/commit/fffebcc3275a6221c6816618b22d79b3b9a0c186,9/21/15,1
lossyrob,what_Ñés the spark submit command you_Ñére using?,9/21/15,1
pomadchin,"```JAR=ingest/target/scala-2.10/gt-admin-ingest-assembly-0.1.0-SNAPSHOT.jarDRIVER_MEMORY=8GEXECUTOR_MEMORY=8GINPUT=hdfs://localhost:9000/input/LANDSAT/B-merged2TABLE=nextdayLAYER_NAME=BLUECRS=EPSG:3857PYRAMID=trueCLOBBER=truezip -d $JAR META-INF/ECLIPSEF.RSAzip -d $JAR META-INF/ECLIPSEF.SFspark-submit \--class geotrellis.admin.ingest.AccumuloIngestCommand \--driver-memory $DRIVER_MEMORY \--executor-memory $EXECUTOR_MEMORY \$JAR \--instance gis --user root --password secret --zookeeper localhost \--crs $CRS \--pyramid $PYRAMID --clobber $CLOBBER \--input $INPUT \--layerName $LAYER_NAME \--table $TABLE```",9/21/15,1
lossyrob,alright. I_Ñém able to reproduce.,9/21/15,1
lossyrob,strange,9/21/15,1
lossyrob,"I can_Ñét work on it more tonight, but I_Ñéll look into it tomorrow",9/21/15,1
pomadchin,): bad news hoped that only I have it,9/21/15,1
pomadchin,Ty!,9/21/15,1
pomadchin,g2 sleep too ,9/21/15,1
lossyrob,"no problem, hopefully it_Ñés not too strange of a problem.",9/21/15,1
lossyrob,@pomadchin I think this could be a problem,9/22/15,1
lossyrob,`    val layoutScheme = ZoomedLayoutScheme(LatLng)`,9/22/15,1
lossyrob,in the ingest,9/22/15,1
lossyrob,which that raster is not in LatLng,9/22/15,1
lossyrob,"the destination CRS accordin the the submit job is `CRS=EPSG:3857`, WebMercator",9/22/15,1
pomadchin,"Hm, it makes an additional job to project?",9/22/15,1
lossyrob,Ingest will reproject to the destination CRS,9/22/15,1
lossyrob,the source tile isn_Ñét in LatLng either,9/22/15,1
lossyrob,I changed that line to be `    val layoutScheme = ZoomedLayoutScheme(WebMercator)`,9/22/15,1
lossyrob,and it worked,9/22/15,1
lossyrob,"but really, you should be using the `Etl` stuff to ingest now",9/22/15,1
lossyrob,that stuff is getting deprecated,9/22/15,1
lossyrob,https://github.com/geotrellis/geotrellis/tree/master/spark-etl,9/22/15,1
lossyrob,"also @echeipesh  is working on changing it further; it_Ñés not exactly done, but it doesn_Ñét use the Ingest object, and shouldn_Ñét be many changes in the future (since ingests would only be a couple lines of code to write)",9/22/15,1
pomadchin,"@lossyrob ty, I must have a look at the new api",9/22/15,1
pomadchin,): and what about Pyramid object? It is deprecated too?,9/22/15,1
echeipesh,"Pyramid is going to remain, just does one thing so it_Ñés good. The problem with Ingest object is that its trying to do too much and fails to provide a good interface for it.",9/22/15,1
pomadchin,(: what to see the refactored gt-admin ingest version,9/22/15,1
echeipesh,"yeap, that will have to be done",9/22/15,1
lossyrob,there_Ñés a Pyramid option for ETL that will do the pyramid save (`_ÑÓpyramid`),9/22/15,1
pomadchin,yep th guyz!,9/22/15,1
pomadchin,ty *,9/22/15,1
rfecher,"@nicerobot @timothymschier getting back to the GW/GM evaluation (sorry I'm a couple days behind on it)...it sounds like you've made your decision but if you do end up looking back at GW, definitely hit up the gitter or the mailing list to ask the question regarding controlling your keys, I'm curious what your use case is to give you the most accurate answer, but generally you can implement your own index strategy at a  higher level: https://github.com/ngageoint/geowave/blob/master/core/index/src/main/java/mil/nga/giat/geowave/core/index/NumericIndexStrategy.java or at a lower level you can implement your own space filling curve which is what the default numeric index strategy will use (and the default index strategy's default sfc is the compact hilbert implementation that geotrellis has also been looking at): https://github.com/ngageoint/geowave/blob/master/core/index/src/main/java/mil/nga/giat/geowave/core/index/sfc/SpaceFillingCurve.java",9/23/15,1
rfecher,"also we'll have to look more at documentation, we have this page for reference documentation that we felt was pretty thorough, but what we feel we're lacking is in blogging about use cases - our plan is to write some more blog-style rather than reference material and perhaps complement the blogs with a tech talk in the form of a hangout or something like that",9/23/15,1
rfecher,http://ngageoint.github.io/geowave/documentation.html,9/23/15,1
rfecher,"but please feedback any particulars about documentation that you'd like to address, and we will try to address it",9/23/15,1
rfecher,"oh and re: gw/gm merge, *both* projects think its in the best interest of the community to get on the same page and we know there's a lot of overlap...but the nature of these discussions is *not* a merge of gw into gm, which maybe I read out of context but it seemed may have been your take away @nicerobot ",9/23/15,1
nicerobot,"@rfecher Thanks. I'll pass that index strategy to the dev. We really like Scala which make GeoTrellis and GeoMesa attractive but, of course, solutions come before tech-preferences so if we can get more done faster with GW we might use it instead.Re: our use-case. big-picture: points, polygons, and rasters. and, as probably typical of this field, any combinations (utilizing mostly proprietary algorithms) of any of those elements (including results of analysis) can be utilized for analysis. ""standard"" keys facilitate that typical use-case. we additionally utilize a means of relating rasters and polygons depending on how they are created/related so that is the part where we customize the keys. If i'm being too vague, i may be able to think of a practical example.Re: merge. I wasn't thinking merging one _into_ the other but hoping we'd end up with one project that utilized aspects from both. One of the big problems I see is that these stacks aren't trivial so evaluation itself is an investment. It's partly why community response, documentation, and project activity is important to us.Re: docs. I think the main thing about the docs was figuring out the indexing. With your like to the code, we can probably figure it out from there.",9/23/15,1
rfecher,"@nicerobot yeah, and just to let you know I don't mean to pressure you to go one way or the other, just getting information out there...we use a concept of a dataadapter for actually persisting whatever data you may want, so we have a rasterdataadapter that allows you to work with GridCoverage (geotools' raster data model) and vectordataadapter for simplefeature (geotools' vector data model), but they can both be comingled in the same table so you can easily run analysis with rasters/points/polygons together...I think I would be interested if you have a practical example because it sounds interesting",9/23/15,1
nicerobot,"@rfecher ok.Example use-case: Simple analytics. Our rasters are ""user-defined"" but we uniquely identify each by its extent, category, and message digest of the raster data. Based on the raster id (not just the extent), and maybe more user input, we filter a subset of polygons to analyze. Using the queried set of polygons, we intersect them with rasters (maybe more than just the user-defined one) and run calculations on the values of the rasters within the polygonal intersections. Our rasters and polygons are all uniquely identifiable since we rely on those to allow users to manage and control the analysis.",9/23/15,1
lossyrob,"In GeoTrellis, we call that operation a zonal summary. Are the polygons your running against the rasters for the zonal summary large? Or would they be able to be shipped to each node and used for the calculations? We haev it set up that you pass in a Polygon or MultiPolygon to the zonal summary operation, and that will intersect the polygon(s) with each of the tiles to run the calculation ina distributed way. If the polygon(s) are very heavy or complex enough that having a distributed set of polygons would make sense, we don_Ñét yet support that",9/23/15,1
lossyrob,"That would be an interesting use case for us to attack that case for a good first pass at RDD[(SpatialKey, Polygon)] if that_Ñés needed",9/23/15,1
pomadchin,"`@offtop` guyz, the `catalog.read[K](layerId).map { case (key, tile) => key }.collect() ` array will have all the keys for layerId?",9/23/15,1
lossyrob,yessir,9/23/15,1
pomadchin,:D ty!,9/23/15,1
lossyrob,"if you raster is evenly distributed, you could derive that information fro the keyBounds, but if it_Ñés not, then yeah that_Ñés the best way to go about it",9/23/15,1
nicerobot,"I've seen some of our polygons as big as ~100M but they'll typically be far smaller, maybe between 20K and 2M. That said, even with one analytics job we can be talking about 100s of polygons so as we scale, shipping polygons can become a massive network bottleneck. I'm thinking that instead of min-squaring the polygons, we tile them the same as rasters which should provide better locality.",9/23/15,1
lossyrob,"yeah, if you could get the polygons into an RDD that was RDD[(SpatialKey, MultiPolygon)] then it would be a matter of `rasterRDD.join(multiPolygonRDD).mapValues { (tile, polygons) => _Ñ_ }`",9/23/15,1
lossyrob,and that `_Ñ_` would be using the `Rasterizer` to iterate over the cells covered by the polygons on that tile to do the calculation,9/23/15,1
nicerobot,Cool. Thanks. Hopefully i'll be able to give this a try within the next day or two.,9/23/15,1
lossyrob,"ok. the Rasterizer api is a bit ornery (been meaning to rewrite that for a while), so let us help you with that. I_Ñém going to be out for a while starting Friday, but @echeipesh will be able to help",9/23/15,1
rfecher,"so for us, storing features or rasters with a unique ID and being able to look them up explicitly by that ID is part of the default behavior...every data element ingested into the system has a data ID and you can retrieve by data ID here: https://github.com/ngageoint/geowave/blob/master/core/store/src/main/java/mil/nga/giat/geowave/core/store/DataStore.java#L184",9/23/15,1
pomadchin,"@lossyrob cool; and would be there a difference between `key, tile` in this query from tile, got with tile reader for a single tile?",9/23/15,1
rfecher,"we have an index as a separate table for efficient lookups by data ID, and we store it within every key as well (http://ngageoint.github.io/geowave/documentation.html#architecture-accumulo)",9/23/15,1
lossyrob,"@pomadchin no the tile keys are unique, so it would be the same tiles",9/23/15,1
rfecher,"but then when it comes to distributed processing, @lossyrob seems to be right on with using a geotrellis zonal summary",9/23/15,1
anandthakker,Hey @lossyrob got a sec for a couple geotrellis questions?,9/24/15,1
pomadchin,"@anandthakker you can write questions here, @lossyrob and @echeipesh will be there later (: and every body here will try to help you",9/24/15,1
anandthakker,@pomadchin cool thanks :),9/24/15,1
anandthakker,"i guess my first/main question is: i've got a pipeline using rasterio + gdal_polygonize to do some raster math and then get polygons out of it.  looking to apply it on a pretty large scale (30m raster, worldwide coverage).  do you think geotrellis can help?",9/24/15,1
echeipesh,"@anandthakker Greetings and welcome! You kind of have two options here. GeoTrellis is a scala project, you can certainly use it to do raster math and extract polygons from resulting tiles (good questions are exactly how and what happens to polygons that span tile boundaries). Second option is to just do what you_Ñére doing on top of pyspark, which is tempting since rastario is just so nice convinient. lossyrob has is currently working on a project where he has a pyspark+rasterio pre-processing step before pulling the rasters into geotrellis, that would be a good thing to look at. (let me see if I can fish that out)",9/24/15,1
anandthakker,"@echeipesh oh nice -- having not used spark myself, i'd love to see a pyspark + rasterio model!",9/24/15,1
anandthakker,"as for the polygons: because of the way i'm using them, it's not really important for them to cross tile boundaries--i.e., i'd be happy to extract polygons from within each tile",9/24/15,1
anandthakker,"i guess if rasterio can work w/ pyspark, then maybe gdal_polygonize.py can, too? ",9/24/15,1
echeipesh,"Yes, you_Ñéd just have to make sure that it_Ñés availble on all the executors.",9/24/15,1
lossyrob,@anandthakker here_Ñés the OAM Tiler that runs on EMR,9/24/15,1
lossyrob,it runs both pyspark and GeoTrellis spark,9/24/15,1
lossyrob,https://github.com/hotosm/oam-server-tiler,9/24/15,1
lossyrob,https://github.com/hotosm/oam-server-tiler/blob/master/chunk/chunk.py uses rasterio to tile images out to their max zoom level GeoTiffs,9/24/15,1
lossyrob,the deployment scripts at the root of the repository would be a good place to start,9/24/15,1
lossyrob,to figure out how to run it on EMR,9/24/15,1
anandthakker,"@lossyrob awesome, thanks!",9/24/15,1
lossyrob,"I think if you could just use gdal_polygonize.py from python code, that should work as well",9/24/15,1
lossyrob,it_Ñés pretty dynamic what you can do,9/24/15,1
anandthakker,"yeah, that's pretty sweet -- is it hard to get gdal properly installed & configured for the executors?",9/24/15,1
lossyrob,"Also GeoTrellis has support for doing zonal summary calculations given polygons, not sure what sort of math you_Ñére looking to do, but a more robust approach might be to ingest the rasters into a GeoTrellis catalog and go through the geotrellis spark stuff. Certainly would help if you want to do spatial queries of subsets of tiles to do the operation on",9/24/15,1
lossyrob,but I_Ñém curious to see how the pyspark stuff handles it,9/24/15,1
lossyrob,"well, if it was, luckly @mojodna figured it out :)",9/24/15,1
anandthakker,hah,9/24/15,1
lossyrob,there_Ñés a bootstrap.sh,9/24/15,1
lossyrob,that you have to put somewhere on S3,9/24/15,1
lossyrob,and then feed to the EMR command,9/24/15,1
lossyrob,and that will run against the servers as they are provisioned and install gdal and rasterio,9/24/15,1
lossyrob,"so if you use the same bootstrap process, gdal/rasterio would just be available",9/24/15,1
anandthakker,"re: zonal summaries: what i'm ultimately doing is creating a set of vector tiles, and at the highest zoom I basically want a full fidelity vector version of the raster.  (weird, i know, because this makes vectors out of individual pixels much of the time)",9/24/15,1
anandthakker,well that is just sweeet.,9/24/15,1
anandthakker,maybe i'll try to isolate that bootstrap stuff into a boilerplate repo before i add in my actual processing,9/24/15,1
lossyrob,that would be awesome,9/24/15,1
lossyrob,"oh gotcha re: vector tiles. Curious to how that works out; vector data of pixels feels like it would be pretty huge, but then again that_Ñés probably just my penchant for working with rasters ha",9/24/15,1
anandthakker,yeah it's generally pretty huge,9/24/15,1
anandthakker,heh,9/24/15,1
anandthakker,"but it actually depends: this particular (population density) has big contiguous regions w the same value, so that moderates the size quite a bit",9/24/15,1
anandthakker,"speaking purely, it would make more sense to keep this as raster data, but vector tiles are a better fit for the application in this case",9/24/15,1
anandthakker,"but also to be fair, i'm also just way more familiar with vectors, so that's probably also part of what led me to this approach",9/24/15,1
anandthakker,heh,9/24/15,1
lossyrob,you have worldwide 30m population density?,9/24/15,1
anandthakker,http://www.worldpop.org.uk/,9/24/15,1
anandthakker,"it's from a model, and it's actually South America, Africa, and Asia",9/24/15,1
anandthakker,"""worldwide"" was just shorter to say earlier :)",9/24/15,1
lossyrob,oh nice. I_Ñém working with some 30 pop density for US,9/24/15,1
lossyrob,*30m,9/24/15,1
lossyrob,"as part of a larger weighted overlay operation, including housing vacancy, NLCD, housing prices, some others",9/24/15,1
anandthakker,oh cool - is it open/available?,9/24/15,1
lossyrob,"Yeah, not sure exactly where it comes from",9/24/15,1
lossyrob,I could find out tho,9/24/15,1
anandthakker,i'd love to add it to this worldpop dataset,9/24/15,1
lossyrob,"nice. We have a data wizard at Azavea (Sarah Cordivano) who constructed it. She says: ""i downloaded population by block groups and its from ACS 2008-2013 5 year estimates. then i calculated area of every block group and divided population by area to get density.""",9/24/15,1
anandthakker,that's rad,9/24/15,1
mapadelphia,"Hi!  ""data wizard"" here. ;-) not really a wizard, more of a helpful elf",9/24/15,1
anandthakker,i saw Sarah talk about redistricting at Tcamp,9/24/15,1
mapadelphia,LMK if you have any questions about the data,9/24/15,1
anandthakker,hi @mapadelphia !,9/24/15,1
mapadelphia,oh! hi! yes i remember!,9/24/15,1
mapadelphia,"fun fact, we actually tried to do these calculations with BLOCK population",9/24/15,1
mapadelphia,and my computer exploded,9/24/15,1
anandthakker,haha,9/24/15,1
mapadelphia,not really. but it was a non-starter based on the size of the data,9/24/15,1
anandthakker,yeah i can imagine,9/24/15,1
mapadelphia,there are over 11 million us block polygons ,9/24/15,1
lossyrob,good case for getting GeoTrellis rasterization to work over a RDD[Polygon]. Ah if there was only more time in the day...,9/24/15,1
anandthakker,"yeah i was about to say, sounds like a case for spark/geotrellis!",9/24/15,1
lossyrob,but really that wouldn_Ñét be *too* hard to pull off.,9/24/15,1
anandthakker,"still, block group level detail ain't bad :)",9/24/15,1
mapadelphia,yup! we did DL all the data at block level so i do have it to share too. i can look later,9/24/15,1
lossyrob,"Yeah, that would be a good project for someone to work on for GeoTrellis. Too bad GSoC isn_Ñét until next summer.",9/24/15,1
lossyrob,Any takers in this room? ;),9/24/15,1
mapadelphia,"one thing though, not a big deal",9/24/15,1
lossyrob,How does one contrubite data to world pop?,9/24/15,1
mapadelphia,but i think the block level data is actually 2010 decennial population as opposed to 2008-2013 acs pop data,9/24/15,1
lossyrob,If I was smarter I would know what that meant lol,9/24/15,1
anandthakker,"@lossyrob i don't know if it's something they take contributions to, but we kinda know some of the researchers that run it so we could talk to them",9/24/15,1
anandthakker,or put you in touch w them,9/24/15,1
anandthakker,"@mapadelphia actually that's probably better/more consistent in my particular case, because i'm using 2010 estimates from the worldpop dataset",9/24/15,1
mapadelphia,"no worries at all. so every 10 years (decennial), the census captures a ""full"" survey of population, on the off years, they use extrapolations and sample surveys to estimate what those figures might be, this is know as the American Community Survey or (ACS) in general ACS is relatively reliable especially in urban areas, but in rural areas the margin of error is high",9/24/15,1
mapadelphia,"@anandthakker agreed, 2010 is considered by some to be more accurate, especially if comparing to other countries at the same year",9/24/15,1
lossyrob,ah I see. So the ACS is more up to date but perhaps a higher margin of error in rural areas,9/24/15,1
mapadelphia,yup,9/24/15,1
lossyrob,"@anandthakker it would be interesting to touch base with the researchers and see if they could find the data useful, or just chat about the techniques if you could make that connection",9/24/15,1
lossyrob,maybe it would give good reason to do the block polyginization in GeoTrellis and we_Ñéd get to spend some time on that,9/24/15,1
anandthakker,happy to,9/24/15,1
lossyrob,thanks!,9/24/15,1
anandthakker,"i might actually have Nate do it, since he knows the worldpop folks a little better than i do",9/24/15,1
lossyrob,for sure,9/24/15,1
anandthakker,can i use the same email i already have for you?,9/24/15,1
lossyrob,"yeah that will work, and I_Ñéll include @mapadelphia on it in a reply.",9/24/15,1
lossyrob,or I_Ñéll just pm you her email (I don_Ñét know if there_Ñés bots scraping the pulic channels or if that_Ñés paranoia),9/24/15,1
anandthakker,surely the scraping is happening,9/24/15,1
anandthakker,good call :),9/24/15,1
lossyrob,haha,9/24/15,1
mapadelphia,":) I'm about to head out for the day, but I think I can wrangle the data tomorrow or monday and see what i can get to you, i'd also like to but a bit of documentation together to make sure we are properly detailing any processing we did",9/24/15,1
lossyrob,nice!,9/24/15,1
anandthakker,@lossyrob I realized it might be better manners to send him a private note first before sharing his email.  I'll send the intro note once i hear back,9/24/15,1
anandthakker,heh,9/24/15,1
lossyrob,"that sounds good, thanks!",9/24/15,1
anandthakker,hey quick question:,9/24/15,1
anandthakker,https://github.com/hotosm/oam-server-tiler/blob/master/add-steps.sh#L11,9/24/15,1
anandthakker,"the last argument there, `s3://workspace-oam-hotosm-org/test-req3.json` -- is that equivalent to https://github.com/hotosm/oam-server-tiler/blob/master/test-req-full.json",9/24/15,1
anandthakker,?,9/24/15,1
lossyrob,yes,9/24/15,1
lossyrob,that_Ñés the request data that the chunk.py will parse out ,9/24/15,1
anandthakker,"oh i see - it's just a string argument, nothing specialized",9/24/15,1
lossyrob,yeah totally,9/24/15,1
anandthakker,@lossyrob is the SNS stuff important?,9/24/15,1
anandthakker,in general?,9/24/15,1
lossyrob,"it_Ñés SQS stuff, do I make a mention to SNS ? I was lookign to use SNS but I switched to SQS",9/24/15,1
lossyrob,"but it_Ñés just for communication about job status, not necessary to run jobs",9/24/15,1
anandthakker,ah yeah - https://github.com/hotosm/oam-server-tiler/blob/master/chunk/chunk.py#L30,9/24/15,1
lossyrob,whoops I need to push up changes,9/24/15,1
lossyrob,just pushed up the changes,9/24/15,1
anandthakker,cool ,9/24/15,1
anandthakker,@lossyrob what's the story with the instance groups?,9/24/15,1
mojodna,@anandthakker: http://docs.aws.amazon.com/ElasticMapReduce/latest/DeveloperGuide/InstanceGroups.html,9/24/15,1
mojodna,EMR provides 3 instance groups so that you can use spot instances for extra capacity without compromising an HDFS cluster made up of core instances,9/24/15,1
mojodna,"since there_Ñés no persistent FS here, the core instances can be spot instances",9/24/15,1
anandthakker,"oh, interesting",9/24/15,1
anandthakker,Thanks @mojodna ,9/24/15,1
mojodna,np,9/24/15,1
mojodna,"actually, in the OAM case, we have 3 reserved instances, so those can make up the core and then anything else we want to throw at it can be bonus (spot instances with very low bids)",9/24/15,1
lossyrob,we can_Ñét actually,9/24/15,1
mojodna,oh?,9/24/15,1
lossyrob,because there_Ñés something weird with it trying to use the HDFS on Task instance groups,9/24/15,1
lossyrob,and it failing because they don_Ñét have Takss,9/24/15,1
mojodna,"boo, ok.",9/24/15,1
lossyrob,yeah weirdly,9/24/15,1
lossyrob,"i didn_Ñét dig too deep, but when I tried to use Task groups as spot instances that failed because of HDFS errors",9/24/15,1
anandthakker,huh.  possibly Spark doing some caching/checkpointing behind the scenes?,9/24/15,1
anandthakker,"(I'm a total spark & EMR newb, so it's possible that that sentence is total garbage. heh.)",9/24/15,1
mojodna,"if i discover anything while @lossyrob is away, i_Ñéll post my findings here (and in hotosm/oam-server)",9/24/15,1
lossyrob,Spark does awesome caching/checkpointing,9/24/15,1
lossyrob,there_Ñés some _ÑÒcheckpointing_Ñù of shuffle data in shuffles between stages. Those are some heavier spark terms.,9/24/15,1
anandthakker,@lossyrob @mojodna https://github.com/developmentseed/spark-rasterio-seed,9/24/15,1
anandthakker,pulled out the tiler-specific logic to get down to a reusable boilerplate there,9/24/15,1
mojodna,"nice.  i did some chipping away at my version while on vacation_Ñ_haven_Ñét converged it yet, but if there_Ñés anything relevant, there will be a PR headed your way",9/24/15,1
anandthakker,oh sweet,9/24/15,1
mojodna,the main thing to look out for is a `Makefile` that puts the stuff that EMR pulls out of S3 in S3 before starting the jobs,9/24/15,1
anandthakker,oh nice -- i was just thinking i should automate that.  i'll hold off if you've already got it on the way,9/24/15,1
mojodna,"not yet, but was going to write it this afternoon",9/24/15,1
lossyrob,"there_Ñés a bunch of `.sh` scripts that do those steps, I also thought of chunking them into a Makefile that calls out to the scripts as to make it clearer how to interface with it. ",9/24/15,1
lossyrob,so I_Ñéll be looking for those changes :),9/24/15,1
pomadchin,"@all Guyz, are there any issues with 1.6.2 vs 1.6.3 api compatibility? mb smb knows",9/25/15,1
echeipesh,AFAIK should be no issues. Accumulo guys take version stability very seriosly.,9/25/15,1
pomadchin,I guess that issues can be only cross major version,9/25/15,1
anandthakker,@mojodna @lossyrob hey guys -- have you had any luck running a pyspark job on EMR with a script that needs a couple local imports?,9/25/15,1
anandthakker,"(the oam tiler, i saw, just references a single .py file on s3...)",9/25/15,1
mojodna,no.  i wound up propagating whatever would have been imported locally using `bootstrap.sh`,9/25/15,1
mojodna,the documentation around distributing zip files w/ local dependencies is pretty limited and i couldn_Ñét get it to work as expected,9/25/15,1
anandthakker,"ah.  that's what i was afraid of, heh.",9/25/15,1
anandthakker,i guess it'll be one big long file then :),9/25/15,1
mojodna,it might be more straightforward if you_Ñére not trying to distribute PIP deps (directory for each module),9/25/15,1
anandthakker,"eh -- this thing i'm doing is pretty quick and dirty anyway, so i'll just go with the copy-paste route for now.  ",9/25/15,1
mojodna,;-),9/25/15,1
anandthakker,i mean... it's friday afternoon,9/25/15,1
anandthakker,@mojodna okay here's one that's proving a bit of a weird doozy:  my input data for this spark job is a bunch of gzipped tiffs on s3,9/25/15,1
anandthakker,"rasterio doesn't seem to accept file contents directly (i.e., wants a file path or uri), but seems like gunzipping locally in the executor is gonna be problematic.",9/25/15,1
anandthakker,any ideas?,9/25/15,1
anandthakker,"(other than, you know, unzip it and re-upload to s3)",9/25/15,1
mojodna, `/vsicurl/vsigzip/http://s3.amazonaws.com/...` may work as an input for you,9/25/15,1
anandthakker,oh no way,9/25/15,1
mojodna,otherwise i_Ñéd download it and uncompress it as part of a map task,9/25/15,1
anandthakker,"yah i figured the latter, but then i'd have the actual file bytes, which i haven't yet found a way to feed to `rasterio`",9/25/15,1
mojodna,temp files ;-),9/25/15,1
anandthakker,that's safe from within a spark job?,9/25/15,1
anandthakker,i've been assuming i should stay away from the fs...,9/25/15,1
mojodna,sure.  use `mkstemp` (https://docs.python.org/2/library/tempfile.html#tempfile.mkstemp) so temp files from different tasks running on the same instance don_Ñét collide,9/25/15,1
mojodna,"fs is safe from within an individual `map` lambda / function, but don_Ñét assume that things will be available outside of that",9/25/15,1
mojodna,"for passing data between stages, numpy arrays (which rasterio can input/output) seem to be the best bet",9/25/15,1
mojodna,"if your rasters contain NODATA values, make sure that you_Ñére using masked arrays",9/25/15,1
anandthakker,ahh okay.  i was being overly cautious about the fs thing but makes sense that it's safe within a single map fn,9/25/15,1
mojodna,"no guarantee that you won_Ñét run out of disk, but is there ever?",9/25/15,1
anandthakker,hah true,9/25/15,1
anandthakker,@mojodna any chance you're around?,9/27/15,1
pomadchin,"guyz, found an interesting issue (mb my fault): https://github.com/geotrellis/geotrellis/blob/master/spark/src/main/scala/geotrellis/spark/io/accumulo/RasterRDDReader.scala#L44When I query the max spatial key, here appears and empty set exception",9/27/15,1
pomadchin,max means max row max col,9/27/15,1
pomadchin,"but, quering this key by tileReader is ok",9/27/15,1
pomadchin,"```java.lang.UnsupportedOperationException: empty.reduceLeft	at scala.collection.LinearSeqOptimized$class.reduceLeft(LinearSeqOptimized.scala:124)	at scala.collection.immutable.List.reduceLeft(List.scala:84)	at scala.collection.TraversableOnce$class.reduce(TraversableOnce.scala:195)	at scala.collection.AbstractTraversable.reduce(Traversable.scala:105)	at geotrellis.spark.io.accumulo.RasterRDDReader.read(RasterRDDReader.scala:44)	at geotrellis.spark.io.accumulo.AccumuloRasterCatalog.read(AccumuloRasterCatalog.scala:40)```",9/27/15,1
pomadchin,oh sry %) didn_Ñét noticed my grid bound clause in a query %) sunday evening,9/27/15,1
pomadchin,"hm very interesting, what_Ñés the difference? :```scalaval filters = new RasterRDDQuery[SpatialKey].where(Intersects(GridBounds(x, y, x, y)))val r = catalog.read[SpatialKey](layerId, filters)r.map { case (_, t) => t }.collect().head```vs. ```scalaval r = catalog.read[SpatialKey](layerId)r.filter{ case (k, _) => k.col == x && k.row == y }.map { case (_, t) => t }.collect().head``` ",9/27/15,1
dzeno,"Hi, kudos for making geotrellis. I am new here, and we would like to try geotrellis in production. Could you tell me please how safe it is to use current milestone release with Spark intergration in production? (we are mainly interested in your integration with Spark). Also is there any documentation we can check on working with geotrellis+spark? Thanks",9/27/15,1
pomadchin,"@dzeno all versions (even master) are _rather_ stable (I am using master or latest bintray release); though its api is not stable and there are possible strong changes (more info can be provided by @lossyrob , @echeipesh and other gt masters o:);documentation is rather bad (no such a thing :D just a couple of `README.md`_Ñés in each project root dir), but there is a nice test code coverage, so you can use it as an example, and there is a nice gt-admin repo, it is possible to use this project as a sample too",9/27/15,1
pomadchin,checkout bintray repo: https://bintray.com/azavea/geotrellis,9/27/15,1
dzeno,"@pomadchin Thanks a lot. Do you have any idea when we can expect 0.10 major release? Let me just give you basic scenario to see if I get how it works: If I want to do a range query (get some points in given long-lat), gt basically does the processing and stores in accumulo, where data is indexed. And when I do range query, I basically read from accumulo?",9/27/15,1
pomadchin,"@dzeno have no ideas about stable release date ): waiting for it so long;yes you read from accumulo if you use accumulo;it is also possible to use hadoop native and s3",9/27/15,1
pomadchin,ofcource with accumulo it will be faster (: ,9/27/15,1
pomadchin,"by the way, there is a cassandra branch by @allixender so there is a possible cassandra support in the near future",9/27/15,1
dzeno,@pomadchin thanks! ,9/27/15,1
pomadchin,"op find out issues ._. my hands curvity :D broked layer metaDatathough is it good that tile reader has no metadata restrctions, while range query has?btw :D gt needs metaData lenses mb to update it",9/28/15,1
echeipesh,"@dzeno, Spark is very much our primary focus right now. We are consciously choosing to make ""a good release"" rather than just _ÑÒa release_Ñù so the date is slipping a bit. Having said that there is only a couple of features we are targetting that are still not worked out: good multi-band support throughout the API, that_Ñés nearly done with #1182 and RDD partitioning control through operations, next up. I would expect to go into polish and release mode by Novembers end. We are dogfooding geotrellis internally on couple of projects even as we develop it so stability (from the standpoint of not blowing up : ) is a priority. API stability will get there soon. Bintray repo pomadchin linked is the best way to get binaries, we_Ñére building that for every PR.",9/28/15,1
echeipesh,"And yes, we index tiles in all our backends (S3, HDFS, Accumulo) and then will only fetch a required bonding box back to operate on. It wasn_Ñét clear if you are talking about vector or raster data for your use case though.",9/28/15,1
echeipesh,"@pomadchin using the where on RasterRDDQuery like `new RasterRDDQuery[SpatialKey].where(Intersects(GridBounds(x, y, x, y)))` will translate the query into specific ranges in underlying index and only fetch those tiles. The second way, where you produce the RDD and call `.filter` on it (spark function) will load the whole layer and then perform the filter discuarding the tiles that do not match.",9/28/15,1
echeipesh,"@pomadchin re: query restriction. Thats correct, when you do a query through the `.query` on a reader it will clip your query to stored metadata. This is because generating the query ranges and looking for them still has performance cost even if tiles are not there (some cost at least). It_Ñés a bit of an oversight that tile reader does not perform similar checks, but in that case there is no such cost since it_Ñés a direct key->value lookup.",9/28/15,1
anandthakker,"@mojodna @lossyrob FYI - I needed python bindings for GDAL in my EMR job and ran into trouble.  Now working on making & packaging a new build of gdal, v2.0.1.  Planning to update the boilerplate repo with it.",9/28/15,1
mojodna,@anandthakker: awesome.,9/28/15,1
mojodna,here are my notes on building it w/o python for AMZ linux: https://gist.github.com/mojodna/2f596ca2fca48f08438e,9/28/15,1
mojodna,probably should include the version of python it_Ñés built against (2.6 vs. 2.7)_ÑÓboilerplate includes environment settings that causes pyspark to use 2.7 (originally because i thought that might have been the source of problems),9/28/15,1
anandthakker,"sweet, thanks",9/28/15,1
anandthakker,ugh -- but I'm starting to understand why you avoided the Python bindings.  Ever gotten a `MemoryError` trying to `easy_install GDAL`?,9/28/15,1
mojodna,"actually, it was because i didn_Ñét need them ;-)  never waded into that mess, fortunately.",9/28/15,1
anandthakker,you are a lucky man,9/28/15,1
anandthakker,@mojodna https://gist.github.com/anandthakker/7f768079987e5243a78f,9/28/15,1
mojodna,@anandthakker: sweet!,9/28/15,1
pomadchin,got a suggestion with the read method: ,9/29/15,1
pomadchin,https://github.com/geotrellis/geotrellis/blob/master/spark/src/main/scala/geotrellis/spark/io/accumulo/RasterRDDReader.scala#L44,9/29/15,1
pomadchin,"Think, it is not an error, to give an empty result, though reduce makes impossible to get the empty RDD, so as the result, there would be thrown an exception and the query: `catalog.query[K](layerId).where(someConditionNotIntersectsIngestedData).toRDD` will thrown an exception. Or there was motivation to use `reduce` not `fold` ?",9/29/15,1
echeipesh,@pomadchin have you gotten that error? It shouldn_Ñét happen:,9/29/15,1
echeipesh,"```scalascala> import org.apache.spark._import org.apache.spark._scala> implicit val sc = SparkUtils.createLocalSparkContext(""local"",""hello"")2015-09-29 13:01:52.097 java[20554:6629355] Unable to load realm info from SCDynamicStoresc: org.apache.spark.SparkContext = org.apache.spark.SparkContext@607cbe65scala> val rdd = sc.parallelize(Array(1,2,3))rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[0] at parallelize at <console>:29scala> val rdds = List(rdd)rdds: List[org.apache.spark.rdd.RDD[Int]] = List(ParallelCollectionRDD[0] at parallelize at <console>:29)scala> rdds.reduce(_ union _)res2: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[0] at parallelize at <console>:29scala> res2.collect()res3: Array[Int] = Array(1, 2, 3)```",9/29/15,1
pomadchin,And on the empty set?,9/29/15,1
echeipesh,"durp, good point.  If you query outside of keyBounds you should am empty sequence of KeyBounds",9/29/15,1
pomadchin,yeeee,9/29/15,1
echeipesh,"thanks, fix will be here: https://github.com/echeipesh/geotrellis/commit/2626096f4276dce9c451c4cdf45d565d82afa33b",9/29/15,1
pomadchin,waiting for the api upgrade :),9/29/15,1
echeipesh,"Yes, I just need to adjust the ETL project and actually launch it on EMR to check that everything works correctly. Often unit-tests passing is not the same as everything working in production _Ñ_ for some reason :)",9/29/15,1
pomadchin,":D yes, always happens 'magic'",9/29/15,1
echeipesh,It would be nice if we could launch and EMR instance to do a bunch of integration tests for every PR _Ñ_ someday.,9/29/15,1
pomadchin,"btw, gt is planning to maintain accumulo 1.7.x?",9/29/15,1
pomadchin,"oh yes, seems to me google has this feature :D",9/29/15,1
echeipesh,"yep, gotta move with the times. At that point we_Ñéll have to break out the accumulo support into subproject so we can support 1.6 and 1.7 in parallel.",9/29/15,1
pomadchin,cool,9/29/15,1
mapadelphia,@anandthakker hey! update on the natl data processing for pop density....,9/30/15,1
mapadelphia,i'm very close to having all pop density calculated (2010 deccenial) by block (11.1 million) for all US!,9/30/15,1
mapadelphia,patience is a virtue :),9/30/15,1
mapadelphia,"anyway, i'll shoot an email when its done, need to figure out how to share the data... and i'll also see you at js.geo next week!",9/30/15,1
mapadelphia,cc: @lossyrob ,9/30/15,1
nicerobot,Anyone have a detailed comparison on Cassandra vs Accumulo specifically regarding spatial use-cases? I liked  the GeoMesa discussion of Accumulo but I've not yet found such depth regarding Cassanda (or other options).,10/2/15,1
pomadchin,"@nicerobot we all here waiting for it, think soon gt guyz will provide us comparison tables ",10/2/15,1
dzeno,"@echeipesh Thank you for your reply! In my example I was talking about the vector data. What I would like to know if it is possible to do with geotrellis is the following: Given the point (long, lat), I would like to get all the points in some given range around it. Hope that now is more clear what I want. Thanks !",10/5/15,1
echeipesh,"@dzeno at the moment GeoTrellis is focused on the raster processing, vector processing will come later. But either GeoWave (https://ngageoint.github.io/geowave/documentation.html) or GeoMesa (http://geomesa.github.io) got your use case covered (both of them index geometries in Accumulo and support nearest neighbor queries.",10/5/15,1
echeipesh,"@nicerobot I don_Ñét know of any current project that really uses Cassandra for spatial indexing. Currently the favoriate teqnique of doing spatial queries on Accumulo relies on SFC decomposition which requires a oredered index to work well. Something different would be required for Cassandra to take advantage of scalability given by hash partitioning. SimpleGeo had a project that used unbalanced zcurve partitined R+ tree (not sure if there is a good name for that structure) that is very interesting: https://www.youtube.com/watch?t=707&v=7J61pPG9j90, http://www.slideshare.net/mmalone/working-with-dimensional-data-in-distributed-hash-tables but it_Ñés gone into the aether now.",10/5/15,1
dzeno,@echeipesh Thanks! ,10/5/15,1
nicerobot,@echeipesh Thanks. It was my cursory conclusion of Cassandra too but we_Ñére just attempting some due dilligence to ensure we can address any questions raised by some folks hot for Cassandra.,10/5/15,1
echeipesh,"Cassandra is definitely attractive. But all the things that make it attractive also make the problem harder. I_Ñém itching to have a go at the problem myself, hopefully soon.",10/5/15,1
allixender,Heia  @nicerobot @dzeno earlier this year starting from a Facebook Academy student and I continued that in GSoC 2015 we've collected some links and documentation about the topic https://github.com/geotrellis/geotrellis/issues/1098,10/5/15,1
allixender,"As of now Cassandra with GeoTrellis uses Geotrellis indexing logic ( Hilbert-type/Z-curve numerical key index), mapped into Cassandra composite row keys. ",10/5/15,1
pomadchin,@allixender cassandra branch wolud be merged to master?,10/5/15,1
allixender,"Interesting was the TupleJump Star-Gate approach, implementing Lucene (Spatial4J) keys as Custom Cassandra secondary indices. However, the Github project last time I checked https://github.com/tuplejump/stargate-core was only working with Cassandra 2.0.x series, and I didn't have the time/skills to  get it upgraded to Cassandra 2.1.x series",10/5/15,1
allixender,"Hi @pomadchin, I have been keeping the Cassandra branch up  to date with master during GSoC but I got a little out of touch due to my studies... and latest GeoTrellis master progress will require also additions in the Cassandra branch",10/5/15,1
allixender,"it would be desirable to get the Cassandra branch into Master, but I guess for the sake of a proper robust 0.10.0 release they might leave cassandra out of master before that release",10/5/15,1
allixender,"I'll have some more time later this year again and keen to get the Cassandra Geotrellis up to speed again. I am using the current branch  snapshot for some personal small research projects with spark cassandra, but not on large scale :-p",10/5/15,1
nicerobot,@allixender Great. We_Ñéll take a look at that branch. Thanks.,10/5/15,1
echeipesh,Updated #1182 description: https://github.com/geotrellis/geotrellis/pull/1182 so far most of the testing is done and looks good. Anticipate merging this in tomorrow.,10/7/15,1
pomadchin,Geotools subproject is maintaining? Test failing with no unapply function for object (smth like): https://github.com/pomadchin/geotrellis/commit/e8ce12bf72d2670c4ea362f889ac2bfbacf2de8c,10/12/15,1
echeipesh,@pomadchin nice. Could you make that a PR against master please ?,10/13/15,1
pomadchin,"@echeipesh ofc, will make it",10/13/15,1
melvinvarkey,"[warn] 	::::::::::::::::::::::::::::::::::::::::::::::[warn] 	::          UNRESOLVED DEPENDENCIES         ::[warn] 	::::::::::::::::::::::::::::::::::::::::::::::[warn] 	:: org.scalaz.stream#scalaz-stream_2.10;0.7a: not found[warn] 	:::::::::::::::::::::::::::::::::::::::::::::: getting this error what to do ",10/14/15,1
pomadchin,And what are you doing? Just building from master?,10/14/15,1
melvinvarkey,yes building from master ! :(,10/14/15,1
melvinvarkey,i have tried to build it in intellij idea but no use getting the same warning ,10/14/15,1
melvinvarkey,i downloaded this jar and placed .ivy2/cache/org.scalaz.stream/scalaz-stream_2.10/jars,10/14/15,1
pomadchin,and what sub project?,10/14/15,1
pomadchin,"Hm never catched this issue, Mb this version was removed from scalaz bintray repo or smth else",10/14/15,1
melvinvarkey,"my project is to read and process a netcdf file, and need to write that in Scala ! ",10/14/15,1
melvinvarkey,"So i found Rob's project similar to mine, so tried to build the project to study",10/14/15,1
pomadchin,"mmm, so you build not a gt project? then just add bintray repo as a resolver```scalalibraryDependencies += ""Scalaz Bintray Repo"" at ""http://dl.bintray.com/scalaz/releases""```",10/14/15,1
pomadchin,"btw, there is a geotrellis bintray repo: ```""http://dl.bintray.com/azavea/geotrellis/""```",10/14/15,1
melvinvarkey,thank you ! :),10/14/15,1
pomadchin,oh typo* `resolvers +=...`,10/14/15,1
echeipesh,@pomadchin I wrote up some thoughts on making a generic layer updater and wrote it up in #1187. Let me know if that jives with what you_Ñéve been working on or if I_Ñéve missed something.,10/15/15,1
pomadchin,"@echeipesh great thanks, will check it",10/15/15,1
pomadchin,"Looks nice; I can try to begin smth to see will it be enough, and if it will be success mb will be possible to use it; +100 for the rdd format object, looks interesting ",10/15/15,1
pomadchin,"In order not to make write object too heavy, and better semantics",10/15/15,1
echeipesh,"It will make it a little heavy since it would handle read/write in one object. But that mirrors the way AvroFormats work, so probably not a bad idea. Also that would allow us to save it_Ñés class name in metadata and use that for reference and error checking.",10/15/15,1
echeipesh,I don_Ñét know if there is a better name for a combination of a reader and a writer :),10/15/15,1
pomadchin,Format is a supa variant :d play json lib has same namings too,10/15/15,1
pomadchin,Cool,10/15/15,1
pomadchin,"heh, found publishing problems (example):https://dl.bintray.com/azavea/geotrellis/com/azavea/geotrellis/geotrellis-gdal_2.11/",10/16/15,1
hectcastro,Having trouble spotting the problem.,10/16/15,1
lossyrob,"hey all, I_Ñém back from vacation_Ñ_still in Europe, going to be on European time, but I_Ñéll be back watching the channel et al.",10/19/15,1
pomadchin,@lossyrob yo! really was not here for a long time (:,10/19/15,1
lossyrob,yeah :) been traveling Spain for a bit on vacation. ,10/19/15,1
echeipesh,"@lossyrob @pomadchin bringing a conversation in here: For layer update feature, if ther user is trying to update a layer that does not exist_Ñ_ should the layer be created or should an error be thrown?",10/19/15,1
echeipesh,"my gut feeling is that it should be similar to SQL update, where it throws an error. But Upsert is useful API, so a little split on this issue.",10/19/15,1
lossyrob,eh,10/19/15,1
lossyrob,there should be functionality for checking if a layer exists,10/19/15,1
lossyrob,and then an error thrown for an update of no layer,10/19/15,1
pomadchin,@echeipesh thoughts are right; mb to have an alternative? `createOrUpdate` like some SQL dsls have,10/19/15,1
lossyrob,"if we do that enough that it really becomes a pain, then we can add that sugar",10/19/15,1
lossyrob,"I_Ñém thinking that in `ETL`, we should have another step in the `IngestPlugin`",10/21/15,1
lossyrob,"so instead of `load` bringing in the tiles and merging hem into the key type and container type, have source tiles that the user could potentially merge in a different way",10/21/15,1
lossyrob,"@echeipesh it seems like the ETL stuff is setup so that an `IntestPlugin` could be reading from a catalog, and an `IngestInputPlugin` would be something that would take a source and do tiling. Should there be implementations of a catalog input plugin for the different backend types, or at least a naming structure that differentiates what_Ñés there with the potential catalog backends? e.g. `S3IngestInput` as opposed to `S3Input`",10/21/15,1
echeipesh,"@lossyrob It_Ñés almost there, that_Ñés been a long term plan. But if you look at the types for the Accumulo ingest plugin you see that a lot of argumenst are straight up not required. For instance there is no reason to ask for layout scheme when you_Ñére going to be reading from a catalog. So there should be more work to create `InputPlugin` and some flow from params to it. But basically both `InputPlugin` and `IngestInputPlugin` have the same output type, which makes it composable.",10/21/15,1
pomadchin,"Hm looks ugly, an update fuction in a write object",10/24/15,1
lossyrob,@pomadchin @echeipesh so I_Ñém looking at the update function,10/26/15,1
lossyrob,and I_Ñém wondering about the `Format` approach,10/26/15,1
lossyrob,because the Format is just doing what readers and writers already do,10/26/15,1
lossyrob,and it_Ñés really just an update function we want,10/26/15,1
lossyrob,I was mentioning a bit ago that my preference would be an `update` function on the Writer,10/26/15,1
lossyrob,that takes an RDD of the appropriate type,10/26/15,1
lossyrob,"that overwrites any tiles that are already in the layer, adds new tiles, and updates the metadata",10/26/15,1
lossyrob,"and then the merging could happen at antoher stage. For example, you_Ñéd have the tiles you_Ñéd want to have updated. You_Ñéd use a reader to fetch the layer as it already is, perform a merge, and then call update on the writer",10/26/15,1
echeipesh,"So to write some code:```scalaval reader = S3LayerReader[SpatialKey, Tile, RasterRDD](""bucket"",""key"")val writer = S3LayerWriter[SpatialKey, Tile, RasterRDD](""bucket"", ""key"", ZCurveIndexMethod)val update: RasterRDD[SpatialKey] = ??? //got it this from somewhereval id: LayerId = ??? // Id of layer we are trying to updateval existing = reader.query(id).where(Intersects(update.keyBounds)).toRDDval update2 = existing merge updatewriter.update(id, update2)```That_Ñés a bit of work right there in a middle that_Ñés just book-keeping, I_Ñém not thrilled with that.",10/26/15,1
lossyrob,what do you mean by book-keeping?,10/26/15,1
echeipesh,"getting the keybounds, using those key bounds to query for layer to update, merging the layer. The last is potentially very variable, but the first two are always the same.",10/26/15,1
lossyrob,then there can be some sugar functions in front of it,10/26/15,1
lossyrob,reader.query(id).where(Intersects(update)) merge update,10/26/15,1
echeipesh,"Intersects on RasterRDD, yeah, thats not bad.",10/26/15,1
lossyrob,and then if we really wanted we could throw sugar in front of that too,10/26/15,1
echeipesh,My feeling `update` probably should be on writer because it shares so much logic.,10/26/15,1
lossyrob,"```scalaval writer = ???writer.mergeUpdate(reader, id) { tilesToUpdate => tilesToUpdate merge update }```",10/26/15,1
lossyrob,yeah I_Ñém thinking the same thing,10/26/15,1
lossyrob,"`writer.mergeUpdate(reader, id)(_ merge update)` seems pretty short",10/26/15,1
echeipesh,"I_Ñéd be ok with that actually being a function on writer, not just sugar",10/26/15,1
lossyrob,well it would expand to the same thing you wrote,10/26/15,1
lossyrob,"so it would still be sugar, just defined on writer",10/26/15,1
echeipesh,"One weird part, and this could just be naming, is that writer already has `keyIndexMethod`, which we have to disregard on update (and use the method in attribute store).",10/26/15,1
lossyrob,that is weird,10/26/15,1
lossyrob,why is that not an argument to write?,10/26/15,1
echeipesh,"another potential point is that you could be reading tiles from reader that_Ñés not actually connected to the wrighter, like from S3 and update to hadoop.",10/26/15,1
echeipesh,"it would break the `Writer.write(key, value)`contract, which is quite nice.",10/26/15,1
echeipesh,Plus it would make it easier to remove if/when we decide to standardize on one SFC.,10/26/15,1
lossyrob,"I don_Ñét think we_Ñéll ever want to pin down only one SFC, since that could be used to optimize say against focal operations",10/26/15,1
echeipesh,"Well, I think naming that constructor argument _ÑÒdefaultKeyIndexMethod_Ñù might make that clearlier.",10/26/15,1
echeipesh,@pomadchin `LayerFormat` almost feels like it wants to be a type class that abstracts over reading/writing/updating for some types to some backend. Where you thinking about particular use case for that from client code?,10/26/15,1
echeipesh,,10/26/15,1
lossyrob,yeah. `defaultKeyIndexMethod` might make sense,10/26/15,1
lossyrob,though it seems like it might deserve a bit of separation between what has the update method and what is actually going to write the layer (producing a Writer of the nice trait implementation),10/26/15,1
lossyrob,it also has some arguments that don_Ñét make sense in the update case,10/26/15,1
lossyrob,"```scalabucket: String,    keyPrefix: String,    clobber: Boolean = true```",10/26/15,1
lossyrob,(in S3_Ñés case),10/26/15,1
lossyrob,wait nevermind,10/26/15,1
echeipesh,"It_Ñés true, all of those would be read from existing attribute store.",10/26/15,1
lossyrob,oh right. what happens if the attribute store points at a different catalog,10/26/15,1
lossyrob,then it_Ñés not really an update,10/26/15,1
echeipesh,You mean that attributeStore that is contained by the writer?,10/26/15,1
lossyrob,the attribute store contained by the reader,10/26/15,1
lossyrob,points to a specific bucket and such,10/26/15,1
lossyrob,"like you were saying, s3 to hadoop is possible",10/26/15,1
lossyrob,but that_Ñés not an update,10/26/15,1
echeipesh,"I_Ñéd do the reading from `writer.attributeStore`, but that doesn_Ñét change the source of the tiles. Could be a user-be-ware scenario, can_Ñét raelly think of a way to forbid it.",10/26/15,1
echeipesh,Actually we could read both the reader and writer attributeStores and check that the location matches.,10/26/15,1
echeipesh,Throw out a warning if it is not the same type or location is not the same. ,10/26/15,1
lossyrob,warning and then continue with a plain write?,10/26/15,1
echeipesh,"Not sure, it could be a legitimate case where you are doing cross backend update. It would be weird, but why cut that use case if it can be done predictably?",10/26/15,1
echeipesh,add a _ÑÒforce: Boolean = false_Ñù argument on `mergeUpdate` just for this case ?,10/26/15,1
lossyrob,eh. I guess I_Ñém confused.,10/26/15,1
lossyrob,"but the writer has an attribute store, and is going to be pulling out the metadata for that layer, right",10/26/15,1
lossyrob,and updating it with the new metadata,10/26/15,1
echeipesh,correct,10/26/15,1
lossyrob,"so if you called update on a layer that didn_Ñét exist, then it seems like that would be an error",10/26/15,1
echeipesh,"Also, the reader attributeStore needs to be updated somehow, but that_Ñés different issue.",10/26/15,1
echeipesh,"I_Ñém talking about the case where layer exists both in reader and writer but reader is not pointing to the same format as the writer. So the update COULD happen without error, it_Ñés just weird.",10/26/15,1
lossyrob,what do you mean format?,10/26/15,1
echeipesh,"writer on s3 and reader on hadoop or writer on s3(bucket1, prefix) and reader is on s3(bucket2, prefix)",10/26/15,1
lossyrob,"well, the reader is done by that time",10/26/15,1
lossyrob,"you read out tiles, and then do stuff",10/26/15,1
lossyrob,"and then later, you write them to the later to update them",10/26/15,1
lossyrob,so it doesn_Ñét matter where they came from,10/26/15,1
echeipesh,"not if it_Ñés passed to `writer.mergeUpdate(reader, id) { tilesToUpdate => tilesToUpdate merge update }`",10/26/15,1
lossyrob,in the sugared `mergeUpdate` it just does it in a way where it_Ñés working with the same layer and same catalog,10/26/15,1
lossyrob,"oh right, you pass the layer",10/26/15,1
lossyrob,"but even there, it_Ñés not a hard requirement that they are the same",10/26/15,1
echeipesh,not at all,10/26/15,1
lossyrob,i guess it would be quite weird,10/26/15,1
lossyrob,because the locations could be different but for same layerId,10/26/15,1
echeipesh, exactly. I_Ñéd say either don_Ñét worry about it or add a `force` if two `attributeStores`don_Ñét contain the same header for the layer id. Preference on the former at the moment. ,10/26/15,1
lossyrob,"does sort of call for a backend thing that can point to say a bucket and a key, provide both a writer and a reader, and just have a plain update on it for the id_Ñ_.something like, maybe a catalog_Ñ_ahh!",10/26/15,1
pomadchin,"hm yep, just an update job with `Format` object looks much prettier as a DSL though also though about update function in writer, and passing add reader looks not very ._.",10/26/15,1
pomadchin,yes `Format` is heavy too,10/26/15,1
echeipesh,"sorry, I don_Ñét know this word _Ñ_ `cat-alog` Something from youtube ?",10/26/15,1
echeipesh,I think it_Ñéd be possible to have something that extends both reader and writer? Why did we discount that?,10/26/15,1
lossyrob,"that_Ñés the Format, which is just delegating to a reader and writer you pass in",10/26/15,1
echeipesh,but do it without delegation?,10/26/15,1
lossyrob,then always have a writer when you have a reader?,10/26/15,1
echeipesh,well yeah,10/26/15,1
lossyrob,always have to put in the writer stuff when you need a reader,10/26/15,1
lossyrob,feels right to allow them to be separate,10/26/15,1
echeipesh,"one other thing I mentioned, LayerWriter has everything* it needs to construct a LayerReader",10/26/15,1
lossyrob,always?,10/26/15,1
echeipesh,* - nearly,10/26/15,1
lossyrob,or just under the backends we have now,10/26/15,1
echeipesh,"basically always, I am having hard time thinking of a case it would not be",10/26/15,1
echeipesh,If anything writer always needs to know more about backend than the reader,10/26/15,1
lossyrob,I_Ñéd still like to be able to update from tiles that completely originate outside of the layer,10/26/15,1
lossyrob,i.e. do the desugared example,10/26/15,1
echeipesh,"so have `.update` and `.mergeUpdate`, the later would create a reader?",10/26/15,1
lossyrob,"so if the update necessarily creates a reader, i think that would be restricting",10/26/15,1
echeipesh,"anyway, these would need to be fused, and then you can always  make a reader: https://github.com/geotrellis/geotrellis/blob/master/spark/src/main/scala/geotrellis/spark/io/s3/S3LayerWriter.scala#L29",10/26/15,1
lossyrob,"I think working towards a `writer.update(layerId, rdd: BoundedRDD[(K, V)])` would make sense as a first step",10/26/15,1
lossyrob,what would need to be fused?,10/26/15,1
echeipesh,"`RDDWriter[K, V] and RDDReader[K, V]` into `RDDFormat[K, V]`",10/26/15,1
lossyrob,why are we trying to fuse them now? didn_Ñét we just pull them apart lol,10/26/15,1
echeipesh,"the layer readers, not but RDD reader, I don_Ñét see why not, *shrug* ",10/26/15,1
lossyrob,"I_Ñéd be against fusing, if not for just to try to stop the cycle of fuse and defuse",10/26/15,1
lossyrob,"reading and writing seem to be useful as separate concerns, so let_Ñés keep them that way. and then figure out what cases they need to be combined and only combine them there",10/26/15,1
echeipesh,"sounds like another +1 to `writer.mergeUpdate(reader, id) { tilesToUpdate => tilesToUpdate merge update }` ?",10/26/15,1
lossyrob,"I think that_Ñés sugar, so I_Ñéd go with nailing `writer.update` first",10/26/15,1
lossyrob,and making sure that works and makes sense,10/26/15,1
echeipesh,agreed,10/26/15,1
lossyrob,@pomadchin thoughts?,10/26/15,1
pomadchin,would not it be an overhead to init _ÑÉreader' inside writer? ,10/26/15,1
lossyrob,"it would, if you were doing multiple updates",10/26/15,1
lossyrob,but we_Ñére stepping away from that now,10/26/15,1
lossyrob,and talking about an API like what Eugene put above,10/26/15,1
lossyrob,I_Ñéll restate it,10/26/15,1
pomadchin,"kk, looks not bad lets start with `writer.update(layerId, rdd: BoundedRDD[(K, V)])`",10/26/15,1
lossyrob,"```scalaval reader = S3LayerReader[SpatialKey, Tile, RasterRDD](""bucket"",""key"")val writer = S3LayerWriter[SpatialKey, Tile, RasterRDD](""bucket"", ""key"", ZCurveIndexMethod)val update: RasterRDD[SpatialKey] = ??? //got it this from somewhereval id: LayerId = ??? // Id of layer we are trying to updateval existing = reader.query(id).where(Intersects(update)).toRDDval merged: BoundedRDD[(SpatialKey, Tile)] = existing merge updatewriter.update(id, merged)```",10/26/15,1
lossyrob,cool. I think that scopes it to what the task is pretty well. Then figuring out nicety of API can happen afterwards,10/26/15,1
pomadchin,"kk, will remove this fat Format; though tested that it works with Hadoop and with Accumulo ",10/26/15,1
echeipesh,"nice, probably means the update on writer is going to work just as well.",10/26/15,1
lossyrob,nice. The components of the `update` function should basically be the same,10/26/15,1
pomadchin,yep,10/26/15,1
lossyrob,"but make sure to keep it around in `git` just in case things take a left turn, as per usual :)",10/26/15,1
pomadchin,:D ok,10/26/15,1
pomadchin,"hm, seems to me smth is missing here:```scalaval existing = reader.query(id).where(Intersects(update)).toRDDval merged = existing merge updatewriter.update(id, merged)```",10/26/15,1
pomadchin,metadata! ,10/26/15,1
pomadchin,"so mb rdd merge function should encapsulate metadata merging?or using this explicit `update` user going explicitly write metadata? ",10/26/15,1
pomadchin,ouch don't read it :d my night foughts it's all clear ,10/26/15,1
pomadchin,th * though questions about meta are actual,10/27/15,1
lossyrob,I wasn_Ñét aware that we didn_Ñét have bounds on the BoundRDD yet,10/27/15,1
lossyrob,https://github.com/geotrellis/geotrellis/blob/master/spark/src/main/scala/geotrellis/spark/BoundRDD.scala#L30,10/27/15,1
lossyrob,"but once we do, that_Ñés really all the metadata we need, no?",10/27/15,1
pomadchin,hmm,10/27/15,1
pomadchin,the `MetaData` (`RasterMetaData`) exactly should be updated,10/27/15,1
lossyrob,updated with what new information?,10/27/15,1
lossyrob,"the rastermetadata should be updated in the store for sure. but what new information we need from the update RDD (the set of tiles we want to update the layer with), I don_Ñét think we need that to be a whole RasterRDD[K]",10/27/15,1
lossyrob,"we just need the RDD[(K, V)] and the keybounds",10/27/15,1
lossyrob,which is what BoundRDD should be/will be,10/27/15,1
pomadchin,hm,10/27/15,1
pomadchin,and what about extent?,10/27/15,1
lossyrob,the extent follows the grid bounds,10/27/15,1
lossyrob,i think. let me check,10/27/15,1
pomadchin,"```scalaattributeStore.writeLayerAttributes(id, existingHeader, combinedMetaData, combinedKeyBounds, existingKeyIndex, existingSchema)```",10/27/15,1
lossyrob,yeah,10/27/15,1
lossyrob,so the extent gets carried around with the metadata,10/27/15,1
lossyrob,"and is really derived from the keybounds, no?",10/27/15,1
lossyrob,hmm.. I guess on ingest it_Ñés read from the raw tiles,10/27/15,1
lossyrob,so what happens if we want to update with an arbitrary set of tiles? Should the extent grow to encompass new tiles? or should an extent be passed in?,10/27/15,1
pomadchin,yes and on in the Write object it writes metadata got from the Container,10/27/15,1
pomadchin,think extent should be changed if the update rdd is wider than already ingested,10/27/15,1
pomadchin,or somehow intersects it,10/27/15,1
lossyrob,"right, but changed by the derived extent from the KeyBounds?",10/27/15,1
lossyrob,or are we going to need an explicitly stated extent?,10/27/15,1
lossyrob,"e.g. one from RasterMetaData, or one passed in by the user",10/27/15,1
pomadchin,hm,10/27/15,1
lossyrob,"I can imagine instances where really I_Ñéd just want to be able to work with part of a layer as an `RDD[(SpatialKey, Tile)]`, and eventually update the layer with that",10/27/15,1
lossyrob,which would maybe have to derive the KeyBounds from the RDD,10/27/15,1
lossyrob,and then the extent from the KeyBounds,10/27/15,1
lossyrob,"how about that? having an overload that takes a `RDD[(K, V)]` that derives the keyboudns and the extent, an over overload that takes `BoundRDD[(K, V)]` that already has keybounds and derives the extent, and one that takes `RasterRDD[K]` and then the function doesn_Ñét have to compute the keybounds or the extent ",10/27/15,1
pomadchin,hm think that makes sense,10/27/15,1
lossyrob,@echeipesh what needs to be done to get keybounds on BoundRDD? Is there potential for doing that before the whole partitioner feature?,10/27/15,1
lossyrob,be back in a bit,10/27/15,1
pomadchin,(:,10/27/15,1
pomadchin,"if it is a separate pr, it is possible mb to define a generic merge on `Container with RDD[(K, V)]` type, so the result will be merged explicit metaData, not derived from the keyBounds; and after that make same with the `BoundRDD` type, with an overloaded constructor",10/27/15,1
lossyrob,Can we talk through the use case for a second?,10/27/15,1
lossyrob,The new tiles that are coming in,10/27/15,1
lossyrob,"do they have MetaData associated iwth them? if so, how?",10/27/15,1
lossyrob,"Do you do an _ÑÒingest_Ñù (without sink) on the RDD of new tiles, and then you want to call update instead of sink?",10/27/15,1
pomadchin,"yes just an ingest, but updating",10/27/15,1
lossyrob,ok. then it makes sense to just go with satisfying that use case for now,10/27/15,1
lossyrob,"and later we can add the ability to update with a `BoundRDD[(K, V)]` and `RDD[(K, V)]` later",10/27/15,1
pomadchin,"(' tiles inside, already ingested', ' tiles outside, not ingested') ~~~> ('update already ingested layers with the new, outside tiles') ; so there comes out that the merge tiles function can be different, not only mosaic, but overlaping, etc (mb makes sense)",10/27/15,1
pomadchin,":D so interesting, want to make with BoundRDD too :d so will start now, think @echeipesh has some thoughts",10/27/15,1
lossyrob,"ok, here_Ñés a thing",10/27/15,1
lossyrob,"if you_Ñére ingesting from raw tiles for the update, then you_Ñére going to want the layout scheme for the layer",10/27/15,1
pomadchin,"eh, yes; would be fail with different schemes",10/27/15,1
lossyrob,writing some faux client code...,10/27/15,1
pomadchin,so anyway this thing should be handled in the update function,10/27/15,1
lossyrob,let_Ñés figure out how to handle it in client code and then see where to wrap things in a function,10/27/15,1
lossyrob,we don_Ñét store a layoutScheme with the layer,10/27/15,1
lossyrob,oh but there_Ñés an overload of RasterMetaData that doesn_Ñét need it,10/27/15,1
lossyrob,"ok, let_Ñés take a look at this and see what it tells us",10/27/15,1
lossyrob,"```scalaval bucket = ???val prefix = ???val layerId = ???val attributeStore = new S3AttributeStore(bucket, prefix)val existingMetaData = attributeStore.cacheRead[RasterMetaData](layerId, AttributeStore.Fields.metaData)val crs = existingMetaData.crsval layout = existingMetaData.layout// Need to get the correct key index method. Currently we can't.// This would necessitate having a ""KeyIndexMethod"" field on KeyIndex[K]val keyIndex = attributeStore.cacheRead[KeyIndex[SpatialKey]](layerId, AttributeStore.Fields.keyIndex)val keyIndexMethod = keyIndex.method //doesn't existval sourceTiles: RDD[(IngestKey, Tile)] = ???val newMetaData = RasterMetaData.fromRdd(sourceTiles, crs, layout) { key: IngestKey => key.projectedExtent.extent }val tiles = sourceTiles.tile[K](newMetaData, NearestNeighbor)val reader = new S3LayerReader[SpatialKey, Tile, RasterRDD](attributeStore, new S3RDDReader[SpatialKey, Tile])val existing = reader.query(id).where(Intersects(tiles)).toRDDval merged: BoundedRDD[(SpatialKey, Tile)] = existing merge tilesval writer = S3LayerWriter[SpatialKey, Tile, RasterRDD](attributeStore, new S3RDDWriter[SpatialKey, Tile], keyIndexMethod, bucket, prefix)writer.update(layerId, tiles)```",10/27/15,1
lossyrob,one question this brings about is about the KeyIndex,10/27/15,1
lossyrob,what happens if we are updating with tiles that go outside the KeyIndex_Ñés bounds?,10/27/15,1
lossyrob,"I think we_Ñéd have to throw an exception. and that_Ñés the KeyBounds that the index was created with, not the KeyBounds of the layer",10/27/15,1
lossyrob,do we store the KeyBounds of the KeyIndex anywhere?,10/27/15,1
pomadchin,"```scalaval decompose = (bounds: KeyBounds[K]) =>        existingKeyIndex.indexRanges(bounds).map { case (min, max) =>          new AccumuloRange(new Text(long2Bytes(min)), new Text(long2Bytes(max)))        }```we calc it",10/27/15,1
lossyrob,?,10/27/15,1
lossyrob,That_Ñés for index ranges of other key bounds,10/27/15,1
lossyrob,but I mean the key bounds of the entire indexed space,10/27/15,1
pomadchin,oh ye in metadata document,10/27/15,1
pomadchin,"```scalaattributeStore.readLayerAttributes[AccumuloLayerHeader, MetaDataType, KeyBounds[K], KeyIndex[K], Schema](id)```",10/27/15,1
lossyrob,That key bounds is the key bounds of the layer,10/27/15,1
lossyrob,not of the index space,10/27/15,1
pomadchin,yes,10/27/15,1
lossyrob,they can be different,10/27/15,1
lossyrob,"and if they aren_Ñét, then the key index would have the keybounds of the layer, and if you try to update tiles outside of the layer keybounds, then what happens?",10/27/15,1
pomadchin,"seems to me in this case, all layer should be reindexed?",10/27/15,1
lossyrob,yeah. that or an error thrown. not really another way to handle it,10/27/15,1
lossyrob,so now I_Ñém trying to figure out how to get the keybounds of the index space,10/27/15,1
lossyrob,because you could have been smart when ingesting,10/27/15,1
lossyrob,and known that you are going to actually update the layer to a larger area later,10/27/15,1
lossyrob,the the KeyIndex you used had a bigger key bounds than the original layer,10/27/15,1
pomadchin,and to preset the the bounds,10/27/15,1
lossyrob,"so if you did that, we still need to check that the KeyBounds of the index space is not violated",10/27/15,1
lossyrob,"so the KeyBounds of the layer (and the one in the attribute store) would still be the smaller one of the RDD, but the index space would have larger bounds",10/27/15,1
lossyrob,and this line actually doesn_Ñét allow what I_Ñém talking about,10/27/15,1
lossyrob,https://github.com/geotrellis/geotrellis/blob/master/spark/src/main/scala/geotrellis/spark/io/s3/S3LayerWriter.scala#L51,10/27/15,1
lossyrob,so we_Ñéd need to figure out a way to be able to do that,10/27/15,1
lossyrob,"I gotta switch spots, talk to you later",10/27/15,1
pomadchin,kk,10/27/15,1
pomadchin,"seems `_.indexRanges(bounds)` solves the problem",10/27/15,1
pomadchin,"if `(min, max)` pair is out of the existing bounds the would be weird thing",10/27/15,1
lossyrob,what bound are you comparing them to?,10/27/15,1
echeipesh,"@pomadchin @lossyrob I think a pretty convincing argument could be made to remove extent from `RasterMetaData`, so that would removes one concern. We will have NoData in the edge tiles, so it_Ñés not clear why extent for the data would be really useful. As far as KeyBounds, they can be collected using `Boundable` type class until they are appear on `BoundedRDD`. Also, I don_Ñét know if you want to consider this but some Indexing schemes aren_Ñét concerned by KeyBounds, for instance Z index will index the same regardless of what bounds are.",10/27/15,1
lossyrob,"Hilbert is though, and I_Ñém still convinced we_Ñéll switch to that after we start cluster testing in earnest",10/27/15,1
lossyrob,"also there_Ñés no way to pass in a different keybounds to index than the layer being saved, which is a problem that we have to consider 1) for updating and also 2) for accumulo writing to the same table",10/27/15,1
echeipesh,"Possibly, just pointing that case out for consideration.",10/27/15,1
lossyrob,true. Z index would probably be fine even with the bounds not being considered,10/27/15,1
echeipesh,"Right, so either it_Ñés magically ok, as in Z curve. Or it_Ñés invalid if out of bounds, as in Hilbert.",10/27/15,1
echeipesh,"pomadchin tested that, Z curve is def fine without considering init bounds.",10/27/15,1
lossyrob,"ok, cool-ish",10/27/15,1
lossyrob,half cool ha,10/27/15,1
pomadchin,not sure about ZCurve(!),10/27/15,1
lossyrob,quater-cool,10/27/15,1
pomadchin,:D,10/27/15,1
lossyrob,either way we have to figure out how we want to think about the keybounds of the index space as independant from the keybounds of the layer,10/27/15,1
pomadchin,"): now testing to load rather big maps on a data set (2000 - 2015 years, space time key)",10/27/15,1
pomadchin,"but, indexing every 8 days",10/27/15,1
pomadchin,seems to me that only rowmajor index works correct with out of bounds keys,10/27/15,1
echeipesh,"Interesting, how does it fail? key bounds aren_Ñét even part of construction for either zcruve indicies: https://github.com/geotrellis/geotrellis/blob/master/spark/src/main/scala/geotrellis/spark/io/index/zcurve/ZSpatialKeyIndex.scala#L7",10/27/15,1
echeipesh,https://github.com/geotrellis/geotrellis/blob/master/spark/src/main/scala/geotrellis/spark/io/index/zcurve/ZSpaceTimeKeyIndex.scala#L22,10/27/15,1
pomadchin,"think the problem can be here ```val decompose = (bounds: KeyBounds[K]) =>        existingKeyIndex.indexRanges(bounds).map { case (min, max) =>          new AccumuloRange(new Text(long2Bytes(min)), new Text(long2Bytes(max)))        }```",10/27/15,1
pomadchin,ouh,10/27/15,1
pomadchin,"```scalaval getRowId = (key: K) => index2RowId(existingKeyIndex.toIndex(key))```",10/27/15,1
pomadchin,"using on a cluster a lil bit diff version (where appeared err), will check out everyting",10/27/15,1
echeipesh,"kk, I_Ñém not seeing a possible problem there. I might blame the date chunker function since you say you_Ñére using spacetime index?",10/27/15,1
pomadchin,:D mb mb but noticed that weird behaivour only on my mac %),10/27/15,1
pomadchin,"btw, liked that tip, about predefining keyBounds ranges",10/27/15,1
pomadchin,"it will be useful to have a possibility for hilbert, to have a layer with ranges for example  for 20 years; or to have some job, with modifying exist layers and reindexing tiles, in order to win time during updating",10/27/15,1
echeipesh,tbh I think _ÑÒre-indexing_Ñù tiles can only really exist on Accumulo and you still have to be super-careful about interuptions. ,10/27/15,1
pomadchin,+,10/27/15,1
lossyrob,"well, reindexing for say S3 would be moving files around (or deleting and writing). Which would be a pain, but it_Ñés possible",10/27/15,1
echeipesh,My faith in the process actually completing and not _ÑÒexiting_Ñù and losing state on regular basis is pretty low : ),10/27/15,1
echeipesh,"Oh, maybe we can have red-green layer update? Where we keep two indexed schemes and switch between them once the udpate is done. hehe",10/27/15,1
pomadchin,sounds safe :D,10/27/15,1
pomadchin,more safe,10/27/15,1
lossyrob,yeah.,10/27/15,1
lossyrob,"It would mean reading the whole layer, merging in the new tiles, and then writing with the new index",10/27/15,1
lossyrob,and then deleting layers,10/27/15,1
lossyrob,which is a feature we don_Ñét have,10/27/15,1
lossyrob,which we could use,10/27/15,1
pomadchin,hm deleting is a nice feature,10/27/15,1
pomadchin,a like deleting tiles o: lots of space after that appears,10/27/15,1
lossyrob,"yeah, except when it_Ñés on S3 and you_Ñére paying for the deletes",10/27/15,1
pomadchin,)`: i got no S3,10/27/15,1
lossyrob,like when I moved a lot of data to Glacier from S3_Ñ_cost soooo much money in S3 deletes,10/27/15,1
echeipesh,"S3: ""You pay on the way in, you pay on the way out. Muhahahaha""",10/27/15,1
echeipesh,"Hmm, would you still have to pay if you set deletion policy and let the cleanup cycle handle it ?",10/27/15,1
echeipesh,rather expiration policy,10/27/15,1
lossyrob,yup,10/27/15,1
lossyrob,I sent it to glacier via the lifecycle policy,10/27/15,1
lossyrob,oh wait. hmm I dunno about deleting,10/27/15,1
lossyrob,delete lifecycle. seems like you would tho,10/27/15,1
lossyrob,oh wait,10/27/15,1
lossyrob,it_Ñés not delete,10/27/15,1
lossyrob,It_Ñés Glacier Archive requests,10/27/15,1
lossyrob,deletes are free,10/27/15,1
pomadchin,nice,10/27/15,1
pomadchin,"hm, so what_Ñés the plan? o: ",10/27/15,1
lossyrob,lol,10/27/15,1
lossyrob,umm,10/27/15,1
lossyrob,about which part?,10/27/15,1
lossyrob,sorry I_Ñém between this and a bunch of other stuff,10/27/15,1
pomadchin,yep ,10/27/15,1
pomadchin,about an upate pr,10/27/15,1
lossyrob,well yeah. but what_Ñés the current point where we need a plan?,10/27/15,1
lossyrob,We could do the code that I put above,10/27/15,1
pomadchin,"hm, we found out that we need to remove meta data, that there is a problem with hilbert, and not complete BoundedRDD ",10/27/15,1
lossyrob,so the client code I put above,10/27/15,1
lossyrob,"we could do that, if the keybounds of the index space are not violated",10/27/15,1
lossyrob,which means that we aren_Ñét updating tiles that are outside of the layer,10/27/15,1
pomadchin,kk,10/27/15,1
lossyrob,and so we_Ñéd have to either reindex when the bounds are changed,10/27/15,1
lossyrob,which seems like a bunch of work,10/27/15,1
lossyrob,or require that the index bounds be set big enough on the layer,10/27/15,1
lossyrob,"we need a delete layer feature, which can be separate but is pretty important. that way re-indexing is just loading up the layer, merging in whatever, and then saving it a new layer",10/27/15,1
lossyrob,perhaps we need a rename operation along with a delete operation,10/27/15,1
lossyrob,"but as a first step, nailing the case where the bounds are not violated",10/27/15,1
lossyrob,and throwing an exception if they are,10/27/15,1
pomadchin,got it,10/27/15,1
lossyrob,might be the first good nudge along this path,10/27/15,1
pomadchin,"seems zcurve is ok; though  special intersection should be defined for the keybounds (space time case), and not to get intersection cube, but some plane, for a certain time",10/27/15,1
pomadchin,"or it should be as a cube o: anyway it is not a problem, and easy redefining on the client side; so the only ingest problem can be really with hilbert",10/27/15,1
pomadchin,o:,10/27/15,1
pomadchin,even on the client side**,10/28/15,1
lossyrob,"ok. good to know the zcurve works, but that_Ñés sort of incidental to the implementation; just by reading the code it seems like it could easily not work (and the hilbert curve doesn_Ñét have an implementation that makes it work).",10/28/15,1
lossyrob,What do you mean by the keybounds intersection need? I_Ñém not clear on what you meant by thta,10/28/15,1
lossyrob,@echeipesh this might be of interest to your research into spatial joins in Spark: http://spatialhadoop.cs.umn.edu/publications/p1602-eldawy.pdf,10/28/15,1
lossyrob,"has a section, ""3.3 Spatial Join Performance""",10/28/15,1
pomadchin,"@lossyrob ment smth like:```scala// for SpaceTimeKeydef maxSpatialBound(a: SpaceTimeKey, b: SpaceTimeKey): Option[SpaceTimeKey] = {      if (a.time == b.time) Some(SpaceTimeKey(math.max(a.col, b.col), math.max(a.row, b.row), a.time))      else None    }```",10/28/15,1
lossyrob,what is that used for?,10/28/15,1
pomadchin,"just a thought about generic ingest, or i_Ñém missing smth?if there ingested layers for years 2010, 2011, 2013 and we want to update 2011 year, just a KeyBounds intersection will not give us a cube of keys?",10/28/15,1
lossyrob,KeyBounds would be 3 dimensional,10/28/15,1
lossyrob,"so if the keybounds is just in one time value, then the intersection should reflect that",10/28/15,1
pomadchin,"yes see it, sorry dealing with some other stuff too %)",10/28/15,1
echeipesh,"thanks lossyrob, looks like good paper.",10/28/15,1
lossyrob,re: your focal PR,10/28/15,1
lossyrob,https://github.com/geotrellis/geotrellis/pull/1194/files#diff-d80862d44602585532b32292b32ad8fbR31,10/28/15,1
lossyrob,"I wonder what would be more effective, a `cogroup` or a `groupByKey`",10/28/15,1
lossyrob,p.s. more papers here: http://spatialhadoop.cs.umn.edu/publications.html,10/28/15,1
lossyrob,"groupByKey would require you to figure out the target key, but doesn_Ñét combine two RDDs",10/28/15,1
lossyrob,"and then you drop the equals check for `if !(c == col && r == row) && bounds.contains(c, r)`",10/28/15,1
lossyrob,"and perhaps instead of mapping the key into the `(K, V)` element of the result, map `((c - col, r - row), tile)`",10/28/15,1
lossyrob,"so `(0, 0)` would be the target key in the `SeqTileNeighbors.fromKeys`",10/28/15,1
lossyrob,naw mean?,10/28/15,1
echeipesh,"not yet, compiling",10/28/15,1
echeipesh,,10/28/15,1
echeipesh,"ah, nvm, I see",10/28/15,1
echeipesh,Lemme see if that runs faster,10/28/15,1
lossyrob,"yeah. Dunno if that helps because of the one RDD instead of two, but I think so because of the partitioning (at least until we have the smae partitioner",10/28/15,1
lossyrob,(,10/28/15,1
lossyrob,this seems cool,10/28/15,1
lossyrob,https://sidecar.gitter.im/,10/28/15,1
echeipesh,"@lossyrob: it_Ñés _ÑÒmaybe_Ñù faster, still in the same variance as all other tests",10/28/15,1
lossyrob,"my suspicion is that when there_Ñés multiple machines involved, that would be faster",10/28/15,1
lossyrob,this is also cool,10/28/15,1
lossyrob,http://knsv.github.io/mermaid/,10/28/15,1
echeipesh,"*shrug*, wish we had an environment for integration tests that could spin up an EMR cluster to run some non-trivial tests and benchmarks.",10/28/15,1
echeipesh,"Anyway, I_Ñém going to push the update, I like it better as well since it_Ñés transformations on single RDD rather than a cogroup. Feels like that should be faster/better somewhere.",10/28/15,1
lossyrob,cool cool,10/28/15,1
echeipesh,"Oh hey! I take my slanderous lies back, benchmarked it on cached RDD with some iteration and there is def a performance improvement:```shell[info] SumSpec:                                                                 [info] Sum Focal Spec[info] - should square sum r = 1 for raster rdd (477 milliseconds)[info] - should square sum with 5x5 neighborhood (273 milliseconds)[info] - should circle sum for raster source (225 milliseconds)[info] + zoom = 13, bounds = GridBounds(0,0,7935,6911) [info] - focal flatMap/cogroup (run: 0) (2 seconds, 914 milliseconds)           [info] - focal flatMap/cogroup (run: 1) (2 seconds, 515 milliseconds)           [info] - focal flatMap/cogroup (run: 2) (2 seconds, 316 milliseconds)           [info] - focal flatMap/cogroup (run: 3) (2 seconds, 418 milliseconds)           [info] - focal flatMap/cogroup (run: 4) (2 seconds, 214 milliseconds)           [info] - focal flatMap/cogroup (run: 5) (2 seconds, 154 milliseconds)           [info] - focal flatMap/cogroup (run: 6) (2 seconds, 240 milliseconds)           [info] - focal flatMap/cogroup (run: 7) (2 seconds, 249 milliseconds)           [info] - focal flatMap/cogroup (run: 8) (2 seconds, 296 milliseconds)           [info] - focal flatMap/cogroup (run: 9) (2 seconds, 241 milliseconds)           [info] - focal flatMap/cogroup (run: 10) (2 seconds, 227 milliseconds) [info] SumSpec:                                                                 [info] Sum Focal Spec[info] - should square sum r = 1 for raster rdd (395 milliseconds)[info] - should square sum with 5x5 neighborhood (138 milliseconds)[info] - should circle sum for raster source (142 milliseconds)[info] + zoom = 13, bounds = GridBounds(0,0,7935,6911) [info] - focal flatMap/groupByKey/flatMap (run: 0) (2 seconds, 752 milliseconds)[info] - focal flatMap/groupByKey/flatMap (run: 1) (2 seconds, 179 milliseconds)[info] - focal flatMap/groupByKey/flatMap (run: 2) (2 seconds, 16 milliseconds) [info] - focal flatMap/groupByKey/flatMap (run: 3) (1 second, 824 milliseconds) [info] - focal flatMap/groupByKey/flatMap (run: 4) (1 second, 825 milliseconds) [info] - focal flatMap/groupByKey/flatMap (run: 5) (1 second, 834 milliseconds) [info] - focal flatMap/groupByKey/flatMap (run: 6) (1 second, 845 milliseconds) [info] - focal flatMap/groupByKey/flatMap (run: 7) (1 second, 940 milliseconds) [info] - focal flatMap/groupByKey/flatMap (run: 8) (1 second, 919 milliseconds) [info] - focal flatMap/groupByKey/flatMap (run: 9) (2 seconds, 102 milliseconds)[info] - focal flatMap/groupByKey/flatMap (run: 10) (1 second, 997 milliseconds)```",10/28/15,1
lossyrob,woot,10/28/15,1
pomadchin,o:,10/28/15,1
echeipesh,"wonder if we can make it even more better by shipping just the borders, not the whole neighborhood tiles around",10/28/15,1
lossyrob,"yeah, that would take some processing time to cut the borders out of tiles, but then again we save on shipping costs",10/28/15,1
lossyrob,"one thing is, I don_Ñét know if the underlying stuff (which I think uses CompositeTile) allows for differently-sized components",10/28/15,1
echeipesh,"There is a bit of work to test that theory out, and it would need to be tested on a cluster. I wish somebody would do it  :)",10/28/15,1
lossyrob,add it to the list of things to test once we start our cluster benchmarking,10/28/15,1
echeipesh,Going to write up an issue for it and pray to open source gods to send a hero foretold in the chronicles of GitHub.,10/28/15,1
echeipesh,"Hmm, another thought is that we could do better by building a larger neighborhood, 2x2 tiles in the center or 4x4 tiles in the cetner.",10/28/15,1
lossyrob,"that_Ñés sort of based on how big the tiles are ingest at, right?",10/28/15,1
echeipesh,"yes, you_Ñéd need to target some ideal neighborhood tile size",10/28/15,1
echeipesh,"But `CellGrid` tells us that information, so doesn_Ñét sound unreasonable",10/28/15,1
lossyrob,"would perhaps be good to have it as a parameter into the focal method, then",10/28/15,1
echeipesh,"Yeah, I_Ñém going to spend an hour or two actually testing that. Did some back of the envelope and it potentially has a huge impact on the groupBy complexity",10/28/15,1
lossyrob,that effect is going to be hard to test without a cluster,10/28/15,1
lossyrob,what are you proposing again? maybe a bit unclear,10/28/15,1
echeipesh,I think it_Ñés going to be visible even locally. If it_Ñés at all measurable I_Ñéll spin on up,10/28/15,1
echeipesh,basically do focal calcs on meta-tiles,10/28/15,1
echeipesh,assemble CompositeTile from 2x2 or 3x3 tiles in the center + borders vs always doing 1 tile in the center then then borders.,10/28/15,1
lossyrob,hmm ok,10/28/15,1
lossyrob,what would the paramter be in that case?,10/28/15,1
echeipesh,"basically with 1x1 meta tile each tile contributes up to 9 rows in flatMap. With 2x2 center each tile contributs to 3 rows in flatMap, with 3x3 tiles in the center some tiles contribute to only 2 rows and some to only 1 row.",10/28/15,1
echeipesh,"the size of the center, but we can determine that from tile layout for RasterRDD case.",10/28/15,1
lossyrob,"so intsead of holding 9 tiles in memory at a time (or less with border cuts), we_Ñére holding 4 + border (up to 12) or 9 + border (up to 16)",10/28/15,1
lossyrob,so perhpas there_Ñés a sweet spot where you_Ñére not blowing up memory but decrease combines,10/28/15,1
echeipesh,"We_Ñére always holding all the tiles _ÑÒin memory_Ñù, it_Ñés just when we_Ñére doing groupBy, it can be over smaller list.",10/28/15,1
lossyrob,we don_Ñét always hold all the tiles in memory,10/28/15,1
echeipesh,"exactly. And statistically you_Ñére _ÑÒWasting_Ñù less border pixels for larger center tile, even without cutting it up.",10/28/15,1
lossyrob,https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/rdd/PairRDDFunctions.scala#L503,10/28/15,1
echeipesh,"Oh, sorry, I misunderstood, yes I agree.",10/28/15,1
echeipesh,You meant holding tiles in memory per focal operation round,10/28/15,1
lossyrob,yeah. smaller tile groups (in number per group) lets the work be chunked up more,10/28/15,1
echeipesh,"Yep, but tiles are never that big 16 256x256 tiles of doubles is only 8MB",10/28/15,1
lossyrob,"if it_Ñés parameterized, then once we start cluster benchmarking we can do 1x1 vs 3x3 in the center",10/28/15,1
echeipesh,yep yep,10/28/15,1
lossyrob,"well, you might be working with bigger tiles",10/28/15,1
lossyrob,"1024x1024, multiband, starts getting to some limits methinks",10/28/15,1
echeipesh,"right, but you know the tile size from `TileLayout`, so you can fix the size of the focal area in pixels and float the size of the focal area in tiles",10/28/15,1
lossyrob,well that_Ñés where the parameter is a question. do we say number of tiles or number of pixels?,10/28/15,1
lossyrob,"I_Ñéd prefer number of tiles, and externalize the trickery with the TileLayout math for the client to do if they_Ñére smart",10/28/15,1
echeipesh,"Well, there is going to be implemintation on naked RDD[(K, V)] and that HAS to be parameterized by tile count.",10/28/15,1
echeipesh,"method for RasterRDD can try to overload the parameter to both, we can make a good guess and give a door for alternative.",10/28/15,1
echeipesh,"ever seen class family like this:```scalatrait Grid {  val cols: Int  val rows: Int}trait MetaGrid extends Grid {  val subgrid: Grid  val cols: Int  val rows: Int}```",10/28/15,1
lossyrob,no need to declare `cols` and `rows` in MetaGrid,10/28/15,1
echeipesh,"true, but question stands :)",10/28/15,1
lossyrob,"looks like a Node sort of setup, with a parent",10/28/15,1
pomadchin,"oops guys today was fixing my cluster (that zcurve time problem, figuring out where that version got bugs ) ): today will push some changings to the feature update to discuss; though working method update available now",10/28/15,1
pomadchin,In writer; but it's dirty a little ._.,10/28/15,1
lossyrob,"@pomadchin ok cool, let me know when you push the changes so can take a look",10/28/15,1
melvinvarkey,"```mel@django ~/myproject/geo/geot $ ./run.sh [info] Set current project to NetCDF GeoTrellis Test (in build file:/home/mel/myproject/geo/geot/) [info] Updating {file:/home/mel/myproject/geo/geot/}geot... [info] Resolving org.fusesource.jansi#jansi;1.4 ... [info] Done updating. [info] Running netcdf.Main 18:59:16 NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable 18:59:17 Slf4jLogger: Slf4jLogger started 18:59:17 Remoting: Starting remoting 18:59:17 Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriver@127.0.0.1:35055] 18:59:17 Server: jetty-8.y.z-SNAPSHOT 18:59:17 AbstractConnector: Started SocketConnector@0.0.0.0:38536 18:59:17 Server: jetty-8.y.z-SNAPSHOT 18:59:17 AbstractConnector: Started SelectChannelConnector@0.0.0.0:4040 18:59:18 Executor: Exception in task 0.0 in stage 0.0 (TID 0) java.lang.UnsatisfiedLinkError: org.gdal.gdalconst.gdalconstJNI.GTO_TIP_get()I at org.gdal.gdalconst.gdalconstJNI.GTO_TIP_get(Native Method) at org.gdal.gdalconst.gdalconstConstants.<clinit>(gdalconstConstants.java:122) at geotrellis.gdal.Gdal$.open(Gdal.scala:20) at geotrellis.spark.io.hadoop.formats.GdalRecordReader.initialize(GdalInputFormat.scala:102) at org.apache.spark.rdd.NewHadoopRDD$$anon$1.<init>(NewHadoopRDD.scala:133) at org.apache.spark.rdd.NewHadoopRDD.compute(NewHadoopRDD.scala:104) at org.apache.spark.rdd.NewHadoopRDD.compute(NewHadoopRDD.scala:66) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277) at org.apache.spark.rdd.RDD.iterator(RDD.scala:244) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277) at org.apache.spark.rdd.RDD.iterator(RDD.scala:244) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277) at org.apache.spark.rdd.RDD.iterator(RDD.scala:244) at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:70) at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41) at org.apache.spark.scheduler.Task.run(Task.scala:70) at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:213) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) at java.lang.Thread.run(Thread.java:745) 18:59:18 SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[Executor task launch worker-0,5,run-main-group-0] java.lang.UnsatisfiedLinkError: org.gdal.gdalconst.gdalconstJNI.GTO_TIP_get()I at org.gdal.gdalconst.gdalconstJNI.GTO_TIP_get(Native Method) at org.gdal.gdalconst.gdalconstConstants.<clinit>(gdalconstConstants.java:122) at geotrellis.gdal.Gdal$.open(Gdal.scala:20) at geotrellis.spark.io.hadoop.formats.GdalRecordReader.initialize(GdalInputFormat.scala:102) at org.apache.spark.rdd.NewHadoopRDD$$anon$1.<init>(NewHadoopRDD.scala:133) at org.apache.spark.rdd.NewHadoopRDD.compute(NewHadoopRDD.scala:104) at org.apache.spark.rdd.NewHadoopRDD.compute(NewHadoopRDD.scala:66) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277) at org.apache.spark.rdd.RDD.iterator(RDD.scala:244) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277) at org.apache.spark.rdd.RDD.iterator(RDD.scala:244) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277) at org.apache.spark.rdd.RDD.iterator(RDD.scala:244) at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:70) at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41) at org.apache.spark.scheduler.Task.run(Task.scala:70) at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:213) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) at java.util```",10/29/15,1
melvinvarkey,"i am keep getting this error:worried:showing gdal errorbut i have installed this gdaldo you have any link to install gdal that you used in this project ? ",10/29/15,1
melvinvarkey,"```mel@django ~/myproject/geo/geot $ ./run.sh [info] Set current project to NetCDF GeoTrellis Test (in build file:/home/mel/myproject/geo/geot/) [info] Updating {file:/home/mel/myproject/geo/geot/}geot... [info] Resolving org.fusesource.jansi#jansi;1.4 ... [info] Done updating. [info] Running netcdf.Main 18:59:16 NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable 18:59:17 Slf4jLogger: Slf4jLogger started 18:59:17 Remoting: Starting remoting 18:59:17 Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriver@127.0.0.1:35055] 18:59:17 Server: jetty-8.y.z-SNAPSHOT 18:59:17 AbstractConnector: Started SocketConnector@0.0.0.0:38536 18:59:17 Server: jetty-8.y.z-SNAPSHOT 18:59:17 AbstractConnector: Started SelectChannelConnector@0.0.0.0:4040 18:59:18 Executor: Exception in task 0.0 in stage 0.0 (TID 0) java.lang.UnsatisfiedLinkError: org.gdal.gdalconst.gdalconstJNI.GTO_TIP_get()I at org.gdal.gdalconst.gdalconstJNI.GTO_TIP_get(Native Method) at org.gdal.gdalconst.gdalconstConstants.<clinit>(gdalconstConstants.java:122) at geotrellis.gdal.Gdal$.open(Gdal.scala:20) at geotrellis.spark.io.hadoop.formats.GdalRecordReader.initialize(GdalInputFormat.scala:102) at org.apache.spark.rdd.NewHadoopRDD$$anon$1.<init>(NewHadoopRDD.scala:133) at org.apache.spark.rdd.NewHadoopRDD.compute(NewHadoopRDD.scala:104) at org.apache.spark.rdd.NewHadoopRDD.compute(NewHadoopRDD.scala:66) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277) at org.apache.spark.rdd.RDD.iterator(RDD.scala:244) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277) at org.apache.spark.rdd.RDD.iterator(RDD.scala:244) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277) at org.apache.spark.rdd.RDD.iterator(RDD.scala:244) at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:70) at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41) at org.apache.spark.scheduler.Task.run(Task.scala:70) at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:213) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) at java.lang.Thread.run(Thread.java:745) 18:59:18 SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[Executor task launch worker-0,5,run-main-group-0] java.lang.UnsatisfiedLinkError: org.gdal.gdalconst.gdalconstJNI.GTO_TIP_get()I at org.gdal.gdalconst.gdalconstJNI.GTO_TIP_get(Native Method) at org.gdal.gdalconst.gdalconstConstants.<clinit>(gdalconstConstants.java:122) at geotrellis.gdal.Gdal$.open(Gdal.scala:20) at geotrellis.spark.io.hadoop.formats.GdalRecordReader.initialize(GdalInputFormat.scala:102) at org.apache.spark.rdd.NewHadoopRDD$$anon$1.<init>(NewHadoopRDD.scala:133) at org.apache.spark.rdd.NewHadoopRDD.compute(NewHadoopRDD.scala:104) at org.apache.spark.rdd.NewHadoopRDD.compute(NewHadoopRDD.scala:66) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277) at org.apache.spark.rdd.RDD.iterator(RDD.scala:244) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277) at org.apache.spark.rdd.RDD.iterator(RDD.scala:244) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277) at org.apache.spark.rdd.RDD.iterator(RDD.scala:244) at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:70) at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41) at org.apache.spark.scheduler.Task.run(Task.scala:70) at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:213) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) at java.util```",10/29/15,1
pomadchin,"some time ago we talked with @melvinvarkey so he experimented with https://github.com/lossyrob/netcdf-geotrellis-spark-read-test project and had some gdal issues too in the final result ",10/29/15,1
lossyrob,@melvinvarkey what do you get for `gdalinfo _ÑÓversion`?,10/29/15,1
pomadchin,as i remember there was smth like `1.10.1`,10/29/15,1
pomadchin,needs for project ** 1.10.1,10/29/15,1
pomadchin,ouch readme.md there :D ,10/29/15,1
melvinvarkey,"```mel@django ~/myproject/geo/geot $ ./run.sh [info] Set current project to NetCDF GeoTrellis Test (in build file:/home/mel/myproject/geo/geot/)[info] Running netcdf.Main 20:13:28 NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable20:13:29 Slf4jLogger: Slf4jLogger started20:13:29 Remoting: Starting remoting20:13:29 Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriver@127.0.0.1:38266]20:13:30 Server: jetty-8.y.z-SNAPSHOT20:13:30 AbstractConnector: Started SocketConnector@0.0.0.0:5073320:13:30 Server: jetty-8.y.z-SNAPSHOT20:13:30 AbstractConnector: Started SelectChannelConnector@0.0.0.0:404020:13:31 Executor: Exception in task 0.0 in stage 0.0 (TID 0)java.lang.UnsatisfiedLinkError: org.gdal.gdalconst.gdalconstJNI.GTO_TIP_get()I	at org.gdal.gdalconst.gdalconstJNI.GTO_TIP_get(Native Method)	at org.gdal.gdalconst.gdalconstConstants.<clinit>(gdalconstConstants.java:122)	at geotrellis.gdal.Gdal$.open(Gdal.scala:20)	at geotrellis.spark.io.hadoop.formats.GdalRecordReader.initialize(GdalInputFormat.scala:102)	at org.apache.spark.rdd.NewHadoopRDD$$anon$1.<init>(NewHadoopRDD.scala:133)	at org.apache.spark.rdd.NewHadoopRDD.compute(NewHadoopRDD.scala:104)	at org.apache.spark.rdd.NewHadoopRDD.compute(NewHadoopRDD.scala:66)	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)	at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)	at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)	at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:70)	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)	at org.apache.spark.scheduler.Task.run(Task.scala:70)	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:213)	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)	at java.lang.Thread.run(Thread.java:745)20:13:31 SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[Executor task launch worker-0,5,run-main-group-0]java.lang.UnsatisfiedLinkError: org.gdal.gdalconst.gdalconstJNI.GTO_TIP_get()I	at org.gdal.gdalconst.gdalconstJNI.GTO_TIP_get(Native Method)	at org.gdal.gdalconst.gdalconstConstants.<clinit>(gdalconstConstants.java:122)	at geotrellis.gdal.Gdal$.open(Gdal.scala:20)	at geotrellis.spark.io.hadoop.formats.GdalRecordReader.initialize(GdalInputFormat.scala:102)	at org.apache.spark.rdd.NewHadoopRDD$$anon$1.<init>(NewHadoopRDD.scala:133)	at org.apache.spark.rdd.NewHadoopRDD.compute(NewHadoopRDD.scala:104)	at org.apache.spark.rdd.NewHadoopRDD.compute(NewHadoopRDD.scala:66)	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)	at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)	at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)	at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:70)	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)	at org.apache.spark.scheduler.Task.run(Task.scala:70)	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:213)	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)	at java.lang.Thread.run(Thread.java:745)```",10/29/15,1
melvinvarkey,"GDAL 2.1.0dev, released 2015/99/99",10/29/15,1
melvinvarkey,this is the version now it is showing ,10/29/15,1
melvinvarkey,"```https://gist.github.com/lossyrob/4348503```",10/29/15,1
melvinvarkey,by following this link i installed ,10/29/15,1
lossyrob,that_Ñés a higher version then I think I was using for that project,10/29/15,1
melvinvarkey,java.lang.UnsatisfiedLinkError: org.gdal.gdalconst.gdalconstJNI.GTO_TIP_get(),10/29/15,1
melvinvarkey,why this error is coming ,10/29/15,1
melvinvarkey,yeah but still getting the same erro ,10/29/15,1
lossyrob,getting the same error?,10/29/15,1
melvinvarkey,yes ,10/29/15,1
lossyrob,using a differently installed verison of GDAL to match the 1.11 version?,10/29/15,1
lossyrob,what has changed between things where you_Ñére saying your getting the _ÑÉsame_Ñé error,10/29/15,1
melvinvarkey,"i installed again by following the link you gave me ",10/29/15,1
melvinvarkey,nothing changed ,10/29/15,1
melvinvarkey,both log is same,10/29/15,1
lossyrob,and your gdalinfo _ÑÓverison is now 1.11?,10/29/15,1
melvinvarkey,nope,10/29/15,1
melvinvarkey,"GDAL 2.1.0dev, released 2015/99/99",10/29/15,1
lossyrob,and your sure the `.so` gdal binaries in `/usr/local/lib` has changed?,10/29/15,1
melvinvarkey,this onw ,10/29/15,1
lossyrob,ok. so intead of trying to downgrade,10/29/15,1
lossyrob,you can upgrade the jar file,10/29/15,1
lossyrob,try using this dependency v,10/29/15,1
lossyrob,http://search.maven.org/#artifactdetails%7Corg.gdal%7Cgdal%7C2.0.0%7Cpom,10/29/15,1
melvinvarkey,[![Screenshot from 2015-10-29 20:18:38.png](https://files.gitter.im/geotrellis/geotrellis/uA9F/thumb/Screenshot-from-2015-10-29-20_18_38.png)](https://files.gitter.im/geotrellis/geotrellis/uA9F/Screenshot-from-2015-10-29-20_18_38.png),10/29/15,1
melvinvarkey,see this is the /usr/local/lib folder,10/29/15,1
lossyrob,it_Ñés basically going to take some work to line up the GDAL java binding versions,10/29/15,1
lossyrob,that_Ñés what you should be troubleshooting,10/29/15,1
melvinvarkey,yeah could you please help me here ,10/29/15,1
lossyrob,I don_Ñét have anything exact to say. it_Ñés going to take troubleshooting,10/29/15,1
lossyrob,it_Ñés not an error i_Ñéve run into before,10/29/15,1
melvinvarkey,is it ok if i changed the build.sbt like this ,10/29/15,1
melvinvarkey,"```name := ""NetCDF GeoTrellis Test""version := ""0.1.0""scalaVersion := ""2.10.5""libraryDependencies += ""org.apache.spark"" %% ""spark-core"" % ""1.4.1""libraryDependencies += ""org.apache.hadoop"" % ""hadoop-client"" % ""2.6.0""libraryDependencies += ""com.azavea.geotrellis"" %% ""geotrellis-spark""  % ""0.10.0-M1""libraryDependencies += ""org.gdal"" % ""gdal"" % ""2.0.0""libraryDependencies += ""com.azavea.geotrellis"" % ""geotrellis-gdal_2.10"" % ""0.10.0-M1""libraryDependencies += ""com.azavea.geotrellis"" % ""geotrellis-spark_2.10"" % ""0.10.0-M1""resolvers += ""Scalaz Bintray Repo"" at ""http://dl.bintray.com/scalaz/releases""mainClass := Some(""netcdf.Main"")javaOptions += ""-Djava.library.path=/usr/lib/jni""scalacOptions ++= Seq(""-unchecked"", ""-deprecation"")```",10/29/15,1
melvinvarkey,and run.sh as this ,10/29/15,1
melvinvarkey,"```#!/usr/bin/env bash./sbt run -Djava.library.path=/usr/lib/jni/```",10/29/15,1
lossyrob,I_Ñém not sure. that might conflict with the geotrellis-gdal version being pulled in,10/29/15,1
lossyrob,you might want to try to check out `geotrellis` and try to upgrade the `gdal` dependnecy there,10/29/15,1
melvinvarkey,"```20:22:26 Server: jetty-8.y.z-SNAPSHOT20:22:26 AbstractConnector: Started SelectChannelConnector@0.0.0.0:4040Native library load failed.java.lang.UnsatisfiedLinkError: no gdaljni in java.library.path20:22:27 Executor: Exception in task 0.0 in stage 0.0 (TID 0)java.lang.UnsatisfiedLinkError: org.gdal.gdal.gdalJNI.AllRegister()V	at org.gdal.gdal.gdalJNI.AllRegister(Native Method)	at org.gdal.gdal.gdal.AllRegister(gdal.java:456)```",10/29/15,1
melvinvarkey,now getting this ,10/29/15,1
melvinvarkey,Native library load failed ,10/29/15,1
lossyrob,"sorry @melvinvarkey, I don_Ñét think pasting every error is going to be a good way to work through this",10/29/15,1
pomadchin,"seems to me add exclusion may solve the problem; but feels likethe only way to build own version```scala exclude(_ÑÒorg.gdal"" %% _ÑÒgdal"")```",10/29/15,1
lossyrob,"do you understand the core problem? That there is a native set of `.so` files that are installed by GDAL java bindings, into `/usr/local/lib`. Those have to be of the same version as the `gdal.jar` being built with your scala project. `geotrellis-raster` pulls in `gdal.jar` in a specific version, which will most likely conflict with if you try to pull in a `2.0.0` jar. So you need to figure out a way to line those versions all up on the machine, and then it should work. Barring any code changes you_Ñéll need for the gdal java code that changed between 1.11 and 2.0",10/29/15,1
melvinvarkey,i think the gdal.jar is missing in my system ,10/29/15,1
melvinvarkey,i could not locate this gdal.jar file,10/29/15,1
lossyrob,gdal.jar would be pulled in as part of the build process,10/29/15,1
melvinvarkey,nope i checked ,10/29/15,1
lossyrob,so it would be in `~/.ivy2/cache/org/osgeo/_Ñ_`,10/29/15,1
melvinvarkey,no file in this location ,10/29/15,1
melvinvarkey,see that may be the prblel ,10/29/15,1
melvinvarkey,problem ,10/29/15,1
melvinvarkey,no jar file in any location ,10/29/15,1
melvinvarkey,can you send me 2.0.0 version jar file and i will paste in this location ,10/29/15,1
lossyrob,it_Ñés on maven central,10/29/15,1
melvinvarkey,ok thanks,10/29/15,1
lossyrob,you should be able to pull it into a build process if it_Ñés on maven central,10/29/15,1
melvinvarkey,which location should i put this jar ,10/29/15,1
lossyrob,it should be brought into SBT as a declared dependnecy,10/29/15,1
melvinvarkey,ok ,10/29/15,1
lossyrob,https://github.com/lossyrob/netcdf-geotrellis-spark-read-test/blob/master/build.sbt#L8,10/29/15,1
lossyrob,just as this line brings in spark as a dependnecy,10/29/15,1
lossyrob,you need to bring in the gdal dependency. also you_Ñéll have to exclude it from `geotrellis-spark` like @pomadchin was saying,10/29/15,1
melvinvarkey,yeah thanks ,10/29/15,1
melvinvarkey,i got the whole idea but,10/29/15,1
melvinvarkey,do changing the version to higher will help ? ,10/29/15,1
lossyrob,the other option is to move your system_Ñés installed gdal to 1.11,10/29/15,1
melvinvarkey,how ?,10/29/15,1
lossyrob,"I predict that either way will be a bit painstaking, to be honest. I_Ñéve fought with getting installed gdal java bindings to run on a machine for hours before, so I know your pain :)",10/29/15,1
melvinvarkey,haha,10/29/15,1
melvinvarkey,:),10/29/15,1
lossyrob,"either follow the instructions on the GDAL site for how to build from source and install the java bindings, and make sure that they are not conflicting with pre-installed java bindings",10/29/15,1
melvinvarkey,yeah ok ,10/29/15,1
vishalanand,"Hi everyone, I was looking into parallelization of raster data processing in geotrellis via Apache Spark after my office-hours. I was using Cloudera VM for this, but I am currently facing issues with the setting up of Spark projects. As a sample, I was building decision trees in Scala with mllib (Spark).I was wondering if someone could give me few pointers on setting it up(on Cloudera, or without) for a first Spark project?",10/29/15,1
lossyrob,I_Ñéve normally just downloaded the distribution from the spark website,10/29/15,1
lossyrob,"and that gives you scripts in `bin/`, like `spark-submit`",10/29/15,1
lossyrob,"or otherwise I run it by including spark as a dependnecy in my scala project, and then create a SparkContext from inside the code",10/29/15,1
vishalanand,"Thank you so much.I will try this and get started with the Spark project for parallelization!",10/29/15,1
lossyrob,cool! nice to hear from you again Vishal :),10/29/15,1
vishalanand,:D :D,10/29/15,1
pomadchin,o: wouch,10/29/15,1
pomadchin,** also may use Docker containers; if you going to use localy it even possible to emulate cluster on a one machine,10/29/15,1
pomadchin,but fat machine (:,10/29/15,1
lossyrob,for sure. though that_Ñés more of a stage two ;),10/29/15,1
lossyrob,"for those not in the know, Vishal is the Google Summer of Code student responsible for the `KrigingInterpolation` code in `geotrellis.raster`. Check that code out, it_Ñéll make your head spin (in a good way) :)",10/29/15,1
pomadchin,"@lossyrob #1192 seems to me now it works (testing now on Hadoop & Accumulo); but the only question is how to pass BoundRDD type to the update function declaration in the Writer trait; the usage of the third type param seems not a good idea;  also it is not difficult to add thad function sugar wrapperp.s. mb it is a good idea to add more specific error thrown by a Reader read function (LayerNotExists) ",10/29/15,1
lossyrob,"Also _ÑÒlayer not exist_Ñù is wrong, _ÑÒlayer does not exist_Ñù instead",10/29/15,1
pomadchin,"and I though on the idea of `update` for indexes differs from hilbertsounds good to let api to update layer if index method = rowmajor / zcurve, cause in the current index implementation it does not depend on the init params",10/29/15,1
lossyrob,I don_Ñét think that would be a good abstraction,10/29/15,1
pomadchin,yes ):,10/29/15,1
lossyrob,"how do you mean, pass the BoundRDD type t the update function in the trait?",10/29/15,1
lossyrob,you mean selectively let through out of bounds ranges because the index implementations allow for it?,10/29/15,1
lossyrob,"sorry my connection is weird, my messages look like they are being submitted out of order",10/29/15,1
pomadchin,kk,10/29/15,1
pomadchin,yes mb selectively some how; in certain cases it can be usefull; but this can be painful for generic abstraction,10/29/15,1
pomadchin,AAA mixed up traits %),10/29/15,1
echeipesh,"@lossyrob: couldn_Ñét resist, took an hour to write the focal operation where we only slice the window: ```shell[info] SumSpec:                                                                 [info] Sum Focal Spec[info] - should square sum r = 1 for raster rdd (455 milliseconds)[info] - should square sum with 5x5 neighborhood (166 milliseconds)[info] - should circle sum for raster source (168 milliseconds)[info] + zoom = 13, bounds = GridBounds(0,0,7935,6911) [info] - focal Slice (run: 0) (1 second, 85 milliseconds)                       [info] - focal Slice (run: 1) (650 milliseconds)[info] - focal Slice (run: 2) (577 milliseconds)[info] - focal Slice (run: 3) (498 milliseconds)[info] - focal Slice (run: 4) (457 milliseconds)[info] - focal Slice (run: 5) (543 milliseconds)[info] - focal Slice (run: 6) (416 milliseconds)[info] - focal Slice (run: 7) (429 milliseconds)[info] - focal Slice (run: 8) (464 milliseconds)[info] - focal Slice (run: 9) (467 milliseconds)[info] - focal Slice (run: 10) (516 milliseconds)```",10/29/15,1
echeipesh,thats 3x improvement from previous and 5x improvement from original implemintation. ,10/29/15,1
pomadchin,"@echeipesh had you such kind of error?: ```Job aborted due to stage failure: Serialized task 0:0 was 155119290 bytes, which exceeds max allowed: spark.akka.frameSize (10485760 bytes) - reserved (204800 bytes).```",10/29/15,1
pomadchin,it is on a some query o: ,10/29/15,1
echeipesh,"Guess it has too much code it tried to serialize? I'm sure you can bump that limit in application.conf, but it would be nice to know the root cause. Maybe some large data structure was pulled in and can be broadcast instread?",10/29/15,1
pomadchin,heh not sure,10/29/15,1
pomadchin,"is smth like```scalaval filters = new RasterRDDQuery[SpaceTimeKey].where(Intersects(GridBounds(key.col, key.row, key.col, key.row)))val rdd = catalog.read[SpaceTimeKey](layer, filters)```",10/29/15,1
pomadchin,"but! there not a lot data at all; mb some sort of bug?",10/29/15,1
echeipesh,"Could be, will look around tomorrow",10/29/15,1
echeipesh,Which backend?,10/29/15,1
pomadchin,"accumulo ._. looking now through logs; think it may be caused by possible inconsistenty of tables smth wrong with data inside; or smth else now trying same things on a _ÑÒclean_Ñù db",10/29/15,1
melvinvarkey,"```[warn] 	::::::::::::::::::::::::::::::::::::::::::::::[warn] 	::          UNRESOLVED DEPENDENCIES         ::[warn] 	::::::::::::::::::::::::::::::::::::::::::::::[warn] 	:: org.gdal#gdal;1.10.1: not found[warn] 	::::::::::::::::::::::::::::::::::::::::::::::```",10/30/15,1
melvinvarkey,i checked this link ,10/30/15,1
melvinvarkey,"```http://mvnrepository.com/artifact/org.gdal/gdal```",10/30/15,1
melvinvarkey,"1.10.1 is not present, so any other idea ?",10/30/15,1
pomadchin,Osgeo repo http://download.osgeo.org/webdav/geotools/  resolves the problem,10/30/15,1
pomadchin,or you can use this repo: http://repo.boundlessgeo.com/main/,10/30/15,1
melvinvarkey,hey yeah ,10/30/15,1
melvinvarkey,but i copied the jar file to the iv2/cache/org.gdal/gdal ,10/30/15,1
melvinvarkey,no use ,10/30/15,1
pomadchin,add it as a resolver into build.sbt,10/30/15,1
melvinvarkey,"```resolvers += ""gdal repo"" at ""http://repo.boundlessgeo.com/main/""```",10/30/15,1
melvinvarkey,is this ok ???? ,10/30/15,1
pomadchin,ye,10/30/15,1
melvinvarkey,okk,10/30/15,1
melvinvarkey,"```/usr/lib/jvm/java-7-openjdk-amd64/bin/java -Didea.launcher.port=7534 -Didea.launcher.bin.path=/home/achayoo/NETCDF/DeepakWorking/sysreq/idea-IC-139.659.2/bin -Dfile.encoding=UTF-8 -classpath /usr/lib/jvm/java-7-openjdk-amd64/jre/lib/javazic.jar:/usr/lib/jvm/java-7-openjdk-amd64/jre/lib/management-agent.jar:/usr/lib/jvm/java-7-openjdk-amd64/jre/lib/jsse.jar:/usr/lib/jvm/java-7-openjdk-amd64/jre/lib/rhino.jar:/usr/lib/jvm/java-7-openjdk-amd64/jre/lib/compilefontconfig.jar:/usr/lib/jvm/java-7-openjdk-amd64/jre/lib/rt.jar:/usr/lib/jvm/java-7-openjdk-amd64/jre/lib/resources.jar:/usr/lib/jvm/java-7-openjdk-amd64/jre/lib/charsets.jar:/usr/lib/jvm/java-7-openjdk-amd64/jre/lib/jce.jar:/usr/lib/jvm/java-7-openjdk-amd64/jre/lib/ext/icedtea-sound.jar:/usr/lib/jvm/java-7-openjdk-amd64/jre/lib/ext/zipfs.jar:/usr/lib/jvm/java-7-openjdk-amd64/jre/lib/ext/dnsns.jar:/usr/lib/jvm/java-7-openjdk-amd64/jre/lib/ext/sunpkcs11.jar:/usr/lib/jvm/java-7-openjdk-amd64/jre/lib/ext/localedata.jar:/usr/lib/jvm/java-7-openjdk-amd64/jre/lib/ext/sunjce_provider.jar:/usr/lib/jvm/java-7-openjdk-amd64/jre/lib/ext/java-atk-wrapper.jar:/home/achayoo/NETCDF/DeepakWorking/DeepakWorking/NetCDF-Test2/target/scala-2.10/classes:/home/achayoo/.ivy2/cache/com.esotericsoftware.minlog/minlog/jars/minlog-1.2.jar:/home/achayoo/.ivy2/cache/com.google.code.findbugs/jsr305/jars/jsr305-1.3.9.jar:/home/achayoo/.ivy2/cache/commons-beanutils/commons-beanutils/jars/commons-beanutils-1.7.0.jar:/home/achayoo/.ivy2/cache/commons-beanutils/commons-beanutils-core/jars/commons-beanutils-core-1.8.0.jar:/home/achayoo/.ivy2/cache/commons-collections/commons-collections/jars/commons-collections-3.2.1.jar:/home/achayoo/.ivy2/cache/commons-configuration/commons-configuration/jars/commons-configuration-1.6.jar:/home/achayoo/.ivy2/cache/commons-digester/commons-digester/jars/commons-digester-1.8.jar:/home/achayoo/.ivy2/cache/commons-httpclient/commons-httpclient/jars/commons-httpclient-3.1.jar:/home/achayoo/.ivy2/cache/log4j/log4j/bundles/log4j-1.2.17.jar:/home/achayoo/.ivy2/cache/org.apache.commons/commons-math/jars/commons-math-2.1.jar:/home/achayoo/.ivy2/cache/org.uncommons.maths/uncommons-maths/jars/uncommons-maths-1.2.2a.jar:/home/achayoo/.ivy2/cache/xmlenc/xmlenc/jars/xmlenc-0.52.jar:/home/achayoo/.ivy2/cache/org.apache.avro/avro/jars/avro-1.7.4.jar:/home/achayoo/.ivy2/cache/org.apache.commons/commons-compress/jars/commons-compress-1.4.1.jar:/home/achayoo/.ivy2/cache/org.mortbay.jetty/jetty-util/jars/jetty-util-6.1.26.jar:/home/achayoo/.ivy2/cache/org.tukaani/xz/jars/xz-1.0.jar:/home/achayoo/.ivy2/cache/c3p0/c3p0/jars/c3p0-0.9.1.1.jar:/home/achayoo/.ivy2/cache/com.beust/jcommander/jars/jcommander-1.35.jar:/home/achayoo/.ivy2/cache/com.google.guava/guava/bundles/guava-17.0.jar:/home/achayoo/.ivy2/cache/com.google.protobuf/protobuf-java/bundles/protobuf-java-2.5.0.jar:/home/achayoo/.ivy2/cache/commons-codec/commons-codec/jars/commons-codec-1.6.jar:/home/achayoo/.ivy2/cache/edu.ucar/cdm/jars/cdm-4.5.5.jar:/home/achayoo/.ivy2/cache/edu.ucar/httpservices/jars/httpservices-4.5.5.jar:/home/achayoo/.ivy2/cache/edu.ucar/netcdf/jars/netcdf-4.2.20.jar:/home/achayoo/.ivy2/cache/edu.ucar/udunits/jars/udunits-4.5.5.jar:/home/achayoo/.ivy2/cache/edu.ucar/unidataCommon/jars/unidataCommon-4.2.20.jar:/home/achayoo/.ivy2/cache/net.jcip/jcip-annotations/jars/jcip-annotations-1.0.jar:/home/achayoo/.ivy2/cache/net.sf.ehcache/ehcache-core/jars/ehcache-core-2.6.2.jar:/home/achayoo/.ivy2/cache/org.apache.httpcomponents/httpmime/jars/httpmime-4.2.6.jar:/home/achayoo/.ivy2/cache/org.itadaki/bzip2/jars/bzip2-0.9.1.jar:/home/achayoo/.ivy2/cache/org.jdom/jdom2/jars/jdom2-2.0.4.jar:/home/achayoo/.ivy2/cache/org.quartz-scheduler/quartz/jars/quartz-2.2.0.jar:/home/achayoo/.ivy2/cache/aopalliance/aopalliance/jars/aopalliance-1.0.jar:/home/achayoo/.ivy2/cache/asm/asm/jars/asm-3.2.jar:/home/achayoo/.ivy2/cache/com.amazonaws/aws-java-sdk-core/jars/aws-java-sdk-core-1.9.34.jar:/home/achayoo/.ivy2/cache/com.amazonaws/aws-java-sdk-kms/jars/aws-java-sdk-kms-1.9.34.jar:/home/achayoo/.ivy2/cache/com.amazonaws/aws-java-sd```",11/3/15,1
melvinvarkey,anybody knows how to fix this issue ? ,11/3/15,1
melvinvarkey,"```java.lang.SecurityException: class ""javax.servlet.FilterRegistration""'s signer information does not match signer information of other classes in the same package```",11/3/15,1
melvinvarkey,signer information does not match ! i googled the error but no use ! :(,11/3/15,1
pomadchin,sounds like mixing deps,11/3/15,1
pomadchin,"try to exclude from some libs via `excludeAll ExclusionRule(organization = ""javax.servlet"")`",11/3/15,1
melvinvarkey,nope ,11/3/15,1
melvinvarkey,still that same error ! :(,11/3/15,1
lossyrob,@pomadchin looking over your PR now. Where are we at with that?,11/3/15,1
pomadchin,"update function, and update with sugar ; just want to know is it a right way ",11/3/15,1
lossyrob,I_Ñéve made some comments on the PR,11/3/15,1
lossyrob,but I think it_Ñés in the right direction,11/3/15,1
lossyrob,"It would be good to see some tests that would act as examples of client code, to see how it looks from the client side",11/3/15,1
pomadchin,+ ok got it; the errors : wanted to specify error kind by it's type,11/3/15,1
lossyrob,`case e: HeaderMatchError[_] => throw e.initCause(e)`,11/3/15,1
lossyrob,I_Ñém confused about what this doest,11/3/15,1
lossyrob,*does,11/3/15,1
lossyrob,it just pretty much rethrows it as is right?,11/3/15,1
pomadchin,yes ):,11/3/15,1
lossyrob,"I_Ñéd say, follow the current convention we have in the project, which is to not catch errors unless you are actually handling it (and that goes for translating exceptions too_Ñ_I_Ñéve never found that useful in practice)",11/3/15,1
pomadchin,yes got it,11/3/15,1
pomadchin,"thought it is nice sometime, but honestly to say, im trying to avoid try catch in scala code ): so wasn't sure about it",11/3/15,1
lossyrob,no worries,11/3/15,1
pomadchin,"and your comment about RDD, can you expand it a little?",11/3/15,1
lossyrob,"that function signature is `def mergeUpdate(id: ID, reader: FilteringLayerReader[ID, K, RDD[(K, V)]], rdd: RDD[(K, V)])`",11/3/15,1
lossyrob,where is the `rdd` parameter being used in the function,11/3/15,1
lossyrob,oh I see,11/3/15,1
lossyrob,nevermind then,11/3/15,1
lossyrob,"I am not sure why, but I feel weird about that merge funciton only taking one RDD",11/3/15,1
lossyrob,I feel like it should take 2,11/3/15,1
lossyrob,"and pass the (rdd, existing) into it",11/3/15,1
lossyrob,"I know you can just close over the rdd, from the client side",11/3/15,1
lossyrob,but something feels weird about that,11/3/15,1
lossyrob,"Like, the funciton I would want to pass in would be `(rdd1, rdd2) => rdd1 merge rdd2`. Though that may get confusing about the order of the RDD_Ñés, which is the existing and which is the update",11/3/15,1
lossyrob,I dunno. @echeipesh @notthatbreezy what do you think?,11/3/15,1
pomadchin,"_ª¥Ô I have a local code with that function version, but then realised that reader wasn't used ",11/3/15,1
pomadchin,._.',11/3/15,1
pomadchin,"hm nice, ty alot!",11/3/15,1
lossyrob,"```scaladef mergeUpdate(id: ID, reader: FilteringLayerReader[ID, K, RDD[(K, V)]], rdd: RDD[(K, V)])(merge: (RDD[(K, V)], RDD[(K, V)]) => RDD[(K, V)]) = {  val existing = reader.query(id).where(Intersects(implicitly[Boundable[K]].getKeyBounds(rdd))).toRDD  update(id, merge(existing, rdd))```Is what I_Ñém thinking",11/3/15,1
echeipesh,"yes, merge should not be a map, it should be a reduce. The direction is not set .. but (existing, update) is kind of expected.",11/3/15,1
pomadchin,":D ouch, next I can think about rebuilding index function or delete function ** ",11/3/15,1
pomadchin,hm _ª¥Ô ,11/3/15,1
notthatbreezy,I like how that looks @lossyrob and makes sense in my head,11/3/15,1
lossyrob,"cool, looks like thats what we should go with",11/3/15,1
pomadchin,"_ª¥Ô in a few hours (1-1.5) will fix it, riding home",11/3/15,1
timothymschier,"Hey guys, apologies for the really noddy qn.I'm having 'fun' with IntelliJ building GeoTrellis. I think I've configured Scala/SBT for 2.11, but I get a compile error in the proj4 library with the au.com.bytecode.opencsv not able to be resolved. Googling for OpenCSV, I've tried changing it (and the reference in .Dependencies) to net.sf.opencsv and com.opencsv but to no avail. I'm sure there's something simple I'm missing as a newbie...",11/4/15,1
pomadchin,@timothymschier yo; is it a master branch?,11/4/15,1
lossyrob,@timothymschier I can find that dependency on maven central http://search.maven.org/#artifactdetails%7Cnet.sf.opencsv%7Copencsv%7C2.3%7Cjar,11/4/15,1
lossyrob,"oh, wait. what is `au.com.bytecode.opencsv`?",11/4/15,1
lossyrob,you might be on an older geotrellis (which I think is what @pomadchin was getting at),11/4/15,1
lossyrob,https://github.com/geotrellis/geotrellis/blob/master/project/Dependencies.scala#L29 is what that dependency should look like,11/4/15,1
timothymschier,"Sorry, I was a bit unclear. Dependencies shows net.sfs.opencsv; it was only in the proj4 that showed au.com.bytecode.opencsv: https://github.com/geotrellis/geotrellis/blob/master/proj4/src/main/scala/geotrellis/proj4/EPSGCSVReader.scala#L19",11/4/15,1
lossyrob,interesting,11/4/15,1
timothymschier,But even when I changed that reference to net.sh.opencsv it still didn't work...,11/4/15,1
timothymschier,... should it be referencing .Dependencies?,11/4/15,1
lossyrob,no it shouldn't,11/4/15,1
timothymschier,ok,11/4/15,1
lossyrob,that_Ñés the import statement. The Dependencies.scala file is what we use to just abstract our dependencies out of our build files,11/4/15,1
timothymschier,"ah yep, of course :)",11/4/15,1
lossyrob,https://github.com/geotrellis/geotrellis/blob/master/proj4/build.sbt,11/4/15,1
lossyrob,"so that points to openCSV, which is that line in Dependencies, which is the net.sfs.opencsv",11/4/15,1
lossyrob,maybe the package naming is weird in that JAR,11/4/15,1
lossyrob,I_Ñéll check,11/4/15,1
timothymschier,"ok so with that referencing .Dependencies, it hsould be pulling in net.sf.opencsv; but the code is au.com.bytecode",11/4/15,1
lossyrob,and it_Ñés that import statement that_Ñés failing to compile right?,11/4/15,1
timothymschier,yep,11/4/15,1
timothymschier,btw (not sure if relevant) I also found somewhere online that said opencsv had moved to com.opencsv (but only one; most said net.sf.opencsv),11/4/15,1
lossyrob,I can import it in the console,11/4/15,1
lossyrob,"like, in sbt console, I do _ÑÒproject proj4_Ñù and then _ÑÒconsole_Ñù, which enters a scala console. Then I do that import statement and it works",11/4/15,1
timothymschier,ah,11/4/15,1
lossyrob,so that_Ñés weird,11/4/15,1
timothymschier,I'll have to try that when I get in to work,11/4/15,1
timothymschier,(1.5 hours away),11/4/15,1
timothymschier,not a scala/compiler version thing? I was using 2.10.x (2.10.5?) but saw some GeoTrellis commits on the migration to 2.11 so have re-setup my IntelliJ to be 2.11.7,11/4/15,1
lossyrob,trying to clear it out the cache...,11/4/15,1
timothymschier,*scala/sbt,11/4/15,1
lossyrob,It uses 2.10 by default,11/4/15,1
timothymschier,ah,11/4/15,1
lossyrob,we_Ñére stuck on 2.10 for now because Spark is,11/4/15,1
timothymschier,ahh,11/4/15,1
timothymschier,"ok, will reconfigure back",11/4/15,1
lossyrob,so you_Ñére saying you were doing it with 2.11?,11/4/15,1
timothymschier,yes,11/4/15,1
timothymschier,though I'm really sure I was getting it with 2.10 too; that was part of the reason for moving to 2.11,11/4/15,1
timothymschier,but small chance I'm misremembering that bit... so if it works for you with a cleared cache maybe don't worry about it until I can reconfigure back...,11/4/15,1
lossyrob,"weird. I deleted it out of my ivy cache, then did a 2.11.5 console, and it still worked",11/4/15,1
timothymschier,"huh, ok I'll try from the console a little later on this morning",11/4/15,1
timothymschier,"gtg, will post up how I go",11/4/15,1
lossyrob,"ok. maybe it_Ñés pulling in that new opencsv dependency for some reason that I_Ñém not, just through ./sbt.",11/4/15,1
lossyrob,"ok, cool. see ya",11/4/15,1
pomadchin,"hm, really weird; build scala 2.10* version on a clean machine (in a Docker container) yesterday",11/4/15,1
timothymschier,"So after coming back in this morning, this doesn't seem to be an issue. I think IntelliJ was using a background task to resolve the dependencies, and that task just hadn't finished... dunno. At any rate that error no longer exists, yay!",11/5/15,1
timothymschier,"Hey folks, next GeoTrellis new guy qn - trying to run the geotrellis-chatta-demo and when I run ```./sbt run``` I get```[info] Compiling 4 Scala sources to /gde/dev/geotrellis-chatta-demo/geotrellis/target/scala-2.10/classes...[error] /gde/dev/geotrellis-chatta-demo/geotrellis/src/main/scala/Model.scala:68: erroneous or inaccessible type[error]                     def apply(col:Int, row:Int, g:Geometry[Any]) {[error]                                                                  ^[error] one error found```(in monospaced font the ```^``` lines up under the ```{```",11/5/15,1
timothymschier,":) ah, as it does within a code section in gitter",11/5/15,1
timothymschier,this is a fresh pull of master btw... I'm quite new to Scala so not sure how to interpret the error,11/5/15,1
pomadchin,"@echeipesh investigting an issue i caught few days ago: https://gist.github.com/pomadchin/753734d1ff0ac3ab9571accumulo log, on a query: ```scalaval rdd = spaceTimeLayerReader.read(                layer,                new RDDQuery[SpaceTimeKey, spaceTimeLayerReader.MetaDataType]                  .where(Intersects(GridBounds(key.col, key.row, key.col, key.row))))```",11/5/15,1
pomadchin,"the data set is _small_; on the same data set, same cluster had not got errors;noticed that this err appers after a certain amount of tiles ingested",11/5/15,1
pomadchin,same means that same machines; but Accumulo 1.6.2 instead of Accumulo 1.5.2,11/5/15,1
pomadchin,amount of tiles ingested into one table,11/5/15,1
lossyrob,@echeipesh What_Ñés the reasoning behind catching a specific error here: https://github.com/geotrellis/geotrellis/blob/master/spark/src/main/scala/geotrellis/spark/io/accumulo/AccumuloLayerReader.scala#L45,11/6/15,1
lossyrob,and a generic error here: https://github.com/geotrellis/geotrellis/blob/master/spark/src/main/scala/geotrellis/spark/io/s3/S3LayerReader.scala#L56,11/6/15,1
lossyrob,and why catch it at all?,11/6/15,1
echeipesh,"@lossyrob: More general version is correct since I added the `.initCause` call to preserve the original exception. The thought was to give context to the underlying exception, what layer we_Ñére loading, in order to give users something to switch on.",11/6/15,1
echeipesh,@pomadchin: That is really strange. That should literally be looking for a single row in a table. I wonder if guys on accumulo IRC might have something to say about that.,11/6/15,1
pomadchin,"@echeipesh agree with you, found table.scan.max.memory setting, it sets scanner buffer memory before sending to client; but not sure it is going to help - think it is for speed tuning",11/6/15,1
echeipesh,is this on space-time layer ?,11/6/15,1
pomadchin,Yes,11/6/15,1
echeipesh,using the new format or the old format?,11/6/15,1
pomadchin,"Z3, by days, using new, reingest master",11/6/15,1
echeipesh,I wonder if there is something about the iterators that we_Ñére using for filtering that is forcing the whole row (all the column families) to be put into memory and that_Ñés blowing the heap. That would be a good guess if the table has a lot of layers.,11/6/15,1
pomadchin,"+1 for the idea, will try on a big data set with RowMajorIndex",11/6/15,1
echeipesh,"heh, row major, bringing ti back to the basics",11/6/15,1
pomadchin,:d just cause got full layer,11/6/15,1
pomadchin,another step will be to try same data set on z3 ,11/6/15,1
pomadchin,so will be only one time layer ,11/6/15,1
pomadchin,"I've got a feeling that's defiantly right idea, the err appeared after having ~ 7 time layers",11/6/15,1
lossyrob,"@echeipesh I_Ñém wondering the worthwhileness of catching generic excpetions and wrapping it other excpetions. If it_Ñés something worth doing, we should make a pattern of it so it_Ñés clear when to do it, if it_Ñés not we should just not do the catch&throw",11/6/15,1
lossyrob,"and i_Ñém not sure why it_Ñés worth doing, but I could be convinced if there_Ñés convincing arguments",11/6/15,1
echeipesh,"I get your drift. It_Ñés a little hard to answer without client code to reference. But here is one use case: If I_Ñém wrapping exceptions in a spray service and showing the message, then a exception that says _ÑÒtried to laod a layer X, failed_Ñù is much more helpufl than _ÑÒFailed to find attribute X_metadata on s3_Ñù or something like that.",11/6/15,1
echeipesh,There is a java argument for doing it: http://tutorials.jenkov.com/java-exception-handling/exception-wrapping.html,11/6/15,1
lossyrob,"I think you would just catch the general exception in the spray code and state that then, and then the middle exception isn_Ñét worthwhile, yeah?",11/6/15,1
lossyrob,"with java, you have to declare exceptions that can be thrown. and that would be a reason, is a java reason",11/6/15,1
lossyrob,"I guess, if you want the exception to be caught from the spray code that specifically says it_Ñés in the load portion. and you want to match over exception types in an outer try catch, and depend on matching to the correct _ÑÉarea_Ñé that the exception would have been thrown. But that would take a pretty consistent wrapping of exceptions based on code areas in order to be consistently useful I think",11/6/15,1
echeipesh,Is that an argument for wrapping exceptions or against ?,11/6/15,1
lossyrob,"it_Ñés saying, in order for wrapping exceptions to be useful in the case of the client knowing which ones to catch and make some use of so that they can talk about where the exception came from in the middle instead of just reporting the source, then we_Ñéd have to be very consistent in how we wrapped exceptions and at what levels we wrapped exceptions, and how those wrappers were communicated to the consumers of the library. Which I think is more work than the payoff merits. But if we don_Ñét do it consistently, then there_Ñés no payoff. So I_Ñém in favor of not wrapping.",11/6/15,1
nicerobot,Has anyone here evaluated http://geospark.datasyslab.org/ yet? ,11/8/15,1
echeipesh,"Have not seen that yet, but looks very interesting. Going to look over it today actually.",11/9/15,1
timothymschier,"Hey folks. I'm still getting this weird ```erroneous or inaccessible type``` compile error with the chatta demo (a clean pull of master). It didn't happen on a setup that I had going 2 weeks ago. Googleing for that error seems to suggest Scala compiler issues. Is anyone else seeing this? Any suggestions on how I might trouble-shoot it? I was hoping to use the chatta demo as a platform for me experimenting/ getting my own GeoTrellis demo up (ingesting Landsat data).",11/10/15,1
timothymschier,"OK, scrap that one. I had a weird network file share permissions issue that (I think) was causing issues with cached libs. I fixed the permissions, did an ```./sbt clean``` and ```./sbt run``` and we're all good. Weird error though.",11/10/15,1
timothymschier,"Hey folks, can I ask for a bit of a lead on a task I'm trying out.The task is to ingest ~40GB of Landsat GeoTIFFs that cover a few hundred square kilometres, and then run a temporal/geographic query on it that can produce an image output (either rendered on screen, or output to a file). Using the GeoTrellis Ansible scripts I've got a HDFS/Accumulo stack going.I've only been looking at 0.10.0-SNAPSHOT code and tests so far (not seen any doc or other sample code yet?). I've seen the SingleBandGeoTiff class, but it seems useful for converting the tiff to arrays, not really ingesting for future querying. Is the spark.ingest package where I should be looking? I'm not sure Spark is up and would be happy enough to just use Accumulo if that's a possibility? The spark.io.accumulo package doesn't seem to have direct ingest support?Any input is welcome, as I'm keen for every time saving I can find at this point! Is anyone working on a dev branch with sample code or tests that I can look over? Or maybe list of a few classes that I can jimmy together to get something up?Thanks all :)",11/11/15,1
pomadchin," @timothymschier , Yo! The example of the gt usage (on spray) is here: https://github.com/geotrellis/gt-admin (ingest sample and server projects) But the api is deprecated and not up to master branch ): This code can give you an idea, that it is possible to get tiles by key (spatial / space time) easily, and make a png / tiff output in a desirable projection (though there are some limitations, not all projections still supported), and it is possible to make queries by extent , spatial bounds, time bounds.Gt works on top of spark, so you need it (btw, dunno better idea how to manage lots of big files :D), you can try to use Docker containers (used them on a fat machine to emulate real-like dev cluster (3 nodes) to work with gt apps, there is spark included): https://github.com/pomadchin/accumulo-spark(only spark project):ingesting :https://github.com/geotrellis/geotrellis/blob/master/spark/src/main/scala/geotrellis/spark/ingest/Ingest.scalapyramid: https://github.com/geotrellis/geotrellis/blob/master/spark/src/main/scala/geotrellis/spark/ingest/Pyramid.scalaAlso writer and reader:  https://github.com/geotrellis/geotrellis/blob/master/spark/src/main/scala/geotrellis/spark/io/accumulo/AccumuloLayerWriter.scala https://github.com/geotrellis/geotrellis/blob/master/spark/src/main/scala/geotrellis/spark/io/accumulo/AccumuloLayerReader.scala",11/11/15,1
pomadchin,"Btw, strongly recommend to look into the ETL project",11/11/15,1
timothymschier,"Thanks heaps @pomadchin, I'll look into all those projects and code. ",11/11/15,1
timothymschier,"I was asking about Spark because I was wondering if the rasters may be stored in straight HDFS or tiled into Accumulo blobs, or something else (one example I've seen is each pixel as a vector!). So it's good to know that Spark is mandatory. ",11/11/15,1
pomadchin,,11/11/15,1
timothymschier,"Btw I've been using the project at https://github.com/geotrellis/geotrellis-ec2-cluster to configure a local cluster, and it does include Ansible tasks for Spark. I've not tested those tasks because I haven't needed Spark until now, but I'm guessing it will work as well as all the rest did :smile: ",11/11/15,1
timothymschier,I'm clearly quite the beginner so I appreciate all the input too :smile: ,11/11/15,1
pomadchin,"ye, just write questions here, and the supa guyz (there are lots in this chat) will help you :DActually you mensioned raster storage, so there are possible ways: HDFS / Accumulo / S3 ",11/11/15,1
timothymschier,"Ah OK, I was using the wrong term then. Rasters that are just stored in ""raster storage"", are they indexed for queries? Or do you have to ingest into Spark for that? ",11/11/15,1
pomadchin,"Spark is used for in memory operations. For indexing used SpaceFilling curves (ZCurve, Hilbert curve & RowMajor). The idea that you have some tiles (somewhere, in hdfs folder, for example) (tiff / netcdf) and want to ingest into the storage. During this ingest process your tiles will be ingested according zoomed layout scheme for a choosed projection, each raster will be cut into tiles of this scheme for a certain zoom, and index builds according to choosed strategy (ZCurve / Hilbert curve/ RowMajor) for a certain tile. (sry for a rough explanation)",11/11/15,1
timothymschier,"That's a helpful explanation, thanks. I see from gt-admin that there is a spark-submit command which I can use to get data from my file  system into Spark, and indexed in Gt...",11/11/15,1
timothymschier,"I think that's enough for me for tonight.  I'm off to bed, to get ready so I can tackle this tomorrow :smile: ",11/11/15,1
lossyrob,"hey @timothymschier , the way to ingest the landsat data is using the spark-etl subproject (see the README here https://github.com/geotrellis/geotrellis/tree/master/spark-etl)",11/11/15,1
lossyrob,"you would ingest the data into S3 or Accumulo. Depending if you want to ingest separate layers for each band (if the bands are each single band GeoTiffs) or want them as one MultiBand layer, you might have to do some work on transforming the data before ingest or write some adapters for ingesting multiple single band tiffs into one multiband raster.",11/11/15,1
lossyrob,"at that point, they will be accessible as a Layer through the LayerReader (either AccumuloLayerReader or S3LayerReader, in geotrellis.spark.io.{accumulo|spark}",11/11/15,1
lossyrob,"you can query a layer via a spatial or spatio-temporal bound, and then once the tiles are returned (as an RDD), either collect them and work with them on the driver machine or do some transformations as RDDs and then collect them. for instance, you could do the rendering into a PNG through RDD operations and then serve out the image",11/11/15,1
owcm,"I am new to Geotrellis and trying to use 0.10.0-M1.  Trying to rewrite some code I had working under 0.9.  I am reading in a GeoTiff using GeoTiffReader and want to convert to a RasterSource so I can do some simple elevation and focal operations. I was doing all my operations on Rastersources in 0.9 , is that not the way to go in 0.10.  So I am trying to convert my GeoTiff raster or tile to a Rastersource.",11/11/15,1
owcm,"Figured out, pass in the Tile associated with the raster and the extent to RasterSource: val geoTiff = GeoTiffReader.readSingleBand( filePath ).raster// Move from rasterSource to rasterval rs = RasterSource( geoTiff.TIle, geoTiff.extent )",11/11/15,1
timothymschier,"thanks @lossyrob, am on it...",11/12/15,1
lossyrob,"hey @owcm, you can probably just run the operations on the tile as well. Operations are added to the `Tile` type through implicits that are imported by packages, so for example if you do `import geotrellis.raster.op.focal_` or `import geotrellis.raster.op.elevation._`, you should be able to call `tile.hillshade` etc on the tile directly instead of going through the RasterSource",11/12/15,1
gfinch,"Hi ... we're evaluating Geotrellis for a new project.  So far, it looks fantastic.  One question ... it looks like Geotrellis can extract Quantile breaks from raster histograms .  We have a need for Jenks natural breaks.  Any plans to add that?",11/12/15,1
pomadchin,"@echeipesh have tested that issue with the issue with an accumulo iterator overflow: it fetches all rows between space time keys, i tried for 4 layers, and made a query:```scala.where(Intersects(GridBounds(key.col, key.row, key.col, key.row))).where(Between(...))```In a reader we set ranges: ```scalaInputFormatBase.setRanges(job, ranges)ranges.length //> 181399```Mb the reason smth here? fetches all the data from that range?",11/13/15,1
lossyrob,"@gfinch there_Ñés plans to add it if there_Ñés interest, or someone wants to add it ;)",11/13/15,1
lossyrob,"but really, I_Ñéve only used quantile breaks, but always thought it would be great to have a range of break calculation capabilities.",11/13/15,1
lossyrob,The best way to get the ball rolling on that would be to make a github issue for Jenks natural breaks on the geotrellis repo.,11/13/15,1
lossyrob,"if you could include some information about that type of breaks (I_Ñém not familiar with them), any sort of resources and specific qualities the feature would need to have, that would really help",11/13/15,1
lossyrob,"also, if it_Ñés something you feel like you might want to contribute, I_Ñéd be happy to help guide you along with the development",11/13/15,1
jwalgran,I am working through updating a spray service to use `S3LayerReader` instead of `S3RasterCatalog` and I am having trouble figuring out how to correctly create an `S3TileReader`.,11/13/15,1
jwalgran,"I was using this code to fetch a single tile from an `S3RasterCatalog` given a layer name and ZXY coordinates, TMS style``` val reader = catalog.tileReader[SpatialKey]((layer, z)) val tile = reader(SpatialKey(x, y))```",11/13/15,1
jwalgran,"I was misreading the source. It is just a simple `S3TileReader(bucket, prefix)`",11/13/15,1
gfinch,"@lossyrob Thanks for the info about breaks.  I'll get an issue into github sometime next week on this with some details.  We may be able to contribute - if so, I'll reach out again for some guidance to get us going.",11/13/15,1
pomadchin,@jwalgran everything is ok now?,11/13/15,1
jwalgran,"I am now having trouble finding the best way to get the CRS for an S3 layer. With an `S3RasterCatalog` I was able to do this:```val rasterMetaData = catalog.getLayerMetadata(layerId).rasterMetaData```",11/13/15,1
jwalgran,The use case is that I have a service that takes a coordinate and returns the value of a raster at that point. I need to fetch the CRS of the layer so I can project the point to the propert coordinate system.,11/13/15,1
pomadchin,"You can try to get metadata out of the attributeStore:```scalaval instance = AccumuloInstance(_Ñ_)val attributeStore = AccumuloAttributeStore(instance.connector)def getSpaceTimeAttributes(id: LayerId) = attributeStore.readLayerAttributes[    AccumuloLayerHeader, RasterMetaData, KeyBounds[SpaceTimeKey], KeyIndex[SpaceTimeKey], Schema](id)```",11/13/15,1
pomadchin,"or more generic:```scaladef getAttributes[K](id: LayerId) = attributeStore.readLayerAttributes[    AccumuloLayerHeader, RasterMetaData, KeyBounds[K], KeyIndex[K], Schema](id)```",11/13/15,1
pomadchin,"```scalagetAttributes(LayerId(_ÑÒlayer_Ñù, zoom))._2```",11/13/15,1
jwalgran,That is where I was headed. Thanks. Bumping into a `could not find implicit value for evidence parameter of type spray.json.JsonFormat[geotrellis.spark.RasterMetaData]` compile error at the moment.,11/13/15,1
pomadchin,"```scalaimport geotrellis.spark.io.json._```",11/13/15,1
jwalgran,Compiles! Thanks.,11/13/15,1
pomadchin,glad to hear it (:,11/13/15,1
pomadchin,"@echeipesh again with this query overflow: seems to me like smth wron gone with index method serializing: ```scalaval zbounds: KeyBounds[SpaceTimeKey] = lmd._3val zindex: KeyIndex[SpaceTimeKey] = lmd._4val zindexFake: KeyIndex[SpaceTimeKey] = ZCurveKeyIndexMethod.byPattern(""YM"").createIndex(zbounds)zindex.indexRanges(zbounds.minKey, zbounds.maxKey) //> length1zindexFake.indexRanges(zbounds.minKey, zbounds.maxKey) //> length2// not eqauled```Tiles were ingested `byPattern(_ÑÒYM"")`, and seems like the `DateTime => Int` function after deserialization differs",11/16/15,1
lossyrob,Hmmm that would certainly be a bug,11/16/15,1
lossyrob,"could we write a unit test to prove that? Create teh key index, serialize it, deserailize it, and then compare the results with an assert?",11/16/15,1
pomadchin,"@lossyrob testing it right now, mb as usual my hands curvity  %)",11/16/15,1
lossyrob,"haha I had to check with Eugene about the Russian slang, but I get it :)",11/16/15,1
lossyrob,"also made some comments on the PR, getting real close",11/16/15,1
pomadchin,":D ye, it sounds like _ÑÒkrivizna ruk_Ñù, mb similar to ""both left hands_Ñù ",11/16/15,1
lossyrob,all thumbs is another one,11/16/15,1
pomadchin,ouch,11/16/15,1
pomadchin,:D ty!,11/16/15,1
lossyrob,haha not saying that_Ñés you. just saying that_Ñés an expression,11/16/15,1
pomadchin,got it ofcource ;),11/16/15,1
pomadchin,"hm according to tests, is it acceptable to make a manual keys update for a `sample` rdd in tests, or we need the new files, not intersected? ",11/16/15,1
lossyrob,I think a properly concocted manually setup test works fine,11/16/15,1
pomadchin,ok,11/16/15,1
pomadchin,"@lossyrob there is a `PersistenceSpec` with too generic `Key`, so two variants: to add additional `(Spatial|Space)PersistenceSpec` trait, or to write them to `AllOnesTestTileTests|CoordinateSpaceTimeTests` ",11/16/15,1
lossyrob,hmmm lemme check it out,11/16/15,1
lossyrob,"so, yeah that seems a little messy",11/16/15,1
lossyrob,"for one I think the AllOnesTest* trait and CoordinateSpaceTime* trait should be broken out to their own files, for clarity",11/16/15,1
lossyrob,"Yeah, I think adding it to one of those makes sense",11/16/15,1
lossyrob,because the logic for the updater would be the same between the two different types of keys,11/16/15,1
pomadchin,kk done,11/16/15,1
lossyrob,nice. Can you break those traits out into their own files? AllOnestTestTileTests and CoordinateSpaceTimeTests. After that I_Ñém +1. I_Ñéll leave it up to Eugene to +1 and then we_Ñére good to go,11/16/15,1
pomadchin,:D,11/16/15,1
pomadchin,no problems,11/16/15,1
pomadchin,(: waiting for comments ,11/16/15,1
lossyrob,"nice. we_Ñéll let @echeipesh give a look-see, but +1 from me",11/16/15,1
pomadchin,"@lossyrob @echeipesh I am glad to tell you that no bugs detected_Ñ_ :D i_Ñéve checked everything_Ñ_ so the problem was with my hands :D OMGI ingested tiles with index, `byPattern(""YD_Ñù)`: that was the main fault, as we know java builds datetime withoud leading zeros )': , so `_ÑùYD_Ñù, 2015-001` equals to `20151`, but `2015-256` to `2015256`...the right _ÑÉevery day index_Ñé is `byPattern(_ÑÒYddd_Ñù)`the range will be much smaller",11/16/15,1
lossyrob,oh whoa,11/16/15,1
lossyrob,yeah string date parsing has bit me very hard in the past,11/16/15,1
pomadchin,"yea_Ñ_ that zeros, ffff",11/16/15,1
echeipesh,"I_Ñém going to take a little time squinting at #1192, but looks good so far.",11/17/15,1
echeipesh,"btw, I threw up some notes on my current direction in spatial partitioning and spatial joins here: https://github.com/geotrellis/geotrellis/issues/1206",11/17/15,1
pomadchin,@echeipesh spatial join sounds very very cool ,11/17/15,1
lossyrob,"@echeipesh it_Ñés a good write up, but I_Ñém confused and I can_Ñét tell if that_Ñés because I_Ñém too dumb or things need clarification",11/17/15,1
lossyrob,"""By dropping r least significant bits we group points in the (quad/oct)tree region implied by the prefix bits._Ñù Drop r least significant bits in what?",11/17/15,1
lossyrob,"""This operation is valid for both Morton and Hilbert curves._Ñù What about Z?",11/17/15,1
pomadchin,"@lossyrob z is the other name for morton, learned it yesterday while debugging my mistakes",11/17/15,1
lossyrob,oh nice,11/17/15,1
lossyrob,"see, I_Ñém just dumb ;)",11/17/15,1
pomadchin,":D if u are dumb, don't want to know who I am :D",11/17/15,1
lossyrob,"""Given an arbitrary region Q we can decompose it into a set of prefixes using same mapping to form Pq. We may then only inspect partitions corresponding to intersection of Pq and Pr. _ÑÒ I think I know where Pr is coming from, but I think the setup to this section could be clearer",11/17/15,1
lossyrob,actually I take that back I don_Ñét know what Pr is,11/17/15,1
lossyrob,"my take away is starting to be that, you are trying to use a sort of higher level tile layout to partition the tiles",11/17/15,1
lossyrob,"""If both left and right RDDs have a SpacePartitioner we can define OneToOneDependency between each partition of the left RDD and each partition of right RDD. _ÑÒ Do you mean, the same SpacePartitioner?",11/17/15,1
lossyrob,"It seems like if there_Ñés multiple SPacePartitioners, those should be merged for maximum efficiency and that merged partitioner used for the shuffle",11/17/15,1
lossyrob,"For clarity, the last bit about location preference is only for spatial joins with more than on RDD, so if there is a partition of the first RDD that lives on node A, and the partitions of the other RDDs live on node B, then the cogroup logic might choose node A over B, where as the partitionaware logic would pick node B. is that correct?",11/17/15,1
lossyrob,"I think these are great ideas, and maybe after doing some clarifying edits we could post this around to try to get some opinions on it from the community (geowave, geomesa, geojinni, etc)",11/17/15,1
echeipesh,"@lossyrob Kind of in order: Pq are the set of prefixes that intersect query region, Pr is set of prefix that intersect the RDD. (I agree, setup needs to be better). 2) The trick is that there are multiple SpacePartitioners that have different order of prefixes. But that check to align partitions can be done on driver side as RDDs are chained. So they are effectivaly merged on computations (on inner join that is) 3) locality preference: yes. It only matters when joining more than one RDD, that_Ñés a good thing to point out.",11/17/15,1
echeipesh,"I_Ñéll re-read and refine it further, I_Ñéd love to have some feedback and discussion. Thanks for the comments.",11/17/15,1
lossyrob,can you clarify what you mean by _ÑÒdifferent order of prefixes_Ñù?,11/17/15,1
echeipesh,"Sure, if two RDDs cover different regions they will be covered by different set of prefixes. For the purprose of RDD partition index we_Ñére just going to sort them and use that index. So prefix 9ad4 might be at index 3 in RDD A and might be at index 6 in RDD B (or might not be there at all).",11/17/15,1
lossyrob,ok. maybe there_Ñés a better term then prefix?,11/17/15,1
echeipesh,region ?,11/17/15,1
lossyrob,"so, prefix is meaning, the specific region of the space filling curve that is of a lower resolution than the actual curve",11/17/15,1
echeipesh,"quadtree wiki uses terms quadrant or region. Since this could be 3 d, region seems better.",11/17/15,1
lossyrob,partition region?,11/17/15,1
echeipesh,"yes, thats a better way of saying it.",11/17/15,1
lossyrob,"So there_Ñés two partition regions that match, but they are assigned different partition numbers in the respective SpaceParititioners",11/17/15,1
echeipesh,yep,11/17/15,1
lossyrob,so the two SpacePartitioners need to have the same Region resolution,11/17/15,1
lossyrob,or could one have less resolution,11/17/15,1
echeipesh,For the moment yes. Although interestingly enough `NarrowDependency` allows to return a List of partitions.,11/17/15,1
echeipesh,"So I_Ñém guessing if that list is always lower than some fairly low constant, like 4, it would be ok.",11/17/15,1
echeipesh,"In future we might be able to join two RDDs that are partitioned one zoom level apart, so to speak. So one partition might match 4 partitions in another RDD.",11/17/15,1
lossyrob,right. and a lower-resolution region partition bounds should always fully contain or not contain any region of a higher resolution partitioner,11/17/15,1
echeipesh,correct,11/17/15,1
lossyrob,is it worth trying to bake that in now? Can we come up with use cases for that?,11/17/15,1
echeipesh,I don_Ñét think so,11/17/15,1
lossyrob,I can in the vector case,11/17/15,1
echeipesh,"I think it_Ñés mostly going to control for partition size, but there isn_Ñét an exact target there, _ÑÒnot too small_Ñù and _ÑÒnot too big_Ñù can be covered by a single value.",11/17/15,1
lossyrob,"for larger features, it might have a lower region resolution, and for more dense point data say, it would have higher",11/17/15,1
echeipesh,Plenty of things to worry about here before dealing with vector ideas.,11/17/15,1
lossyrob,yeah. I just don_Ñét want to have to do deep refactors if we can account for it now,11/17/15,1
echeipesh,"I think that part of it should be pretty hidden behind RDD API, so refactors shouldn_Ñét be very deep.",11/17/15,1
echeipesh,"Anyway, partitioning geometries of non-uniform size is much harder problem I think.",11/17/15,1
lossyrob,what about two RasterRDDs of different density,11/17/15,1
lossyrob,and if we_Ñére tooling to partition size as in number of tiles stored,11/17/15,1
echeipesh,you mean with two different tile sizes ?,11/17/15,1
lossyrob,the region resolution of the less dense one will be less,11/17/15,1
lossyrob,no. same tile size,11/17/15,1
echeipesh,so how could they have different density ?,11/17/15,1
lossyrob,"so, NEX has a specific density, full coverage uniform monthly",11/17/15,1
lossyrob,and I have a much less dense imagery set,11/17/15,1
echeipesh,so annual or something ?,11/17/15,1
lossyrob,or some yearly set say. that maybe doesn_Ñét cover the whole land,11/17/15,1
lossyrob,yeah,11/17/15,1
lossyrob,"so in order to have the aprtition size correct on either RDD, the region resolution is different",11/17/15,1
lossyrob,(I_Ñém not sure how we come up with that region resolution number),11/17/15,1
echeipesh,"That actually auto scales, two RDDs will have different number of partitions.",11/17/15,1
echeipesh,So each region can have at most N elements.,11/17/15,1
lossyrob,"right, but will they have different _ÑÒleast significant bits""",11/17/15,1
lossyrob,"so in order for the more dense region to have at most N elements, that region needs to have more significant bits. and for the less dense, it needs less significant bits. So the matching gets into this one-to-many dependency thing",11/17/15,1
echeipesh,"Ah, I see what you_Ñére getting that. There will be 12 keys from first RDD and 1 key from right RDD. I think that_Ñés actually really good argument for CoGroup like interface. where you would get Iterator[A] with 12 tiles and Iterator[B] with 1 tile.",11/17/15,1
lossyrob,but the keys are the same,11/17/15,1
lossyrob,"and in the tiles case, will still just have 1 tile from rdd A and 1 tile from rdd B",11/17/15,1
echeipesh,I don_Ñét think I follow. They keys can_Ñét be the same. Otherwise you have 12 records in RDD a with the same key,11/17/15,1
lossyrob,sorry. disregard that statement,11/17/15,1
lossyrob,but the cogroup iterables are not based on the partitions,11/17/15,1
lossyrob,they are based on the values of the keys in each rdd,11/17/15,1
echeipesh,"But I see what you_Ñére actually asking. It_Ñés a good question, I don_Ñét have a fully baked answer for that actually, let me think about it.",11/17/15,1
echeipesh,"Sure, you just need to have a rule that matches keys to partitions to be able to discard partitions fully before you do per-key match.",11/17/15,1
lossyrob,yeah. I_Ñém just wondering how that plays into the narrow dependency thing. Where we_Ñéd have a set of narrow dependencies from one RDD and one narrow dependency from the other,11/17/15,1
echeipesh,What you are saying is that its possible to have dataset that are sparse and you would never reach full density provided by partitining your space. That_Ñés true. You essnetially have unused space because of your data layout.,11/17/15,1
echeipesh,I think the question I need to answer is how to join two RDDs where one is indexed annually and another is indexed monthly (as their maximum densities).,11/17/15,1
lossyrob,"well, not _ÑÒindexed_Ñù as in the SFC index that is used for storage",11/17/15,1
lossyrob,but the SFC that_Ñés used for the partition regions,11/17/15,1
lossyrob,"so one space is using a SFC with a lower resolution, and the other an SFC with a higher resolution (to index the regions), and then how do we combine those",11/17/15,1
echeipesh,"yep, thats the question",11/17/15,1
lossyrob,"this is assuming that if I put a SpacePartitioner on the lower density RDD, it would come up with the lower resolution SFC for the region partitions, and if I applied a SpacePartitioner to a higher density it would higher density on the region partitioners. And then I do a join between them.",11/17/15,1
lossyrob,yeah,11/17/15,1
echeipesh,I_Ñéll have to see what the limist of NarrowDependency are exactly,11/17/15,1
echeipesh,updated spatial partitioning writeup to make it more readable. Also switching my opinion that CoGroupRDD is a better model.,11/19/15,1
echeipesh,@pomadchin: I just wanted to double check that hadoop MapFile is actually updatable like that? It doesn_Ñét do something crazy like overriding whole part files but just updates only the right records?,11/19/15,1
pomadchin,"@echeipesh checked it on a small amount of data, and didn_Ñét noticed slow speed, seems yes, but can check it right now",11/19/15,1
pomadchin,*will,11/19/15,1
echeipesh,I_Ñém more worried about it blowing away records that are not part of the update.,11/19/15,1
pomadchin,"I checked so: loaded couple tiles, which intersects each other, and the updated them in different order one by one; checked them looking on a tms map (visual)",11/19/15,1
echeipesh,I_Ñém kind of googling around on this: https://hadoop.apache.org/docs/current/api/org/apache/hadoop/io/MapFile.html,11/19/15,1
echeipesh,"and Quora, sigh: https://www.quora.com/How-do-I-update-a-MapFile-in-Hadoop",11/19/15,1
pomadchin,looks suspicious (my code),11/19/15,1
echeipesh,"What I am afraid is might be happening is if the written dataset has `part1,part2, part3` and an update is happening on `part2` records: we load the `part2` records, merge them and write, but they get written as `part1`.",11/19/15,1
echeipesh,"So we end up with `part1` overwritten, `part2` and `part3` unchanged, but now with indecies out of order.",11/19/15,1
pomadchin,"reading now about this problem, but during test mb was too lucky and lose nothing; even updated (but with custom updater) tiles out of current bounds (zcurve)",11/19/15,1
bneff,I've added my own raster to the geotrellis-spray-tutorial.  The NODATA for this raster is -99999.  Is there a way I can specify the NODATA so the statistics don't include these values?,11/19/15,1
echeipesh,@bneff: GeoTrellis uses constant NoData values for performance reasons. For int tiles it_Ñés the contstant NODATA. If you_Ñére reading your layer from GeoTiff the conversion will happen if the GeoTiff tags are set to mention NoData value. If your ingest process is different you can .map over Tile and replace the values manually.,11/19/15,1
bneff,"@echeipesh: ok got it.  I was amazed how how fast everything was, but adding that map process is like 10x slower.  I converted my GeoTiff to ARG using gdal_translate, so I'll look at that for a bit unless you have the options handy.",11/19/15,1
echeipesh,did you GeoTiffs have NoData tag on them? I guess gdal_translate didn_Ñét respect that ?,11/19/15,1
bneff,"@echeipesh thanks for pointing me in the right direction.  The source GeoTiff has the correct NoData tag, but if I specify the -of arg in the command line It sets NoData to nan.  If I don't specify anything and just do gdal_translate <source.tiff> <output.arg> then NoData is correct...  I'll keep playing around.  Thanks again!",11/19/15,1
lossyrob,@bneff glad to hear you got that worked out. I_Ñém suprised the inclusion of the map step was 10 times slower! Can you point to where in the code you put that map?,11/19/15,1
bneff,"Hi @lossyrob :  I added this inside the rasters/stats route: val processed = raster.localMap{ x => if (x > 0) x else NODATA } .  I'm still very new so this could be completely wrong, but I'm eager to learn the proper ways.  Also when I say 10x slower - It goes from ~30ms without it to ~350ms with it.  So it isn't unusable, but I couldn't help but think it doesn't seem necessary if I can ingest properly",11/19/15,1
lossyrob,@bneff here? https://github.com/geotrellis/geotrellis-spray-tutorial/blob/master/src/main/scala/GeoTrellisService.scala#L73,11/19/15,1
lossyrob,so,11/19/15,1
lossyrob,"```scalaval histogramSource: ValueSource[Histogram] = raster.localmap { x => if(x > 0) x else NODATA }.histogram()```",11/19/15,1
lossyrob,what_Ñés the data look like? What datatype/how big,11/19/15,1
bneff,@lossyrob can I delete my prior comments?  I just added it back in to test and its ~25ms.  I swear I'm not crazy!,11/19/15,1
lossyrob,:) no worries,11/19/15,1
lossyrob,timings can be weird sometimes,11/19/15,1
lossyrob,"we use a microbenchmarking framework to get some consistency and make sure we_Ñére not making changes that will effect performance, and sometimes the timings I see from that are very strange.",11/19/15,1
bneff,"@lossyrob sorry for the false positive.  Thats what happens when I work unsupervised! =).  If you see Mr. Cheetham, give him a nudge for me!",11/19/15,1
pomadchin,"@echeipesh seems you are right, and in my test cases just updated lucky parts;",11/19/15,1
pomadchin,weird that it is working,11/19/15,1
echeipesh,I think it works for the cases where all the tiles fit within one block on HDFS.,11/19/15,1
pomadchin,++,11/19/15,1
pomadchin,so a variant to copy it with update sounds not bad ,11/19/15,1
pomadchin,"smth like (possible not best solution)?```scalaval updatedPart = rdd      .map { case (key, value) =>        val kw = fmt.kClass.newInstance()        kw.set(closureKeyIndex.toIndex(key), key)        val kv = fmt.vClass.newInstance()        kv.set(value)        (kw, kv)      }.sortByKey(numPartitions = partitions)val allParts = readFully(path)(allParts merge updatedPart).saveAsNewAPIHadoopFile(_Ñ_) // to a new path // + updateLayer metaData to point to a new dir```",11/19/15,1
pomadchin,"hm, but mb it doesn't read the wole mapfile",11/19/15,1
pomadchin,Whole*,11/19/15,1
pomadchin,"or Mb it is possible to merge at least with this bug, and then to move forward, or try to solve it during this commit :)",11/19/15,1
lossyrob,I think we should merge in a bug :p,11/19/15,1
lossyrob,*shouldn't,11/19/15,1
lossyrob,"@bneff Cheetham nudged, say hello :)",11/20/15,1
bneff,@lossyrob thank you!  I'll shoot him another email,11/20/15,1
lossyrob,sorry that was supposed to say _ÑÒsays hello_Ñù lol,11/20/15,1
lossyrob,"@anandthakker hey, working on a scala client for `landsat-api`, thought you might be interested: https://github.com/azavea/scala-landsat-util",11/21/15,1
pomadchin,"@lossyrob nice option will be to add the second google src, and (if it's possible) from the official usgs server, (noticed some delay on amazon / google)",11/21/15,1
pomadchin,cool,11/21/15,1
lossyrob,"@pomadchin this just gives what the `lansat-api` gives as far as information goes (and a few convenience methods for s3 paths). Adding similar methods for those sources would be a good idea (I_Ñém less familiar with those sources, but if someone in the know wants to add, please do!). I_Ñém going to use it to do geotrellis ingests, which I_Ñém just going to start with images available through the s3 bucket for simplicity, but that could end up being expanded to check all sources",11/21/15,1
pomadchin,@lossyrob seems like I can try (hope i can) to help you :) to start with google: it also has a nice api,11/21/15,1
pomadchin,but with the official usgs server there can be problems,11/21/15,1
pomadchin,* but first of all i will finish that long and very very interesting #1192,11/21/15,1
pomadchin,:D ):,11/21/15,1
lossyrob,"yeah, that HDFS fix is a tough one. I_Ñém not entirely sure what the approach is to solve that",11/21/15,1
lossyrob,you basically have to overwrite the entire map file that any of the update tiles belong to,11/21/15,1
lossyrob,"tbh, what I_Ñéve read about mapfiles is, don_Ñét try to update them. they are not meant for updates. at least that_Ñés waht I remember reading",11/21/15,1
lossyrob,"https://hadoop.apache.org/docs/current/api/org/apache/hadoop/io/MapFile.html: Map files are created by adding entries in-order. To maintain a large database, perform updates by copying the previous version of a database and merging in a sorted change list, to create a new version of the database in a new file. Sorting large change lists can be done with SequenceFile.Sorter.",11/21/15,1
pomadchin,"@echeipesh suggested to make copy on write, so i think a nice idea would be the following: * read only intersected tiles (already ingested, will call them old)* merge them with new (`val rdd = newRdd merge oldRdd`)* read all tiles except interseted, and make union:  (`rdd union neIntersectedRdd`)* wirte to a _new dir_* rename _old dir_ (or fix metadata dir for layer)* rename _new dir_ to _old dir_",11/21/15,1
lossyrob,"why not just do the merge on the entire layer, if you_Ñére going to read it in anyway?",11/21/15,1
pomadchin,"hm or ye, to merge all",11/21/15,1
lossyrob,"right, because the merge function handles the first RDD having more tiles than the other, right? is there any performance impact you can imagine of doing it that way?",11/21/15,1
pomadchin,"in general case seems no; (just thought about the case, when we need to update a very very small amount of tiles compared to already ingested), but just looked through the merge function, and we need anyway the entire rdd, so it is cheaper just to make one merge on an entire set, rather my weird flow example %)",11/21/15,1
lossyrob,yeah. I think so. I think it should be clearly documented that HDFS is really bad for updates.,11/21/15,1
pomadchin,yeah,11/21/15,1
lossyrob,"but it_Ñés really not meant to be a RW database, so that makes sense",11/21/15,1
pomadchin,"and with the old dataset (not updated), where should we put it? just renaming?",11/21/15,1
lossyrob,I_Ñém not sure,11/21/15,1
lossyrob,you know what. If it_Ñés really going to be a whole-layer replace,11/21/15,1
lossyrob,requiring that we move the old layer ,11/21/15,1
lossyrob,"or delete it, or rename the new layer",11/21/15,1
lossyrob,then I don_Ñét really think this is an update,11/21/15,1
lossyrob,in the sense that there should be a LayerUpdater for it,11/21/15,1
pomadchin,yes yes,11/21/15,1
lossyrob,"I think we need good functionality for layer management, move, delete, rename",11/21/15,1
lossyrob,but that_Ñés a separate issue,11/21/15,1
lossyrob,"so perhaps the thing to do here is to just remove the Hadoop layer updater, and then make clear documentation about why hadoop doesn_Ñét have one and why",11/21/15,1
pomadchin,"so as a possible variant, we can not to make such updater for hadoop, and to wait layer management",11/21/15,1
pomadchin,oops lags,11/21/15,1
pomadchin,"cool! btw really want to start `layer manager` too o: sounds awesome :D ",11/21/15,1
lossyrob,yeah those functions are something I think we want to implement before the 0.10 release,11/21/15,1
lossyrob,the most overdue release ever :x ,11/21/15,1
lossyrob,"but once Eugene finishes the spatial partitioning work, that_Ñés the last big feature I want I think is necessary for the release. so things like that, then testing/benchmarking/documentation phase, then release",11/21/15,1
pomadchin,great!,11/21/15,1
pomadchin,is there a full roadmap of todos for the realease? (including tests/bench/docs),11/21/15,1
pomadchin,"ok, so will fix the pr as fast as can to try to help ",11/21/15,1
lossyrob,there are issues that are marked for 0.10,11/21/15,1
lossyrob,but not really. I should perhaps put one together,11/21/15,1
lossyrob,@pomadchin congrats on getting the PR merged in!,11/23/15,1
pomadchin,@lossyrob yeeeee,11/23/15,1
pomadchin,need to work harder to include more cool features into gt and faster make them ready to merge:0 ,11/23/15,1
lossyrob,@echeipesh ping,11/24/15,1
lossyrob,I responded to your PR comment.,11/24/15,1
lossyrob,"Also looking back over your SpatialJoinRDD issue. There_Ñés the line: ""We also do not have to consider sparse tile layouts, assuming that tiles are, or nearly, continuous for some region.""",11/24/15,1
lossyrob,I thought that was a clear case that we needed to support? Sparse tiles as in a layer of satellite images that are taken at odd time intervals and do not necessarily cover a whole spatial region?,11/24/15,1
echeipesh,"By sparse in spatial sense I mean there isn_Ñét a region here and region there. So if there is some region thats contionous in our space, I wouldn_Ñét call that sparse. Perhaps better terminology is needed ?",11/24/15,1
echeipesh,I guess sparse with regards to KeyBounds.,11/24/15,1
lossyrob,"Perhaps. So, if I have a set of images, some may be clustered around a certain time (say DigitalGlobe was tasked with taking images around an area), and other images are far out from there (we are combining with Landsat), and the area that is covered is pretty large but not necessarily covered by many images, or there are a lot of holes if you consider the region as a bounding box. Wouldn_Ñét that be sparse for KeyBounds?",11/24/15,1
echeipesh,Odd time intervals can make it sparse wrt to KeyBounds. I_Ñém not sure if that invalidates the whole space partitioning approach. The alternative is to do some kind of sampling to do data partitioning.,11/24/15,1
echeipesh,"If there are a lot of holes in the bounding box that would be sparse, yes.",11/24/15,1
lossyrob,"Ok. I don_Ñét know if we should make that assumption then. It could be that, the spatial partitioning works better if the dataset is more uniform (which would make sense), but we should keep that use case on the table",11/24/15,1
lossyrob,"Does it have disastorous effects on that use case, or is it just, the more sparse the less the partitions are even?",11/24/15,1
echeipesh,sparse data would create skinny of empty partitions,11/24/15,1
lossyrob,"And what happens if there are empty partitions under the scheme, do they get excluded or would the RDD have empty partitions?",11/24/15,1
echeipesh,They can_Ñét be excluded because they_Ñére built without examining the data. So nobody knows they will be empty until the operations start running.,11/24/15,1
echeipesh,I don_Ñét know what the practical implications of it are exactly. Spark will of course examine and chug through empty partitions quickly,11/24/15,1
lossyrob,Ok. that sounds similar to a RangePartitoner that doesn_Ñét have data in some ranges,11/24/15,1
lossyrob,I wonder what happens there,11/24/15,1
echeipesh,I think a course of action would be to implement the current plan and benchmark on sparse datasets to get sense of the tradeoff.,11/24/15,1
echeipesh,RangePartitioner actually samples the data with replacement to try to build balanced ranges.,11/24/15,1
lossyrob,"ok. That makes sense, but I_Ñéd take out the no-sparse assumption from the list",11/24/15,1
echeipesh,"Alright, it makes sense.",11/24/15,1
lossyrob,"I_Ñém actually doing some edits to the issue, just for clarifying sentences and such. I can either email you with that copy or just edit the comment directly if you don_Ñét mind",11/24/15,1
lossyrob,I_Ñéll bring up any actual changes I_Ñéd want to make (like taking out that assumption) here,11/24/15,1
echeipesh,"I don_Ñét mind the edits, I think it would be worth while to make an extra post for significant ones like _ÑÒcan not assume sparse data""",11/24/15,1
lossyrob,"Like, post it in the comments? I just want to send it around (someone is talking about spatial joins in the geowave gitter), and don_Ñét want them to have to read through the comments to get the final picture",11/24/15,1
echeipesh,"I meant for an edit log, but I guess nobody has really read it yet, so no matter.",11/24/15,1
lossyrob,"Yeah, it_Ñés too bad that there_Ñés not a commit history on it",11/24/15,1
echeipesh,"Hmm, Gist has commit history, but then it_Ñés hard to find.",11/24/15,1
lossyrob,we could work it out in a gist at first,11/24/15,1
lossyrob,"want to copy the markdown and paste it into a public gist, then I can make an edit to it? Then we can link the final issue text to the gist in case people want to see it_Ñés development",11/24/15,1
lossyrob,"Assume we are given an `RDD[(K,V)]`, where `K` is some key type that can be converted to vectors in some __d__ dimensional space, which we'll call __N__<sup>d</sup>. We'll define __K__ as the set of possible keys of type `K`. We are given a `KeyBounds[K]`, which describes the bounds of all keys in the given `RDD[(K, V)]`, and so describes some subspace of __N__<sup>d</sup>, which we'll call __N'__. Further we have some space filling curve over __N'__, which we'll call __C__<sub>K</sub>, that maps __N'__ to a one dimensional index space.Using a lower resolution space filling curve defined over __N'__, which we'll call __C__<sub>R</sub>, define a set of regions __R__ that are the N-dimensional regions (sub-spaces) indexed by __C__<sub>R</sub> in __N'__. Each region will contain a unique set of keys indexed by __C__<sub>K</sub>. This creates a many-to-one, surjective mapping of keys in __K__ to regions in __R__.A way to map a key in __K__ to a region in __R__ is to determine what significant bits are dropped between one dimensional indices mapped from __C__<sub>K</sub> and one dimensional indices mapped from __C__<sub>R</sub>. Call that difference __r__, and so the index __i__<sub>K</sub> of a __k ___ K__ under __C__<sub>K</sub> is mapped to the index __i__<sub>R</sub> of some __r ___ R__ under __C__<sub>R</sub> by dropping the __r__ least significant bits of __i__<sub>K</sub>.",11/24/15,1
echeipesh,We can map from  __C__<sub>K</sub>(__k__) to __C__<sub>R</sub>(__k__) by dropping __r__ least significant bits.,11/24/15,1
lossyrob,"Assume we are given an `RDD[(K,V)]`, where `K` is some key type that can be converted to a vector in some __d__ dimensional space, which we'll call __N__<sup>d</sup>. We'll define __K__ as the set of possible keys of type `K`. We are given a `KeyBounds[K]`, which describes the bounds of all keys in the given `RDD[(K, V)]`, and so describes some subspace of __N__<sup>d</sup>, which we'll call __N'__. Further we have some space filling curve over __N'__, which we'll call __C__<sub>K</sub>, that maps __N'__ to a one dimensional index space __U__<sub>K</sub>.Using a lower resolution space filling curve defined over __N'__, which we'll call __C__<sub>R</sub>, define a set of regions __R__ that are the N-dimensional regions (sub-spaces) indexed by __C__<sub>R</sub> in __N'__. __C__<sub>R</sub> then maps __N'__ into a one dimensional index space __U__<sub>R</sub>. Each region will contain a unique set of keys indexed by __C__<sub>K</sub>. This creates a many-to-one, surjective mapping of keys in __K__ to regions in __R__.There is a mapping from __U__<sub>K</sub> and __U__<sub>R</sub> defined as: __i__<sub>K</sub> _____ U__<sub>K</sub> is mapped to __i__<sub>R</sub> _____ U__<sub>R</sub> by dropping __r__ least significant bits of __i__<sub>K</sub>, where __r__ is determined by the differences in resolution between __C__<sub>K</sub> and __C__<sub>R</sub>. Since the mapping of __R__ to __U__<sub>R</sub> is bijective, we can then create the composite map from __k ___ K__ to __r ___ R__.",11/24/15,1
echeipesh,,11/24/15,1
pomadchin,@lossyrob now it is clearer ._. ,11/25/15,1
echeipesh,It is always fascinating to me how difficult it is to convey simple ideas to a group of people. Maybe it_Ñés just me through :),11/25/15,1
pomadchin,:D but so cool idea; I had to write it on a paper to got it ):,11/25/15,1
lossyrob,"@pomadchin lol at the ._. face. But actually what we wrote was slightly off, the updated version will be slightly less complicated I think. I think in code too it'll be pretty clear",11/25/15,1
pomadchin,"@lossyrob this version is _ª¥Ô too; ye, think same",11/25/15,1
bneff,"Can I create a RasterSource from a GeoTiff just using a file path?  I'm basically trying to mimic the geotrellis-spray example, but load a tiff instead of using an arg/catalog.  Still learning the API so apologies if this doesn't make sense",11/25/15,1
lossyrob,"I believe you can do something like, `val rs = RasterSource(SingleBandGeoTiff(path).raster)`",11/25/15,1
lossyrob,that_Ñés a guess though. I_Ñéd have to remind myself of the RasterSource API,11/25/15,1
lossyrob,actually,11/25/15,1
lossyrob,"```scalaval gt = SingleBandGeoTiff(path)RasterSource(gt.tile, gt.extent)```",11/25/15,1
lossyrob,https://github.com/geotrellis/geotrellis/blob/master/engine/src/main/scala/geotrellis/engine/RasterSource.scala#L481,11/25/15,1
bneff,@lossyrob thank you soon much!! Worked like a charm!,11/25/15,1
lossyrob,nice :),11/25/15,1
lossyrob,@echeipesh this is the geomesa z curve change that we need to figure out if we need to include: https://github.com/locationtech/geomesa/commit/5789033c3789aca00c3fd4967eaa88b1a747e203,11/25/15,1
pomadchin,"/offtop/ quick json libs comparison (fresh) http://manuel.bernhardt.io/2015/11/06/a-quick-tour-of-json-libraries-in-scala/ (mean mb to change in future, or whenever spray-json, but now see no need in it)",11/30/15,1
pomadchin,https://github.com/pomadchin/geotrellis/tree/feature/layer-management,11/30/15,1
pomadchin,https://github.com/pomadchin/geotrellis/blob/feature/layer-management/spark/src/main/scala/geotrellis/spark/io/accumulo/AccumuloLayerManager.scala,11/30/15,1
bneff,"I have two GeoTiff's that I'm loading into a raster source and then rendering to a PNG.  1 sample GeoTiff works, but another, outputs ""Message: matrices not fitting together"".  Any pointers where to look?",11/30/15,1
lossyrob,what geotrellis version are you using? A locally published one?,11/30/15,1
bneff,0.10.0-97834e6 from bintray,11/30/15,1
lossyrob,"ok. I_Ñém going to merge in an outstanding PR that deals with the code that throws that error, and I_Ñém wondering if that will fix it. We_Ñéll have to wait for travis to push up the new version to bin tray ",11/30/15,1
bneff,ok awesome.  thanks for the heads up!,11/30/15,1
lossyrob,When this finishes: https://travis-ci.org/geotrellis/geotrellis,11/30/15,1
lossyrob,"the new bintray version should be up, and you can swith versions",11/30/15,1
bneff,I'll follow it and post here if this resolves the issue.  Thanks again!,11/30/15,1
lossyrob,it can take up to 3 hours though :-o,11/30/15,1
bneff,so you're saying I can go home early!! sweet!,11/30/15,1
lossyrob,lol,11/30/15,1
lossyrob,"also you could pull down master and run the `./publish-local.sh` script, which will publish `0.10.0-SNAPSHOT` versions to your local ivy2 cache, and so you could change the version to `0.10.0-SNAPSHOT` and use the changes right away",11/30/15,1
bneff,"oh, thats a great idea.  I saw that referenced in a few projects but didn't know how to do it until now.  thanks rob",11/30/15,1
lossyrob,no problem!,11/30/15,1
bneff,"so with the snapshot build I now get ""Message: y: 41.30668099999972 to 41.3030799999997"", does that give any hints?",11/30/15,1
lossyrob,hmmm,11/30/15,1
lossyrob,what is the stack trace?,11/30/15,1
bneff,http://pastebin.com/mAb6rfNt,11/30/15,1
lossyrob,it seems like it_Ñés just not able to read the extent out of this geotiff,11/30/15,1
lossyrob,what_Ñés the gdalinfo look like?,11/30/15,1
bneff,"it looks similar to the working file, I'll paste it",11/30/15,1
bneff,http://pastebin.com/UUamgp1u,11/30/15,1
lossyrob,the upper coordinate of the bounding box has a lower value then the lower coordinate,11/30/15,1
lossyrob,*higher,11/30/15,1
lossyrob,"```Upper Left  ( -89.6120100,  41.3030800) ( 89d36'43.24""W, 41d18'11.09""N)Lower Left  ( -89.6120100,  41.3066810) ( 89d36'43.24""W, 41d18'24.05""N)Upper Right ( -89.6047640,  41.3030800) ( 89d36'17.15""W, 41d18'11.09""N)Lower Right ( -89.6047640,  41.3066810) ( 89d36'17.15""W, 41d18'24.05""N)Center      ( -89.6083870,  41.3048805) ( 89d36'30.19""W, 41d18'17.57""N)```",11/30/15,1
lossyrob,Upper is supposed to be greater than Lower,11/30/15,1
lossyrob,Is that valid? Do GeoTiffs not enforce that map direction? If so we might need to change the GeoTiff read logic to just switch it around if it_Ñés opposite,11/30/15,1
bneff,hmmm well we used something to generate this...let me find that out first,11/30/15,1
bneff,"I don't know if this matters, but the file loads in QGIS",11/30/15,1
lossyrob,"In TiffTags.scala, line 288",11/30/15,1
lossyrob,Try changing:,11/30/15,1
lossyrob,"```scala// Thisprivate def getExtentFromModelFunction(func: Pixel3D => Pixel3D) = {    val modelPixels = getRasterBoundaries.map(func)    val (minX, minY) = (modelPixels(0).x, modelPixels(0).y)    val (maxX, maxY) = (modelPixels(1).x, modelPixels(1).y)    Extent(minX, minY, maxX, maxY)  }// To thisprivate def getExtentFromModelFunction(func: Pixel3D => Pixel3D) = {    val modelPixels = getRasterBoundaries.map(func)    val (minX, minY) = (modelPixels(0).x, modelPixels(0).y)    val (maxX, maxY) = (modelPixels(1).x, modelPixels(1).y)    Extent(math.min(minX, maxX), math.min(minY, maxY), math.max(minX, maxX), math.max(minY, maxY))  }```",11/30/15,1
lossyrob,and publish local again (you could actually just run `./sbt _ÑÒproject raster_Ñù +publish-local` instead of the running the whole script),11/30/15,1
bneff,trying now,11/30/15,1
bneff,it is rendering now. thank you!,11/30/15,1
lossyrob,hmm ok. That should probably be in the codebase then. Want to make a PR?,11/30/15,1
bneff,I'll give it a shot.,11/30/15,1
bneff,"@lossyrob so other than getting practice doing a PR, I'm not sure what the commit msg should be other than ""did what rob said and now it works""",11/30/15,1
lossyrob,haha I guess that_Ñés true.,11/30/15,1
lossyrob,"What happened was, you found a geotiff that had a bounding box in the reverse Y order then an Extent expects, and this fixes it so the Extent is created correctly for that case",11/30/15,1
lossyrob,I could commit it if that makes more sense,11/30/15,1
bneff,probably makes more sense for you to commit =)  Id just be riding on your coat tails,11/30/15,1
lossyrob,ok no problem,11/30/15,1
lossyrob,"@bneff you can use `0.10.0-87fbc6e` from bintray now, it has that fix",12/1/15,1
bneff,@lossyrob awesome thanks again for your help,12/1/15,1
echeipesh,@lossyrob: to clarify your comment from https://github.com/geotrellis/geotrellis/pull/1224#issuecomment-161336133,12/2/15,1
echeipesh,There was a case where exception with `.initCause` was hiding information instead of just adding an explanation ?,12/2/15,1
lossyrob,yes. casey was asking why there was a layer read error,12/2/15,1
lossyrob,he had to search in the logs to get the inner stack trace,12/2/15,1
echeipesh,"Ah, the inner stack trace was not being returned in the service endpoint",12/2/15,1
lossyrob,yeah. It just hides it from the most obvious place you_Ñére looking for an error,12/2/15,1
pomadchin,"hm, it is possible just to wrap exception object",12/2/15,1
lossyrob,what do you mean by wrap?,12/2/15,1
pomadchin,"to incapsulate in our specific exception class```scalacatch { case e: Exception => new LayerUpdateError(id, e) }```",12/2/15,1
pomadchin,"and may be not to catch this generic exception type, but to catch only specific, like in the pr: https://github.com/geotrellis/geotrellis/pull/1224/files#diff-d137ca27a2c10efc9851011d922212caR39",12/2/15,1
echeipesh,Not sure that would solve the stack trace hiding. At least with `.initCause` you have special consideration when it is printed,12/2/15,1
lossyrob,what throws an UnsupportedOperationException?,12/2/15,1
pomadchin,"oops seems like a wrong handled thing, but it happens while updating an empty rdd, sec will check it ",12/2/15,1
pomadchin,"found, so caused by `getKeyBounds` on an empty set (reduce fails)```scaladef getKeyBounds(rdd: RDD[(SpaceTimeKey, X)] forSome {type X}): KeyBounds[SpaceTimeKey] = {      rdd        .map{ case (k, tile) => KeyBounds(k, k) }        .reduce { combine }    }```",12/2/15,1
echeipesh,"Ok, I_Ñém looking around code bases trying to see how people use initCause and the rule seems to be that the thing that is being thrown needs to be an Exception. I think what feels wrong here is that it_Ñés not obvious that something like `LayerNotExistsError` is always exception rather than control flow.",12/2/15,1
echeipesh,"This is an exception, essentially layer is in invalid state: https://github.com/geotrellis/geotrellis/blob/bad3d2436a15ceba20889564ce5bc87f78ca07e3/spark/src/main/scala/geotrellis/spark/io/accumulo/AccumuloTileReader.scala#L50",12/2/15,1
echeipesh,"Hmm, I_Ñéve ran into similar problems before. It_Ñés always like we need a special instance of empty KeyBounds",12/2/15,1
echeipesh,Something that you can `foldeLeft` into.,12/2/15,1
echeipesh,It would also allow `Boundable.intersect` return `KeyBounds` rather than `Option[KeyBounds]`,12/2/15,1
pomadchin,"yes, to remove this side effect, or just to add an additional check and to throw an exception",12/2/15,1
pomadchin,"wow, nice",12/2/15,1
pomadchin,for that we need also empty instances of keys,12/2/15,1
lossyrob,I think the try catch should wrap the thing that could potentially throw it and then give it that specific context. It_Ñés the general catchall try catch that is weird and information-erasing,12/2/15,1
lossyrob,but catching that unsupported around the call that could throw it and then explicity throwing a well described error is a good idea,12/2/15,1
pomadchin,and lots of try catch blocks doesn't look dirty?,12/2/15,1
lossyrob,looking dirty is better than being dirty,12/2/15,1
lossyrob,"I think it_Ñés more explicit. It does sacrifice asthecis, sure, but I don_Ñét mind that when it makes the code more readable and debuggable",12/2/15,1
pomadchin,"ok nice so: * make blocks more specific* and specify more information in a throwed exception",12/2/15,1
lossyrob,"yeah. whenever your catching and throwing, the new thrown error should be more clear, less confusing, and have more information then the thing you originall caught",12/2/15,1
pomadchin,will have a look now too,12/2/15,1
pomadchin,"also, you saw the #1225 , so decided to start with `copy`(looked like it is more obvious):  https://github.com/pomadchin/geotrellis/blob/feature/layer-management-function-copier/spark/src/main/scala/geotrellis/spark/io/LayerCopier.scala But it seems to me it is too heavy (p.s. this branch is covered with tests: https://github.com/pomadchin/geotrellis/blob/feature/layer-management-function-copier/spark/src/test/scala/geotrellis/spark/io/LayerCopySpaceTimeTileTests.scala)",12/2/15,1
lossyrob,how do you mean heavy?,12/2/15,1
pomadchin,it has `layerReader` and `layerWriter` in this implementation,12/2/15,1
lossyrob,that makes sense though,12/2/15,1
pomadchin,"kk, so will make the pr after dealing with the deleter (because it is a fork of deleter branch ): )",12/2/15,1
lossyrob,"ok cool. You can make PRs that depend on each other, just make sure you say that one depends on the other",12/2/15,1
pomadchin,"ok, done; hm, so what to do with exceptions? mb now just to make them more local, and after provide exceptions with more explanations; but the initCause still obfuscates exceptions",12/2/15,1
pomadchin,"btw, cant we just use `setStackTrace` ?",12/2/15,1
lossyrob,"I would just remove the large try blocks, try to wrap some of the known problem points, and not worry too much about it",12/2/15,1
lossyrob,@notthatbreezy there_Ñés a missing set of lines in `spark-etl``_Ñébuild.st`,12/3/15,1
lossyrob,that controls how we merge stuff in the assmebly,12/3/15,1
lossyrob,"```scalaassemblyMergeStrategy in assembly := {  case ""reference.conf"" => MergeStrategy.concat  case ""application.conf"" => MergeStrategy.concat  case ""META-INF/MANIFEST.MF"" => MergeStrategy.discard  case ""META-INF\\MANIFEST.MF"" => MergeStrategy.discard  case _ => MergeStrategy.first}```",12/3/15,1
notthatbreezy,Ah got it,12/3/15,1
notthatbreezy,That worked :thumbsup: ,12/3/15,1
lossyrob,"for context: chris was getting errors about doing an `assembly` on `spark-etl`. The assembly plugin doesn_Ñét know how to merge when there_Ñés multiple versions of the same type in different libraries, so you need to tell it to just take the first. A better strategy is to exclude the conflicting types out of the transitive dependencies, but there are like 200 conflicts, so that would take some might",12/3/15,1
notthatbreezy,"The errors look like the following: ```scala[error] deduplicate: different file contents found in the following:[error] /root/.ivy2/cache/commons-collections/commons-collections/jars/commons-collections-3.2.1.jar:org/apache/commons/collections/FastHashMap.class[error] /root/.ivy2/cache/commons-beanutils/commons-beanutils/jars/commons-beanutils-1.7.0.jar:org/apache/commons/collections/FastHashMap.class[error] /root/.ivy2/cache/commons-beanutils/commons-beanutils-core/jars/commons-beanutils-core-1.8.0.jar:org/apache/commons/collections/FastHashMap.class```",12/3/15,1
notthatbreezy,For reference,12/3/15,1
lossyrob,"@notthatbreezy hey, I remember talking about `geotrellis-ec2-cluster`, and thinking about how we could specify what AZ we would want to launch the instances in. Do you remember what level of effort that would take?",12/3/15,1
lossyrob,or like where that would have to go,12/3/15,1
notthatbreezy,"Yeah, let me take a quick look",12/3/15,1
notthatbreezy,Right now it grabs the [first AZ in the region](https://github.com/geotrellis/geotrellis-ec2-cluster/blob/develop/deployment/cfn/vpc.py#L68),12/3/15,1
notthatbreezy,"You could make it an optional parameter, that if supplied would override that. For instance, passing in the string `us-east-1a` would have the stack use that one, but if not supplied it defaults to the first one listed",12/3/15,1
lossyrob,"nice, that_Ñés not so bad",12/3/15,1
lossyrob,spot prices in eu-central-1 are crazy different for the two AZs for m3.xlarges,12/3/15,1
notthatbreezy,"actually, here's the method that gets the zones http://boto.cloudhackers.com/en/latest/ref/ec2.html#boto.ec2.connection.EC2Connection.get_all_zones; probably better to use that instead because I see we are actually using attributes on the `zone` object for other things",12/3/15,1
notthatbreezy,so the `filters` functionality there,12/3/15,1
notthatbreezy,I've seen that happen before for some of the US regions; definitely good to be flexible,12/3/15,1
lossyrob,"ok cool I_Ñéll use that function instead then, and filter to the configured one if it_Ñés supplied",12/3/15,1
lossyrob,"@notthatbreezy wait, isn_Ñét that the `get_all_zones` we_Ñére using now?",12/3/15,1
notthatbreezy,"Yeah, sorry wasn't super clear there. I'm suggesting the change be that a user can pass in an optional string that can be used in calling `get_all_zones` via the config",12/3/15,1
lossyrob,e.g.,12/3/15,1
lossyrob,"```python        region = self.get_input('Region')        conn = boto.ec2.connect_to_region(region, profile_name=self.aws_profile)        zone_name = self.get_input('AvailabilityZone')        if zone_name:            zones = conn.get_all_zones(filter={ 'name' : zone_name })            if not zones:                raise new Error(""Bad availability zone name: %s"" % zone_name)        else:            zones = conn.get_all_zones()[0]                 zone = zones[0]```",12/3/15,1
notthatbreezy,Yeah,12/3/15,1
lossyrob,cool,12/3/15,1
notthatbreezy,I think you'll have to add it to `INPUTS` as well here: https://github.com/geotrellis/geotrellis-ec2-cluster/blob/develop/deployment/cfn/vpc.py#L16,12/3/15,1
lossyrob,"did that, and added a default of None",12/3/15,1
notthatbreezy,cool,12/3/15,1
lossyrob,"one question is, having a `Global:AvailabilityZone` and `VPC:AvailabilityZone` is totally ok, right?",12/3/15,1
notthatbreezy,"Yeah, that's fine; that specifies where to resolve the input from",12/3/15,1
lossyrob,Response><Errors><Error><Code>InvalidParameterValue</Code><Message>The filter 'name' is invalid</Message></Error></Errors><RequestID>857b96e4-02ef-4bed-b6e4-e51fb55cceae</RequestID></Response>,12/3/15,1
lossyrob,wa,12/3/15,1
lossyrob,"`zone-name`, whoops.",12/3/15,1
lossyrob,@notthatbreezy note: there_Ñés some files that need to be exlcuded or you get an invalid signature when you try to run it through spark. Forgot about that and just ran into it,12/3/15,1
lossyrob,so the merge should be,12/3/15,1
lossyrob,"```scalaassemblyMergeStrategy in assembly := {  case ""reference.conf"" => MergeStrategy.concat  case ""application.conf"" => MergeStrategy.concat  case ""META-INF/MANIFEST.MF"" => MergeStrategy.discard  case ""META-INF\\MANIFEST.MF"" => MergeStrategy.discard  case ""META-INF/ECLIPSEF.RSA"" => MergeStrategy.discard  case ""META-INF/ECLIPSEF.SF"" => MergeStrategy.discard  case _ => MergeStrategy.first}```",12/3/15,1
notthatbreezy,"ah ok, cool; happened to run fine for me I think but I'm just doing a local spark submit",12/3/15,1
lossyrob,I think it_Ñés because we remove types due to the merge and so the hash doesn_Ñét match up,12/3/15,1
lossyrob,"oh, yeah I_Ñém using an older spark version too I think",12/3/15,1
lossyrob,shoud ec2-cluster be able to just bump up to 1.4.1?,12/3/15,1
notthatbreezy,I don't see why not as long as the AMI building still works,12/3/15,1
lossyrob,"I_Ñéll have to do that later when I_Ñém deep in something else, AMI_Ñés take a sad amount of time to build ha",12/3/15,1
notthatbreezy,For sure,12/3/15,1
bneff,"In the ""older"" examples, you could call localMap() on a RasterSource, but in 0.10 localMap is not a member of RasterSource.  It's probably really easy, but I can't seem to connect the to browsing the API...any pointers are greatly appreciated.",12/4/15,1
pomadchin,@bneff seems like it is present in 0.10: https://github.com/geotrellis/geotrellis/blob/master/engine-test/src/test/scala/geotrellis/engine/op/local/LocalMapSpec.scala,12/4/15,1
bneff,"@pomadchin I'm using 0.10.0-87fbc6e and I get a compile error. I new to scala, do I need to add the proper import?",12/4/15,1
pomadchin,let me have a look,12/4/15,1
pomadchin,try `import geotrellis.engine.op.local._`,12/4/15,1
bneff,@pomadchin that did the trick.  Thank you!,12/4/15,1
moradology,"Hey @bneff - this behavior is the result of so-called 'implicit extension'. basically, if you've got a class that you want to monkey patch (in a type-safe fashion), you can do that by defining scala implicits at the call site. Here's what did the work when you imported `geotrellis.engine.op.local` (to give you a sense of what's going on): https://github.com/geotrellis/geotrellis/blob/master/engine/src/main/scala/geotrellis/engine/op/local/package.scala",12/4/15,1
moradology,kind of a cool pattern in that it allows us to organize the functionality of classes and hide away unused features,12/4/15,1
lossyrob,"Good explination @moradology. Yeah we use implicit classes a lot to add functionality to types without having to modify those types. The one downside is that it_Ñés a bit less discoverable, and if you don_Ñét have the correct imports the methods are nowhere to be found. I_Ñéve been trying to think of a good way to mitigate that. Other projects have an `Implicits` object, and the package objects extend traits with all the implicits, and the Implicits object also extends all of those traits, so a `import geotrellis.raster.Implicits._` would import all of the extensions without having to dig around the packages",12/4/15,1
moradology,others do something like `import scalaz._; import Scalaz._`,12/4/15,1
moradology,ugh,12/4/15,1
moradology,Where Scalaz is some big object to hold conveniences for things in scalaz,12/4/15,1
moradology,"which is, i guess, what you just said @lossyrob ",12/4/15,1
lossyrob,yeah not sure what a good name for it would be,12/4/15,1
pomadchin,"mb it would be interseting, there is even an implicit calculus, so this pattern described formal: https://ropas.snu.ac.kr/~wclee/papers/pldi12a.pdf",12/4/15,1
lossyrob,that paper is intense,12/4/15,1
moradology,as much formalism as a wikipedia math article 0.o,12/4/15,1
moradology,"'Our calculus aims at providing a formal model ofimplicits, but there are some noteworthy differences between ï___êand Scala implicits. In contrast to ï___ê, Scala has subtyping. As discussedin Section 5.2 subtyping would require some adaptations toour calculus.'that is a *BIG* difference.",12/4/15,1
bneff,@moradology thanks for the great explanation.  It really helps me digest all of this information.  You are all very helpful,12/4/15,1
pomadchin,"@moradology ye, but think it shows that it's possible, but they are basing not on DOT, but on the classic system F btw, heard from some scala guyz, from epfl, that probably soon there would be a real article about DOT",12/4/15,1
moradology,@pomadchin yeah :) didn't mean to be dismissive - truth be told this paper is a bit over my head,12/4/15,1
moradology,@bneff any time - happy to help,12/4/15,1
pomadchin,:d @moradology glad to see you here btw,12/4/15,1
jamesmcclain,"Hello, I just wanted to start conversation about the polygon intersection task that I am working on.  The basic idea that I want to use for this is to find some set of rectangles that approximate the polygon, then generate a non-overlapping set of keybounds from those rectangles.",12/7/15,1
jamesmcclain,I am suggesting an approximate solution because this page https://en.wikipedia.org/wiki/Polygon_covering seems to imply that computing exact solutions is not generally feasible,12/7/15,1
lossyrob,"tbh, this could use the rasterization code",12/7/15,1
lossyrob,which does find an exact grid cell coverage for polygons,12/7/15,1
jamesmcclain,I thought about that,12/7/15,1
jamesmcclain,"I see, okay.",12/7/15,1
jamesmcclain,"I didn't know whether you wanted to find an algorithmically-driven answer, or an engineering one",12/7/15,1
lossyrob,"the only thing is, the code considers the center of cells as the inclusion/exclusion, where we would want an intersection that doesn_Ñét include the cell center to be included",12/7/15,1
lossyrob,what do you mean by algorithmically driver/engineering?,12/7/15,1
jamesmcclain,"Well, my reasoning is something like this: it should be possible to generate an approximate set that is pretty tight in no more than O(n lgn) time, where n is the number of vertices",12/7/15,1
jamesmcclain,Then generate keybounds from the covering set,12/7/15,1
jamesmcclain,That has an algorithmic flavor,12/7/15,1
jamesmcclain,The other approach is just rasterize the polygon and not worry about the theoretical efficiency,12/7/15,1
jamesmcclain,Which is an engineering approach,12/7/15,1
lossyrob,"well, there_Ñés a comple of efficiencies that we_Ñére worried about, the coverage generation is not the only one",12/7/15,1
lossyrob,because what is the cost of generating the estimate vs the exact,12/7/15,1
lossyrob,and then what is the cost of fetching non-intersecting tiles vs not?,12/7/15,1
jamesmcclain,"Right, I understand that issue",12/7/15,1
jamesmcclain,I just wasn't sure where people came down on this issue,12/7/15,1
lossyrob,"if it_Ñés between the theoretical efficiency of the one algorithm in isolation vs the actual efficiency of the algorithm in the context of the library, the latter is the way to go",12/7/15,1
jamesmcclain,"Okay, so I'll go with the engineering view",12/7/15,1
lossyrob,perhaps the estimate coverage would be useful in another setting,12/7/15,1
lossyrob,good to keep it in the back pocket,12/7/15,1
jamesmcclain,"Sorry, didn't mean to leave abruptly ... my computer keeps crashing",12/7/15,1
lossyrob,no worries,12/7/15,1
jamesmcclain,"Okay, I will start looking through the polygon rasterization code",12/7/15,1
lossyrob,"polygon rasterization is a pretty intense part of the code. I_Ñéve wanted to refactor it for a while, it_Ñés not very easy to read. But the center cell issue is going to be a problem",12/7/15,1
lossyrob,it_Ñés based on a well known algorithm,12/7/15,1
lossyrob,I can_Ñét remember the name and I can_Ñét find it in the comments though lol,12/7/15,1
jamesmcclain,I took a graphics class back in the day,12/7/15,1
lossyrob,https://en.wikipedia.org/wiki/Bresenham%27s_line_algorithm,12/7/15,1
jamesmcclain,Interesting: https://github.com/WikiWatershed/model-my-watershed/issues/1051,12/7/15,1
jamesmcclain,"I thought that I remembered Breshnham's being used for polygon rasterization, but wiki seems to only talk about it in the context of lines",12/7/15,1
lossyrob,"yeah, it finds the line cells",12/7/15,1
lossyrob,"and then per row, iterates over the ranges",12/7/15,1
jamesmcclain,"Cool ... okay, I will modify that other PR then dig into this",12/7/15,1
lossyrob,"so if I have 4 cells that were determined by Bresnham_Ñés in one row, I know that I start at the first, go to the second, then between second and third is a hole, then third and forth on",12/7/15,1
lossyrob,"seems like there_Ñés a need for the partial cell inclusion in MMW, so perhaps this work could help that as well",12/7/15,1
lossyrob,it does blow up the task a bit,12/7/15,1
lossyrob,"and by a bit, I mean a lot :)",12/7/15,1
jamesmcclain,I think I'll try to take it one step at a time :-),12/7/15,1
pomadchin,"guyz want to know yout opinions: https://github.com/geotrellis/geotrellis/issues/1225 (provided here `LayerMover` example)",12/7/15,1
echeipesh,I don_Ñét think we should do this but it_Ñés an interesting idea on how to get polymorism with generic case using implicits:,12/7/15,1
echeipesh,"```scalatrait GenericStorageBackendtrait HadoopStorageBackend extends GenericStorageBackendtrait S3StorageBackend extends GenericStorageBackendtrait AccumuloStorageBackend extends GenericStorageBackendtrait LayerMover[-T] {  def delete(id: LayerId)}object package {  implicit def genericMover = new LayerMover[GenericStorageBackend] { ... //generic case}  implicit def hadoopMover = new LayerMover[HadoopStorageBackend] { ... //special case logic }}class LayerManager[S <: GenericStorageBackend] {  def move(id: LayerId)(implicit mover: LayerMover[S]): Unit { // use the mover }}```",12/7/15,1
moradology,@pomadchin I'm personally not a big fan of implementing a class for copy/delete; seems heavy maybe?,12/7/15,1
pomadchin,"delete and copy makes sense, due to using specific backend api (especially actual for deleter, and for s3 copier in order not to load huge amount of data to spark memory, btw may be in case of hadoop it is also significant)",12/7/15,1
pomadchin,"but ye, seems heavy too ): ",12/7/15,1
echeipesh,"+1, to tie comment from the PR: I think this too simple to compose over. This is the case where `HadoopLayerManager` can `override` the `move` method to use the `FileSystem` object.",12/7/15,1
pomadchin,"But, there is a SparkLayerCopier which is generic and can be used with any backend in the current implementation, and can work as copier and as reindexer ",12/7/15,1
pomadchin,"@echeipesh yea, will try your comments, think it is a right way; and have to think about @moradology words too",12/7/15,1
echeipesh,@pomadchin Where was it that you had problem with not having empty KeyBounds ?,12/7/15,1
pomadchin,"@echeipesh LayerUpdater, getKeyBounds application to the empty rdd",12/7/15,1
pomadchin,Now there is a catch of unsupported operation exception,12/7/15,1
har777,Sorry for the stupid question but I am slightly confused about what geotrellis is. Will it work as a alternative to hadoop gis ?,12/8/15,1
moradology,@harr777 hey - 'who cares?' is never a dumb question in my opinion. can i ask a bit about your use case?,12/8/15,1
moradology,what kind of data are you working with?,12/8/15,1
echeipesh,"@har777 GeoTrellis is mostly a raster processing library, written in Scala, that can be used with Spark. We_Ñére able to store raster layers in HDFS, Accumulo and S3. There is a cool wrapper around jts vector library that makes pleasant to use in scala. So you can use it to implement a _ÑÒhadoop gis_Ñù, it certainly lives in the same ecosystem. If you can tell us about your use cases we can give you a better idea of what there is to help you.",12/8/15,1
har777,"@moradology @echeipesh My use case is pretty broad/undefined. We have a ton(some petabytes) of data having lon, lat fields. Was thinking about how to utilize them. I guess visualization is certainly one aspect. We also do a lot of queries like ""find me data fields where the location is within this polygon"" and so on. Open to other usecases :) ",12/9/15,1
echeipesh, @har777 Sounds like you_Ñére dealing with a lot of vector data that you need to store/index and query. You should check out GeoWave (https://ngageoint.github.io/geowave/) and GeoMessa (http://www.geomesa.org/). They both impelment a spatial index on top of Accumulo/HDFS. Accumulo is HBase like distributed key/value store. Basically implementing distributed PostGIS.,12/9/15,1
echeipesh,For the moment we are really flushing out features for distributed raster/imagery processing. There has been planning to integrate with GeoWave to use it as a store for both raster and vector information with GeoTrellis providing the post-query analytics. That will be very interesting :),12/9/15,1
ahmetartu,"Hi. I'd like to try out geotrellis over our hadoop cluster. I'm going to use spark to process geotiff files. As I checked out your source code, there is no support for BIGTIFF support. so does that mean the input geotiff file shouldn't be larger than 4GB?",12/9/15,1
echeipesh,"@ahmetartu: Our scala geotiff reader doesn_Ñét support BIGTIFF. We_Ñéve read files through GDAL, the deployment becomes a bit more painful though: https://github.com/geotrellis/geotrellis/blob/master/spark/src/main/scala/geotrellis/spark/io/hadoop/formats/GdalInputFormat.scala",12/9/15,1
echeipesh,I_Ñém curious about the idea though. It seems like raster larger than 4GB is bigger than what you_Ñéd want even one spark partition to be. Would you be reading those in and breaking them up or working on them as is?,12/9/15,1
ahmetartu,"Thanks for the reply @echeipesh , OK. so looks like the only way to handle bigger geotiff files is to break it up into small ones, possible into tiles. Right? I'm OK with GDAL installation that I did that before. Although it's really painful, the problem with GDAL it is not supporting parallel writes, parallel read is OK as I tested in our MPI cluster system. But I'm not familiar to the spark currently. what is the usual way for parallel GEOTIFF read/write that geotrellis supports?",12/9/15,1
moradology,it might make sense to dump them onto HDFS and slurp them up from the machines you parallelize over,12/9/15,1
moradology,"```scalaimport org.apache.hadoop.fs.Pathval hdfsPath = new Path(""someHadoopDirectory"")val binFiles: RDD[(String, PortableDataStream)] = sc.binaryFiles(hdfsPath)```",12/9/15,1
echeipesh,"Yep, what we have been doing so far is hosting GeoTiffs on either HDFS or S3, bring them in, operate and write out. If you_Ñére willing to go through the pain of using GDAL you can make an InputFormat that reads the BigTiffs and tiles them out in one step. Otherwise it can be a seperate job, maybe even written in pyspark where you have access to Rasterio.",12/9/15,1
echeipesh,Our own HDFS catalog are essentally spatial MapFile that gives you ability to partially read a tile layer based on some bounding box. That may be useful as pre-processed format.,12/9/15,1
ahmetartu,Thanks guys. How about converting large geotiff file to SequenceFile format where key can be the spatial index and the value is the binary content of the tile?,12/9/15,1
echeipesh,That_Ñés exactly how we store our layers on HDFS: https://github.com/geotrellis/geotrellis/tree/master/spark/src/main/scala/geotrellis/spark/io/hadoop,12/9/15,1
echeipesh,"So the converting large geotiff would be some subclass of GdalInputFormat that would give you RDD of (Extent, Tile). If you go down that road, a contribution would be very appreciated :) A couple of people have asked about something like this.",12/9/15,1
ahmetartu,It is good to know that there is a hope :) I can go for the subclassing of GdalInputFormat that looks way cleaner approach. Our hadoop cluster is pretty large one that would be good to test it out in our facility  ,12/9/15,1
echeipesh,"Hmm, are you big tiffs on HDFS ?",12/9/15,1
ahmetartu,"Mostly storing in tiles but I will test it on bigtiffs that might cover whole US, I haven't decided it though.",12/9/15,1
echeipesh,"It escaped my mind that the main problem with GDAL is that the API is based around reading/writing from filesystem. What `GdalInputFormat` does is copy the the geotiff locally and reads it then. That works fine for smaller tiles but I_Ñém thinking for multi GB tiff that_Ñés going to be suboptimal. If they_Ñére already stored in tiles, then you_Ñére golden, that_Ñés basically what it is designed for.",12/9/15,1
echeipesh,How could your MPI cluster read them in parallel over NFS or something like that ?,12/9/15,1
echeipesh,"It would be really sweet if there was `/vsihdfs/` in GDAL, heh.",12/9/15,1
ahmetartu,MPI programs reading in parallel over GPFS and we also tested it on Lustre file system. I don't think NFS is a good idea for high performance IO,12/9/15,1
echeipesh,Agreed about NFS. I have not worked with either of those. They must provided a mounted file system view for GDAL?,12/9/15,1
ahmetartu,"Oh yeah, not mounted directly but there are file system drivers that the programs access the files like they access to local files",12/9/15,1
echeipesh,"Interesting, could be a viable solution for this problem then. Thank you, going to do some reading.",12/9/15,1
ahmetartu,"but behind the curtain, there are multiple IO servers performing to retrieve data in parallel for you",12/9/15,1
ahmetartu,Greatly appreciated @echeipesh :),12/9/15,1
echeipesh,"No problem, drop in if you run into any questions and let us know where you end up. Interesting stuff.",12/9/15,1
har777,@echeipesh Thanks !,12/10/15,1
moradology,@echeipesh @lossyrob @pomadchin I'm thinking about the colorbreaks object and the rendering of tiles that have noData values and i'm wondering what your thoughts are about inclusion of that kind of information (what color should NoData be?) in the colorbreaks object itself rather than as another piece of data that must be passed along to whatever function needs it,12/10/15,1
moradology,geotrellis/raster/render/RenderMethods.scala illustrates the case I'm referring to,12/10/15,1
pomadchin,it is a very good question,12/10/15,1
pomadchin,as the nodata value varies depending on the tile type,12/10/15,1
echeipesh,"The writer handles that concerns, the question is mapping of NoData to specific color. Right now it_Ñés always transparent. Which we haven_Ñét really had problems with. But it_Ñés a good point that this is something that should be handled by color map. Right now you need a method that takes a color map and a no data color. So if you had cases where different layers required different colarization you_Ñéd need to construct your own data type to track the combination and then branch to the correct rendering call.",12/10/15,1
echeipesh,"I don_Ñét think its a pressing issue, but it shouldn't break the API.",12/10/15,1
pomadchin,"`/about layer copier / mover header function/`thinking of a similar way to define function, like `getS3Client` function in S3LayerWriter / Reader / Attribute Storage; what do you think? would not it be too ugly? Seems like a good way to avoid breaking simplicitly of the api usage",12/10/15,1
echeipesh,a function like that to handle the copy of metadata ?,12/10/15,1
pomadchin,ye,12/10/15,1
echeipesh,Could it just be an abstract function?,12/10/15,1
pomadchin,"yes, of course ",12/10/15,1
echeipesh,"Ah, I don_Ñét think it can be because you don_Ñét know the type of the layer header based on the instance of layer reader.",12/10/15,1
pomadchin,"oh ye, and this leads us to provide an abstract class for each backend",12/10/15,1
pomadchin,beacuse it_Ñés possible make an abstract function only when the header type is known,12/10/15,1
pomadchin,"hm, I will start implementing and write down possible solutions there",12/10/15,1
echeipesh,"```scalascala> trait Copier { type HeaderType; def copyHeader(h: HeaderType): HeaderType }defined trait Copierscala> class IntCopier { type HeaderType = Int; def copyHeader(h: Int) = h + 1 }defined class IntCopierscala> new IntCopierres0: IntCopier = IntCopier@21788153```",12/10/15,1
pomadchin,"ye, that looks good",12/10/15,1
echeipesh,You may need to make it a type param if you have to bring in `JsonFormat` to read it.,12/10/15,1
echeipesh,"I think you_Ñéll have two choices: make it a type param to get `JsonFormat[HeaderType]`, read it and pass it to abstract function. Or pass `JsValue` and expect the abstract function to do the parsing, since it will know the concrete type.",12/10/15,1
pomadchin,"mm, `JsonFormat` needs only to read / write, think it is not best idea to incapsulate this logic inside this function",12/10/15,1
echeipesh,"I think I prefer the first option, it gives you a cleaner signature for subclasses and the subclasses are actually going to perform the implicit resolution at definition. So the final class won_Ñét have the JsonFormat constructor parameter. I could be convinced otherwise though.",12/10/15,1
owcm,"Hello, newbie question.  What is the best way, is there way to convert a RasterSource to a Geotiff? Thanks in advance.",12/10/15,1
moradology,"I'm not entirely familiar with the goings on inside geotrellis.engine, but it looks to me like you can do something like: ```RasterSource.converge.map( tile => *CODE TO WRITE TILE TO GEOTIFF*)```",12/10/15,1
moradology,The code inside that black might be something like:,12/10/15,1
moradology,"```scalaimport geotrellis.raster.io.geotiff._tile.toGeoTiffTile.write(*somepath*)```",12/10/15,1
echeipesh,@owcm: This should do the trick:,12/10/15,1
echeipesh,"```scala      import geotrellis.raster.io.geotiff._      import geotrellis.proj4._      val rs = RasterSource(""mtsthelens_tiled_cached"")       val tile = rs.get      val extent =  rs.rasterExtent.get.extent      val geotiff = GeoTiff(tile, extent, LatLng)      geotiff.write(""/my/path/is/best/path/geotiff.tif"")```",12/10/15,1
echeipesh,"RasterSource doesn_Ñét know anything about projections, so you have to know at the time of writing.",12/10/15,1
pomadchin,"in addition to @echeipesh answer, it is possible to convert to a byte array:```scala  import geotrellis.raster.io.geotiff.writer.GeoTiffWriter  GeoTiffWriter.write(geotiff)```",12/10/15,1
owcm,"Okay, thanks  for the quick reponse",12/10/15,1
lossyrob,catching up.,12/10/15,1
lossyrob,"@moradology the renderers have the ability to assign a color to NoData, right? So the color breaks are for actual values, but during the rendering you can pass in a specific color for NoData. If that is there but not exposed through the method API_Ñés then that_Ñés work that needs to be done",12/10/15,1
lossyrob,"@pomadchin @echeipesh I think the way we_Ñére doing metadata right now is not ideal. Trying to read just the mapTransform for an object was a chore, when I was coding a spray service for the workshop. So I_Ñém wondering if we can change the API. Not sure if that overlaps with what you were talking about above, but wanted to bring it up",12/10/15,1
echeipesh,"Not directly, but it_Ñés a good discussion. What were you thinking exactly ?",12/10/15,1
lossyrob,Something that allows me to get RasterMetaData (or whatever the metadata type will be for that layer) without having to know all the other types to try and get from the attribute store,12/10/15,1
echeipesh,"hmm, I_Ñém going to go check it now but you should totally be able to do: `attributeStore.read[RasterMetaData](layerId, _ÑÉmetadata_Ñé)`",12/10/15,1
lossyrob,nope,12/10/15,1
lossyrob,because the RasterMetaData isn_Ñét the root json,12/10/15,1
lossyrob,you would get a serialization error,12/10/15,1
pomadchin,we need a sort of json lens for that,12/10/15,1
pomadchin,"but it is possible to read in a such way:```scaladef readLayerAttribute[T: JsonFormat](id: LayerId, attributeName: String): T =    attributeStore.cacheRead[JsObject](id, Fields.metaData).convertTo[T](fieldLens(attributeName))```",12/10/15,1
lossyrob,"it doesn_Ñét by default try to go through the properties and find the json body property that doesn_Ñét throw a serialization error, like say getting a geometry out of feature GeoJson does",12/10/15,1
echeipesh,"Ah, so you have to do ugly",12/10/15,1
lossyrob,yeah,12/10/15,1
lossyrob,https://github.com/lossyrob/dec2015-workshop/blob/master/demo/src/main/scala/demo/ServerExample.scala#L62,12/10/15,1
echeipesh,"`attributeStore.read[JsValue](layerId, _ÑÉmetadata_Ñé).fields(_ÑÒmetadata_Ñù).convertTo[RasterMetaData]` ?",12/10/15,1
pomadchin,,12/10/15,1
pomadchin,@echeipesh solves,12/10/15,1
echeipesh,If that works I still wouldn_Ñét advocate that as great solution tbh,12/10/15,1
lossyrob,"there should at least be an API in front of it then, because that detail of how the backend fits it_Ñés metadata into it_Ñés json should be opaque to the user",12/10/15,1
pomadchin,"I am thinking about these function inclusion (or similar)```scala  def updateLayerAttribute[T: JsonFormat](id: LayerId, attributeName: String, attribute: T): Unit = {    val json   = attributeStore.cacheRead[JsObject](id, Fields.metaData)    val fields = json.fields    import DefaultJsonProtocol._    attributeStore.cacheWrite[JsObject](id, Fields.metaData, json.copy(fields =      fields.get(attributeName) match {        case Some(obj) => (fields - attributeName) + (attributeName -> attribute.toJson)        case _ => throw new DeserializationException(s""Could not read '$attributeName' in $json"")      }    ))  }  def readLayerAttribute[T: JsonFormat](id: LayerId, attributeName: String): T =    attributeStore.cacheRead[JsObject](id, Fields.metaData).convertTo[T](fieldLens(attributeName))```",12/10/15,1
lossyrob,yeah. I_Ñém not crazy about that either,12/10/15,1
lossyrob,@pomadchin that might be in the right direction,12/10/15,1
lossyrob,"and this brings up where, the AttributeStore[..] in a general sense is nice, but as a user I actually want a catalog, where I can call methods like `catalog.metadataFor[RasterMetaData](layerId)` or something",12/10/15,1
echeipesh,That could hide all the implemintation information. Not sure that `AttributeStore` should have dedicated methods for metadata.,12/10/15,1
lossyrob,attribute store shouldn't,12/10/15,1
lossyrob,but catalog should,12/10/15,1
echeipesh,re: pomadchin snippet. There is whole lot of ugly logic in those functions in attempt to provide pseudo-migrations. I would have liked to remove all that by the time we release. Doesn_Ñét feel right to mutate the data when storing/reading it.,12/10/15,1
lossyrob,can we just remove it now? What datasets are those migration-shims serving?,12/10/15,1
echeipesh,"I_Ñém ok with that. Catalog can do the equivalent of `attributeStore.read[JsValue](layerId, _ÑÉmetadata_Ñé).fields(_ÑÒmetadata_Ñù).convertTo[RasterMetaData]` and it_Ñéd be great.",12/10/15,1
echeipesh,I think we should be able to.,12/10/15,1
lossyrob,yeah. and I think abstracting the catalog is good. because eventually we might not want to use json objects,12/10/15,1
lossyrob,"PostGIS catalog, for instance",12/10/15,1
lossyrob,be able to actually do spatial queries on large sets of layers,12/10/15,1
echeipesh,"That was the line of thinking with attribute stores, you could have a PostGIS attribute store.",12/10/15,1
lossyrob,"yeah, true.",12/10/15,1
echeipesh,Hmm. Kind of starting to sound like catalog would implement AttributeStore inteface ?,12/10/15,1
echeipesh,So for the purpose of readers/writers it would just be key-value store but on that mechanism it could have use facing query API ?,12/10/15,1
lossyrob,"No, I think it would have an attribute store. This idea that the attribute store is _ÑÒkey_Ñù, _ÑÒsub key_Ñù _ÑÒvalue_Ñù is a bit of an implementation detail, that maybe a backend wouldn_Ñét want to follow",12/10/15,1
pomadchin,"@echeipesh yes, just a suggestion, motivated by layer copier / mover functions: ```scalaval (existingLayerHeader, existingMetaData, existingKeyBounds, existingKeyIndex, existingSchema) =   attributeStore.readLayerAttributes[Header, cons.MetaDataType, KeyBounds[K], KeyIndex[K], Schema](from)// change smthattributeStore.writeLayerAttributes[Header, cons.MetaDataType, KeyBounds[K], KeyIndex[K], Schema](  to, headerUpdate(existingLayerHeader), existingMetaData, existingKeyBounds, existingKeyIndex, existingSchema)```",12/10/15,1
echeipesh,I think the next question is going to be if this catalog is going to be the same as layer manager.,12/10/15,1
lossyrob,a layermanager will not be doing the same thing as a catalog,12/10/15,1
lossyrob,"so it might be that a catalog provides a layer manager, or something like that",12/10/15,1
echeipesh,catalog is for discovery and layer manger is for affecting change?,12/10/15,1
lossyrob,"something like that. catalog is the source of truth. LayerManager uses that to do it_Ñés thing, so do readers and writers",12/10/15,1
moradology,I noticed the Statistics object doesn't include a count,12/10/15,1
moradology,"seems like that might be useful, but i suppose you can get that from the tile itself",12/10/15,1
moradology,geotrellis.op.stats,12/10/15,1
lossyrob,Wouldn_Ñét be a bad idea to add it. It would represent the count of non-NODATA values,12/10/15,1
lossyrob,"make it a Long though, if you can imagine running it on a distributed tile set, you could end up with many cells",12/10/15,1
moradology,"yeah, that makes sense",12/10/15,1
moradology,"i just implemented zonalStats and i've got plenty of time left on this task, so i'll see how much is involved to extend the stats object",12/10/15,1
lossyrob,"There_Ñés actually some code that I found that uses RasterExtents to represnet the whole cols and rows for Raster RDD_Ñés, and that shouldn_Ñét be, because they would need to be Long values",12/10/15,1
lossyrob,or just write an issue with it and we_Ñéll add it to the pile,12/10/15,1
ahmetartu,Hi I have a bunch of geotiff files and want to process using spark. I couldn't find a code sample or tutorial for that. ,12/11/15,1
ahmetartu,"and is it ok if I just include the following line to my build.sbt: libraryDependencies += ""com.azavea.geotrellis"" % ""geotrellis-spark_2.10"" % ""0.10.0-M1""",12/11/15,1
lossyrob,"hey @ahmetartu, I_Ñém only on for a minute, but a good place to start would to be to work through an ingest with `spark-etl`, as described in this README: https://github.com/geotrellis/geotrellis/tree/master/spark-etl",12/11/15,1
lossyrob,"And to use a binTray version of GeoTrellis, not `0.10.0-M1`",12/11/15,1
lossyrob,you can do so by adding the following to build.sbt:,12/11/15,1
lossyrob,"```scalaresolvers += Resolver.bintrayRepo(""azavea"", ""geotrellis"")libraryDependencies += ""com.azavea.geotrellis"" %% ""geotrellis-spark-etl"" % ""0.10.0-5650ef1""```",12/11/15,1
lossyrob,The sha in the version comes from the latest version published here: https://bintray.com/azavea/geotrellis,12/11/15,1
lossyrob,we publish those automatically on each PR merged into master,12/11/15,1
lossyrob,And official release should be made around February,12/11/15,1
ahmetartu,"Thanks @lossyrob for that , but looks like there is no code sample for that",12/11/15,1
ahmetartu,"```scalaVersion := ""2.10.5""libraryDependencies += ""org.gdal"" % ""gdal"" % ""1.11.2""libraryDependencies += ""org.scalaz.stream"" % ""scalaz-stream_2.10"" % ""0.7.3""libraryDependencies += ""com.azavea.geotrellis"" % ""geotrellis-spark_2.10"" % ""0.10.0-M1""```looks like that one compiles OK, I'm not sure how it fits my needs though",12/11/15,1
melvinvarkey,"sorry this is a very basic question. :( when we do ncdump foo.nc we can see what are the variables in it rite ? so is their any functions we could use to write an application to read the variable",12/12/15,1
melvinvarkey,just see the variables in that file ,12/12/15,1
melvinvarkey,and find the dimensions ,12/12/15,1
melvinvarkey,just to see the contents of netcdf file ,12/12/15,1
lossyrob,My recommendation would be to use gdal to transform the netcdf into geotiffs before processing with geotrellis. Gdla,12/12/15,1
lossyrob,Gdal can do what your asking I think,12/12/15,1
melvinvarkey,can you point me to some code ?,12/12/15,1
lossyrob,"At an airport, so not currently. It would be command line using gdal_translate and gdalwarp ",12/12/15,1
melvinvarkey,okok thanks ,12/12/15,1
melvinvarkey,"```15/12/12 20:36:55 ERROR SparkContext: Error initializing SparkContext.java.lang.SecurityException: class ""javax.servlet.FilterRegistration""'s signer information does not match signer information of other classes in the same package```",12/12/15,1
melvinvarkey,this error comes any idea ?! ,12/12/15,1
pomadchin,@melvinvarkey and what gt version do you use?,12/12/15,1
pomadchin,sounds like you have some overlapping deps,12/12/15,1
melvinvarkey,am using this one ,12/12/15,1
melvinvarkey,https://github.com/SciSpark/SciSpark,12/12/15,1
melvinvarkey,i mean this project ,12/12/15,1
melvinvarkey,```,12/12/15,1
melvinvarkey,"```import sbt.ResolverassemblyJarName in assembly := ""SciSparkTestExperiments.jar""name := ""SciSparkTestExperiments""version := ""1.0""scalaVersion := ""2.10.5""mainClass in Compile := Some(""org.dia.algorithms.mcc.MainNetcdfDFSMCC"")resolvers ++= Seq(  Resolver.mavenLocal,  Resolver.file(""Local"", file(Path.userHome.absolutePath + ""/.ivy2/local""))(Resolver.ivyStylePatterns))/** * unmanagedBase and unmanagedJars are to provide a way to include custom jars. * This is specifically helpful for nd4s builds. */unmanagedBase := baseDirectory.value / ""lib""unmanagedJars in Compile := (baseDirectory.value ** ""*.jar"").classpath/*val buildSettings = Defaults.coreDefaultSettings ++ Seq {  javaOptions += ""-Xms4000M -Xmx5024M -Xss1M -XX:MaxPermSize=256M""}*//** * Prevents multiple SparkContexts from being launched */parallelExecution in Test := truefork in run := truefork in test := truetest in assembly := {}javaOptions += s""-Djava.library.path=/usr/local/lib:/usr/local/lib/gdal""/** * There are conflicting slf4j versions between spark and nd4j. Due to the * recency of Nd4j and it's development speed it is using the latest slf4j version. */classpathTypes += ""maven-plugin""libraryDependencies ++= Seq(  //""org.scalatest"" % ""scalatest_2.10"" % ""3.0.0-SNAP4"",  ""org.apache.spark"" % ""spark-core_2.10"" % ""1.4.1"" exclude(""org.slf4j"", ""slf4j-api""),  //Math Libraries  //""org.jblas"" % ""jblas"" % ""1.2.3"",  // other dependencies here  //""org.scalanlp"" %% ""breeze"" % ""0.11.2"",  ""org.json4s"" %% ""json4s-native"" % ""3.2.11"",  // native libraries greatly improve performance, but increase jar sizes.  ""org.scalanlp"" %% ""breeze-natives"" % ""0.11.2"",  // Nd4j scala api with netlib-blas backend  //""org.nd4s"" % ""nd4s_2.10"" % ""0.4-rc0"",  ""org.nd4j"" % ""nd4j-x86"" % ""0.4-rc1"",  ""edu.ucar"" % ""opendap"" % ""2.2.2"",  ""joda-time"" % ""joda-time"" % ""2.8.1"",  ""com.joestelmach"" % ""natty"" % ""0.11"",  ""edu.ucar"" % ""netcdf"" % ""4.2.20"",  ""org.apache.hbase"" % ""hbase-server"" % ""0.98.6-cdh5.2.0"" excludeAll(ExclusionRule(organization = ""org.mortbay.jetty"")))resolvers += ""Akka Repository"" at ""http://repo.akka.io/releases/""resolvers += ""Scalaz Bintray Repo"" at ""http://dl.bintray.com/scalaz/releases""assemblyMergeStrategy in assembly := {  case x if x.startsWith(""META-INF"") => MergeStrategy.discard // Bumf  case x if x.endsWith("".html"") => MergeStrategy.discard // More bumf  case x if x.contains(""slf4j-api"") => MergeStrategy.last  case x if x.contains(""org/cyberneko/html"") => MergeStrategy.first  case x if x.contains(""SingleThreadModel.class"") => MergeStrategy.first  case x if x.contains(""javax.servlet"") => MergeStrategy.first  case x if x.contains(""org.eclipse"") => MergeStrategy.first  case x if x.contains(""org.apache"") => MergeStrategy.first  case x if x.contains(""org.slf4j"") => MergeStrategy.first  case PathList(""com"", ""esotericsoftware"", xs@_ *) => MergeStrategy.last // For Log$Logger.class  case x => MergeStrategy.first}```",12/12/15,1
melvinvarkey,this is the SBT ,12/12/15,1
melvinvarkey,build.sbt ,12/12/15,1
pomadchin,and how do you run this app? `sbt run` ?,12/12/15,1
melvinvarkey,nope in Idea intellij,12/12/15,1
melvinvarkey,run in IDE,12/12/15,1
pomadchin,"try to run it via set, not ide",12/12/15,1
melvinvarkey,using sbt run command ?,12/12/15,1
pomadchin,yes,12/12/15,1
melvinvarkey,ok let me c that ,12/12/15,1
melvinvarkey,sbt +compile +run,12/12/15,1
pomadchin,ye,12/12/15,1
pomadchin,"seems like there can be problems with resolving, as intelij resolves deps in its own way, logic incapsulated into scala plugin",12/12/15,1
pomadchin,you try also to look throgh all deps in project settings (in intelij) and to exclude conflicting deps manually; if via sbt works all ok,12/12/15,1
melvinvarkey,ok ,12/12/15,1
melvinvarkey,now showing ,12/12/15,1
melvinvarkey,"```   /home/achayoo/.ivy2/local/org.apache.hbase/hbase-server/0.98.6-cdh5.2.0/ivys/ivy.xml[info] Resolving org.fusesource.jansi#jansi;1.4 ...[warn] 	::::::::::::::::::::::::::::::::::::::::::::::[warn] 	::          UNRESOLVED DEPENDENCIES         ::[warn] 	::::::::::::::::::::::::::::::::::::::::::::::[warn] 	:: org.apache.hbase#hbase-server;0.98.6-cdh5.2.0: not found[warn] 	::::::::::::::::::::::::::::::::::::::::::::::[warn] [warn] 	Note: Unresolved dependencies path:[warn] 		org.apache.hbase:hbase-server:0.98.6-cdh5.2.0 (/home/achayoo/projmelvin/SciSpark-master/build.sbt#L44-63)[warn] 		  +- scisparktestexperiments:scisparktestexperiments_2.10:1.0sbt.ResolveException: unresolved dependency: org.apache.hbase#hbase-server;0.98.6-cdh5.2.0: not found```",12/12/15,1
melvinvarkey,this,12/12/15,1
melvinvarkey,i ll add this in build.sbt,12/12/15,1
pomadchin,"try to add `""Cloudera"" at ""https://repository.cloudera.com/artifactory/public/""` into resolvers ",12/12/15,1
pomadchin,https://repository.cloudera.com/artifactory/public/org/apache/hbase/hbase-server/0.98.6-cdh5.2.0/,12/12/15,1
melvinvarkey,nope not solving ,12/12/15,1
melvinvarkey,same error,12/12/15,1
pomadchin,heh,12/12/15,1
pomadchin,you can add manually this dep and this resolver,12/12/15,1
melvinvarkey,yes downloaded ,12/12/15,1
melvinvarkey,thanks it worked !!!! ,12/12/15,1
melvinvarkey,"```15/12/13 12:29:02 INFO deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length15/12/13 12:29:02 INFO deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file15/12/13 12:29:03 ERROR Executor: Exception in task 0.0 in stage 0.0 (TID 0)java.lang.OutOfMemoryError: Java heap space	at java.util.Arrays.copyOf(Arrays.java:3236)	at java.io.ByteArrayOutputStream.grow(ByteArrayOutputStream.java:118)	at java.io.ByteArrayOutputStream.ensureCapacity(ByteArrayOutputStream.java:93)	at java.io.ByteArrayOutputStream.write(ByteArrayOutputStream.java:153)```",12/13/15,1
melvinvarkey,keep getting this  error ,12/13/15,1
pomadchin,Try to increase memory for executor ,12/13/15,1
pomadchin,Or you have to reduce inmemory tasks size,12/13/15,1
melvinvarkey,how to do that ,12/13/15,1
melvinvarkey,keep getting this erro ,12/13/15,1
melvinvarkey,my system has 8 gb ram ,12/13/15,1
pomadchin,How you start your app?,12/13/15,1
pomadchin,there is a setting for a spark submit: spark.executor.memory ,12/13/15,1
pomadchin,also it is possible to set it right in the code,12/13/15,1
pomadchin,"`conf .set(""spark.executor.memory"", ""4g"")`",12/13/15,1
melvinvarkey,"```#!/usr/bin/env bash./sbt run -Djava.library.path=/usr/local/lib -Xms1024m -Xmx1024m -XX:ReservedCodeCacheSize=512M -XX:MaxPermSize=512M```",12/13/15,1
pomadchin,"you have to set memory for executor, so as you run in a such way, think the best option is to hardcode and to check is it solves the problem: https://spark.apache.org/docs/latest/configuration.html",12/13/15,1
melvinvarkey,yup did like that but no use ,12/13/15,1
lossyrob,"@pomadchin @moradology @jamesmcclain @echeipesh https://github.com/geotrellis/geotrellis/pull/1236 was merged, you_Ñéll have to update your PR_Ñés, if there_Ñés any spark tests it_Ñéll probably have conflicts",12/14/15,1
lossyrob,https://github.com/geotrellis/geotrellis/pull/1231 is ready for review,12/15/15,1
pomadchin,"so this is an example of `reindexer` : https://github.com/pomadchin/geotrellis/blob/feature/layer-management-function-reindexer/spark/src/main/scala/geotrellis/spark/io/LayerReindexer.scalaIt is just a composition of copier and deleter. But not sure, that this implementations suits us: you see that it depends on `LayerCopier` which depends on `KeyIndexMethod`, so if we want to have some generic `LayerManager`, think it would be nice to have smth like `manager.reindex(layerId, ZCurveKeyIndexMethod.byYear)`, but in current implementations this leads to create an instance of `LayerCopier` inside function.And just having wrapper to all layer management functions has its own restrictions (but also usable). ```scalatrait LayerManager[ID] {  val layerDeleter: LayerDeleter[ID]  val layerCopier: LayerCopier[ID]  val layerMover: LayerMover[ID]  val layerReindexer: LayerReindexer[ID]  def delete(id: ID): Unit = layerDeleter.delete(id)  def copy(from: ID, to: ID): Unit = layerCopier.copy(from, to)  def move(from: ID, to: ID): Unit = layerMover.move(from, to)  def reindex(id: ID): Unit = layerReindexer.reindex(id) // we had to init with the right reindexer!  def reindex(id: ID, indexMethod: KeyIndexMethod[K]) = { create new reindexer here }}```",12/15/15,1
lossyrob,so the copier has an option to change the index?,12/15/15,1
pomadchin,its in-memory version: https://github.com/pomadchin/geotrellis/blob/feature/layer-management-function-copier/spark/src/main/scala/geotrellis/spark/io/SparkLayerCopier.scala,12/15/15,1
pomadchin,"guys, mb I_Ñéll write traits and some exaplanations (what i thought implementing these layer management functions) in the issue ?",12/15/15,1
lossyrob,"Yeah that would work. Or if you want to just put the bare traits, and then in the comments of the issue have the explanation, just so we can see the simple bare thing all at once at the top",12/15/15,1
pomadchin,ye ok,12/15/15,1
pomadchin,"btw, about reindexer: let it be still WIP? because i think that possible changes which going to happen with copier and mover will influent alot reindexer",12/15/15,1
lossyrob,it is WIP right?,12/15/15,1
jamesmcclain,Rasterizer WIP PR is up: https://github.com/geotrellis/geotrellis/pull/1244,12/15/15,1
lossyrob,"@jamesmcclain cool thanks. I took a look, looks pretty awesomely terse",12/15/15,1
jamesmcclain,Thanks :-),12/15/15,1
lossyrob,Though I hope there_Ñés no tears when we rip apart the functional beautify of it to make it mutably performant once we determine it_Ñés correctness :),12/15/15,1
jamesmcclain,"Sure, that is expected",12/15/15,1
lossyrob,"@jamesmcclain I_Ñém not sure why your comments aren_Ñét showing up on the lines in the _ÑÒfiles changed_Ñù part of the PR. Are you commenting on the commits directly? If you comment on the _ÑÒfiles changed_Ñù part, they are easier to read in review",12/15/15,1
jamesmcclain,"Yes, I am commenting directly on the commits",12/15/15,1
jamesmcclain,I hope that they show up there,12/15/15,1
lossyrob,they don_Ñét. I think you have to comment on the PR,12/15/15,1
jamesmcclain,"I see, they should be visible there: https://github.com/jamesmcclain/geotrellis/commit/08a7b70daf544482745175b595c9f467230a7e8f.  I will try to comment in the correct place in the future",12/15/15,1
lossyrob,"sure. yeah I feel like github should make them show up on the PR, unfortunately they don't",12/15/15,1
jamesmcclain,Yeah,12/15/15,1
lossyrob,I_Ñém really curious about how this will perform next to the other implementation,12/15/15,1
jamesmcclain,"I am curious, as well",12/15/15,1
jamesmcclain,"For small polygons, almost certainly not as well",12/15/15,1
jamesmcclain,Only changing the way active edges are computed will probably make this faster for sufficiently large polygons,12/15/15,1
jamesmcclain,Because the active-edge calculation in the old code seem to rescan (almost) the entire edge list on every row,12/15/15,1
lossyrob,Hmm interesting,12/15/15,1
jamesmcclain,"But anyway, if this looks worth-pursuing, there is plenty of room for optimization",12/15/15,1
pomadchin,Good news everyone! (ftw i was so blind (just was searching by wrong key words %))) https://accumulo.apache.org/1.6/apidocs/org/apache/accumulo/core/iterators/user/TransformingIterator.html ,12/15/15,1
lossyrob,niice,12/15/15,1
lossyrob,@jamesmcclain for sure,12/15/15,1
echeipesh,"I_Ñém going to have to test this some more, but this is pretty hot:",12/15/15,1
echeipesh,"```scala  /** Define Intersects filter for Extent */  implicit def forExtent[K: SpatialComponent: Boundable, M: (? => {def mapTransform: MapKeyTransform})] =    new RDDFilter[K, Intersects.type, Extent, M] {    def apply(metadata: M, kb: KeyBounds[K], extent: Extent) = {      val bounds = metadata.mapTransform(extent)      val queryBounds = KeyBounds(        kb.minKey updateSpatialComponent SpatialKey(bounds.colMin, bounds.rowMin),        kb.maxKey updateSpatialComponent SpatialKey(bounds.colMax, bounds.rowMax))      implicitly[Boundable[K]].intersect(queryBounds, kb).toSeq    }  }```",12/15/15,1
lossyrob,whoa. So we would get to dumb the *Viewable types,12/15/15,1
echeipesh,"Yep, two questions to answer are: 1) does it perform as expected under all cases? 2) What are the performance implications of typing based on a structural type here.",12/15/15,1
lossyrob,"@non you might be interested in this, finally am leaning on that port of caliper I did oh-so-long-ago: https://github.com/azavea/scaliper",12/15/15,1
lossyrob,"Heads up (mostly @echeipesh) I moved the deprecated `benchmarks` project to my user, so if you even want to pull code from it, it_Ñés at https://github.com/lossyrob/benchmark",12/15/15,1
echeipesh,4-Oct,12/15/15,1
lossyrob,Here is the new benchmark project: https://github.com/geotrellis/geotrellis-benchmark ,12/15/15,1
pomadchin,so with `TransformingIterator` usage it is not possible to copy layer to another table,12/15/15,1
echeipesh,@lossyrob: in scalliper is there a difference between using `setUp` and just doing the setup in the `Benchmark` constructor/body?,12/15/15,1
lossyrob,yes,12/15/15,1
lossyrob,"well, perhaps",12/15/15,1
echeipesh,@pomadchin The the only valid case would be to rename a layer in the same table ?,12/15/15,1
lossyrob,you_Ñéd be closing over the outer object; the idea is to have a clean object that is easily instantiated by the other JVM that will do the benchmarking,12/15/15,1
lossyrob,"it uses Kryo to serialize the benchmark objects. So not having any values actually filled out, and relying on the setUp function that_Ñés called on the other side of the fence, keeps things cleans\closure free",12/15/15,1
pomadchin,"@echeipesh yes, so with this iterator it is posible to change any part of the key (not sure about rowid); but only in one table, it caused by the thing that scanner works only with one table",12/15/15,1
echeipesh,gotcha,12/15/15,1
pomadchin,"I_Ñém making sample, can push to a separate branch and you_Ñéll look through it",12/15/15,1
echeipesh,"Ok, did a little benchmark to compare using structural types vs traits for view bounds:```shRunning benchmarks for Extracting Strings...  1 of 4: Structural, has method          66764.77 ns; ñó=70376.24886 ns @ 10 trials  2 of 4: Structural, implicit clas       88970.43 ns; ñó=93783.06963 ns @ 10 trials  3 of 4: Trait, has method               35172.20 ns; ñó=37074.75573 ns @ 10 trials  4 of 4: Trait, implicit class           85326.08 ns; ñó=89941.58496 ns @ 10 trials  Name                                    us  linear runtime  Structural, has method               66.76  ========  Structural, implicit class           88.97  ==========  Trait, has method                    35.17  ====  Trait, implicit class                85.33  ==========```First two cases are for:```scalaobject StructuralExtractor {  def apply[M: (? => {def extract: String})](thing: M): String = {    thing.extract  }}//has methodclass ExtractableBox(s: String) {  def extract = s}//implicit class    implicit class ExtractableBox(b: Box) {      def extract = b.s    }```Second two cases are for:```scalatrait Extractable {  def extract: String}object TraitExtractor {  def apply[M: (? => Extractable)](thing: M): String = {    thing.extract  }}//has methodclass ExtractableBox(s: String) {  def extract = s}//implicit class implicit class ExtractableBox(b: Box) {    def extract = b.s }```",12/15/15,1
echeipesh,and each one runs for  `10000` times in each iteration.,12/15/15,1
lossyrob,"note that, for things that run in that sort of time (very fast), you have to take the benchmark results with a cup of salt",12/15/15,1
lossyrob,"in general you should be wary of microbenchmarks results, and look for the consistency in the runs",12/15/15,1
echeipesh,"Yes, to be clear I_Ñém basically reading that as _ÑÒno difference_Ñù between structural type and type on a trait. The results are consistent between runs. I_Ñém just checking for bug bears, some crazy reason that it would be a lot slower.",12/15/15,1
lossyrob,@pomadchin confused about the removing of WIP in #1241. Is this because your not working on it? But should it be considered for merging? PR_Ñés that shouldn_Ñét be considered _ÑÒready-to-merge_Ñù should have the WIP tag,12/15/15,1
pomadchin,"@lossyrob clear, it depends on mover / copier",12/15/15,1
pomadchin,"so yes, let it be WIP, i_Ñém confused too ):",12/15/15,1
lossyrob,oh I see. Will it be updated based on the mover/copier code? or just those have to be merged in first?,12/15/15,1
pomadchin,be updated based on mover / copier,12/15/15,1
lossyrob,"ok, yeah it_Ñés still in progress then",12/15/15,1
pomadchin,"ye, ty for explanations",12/15/15,1
lossyrob,"no worries, still trying to pin down the process",12/15/15,1
lossyrob,Commented on https://github.com/geotrellis/geotrellis/issues/1225,12/15/15,1
lossyrob,commented on an old commit: https://github.com/geotrellis/geotrellis/commit/125bb510773c14379db72543f68279be5ac61e6d,12/16/15,1
lossyrob,"@echeipesh vanquished RasterRDD, everything worked out fine: https://github.com/echeipesh/geotrellis/pull/5",12/16/15,1
lossyrob,"The one trick is that the contructor of the `RDD[(K, V)] with Metadata[M]` is generic and called `ContextRDD[K, V, M]`. I_Ñém open to different naming, but it seems like, it_Ñés an RDD with a metadata context.",12/16/15,1
lossyrob,"and that way the contructor is generic and there_Ñés no more bridges that have to be passed around, so it removes a lot of code",12/16/15,1
lossyrob,"The implicit anonymous type restrictions (whatever that is called) works for `M`, as exemplified here: https://github.com/echeipesh/geotrellis/pull/5/files#diff-81994733451a33e191029be9d85748feR12",12/16/15,1
echeipesh,"I like it. It needs to vary over RDD type though, something like this might work:```scalaclass RDD[T]class SpaceRDD[K, V] extends RDD[(K, V)]class ShuffledRDD[K, V, C] extends RDD[(K, V)]case class ContextRDD[F <: RDD[_], M](val rdd: F, val metadata: M) {	def withContext[R <: RDD[_]](f: F => R) =  	new ContextRDD(f(rdd), metadata)}@instrument class Playground {  val context = new ContextRDD(new SpaceRDD[String, Int], ""WKT"") // ContextRDD(SpaceRDD@5d300270,WKT)	  val opResult = context.withContext { rdd => new ShuffledRDD[String, Int, Int] } // ContextRDD(ShuffledRDD@3df4f2a7,WKT) }```",12/16/15,1
dwins,"Hi, i'd like to get involved with geotrellis development. My employer has asked me to see if there's anything I can do to help get closer to the 0.10 release.",12/16/15,1
echeipesh,"Hi David, thanks for getting involved. I believe @lossyrob has concrete plans for you.",12/16/15,1
dwins,"cool, i'm just looking over the open issues for now",12/16/15,1
pomadchin,"@echeipesh wouch, we have to be careful with rdd subtyping ):",12/16/15,1
echeipesh,Yes ^ doesn_Ñét quite do the trick. It also needs to capture the `T` in `RDD[T]` so it can extend it.,12/16/15,1
pomadchin,ye,12/16/15,1
pomadchin,"```scalaclass DupeColumnFamilyTransforming extends TransformingIterator {  val columnFamilyName = ""columnFamily""  var options: java.util.Map[String, String] = null  lazy val columnFamily = options.get(columnFamilyName)  def getKeyPrefix: PartialKey = PartialKey.ROW  def transformRange(input: SortedKeyValueIterator[Key, Value], output: KVBuffer): Unit = {    while (input.hasTop) {      output.append(replaceColumnFamily(new Key(input.getTopKey), columnFamily), input.getTopValue)      input.next()    }  }  override def init(source: SortedKeyValueIterator[Key, Value], options: java.util.Map[String, String], env: IteratorEnvironment) {    super.init(source, options, env)    this.options = options    println(options.toList)  }}```",12/16/15,1
lossyrob,hey @dwins ,12/16/15,1
dwins,hi @lossyrob ,12/16/15,1
lossyrob,Did you end up finding an issue to look at?,12/16/15,1
dwins,"looked at a couple, haven't really dug into anything yet",12/16/15,1
lossyrob,"ok cool. do you want me to find a couple good ones? The github issues aren_Ñét quite fully up to date with the whole 0.10 plan, which is somehting I have to do",12/16/15,1
dwins,"yes, i'd appreciate it",12/16/15,1
lossyrob,for sure,12/16/15,1
lossyrob,@dwins what about this one? https://github.com/geotrellis/geotrellis/issues/1249,12/16/15,1
lossyrob,trying to think of some low hanging fruit/introductory stuff to get you started working on the codebase,12/16/15,1
dwins,ok i'll try and put together a patch for 1249,12/16/15,1
lossyrob,cool. nice to have you helping out!,12/16/15,1
lossyrob,@pomadchin how are things going on your end?,12/16/15,1
pomadchin,@lossyrob there is a dirty version (without fetching by column family): https://github.com/pomadchin/geotrellis/blob/feature/layer-management-function-copier-accumulo/spark/src/main/scala/geotrellis/spark/io/accumulo/AccumuloLayerCopier.scala,12/16/15,1
pomadchin,but it requires Iterator to be available in Accumulo classpath,12/16/15,1
pomadchin,"so yes, it is possible to change family using api (using `TransformingIterator`); and may be even to reindex (may be not using `TransformingIterator` implementation)",12/16/15,1
lossyrob,whoa. looks complicated ha,12/16/15,1
lossyrob,it requires a custom iterator?,12/16/15,1
pomadchin,yes,12/16/15,1
lossyrob,dang,12/16/15,1
lossyrob,"that would be quite a change, to go from not having any custom accumulo iterators to requiring one",12/16/15,1
pomadchin,"ye, may be to delay it, or not to use in near future o: until we'd have smth more to add to Accumulo classpath",12/16/15,1
lossyrob,"ok. So if we don_Ñét have a custom accumulator, we have to do the spark transformation?",12/16/15,1
pomadchin,yes,12/16/15,1
lossyrob,"gotcha. ok, well I guess it makes sense then to allow for that in the abstraction, but have the Accumulo reindexer use the generic for now. We could add the custom iterator stuff later if we think it_Ñéd be worth it",12/16/15,1
echeipesh,"Going over the API with some notes and it all makes sense. Reindexer is the basic problem. Both on S3 and Accumulo it_Ñés hard to imagine how to reindex in place without stomping on yourself. The simple solution is to load to spark, delete, save from spark. But that_Ñés incredibly fragile.",12/16/15,1
echeipesh,https://gist.github.com/echeipesh/812948930d6f5ec7b442,12/16/15,1
lossyrob,That API setup looks good to me,12/16/15,1
echeipesh,"Sure, it_Ñés not too bad, but reindexer question is outstanding :)",12/16/15,1
lossyrob,"ok, so the worry there is that we would have a weird partial reindex",12/16/15,1
lossyrob,which is the stomping on ourselves?,12/16/15,1
echeipesh,"Right, if we_Ñére writing records before the old files are gone, we can_Ñét tell them appart afterwards. If we delete the source files before we write we_Ñére vulenrable to data loss",12/16/15,1
lossyrob,ok. makes sense,12/16/15,1
lossyrob,let_Ñés just go with the safe way. I guess that means generic implementations for all,12/16/15,1
lossyrob,"but also we should keep the trait like you have it, nice and clean, so in the future if we are smarter we can do a smarter thing",12/16/15,1
echeipesh,The safest way would be to Load/Move/Save,12/16/15,1
lossyrob,@pomadchin what do you think?,12/16/15,1
pomadchin,"yes, looks good",12/16/15,1
lossyrob,"ok. The only thing is, do we allow reindex on copy",12/16/15,1
lossyrob,because otherwise we would have to Load/Move/Save (with a delete in there) and then copy,12/16/15,1
echeipesh,"So I would like to suggest a solution from your play book, _ÑÒmake it a users problem_Ñù. Allow only re-index on copy. Then tracking the workflow is not our problem.",12/16/15,1
pomadchin,"just checking hwo it would be with such methods like ```def copy(from: LayerId, to: LayerId, key: String) def copy(from: LayerId, to: LayerId, bucket: String, key: String)```",12/16/15,1
pomadchin,seems all ok;,12/16/15,1
lossyrob,"the bucket and key (for the S3 implementation) should be in the constructor parameters, no?",12/16/15,1
lossyrob,"so it can implement the `def copy(from: LayerId, to; LayerId)`",12/16/15,1
lossyrob,@echeipesh perhaps. or should we just have Copy have an overload that takes a KeyIndexMethod?,12/16/15,1
lossyrob,wait yeah. The implementations you have in that Gist don_Ñét make sense,12/16/15,1
echeipesh,"My thought on that was is that it can always implement `def copy(from: LayerId, to; LayerId)`. Putting target bucket and key in the constructor doesn_Ñét make much sense from standpoint that it charactirizes the copier. So an extra method is ok.",12/16/15,1
lossyrob,I_Ñém going to fork it,12/16/15,1
echeipesh,"kk, great B)",12/16/15,1
pomadchin,"btw, having `KeyIndexMethod` in function will cause making instance of writer ",12/16/15,1
pomadchin,):,12/16/15,1
lossyrob,"`def copy(from: LayerId, to: LayerId, key: String)`, why the overload sans bucket?",12/16/15,1
echeipesh,"API sugar, leave it out ?",12/16/15,1
lossyrob,"yeah, is there a case wher eyou don_Ñét supply the bucket ever? ",12/16/15,1
echeipesh,"Sure, if you_Ñére just re-organizting the layers in your bucket.",12/16/15,1
lossyrob,@pomadchin understood about the writer.,12/16/15,1
echeipesh,Without that you need to use the attribute store to read the bucket for that layer first.,12/16/15,1
pomadchin,"and with copy: ```scaladef copy(from: LayerId, to: LayerId) // > there would be writerdef copy(from: LayerId, to: LayerId, table: String) // > there would be the other writer```",12/16/15,1
lossyrob,"well, that writer could be supplied in the implementation_Ñés constructor",12/16/15,1
pomadchin,the first?,12/16/15,1
pomadchin,ye,12/16/15,1
lossyrob,for the generic ones,12/16/15,1
echeipesh,That_Ñés actually a pretty good point. Why not make Writer.write have an KeyIndex/KeyIndexMethod overload ?,12/16/15,1
pomadchin,but in general it may have different table (this generic writer in constructor),12/16/15,1
lossyrob,Writer doesn_Ñét have that in the def because then it wouldn_Ñét follow the simple Writer trait interface,12/16/15,1
lossyrob,so instead it_Ñés something that_Ñés supplied in the constructor,12/16/15,1
echeipesh,"That_Ñés true, it would have to be `LayerWriter` that has that, that in turn sublasses `Writer`.",12/16/15,1
lossyrob,"but that means if you want to have one thing that can write to many different index structure, you need to create a writer for each one",12/16/15,1
lossyrob,"@pomadchin working with the API gist. Sorry want to make clear_Ñ_the AccumuloLayerCopier will have to use a Spark copy (Reader/Writer) if there_Ñés no custom iterator? Or can that use the accumulo api, and it_Ñés just the reindexer that would need the generic implimentation ",12/16/15,1
pomadchin,"yes you right, we have to use spark even for accumulo copier, if there is no custom iterator",12/16/15,1
lossyrob,"gotcha, thanks",12/16/15,1
lossyrob,"working with the API now, it is a tricky puzzle. Should have something to report in the next hour",12/16/15,1
pomadchin,cool!,12/16/15,1
pomadchin,yes to make it beatiful is really a hard thing,12/16/15,1
dwins,I put up dwins/geotrellis:fix-1249-spatial-index but maybe i should hold off on issuing a PR until I take care of the CLA.,12/16/15,1
lossyrob,"you can issue a work in progress PR, just prefix it with WIP. that way we can code review but know not to merge it in",12/16/15,1
lossyrob,@dwins there_Ñés this issue too: https://github.com/geotrellis/geotrellis/issues/1248,12/16/15,1
lossyrob,"I_Ñém not sure what the benefit of having the CGAlgorithms would be, if there would be performance improvements in some cases. But something to investigate, even if the issue closes with a researched opinion of, no we don_Ñét have to implement these",12/16/15,1
dwins,OK cool I'll put that in the queue too,12/16/15,1
lossyrob,"@pomadchin here_Ñés your reindexer branch, brought up to master: https://github.com/lossyrob/geotrellis/tree/pomadchin-feature-layer-manager",12/17/15,1
lossyrob,"also has a fix where spark-etl was not in buildall.sh, and wasn_Ñét updated to the metadata pr",12/17/15,1
lossyrob,"I added a _ÑÒLayerManager_Ñù interface, and started to play around with that. I modified some of the objects in S3Layer*, to allow for overloads and the like. Take a look at the changes, let me know what you think; I think creating the LayerManager objects will be clarifying as far as the API goes. But everything looks functionally there.",12/17/15,1
lossyrob,"I really want to figure out how to solve the problem that the SpacePartitioner would have with the `RDD[(K, V)] with Metadata[M]` change, because it will remove a lot of code that_Ñés the `bridge` implicits and the container type declaration, which will be really nice",12/17/15,1
pomadchin,"@lossyrob wow, cool, will have a look as fast as I can",12/17/15,1
lossyrob,"I_Ñém going to try to get to bed at a more reasonable hour tonight, so I_Ñéll check about your thoughts in the morning :)",12/17/15,1
lossyrob,"It probably makes sense to pull in my changes into your branch, and then close out the other PRs, just have the Reindexer one be a PR for all the functionality, link back to the old PRs",12/17/15,1
lossyrob,"and if you end up pushing the LayerManager stuff to a point before my tomorrow, that would be cool",12/17/15,1
lossyrob,cheers,12/17/15,1
pomadchin,Yes will merge with the reindeer; sounds really cool,12/17/15,1
pomadchin,Good night Rob!,12/17/15,1
pomadchin,"Hm, have you checked layer manager trait?",12/17/15,1
pomadchin,"```scalatrait LayerManager[ID] {  def delete[K: Boundable: AvroRecordCodec: JsonFormat: ClassTag, V: AvroRecordCodec: ClassTag, M: JsonFormat, C <: RDD[(K, V)]](id: ID): Unit  def copy[K: Boundable: AvroRecordCodec: JsonFormat: ClassTag, V: AvroRecordCodec: ClassTag, M: JsonFormat, C <: RDD[(K, V)]](from: ID, to: ID): Unit  def move[K: Boundable: AvroRecordCodec: JsonFormat: ClassTag, V: AvroRecordCodec: ClassTag, M: JsonFormat, C <: RDD[(K, V)]](from: ID, to: ID): Unit  def reindex[K: Boundable: AvroRecordCodec: JsonFormat: ClassTag, V: AvroRecordCodec: ClassTag, M: JsonFormat, C <: RDD[(K, V)]]     (id: ID, keyIndexMethod: KeyIndexMethod[K])(implicit bridge: Bridge[(RDD[(K, V)], M), C]): Unit}```",12/17/15,1
pomadchin,"seems like `C <: RDD[(K, V)]`may cause problems, due to RDD type invariant",12/17/15,1
pomadchin,"oh sorry, haven_Ñét looked up through the whole code",12/17/15,1
pomadchin,"looks cool, merging it now",12/17/15,1
pomadchin,"very cool that we got rid of `Container => Container with RDD[(K, V)]` evidence",12/17/15,1
pomadchin,I am still wondering: should we create instances of Layer* objects inside LayerManager functions;,12/17/15,1
pomadchin,,12/17/15,1
pomadchin,@echeipesh don_Ñét you remember what was the problem with the `Schema` saving in attribute store for `Hadoop`?,12/17/15,1
echeipesh,"I didn_Ñét think there was such a problem, is there ?",12/17/15,1
pomadchin,https://github.com/geotrellis/geotrellis/blob/master/spark/src/main/scala/geotrellis/spark/io/hadoop/HadoopLayerWriter.scala#L39,12/17/15,1
echeipesh,"Right, it doesn_Ñét support schema evolutions. So it assumes it_Ñés unchanging.",12/17/15,1
pomadchin,"oh yes, got it",12/17/15,1
echeipesh,"Which is kind of weird, I guess it should at least save the Schema even though it will be ignored on read.",12/17/15,1
pomadchin,"yes yes, but anyway understood the problem",12/17/15,1
lossyrob,"@pomadchin FYI you can_Ñét have overloads and default arguments at the same time, e.g. https://github.com/pomadchin/geotrellis/blob/feature/layer-manager/spark/src/main/scala/geotrellis/spark/io/accumulo/AccumuloLayerManager.scala#L49",12/17/15,1
lossyrob,unfortunately it compiles in some cases but there_Ñés weirdness always. should be avoided,12/17/15,1
pomadchin,"@lossyrob yes, it's just a sample, that's why it's in a separate branch",12/17/15,1
lossyrob,oh,12/17/15,1
lossyrob,we could just use the default accumulo write strategy. but the hadoop format does present a problem,12/17/15,1
owcm,"Hello,I have been using the Geotrellis Tutorial as a base to learn the basics of Geotrellis. I have been doing basic focal and local operations.  I now have managed to get a AWS EMR setup with Spark going and was going to start running Geotrellis in that environment.I am assuming that in order to  benefit from running in Spark I should be using the local and focal ops in the Geotrellis Spark package. Should there be one to one mappings?  Are there any good coding examples for Geotrellis in the Spark domain.   ThanksChris",12/18/15,1
lossyrob,"Hey @owcm, sorry for the delayed response, but I_Ñém trying to get together a project that will show you what I think you_Ñére looking for",12/18/15,1
lossyrob,"But in order to start running operations on raster data, you_Ñéd have to do some _ÑÒingesting_Ñù. This takes rasters from their raw format and brings it into an indexed format GeoTrellis can work with",12/18/15,1
lossyrob,what sort of data will you be working with?,12/18/15,1
lossyrob,"I will have a tutorial for how to run GeoTrellis on EMR, hopefully by Monday, which should answer your questions :)",12/18/15,1
pomadchin,"guyz, think #1241 is ready for review",12/18/15,1
owcm,"Hi Rob, thanks for your time.  I am starting with Digital Elevation Model data that is a product of 1 Meter Lidar. Raw format are  Erdas Imagine Images (.img) but I have been using GDAL to create Geotiffs.",12/18/15,1
lossyrob,"Great. Here is a landsat tutorial that I completed the local section of, next week I will be adding the EMR tutorial https://github.com/geotrellis/geotrellis-landsat-tutorial",12/18/15,1
owcm,So is Geotiff the best Raster format that or ARG?,12/20/15,1
lossyrob,"We've moved away from ARG files in 0.10, so GeoTiffs are best to start with. When data is ingested onto a format that you can read using our spark codebase, they are no longer GeoTiffs, but are avro encoded byte arrays.",12/20/15,1
owcm,Is there any WMS capability in Geotrellis or do I use Geoserver in conjunction  with Geoserver.,12/20/15,1
owcm,Should read .. use something like Geoserver in conjuction with Geotrellis to provide WMS services.,12/20/15,1
lossyrob,WMS capabilities is a often requested feature. I don_Ñét think it will make the GeoTrellis 0.10 release_Ñ_but also I view it as a concern outside of the core GeoTrellis library.,12/20/15,1
lossyrob,My current thoughts on it are to just create a WMS implementation in Spray that is implemented by GeoTrellis functionality. That could happen alongside/after/outside of the GeoTrellis release cycle.,12/20/15,1
lossyrob,"GeoServer integration is another choice, however when I looked at that a while ago I found some of the restrictions and requirements that the GeoServer API would put on GeoTrellis in order to work as a layer would hurt GeoTrellis performance and be a lot of work. I_Ñém willing to revisit it though.",12/20/15,1
lossyrob,"@dwins has a lot of GeoServer experience, perhaps he could comment.",12/20/15,1
lossyrob,"But like I said, that is an often requested feature. I would need to find bandwidth to put it together, but if we go with the scala/spray implementation, it_Ñés a pretty clear task. If there_Ñés anyone that wants to help develop it or throw some resources behind it, get in touch!",12/20/15,1
lossyrob,@echeipesh any insight on what this error could be caused by? I_Ñéve been seeing things like this: https://travis-ci.org/geotrellis/geotrellis/jobs/97450134#L5740,12/20/15,1
pomadchin,"@lossyrob @echeipesh looks weird (`Expected exception geotrellis.spark.io.package$LayerNotFoundError to be thrown, but org.apache.accumulo.core.client.TableExistsException`)",12/20/15,1
lossyrob,yeah. like there_Ñés an in memory accumulo that isn_Ñét being cleaned out appropriately or something,12/20/15,1
pomadchin,+,12/20/15,1
dwins,"regarding a WMS implementation, I would also lean toward a Spray-based implementation over GeoServer.  Another option might be to only do XYZ tiling and put GeoWebCache in front to translate to other tile services. GWC can even scale and stitch cached tiles to emulate a non-tiled WMS",12/21/15,1
lossyrob,"That_Ñés interesting about GeoWebCache, I didn_Ñét know it could take a z/x/y tile service and expose it as a WMS endpoint",12/21/15,1
lossyrob,"would we need to do any additional work for that? or just tell clients, if you want to expose the tiles through WMS, throw GeoWebCache in front of it?",12/21/15,1
lossyrob,client applications using GeoTrellis,12/21/15,1
dwins,"lossyrob, actually i double-checked and it doesn't actually have a backend implementation for zxy tile services, there is a pluggable interface but it is only implemented for WMS",12/21/15,1
dwins,so a little improvement in GWC is needed before this could work.,12/21/15,1
dwins,I tried to add a vector subproject to geotrellis-benchmark but got a stack trace when trying to execute a benchmark of no-op. https://gist.github.com/dwins/07d6a5c95595bf2be206,12/21/15,1
dwins,i see the raster module already has jts in scope so I suppose I can do what i was trying to do in there for now,12/21/15,1
dwins,i put my code up here. having a hard time seeing what is different from the raster benchmark. https://github.com/dwins/geotrellis-benchmark/tree/cgalgorithm-benchmark,12/21/15,1
lossyrob,@dwins there's something weird going on in scaliper that I have to fix. But you have to return an object from the test. So returning a string or something might do it,12/21/15,1
dwins,yep that did the trick,12/21/15,1
lossyrob,These travis failures are killing me. I_Ñém going to take out that non-determinant mask spec until @jamesmcclain _Ñés rasterization fixes.,12/21/15,1
lossyrob,"Done. Please update PR_Ñés to master, so that travis will hopefully not fail things when it shouldn_Ñét (although there seems to be other problems with travis and some spark tests)",12/21/15,1
dwins,"More drastic difference than I expected```  Name                                    us  linear runtime  CGAlgorithm.isInRing(coordinat       58.80  ==========  Polygon.contains(Point)             148.83  =========================```",12/21/15,1
dwins,interestingly if I change the benchmark to always test a point outside the ring then Polygon.contains uses hardly any time at all,12/21/15,1
lossyrob,probably because of a bounding box check,12/21/15,1
lossyrob,that is quite a difference,12/21/15,1
dwins,"yep, when i made it outside the ring but inside the bbox the performance difference went away",12/21/15,1
lossyrob,"@pomadchin merged your PR. It looked great, the API seemed on, we have LayerManagers for each backend type, they make sense. If there_Ñés refinements as we move forward we can just make them against master. That was a big one, so congrats :)",12/21/15,1
lossyrob,"@dwins considering the outperformance, I wonder if that the CGAlgorithm version should be the defalut on Polygon, and why it_Ñés not that way in JTS",12/21/15,1
dwins,i can take a peek at the implementations,12/21/15,1
pomadchin,"@lossyrob yee, cool; want to make more commits inside gt :d",12/21/15,1
pomadchin,useful commits of course ,12/21/15,1
lossyrob,https://gist.github.com/lossyrob/23ee71a0436f2fe3c4d5,12/22/15,1
lossyrob,@pomadchin the build errored because of a problem with having overloads with default arguments,12/22/15,1
lossyrob,that error only comes out in packaging which is weird. but it_Ñés one of those things you have to know to avoid.,12/22/15,1
lossyrob,hmmm I_Ñém not sure it was you actually,12/22/15,1
lossyrob,@echeipesh :X https://github.com/geotrellis/geotrellis/blob/master/spark/src/main/scala/geotrellis/spark/io/s3/S3LayerReader.scala,12/22/15,1
pomadchin,"@lossyrob missed it, yes it was before me, this came from master; going to make a fix pr ",12/22/15,1
lossyrob,I changed it,12/22/15,1
lossyrob,just update to master,12/22/15,1
pomadchin,"@lossyrob cool, sorry for that ",12/22/15,1
lossyrob,not your fault ;),12/22/15,1
pomadchin,had to find it though :/,12/22/15,1
pomadchin,@lossyrob see you also don't sleep at night ,12/22/15,1
lossyrob,sometimes I try to haha. Ever since Norway I_Ñéve had trouble adjusting back_Ñ_late nights coding don_Ñét help,12/22/15,1
lossyrob,@dwins did you benchmark the creation of the `PreparedGeometry` when doing the benchmark comparison to `isPointInRing`? (Issue https://github.com/geotrellis/geotrellis/pull/1255),12/22/15,1
dwins,No. i pushed the benchmark code to my branch,12/22/15,1
lossyrob,"I have always been curious about when or when not to use `.prepare`, and it seemed like the up front cost of the call would have to be high in order to prevent it from always being used",12/22/15,1
dwins,https://github.com/dwins/geotrellis-benchmark/commit/c00666183ca23de3988df9711bbb1e3bc7495504,12/22/15,1
dwins,"i'll try it with the prepare as part of the benchmarked code, just a tic",12/22/15,1
lossyrob,cool,12/22/15,1
dwins,"```  Name                                    us  linear runtime  CGAlgorithm.isInRing(coordinat       44.81  ==  Polygon.contains(Point)             175.20  ========  prepare once: prepare(Polygon)        8.78  =  always prepare: prepare(Polygo      552.59  =========================```",12/22/15,1
dwins,seems pretty expensive,12/22/15,1
lossyrob,"preparing geometries is one of those things that I_Ñém sure would help speed up some code a lot, but I never think to do it. And we don_Ñét want to do it by default because if your not going to be intersecting (or whatever the speedup profile is for prepared geometries is, that_Ñés still a big ? for me), then it_Ñés not worth it to prepare",12/22/15,1
lossyrob,"like, when we do polygonal statistics on a raster, we_Ñére potentially doing a lot of interections on the incoming polygon. we should probably be using prepared there",12/22/15,1
lossyrob,because wow that is a lot faster when you just prepare once,12/22/15,1
dwins,it's better than the 3x i mentioned in the PR - there's a special optimization for point-in-polygon in 1.14 and i had not upgraded JTS earlier,12/22/15,1
rfecher,"I just caught this discussion - yeah we've  done a decent amount of benchmarking with prepared geoms...point in polygon grows linearly with # vertices of the poly without prepared geom, but is constant with prepared geoms",12/23/15,1
rfecher,https://github.com/chrisbennight/intersection-test#points,12/23/15,1
rfecher,"well, by ""we"" it was really mostly @chrisbennight and @rwgdrummer digging into that",12/23/15,1
lossyrob,this is really cool stuff @rfecher!,12/23/15,1
lossyrob,"So it seems like the conclusion to draw is, there_Ñés are some cases where prepared geometries are worth it, and some cases where it_Ñés actually less optimal then normal geometries. Intersections of polygons are faster for small polygons, but once the vertex count exceeds some threshold it is less optimal on prepared geometries. Point-in-polygon operations are always going to be faster with a prepared polygon of any sort of complexity (save the simplist cases)",12/23/15,1
lossyrob,not sure how to enact that in the system,12/23/15,1
zeitalexgithub,I am new to Spark and Geotrellis. What we need is to do mapalgebra on Esri Grids. For example to do grid1-f,12/23/15,1
zeitalexgithub,"Sorry, to subtract 2 grids",12/23/15,1
zeitalexgithub,Grid1-grid2. As I understand to reduce shuffle in the best case these 2 grids should be on the same node? Is it possibble with spark and geotrallis?,12/23/15,1
lossyrob,"yes, best case, if you have a set of tiles of 2 large raster layers, in Accumulo, and you want to do a local operation, say subtract, then the tile that overlap will be on read in on the same machine",12/23/15,1
lossyrob,"another option is that the RDD_Ñés are spatially (N-dimensionally) partitioned before the operaiton, which would offer the same guarentee",12/23/15,1
lossyrob,"(soft guarantee, there might be some issues with overflow)",12/23/15,1
lossyrob,"but this is talking about raster layers that are transformed (ingested) into the system, out of their Esri grid format",12/23/15,1
zeitalexgithub,Thanks a lot for your answer.,12/23/15,1
zeitalexgithub,Is it possible to install geotrellis in free ec2?,12/24/15,1
echeipesh,"What do you mean by free ec2, t2.micro instances ?",12/24/15,1
echeipesh,"If so, you might be able to start geotrellis in spark local mode but t2.micro only has 1G memory, so there won_Ñét be much to do anything useful.",12/24/15,1
zeitalexgithub,I would like to test geotrellis. Is the best way to do it is installing it as described here https://github.com/geotrellis/geotrellis-ec2-cluster ?,12/24/15,1
lossyrob,"@zeitalexgithub that is one way to ge a geotrellis cluster running on AWS, with Accumulo installed and ready for a heavy load. You could also use EMR to run batch jobs, or even run Spark and GeoTrellis locally. I would recommend running through some things locally to kick the tires first, you can check out this project for an example: https://github.com/geotrellis/geotrellis-landsat-tutorial",12/26/15,1
lossyrob,@echeipesh @moradology et all: curious about your thoughts on this pattern for dealing with overloads w/ default parameters: https://gist.github.com/lossyrob/63f0d40506ff8b4c3b22,12/26/15,1
pomadchin,"@lossyrob may be it is posible to try this pattern (in a separate branch), and to see how it goes, currying _may_ help I remember that in some projects divided 'not functions' and _ÑÉfunctions', to pass functions as curried arguments, and that looked ok in that cases (but it had an impact on the whole api)",12/27/15,1
lossyrob,"@pomadchin for sure, I'm going to use it as part of a reprojection feature, I hit up against this issue pretty hard with that. If it works out there I'll suggest doing it in other parts of the library where we're having this challenge.",12/27/15,1
lossyrob,"Passing functions as a separate parameter list is pretty common, we do that in GeoTrellis too",12/27/15,1
pomadchin,"@lossyrob yes noticed that somewhere, but not everywhere; cool _ªÉ_ ",12/27/15,1
lossyrob,http://www.gdal.org/frmt_gtiff.html,12/28/15,1
lossyrob,NBITS,12/28/15,1
owcm,"I have a SingleBandGeoTiff object, which extends GeoTiff.  I can use the Geotiff.write method to write to disk.  If I want to get a byte array I was expecting to be able to use the toByteArray trait from GeoTiff.  But I get error ""value toByteArray is not a member of geotrellis.raster.io.geotiff"". I am using Geotrellis 0.10.0-M1.   Code looks as follows:             val lclTile = slopeRS.get            val lclExtent =  slopeRS.rasterExtent.get.extent            val geotiff = GeoTiff(lclTile, lclExtent, LatLng)            geotiff.write( ""/Users/user1/outslope.tiff"")            geotiff.toByteArray()",12/28/15,1
lossyrob,"`geotiff` is a package name, you should name your variable something different",12/28/15,1
lossyrob,"```scalaval lclTile = slopeRS.getval lclExtent = slopeRS.rasterExtent.get.extentval gt = GeoTiff(lclTile, lclExtent, LatLng)gt.write( ""/Users/user1/outslope.tiff"")```should work",12/28/15,1
lossyrob,"@dwins I remembered why I made an extent a Geometry. It_Ñés so I could have Features that were extents, I use them to look at Rasters as Feature[Extent, Tile] here: https://github.com/geotrellis/geotrellis/blob/master/spark/src/main/scala/geotrellis/spark/op/zonal/summary/ZonalSummaryRasterRDDMethods.scala#L48",12/28/15,1
lossyrob,"having Raster be viewable as Features is nice, but I_Ñém not sure how that would work with the type system if Extent wasn_Ñét a Geometry",12/28/15,1
dwins,it did look like that was the main use,12/28/15,1
lossyrob,@pomadchin : should be something like `gdal_translate -ot byte -co NBITS=1 -co compress=deflate in.tif out.tif`,12/29/15,1
pomadchin,@lossyrob :thumbsup: ,12/29/15,1
zeitalexgithub,"while following https://github.com/geotrellis/geotrellis-landsat-tutorial Tutorial I got stacked on ""Transform the tiled image into z/x/y indexed GeoTiffs"" point. I am getting lots of errors:  http://pastebin.com/A7LyaNm1 and data/tiles output folder is not created. What is the minimal hardware requirements for this tutorial? I am using Ubuntu 15.04, 4GB RAM, 40 GB free hd space.",12/29/15,1
lossyrob,"The stack trace is supressed. Can you pastebin the `[trace] Stack trace suppressed: run last compile:run for the full output.` output?",12/29/15,1
owcm,"Back with another question.   I have a RDD[(ProjectedExtent, Tile)] format which I created by using the HadoopSparkContextMethods  hadoopGeoTiffRDD(path: String).  I am interested in running some ElevationRasterRDDMethods.  I need to take the tiles and create RasterRDDs in order to do this .  Is there an approach I can follow, to convert, do I need to create a new sc.newAPIHadoopRDD configuration to handle my case?   Thanks in advance.",12/29/15,1
lossyrob,"you need to tile the source RDD into an indexed set of tiles, cut according to some layout scheme",12/30/15,1
lossyrob,which is essentially what this does https://github.com/geotrellis/geotrellis-landsat-tutorial/blob/master/src/main/scala/tutorial/TileGeoTiff.scala#L60,12/30/15,1
lossyrob,"the `inputTiles` is `RDD[(ProjectedExtent, Tile)]`",12/30/15,1
lossyrob,"well, that_Ñés not true. It_Ñés actually `RDD[(ProjectedExtent, MultiBandTile)]`. But essentially the same",12/30/15,1
lossyrob,"if you know the projection and don_Ñét need to reproject, https://github.com/geotrellis/geotrellis-landsat-tutorial/blob/master/src/main/scala/tutorial/TileGeoTiff.scala#L65 creates the necessary information you need to convert this to a RasterRDD",12/30/15,1
lossyrob,the `Tiler` call here https://github.com/geotrellis/geotrellis-landsat-tutorial/blob/master/src/main/scala/tutorial/TileGeoTiff.scala#L69 does the cutting and indexing. Essentially turning out RDD of random extents and tiles into a set of well-formed tiles of a specific size indexed according to some grid layout,12/30/15,1
lossyrob,"once you cut the tiles, then it_Ñés a matter of `new RasterRDD(cutTiles, rasterMetaData`",12/30/15,1
lossyrob,once you have that,12/30/15,1
lossyrob,`import geotrellis.spark.op.elevation._`,12/30/15,1
lossyrob,to implicitly add those elevation methods to the RDD. Then you can call them,12/30/15,1
lossyrob,"you might want to save those cut, indexed tiles off, so that you don_Ñét have to perform that step again. That_Ñés what a LayerWriter will do, into either the local file system, hadoop, s3, or accumulo",12/30/15,1
lossyrob,@pomadchin nice on the PR. Is the copy bug why it was upcasting the chatta rasters to byte?,12/30/15,1
pomadchin,"@lossyrob yes, it was caused by a generic apply, and I got weird (seemed to be weird) array out of bounds exceptions",12/30/15,1
pomadchin,https://github.com/geotrellis/geotrellis/blob/master/raster/src/main/scala/geotrellis/raster/ArrayTile.scala#L72,12/30/15,1
pomadchin,As a bug usage example: https://github.com/geotrellis/geotrellis/blob/master/raster/src/main/scala/geotrellis/raster/reproject/Reproject.scala#L18,12/30/15,1
zeitalexgithub,"@lossyrob Thanks for your answer. It takes ages to run this tutorial on my notebook. I tried following: ./sbt ""run-main tutorial.TileGeoTiff"" last compile:run > error.log./sbt ""run-main tutorial.TileGeoTiff"" last compile:runMain > error.logecho 4 | ./sbt run last compile:run > error.log, echo 4 | ./sbt last compile:run > error.logBut in all cases it tells:Stack trace suppressed: run last compile:run for the full outputwhat is the correct command to run tutorial.TileGeoTiff with full output?",12/30/15,1
zeitalexgithub,"echo 4 | ./sbt compile:run > error.logseem to work:https://gist.github.com/anonymous/63629dec2d6629bfbb28",12/30/15,1
lossyrob,so there doesn_Ñét seem to be a stack trace there. Is it still erroring for you?,12/30/15,1
lossyrob,that does take a long time_Ñ_on my macbook it takes about a minute or two,12/30/15,1
dwins,"@zeitalexgithub, i believe it should be `./sbt ""last compile:run""`",12/30/15,1
dwins,the quotes aren't necessary if you are using sbt interactively,12/30/15,1
zeitalexgithub,"with `./sbt ""last compile:run""`  it works really fast. But I still getting errors: https://gist.github.com/anonymous/c6f7b68546c960a3a9ae",12/30/15,1
dwins,"@zeitalexgithub, this just shows stored log output, it won't run your program in a different way",12/30/15,1
pomadchin,@zeitalexgithub not enough trace though ):,12/30/15,1
owcm,"Thanks, for the pointer Rob that worked for me, getting to the RasterRDD",12/30/15,1
zeitalexgithub,"cool, it is working now! great job!",12/30/15,1
pomadchin,Happy new year everybody! ,12/31/15,1
jamesmcclain,Is it past midnight in Russia?,12/31/15,1
pomadchin,":d no, but just in order not to forget ",12/31/15,1
lossyrob,Happy new year!,12/31/15,1
jamesmcclain,"Happy new year, everyone",12/31/15,1
pomadchin,"Cool man, New year!",12/31/15,1
lossyrob,"whoa @pomadchin, you_Ñére in a totally different year than us Philly folk :)",12/31/15,1
owcm,"Hi Rob , I took the landsat tutorial from a standalone configuration to a Amazon EMR cluster 1 master and 2 slaves.  If I keep the number of  /landsat/tiles under 160 files tiles sets 1 through 10 everything runs fine.   If go above that number the  system seems to time out or fail on the ""val tiled: RDD[(SpatialKey, MultiBandTile)] = Tiler(inputTiles, rasterMetaData, Bilinear)"" operation.  I get errors like  6/01/03 05:50:42 ERROR executor.CoarseGrainedExecutorBackend: Driver 10.55.4.163:55289 disassociated! Shutting down.16/01/03 05:50:42 WARN remote.ReliableDeliverySupervisor: Association with remote system [akka.tcp://sparkDriver@10.55.4.163:55289] has failed, address is now gated for [5000] ms. Reason: [Disassociated]",1/3/16,1
owcm,Continuation:   Might be more of a Spark issue.  But I believe it is an issue with the number of RDDs and serialization.  This might be outside of the Geotrellis domain but I wondered if anyone else add experience with tuning Spark when handling large files and RDD's. Thanks in advance.,1/3/16,1
lossyrob,"My guess is that the spark submit configuration has to be tweaked. That and perhaps the partition sizes. Some useful information would be: what instances are you using? What options for spark submit? Specifically executor memory, num executors and executor cores. ",1/3/16,1
lossyrob,"Also I didn't think too hard about running that tutorial remotely on that iteration; that's actually what I'm doing now (but got a bit sidetracked with enhancing the accuracy of the reprojection). What I'm working towards is pretty much what it sounds like your doing, a tutorial for ingesting landsat data into an S3 geotrellis catalog with emr (able to scale to any number of images, with configuration worked out). Then to perform NDVI on those images in another emr step (an average ndvi through time). So if he happy to work out what's going on with your end setup here, and hopefully as I get more of the landsat emr tutorial done it'll be helpful as well",1/3/16,1
lossyrob,"Sorry on my phone, ""if he"" = ""I'd be""",1/3/16,1
owcm,"Thanks for your feedback,  given me some things to try out.  I will update you with the configuration changes I come up with.",1/3/16,1
lossyrob,https://github.com/hotosm/oam-server-tiler/blob/master/launch-cluster.sh,1/3/16,1
lossyrob,https://github.com/hotosm/oam-server-tiler/blob/master/add-step2.sh,1/3/16,1
lossyrob,Are some configs to look at for reference,1/3/16,1
owcm,"Was able to get all the tiles to be processes using  the following submit configuration ""spark-submit --class tutorial.TileGeoTiff --deploy-mode cluster --master yarn-cluster --driver-memory 2G  --executor-memory 4G  --executor-cores 2 --num-executors 2 /opt/demo-assembly-0.1.0.jar""",1/3/16,1
lossyrob,Nice!,1/3/16,1
lossyrob,@pomadchin https://gist.github.com/jamesmcclain/388af5dadfb124b81e63,1/4/16,1
pomadchin,https://github.com/geotrellis/geotrellis/pull/1269,1/4/16,1
lossyrob,"@jamesmcclain here_Ñés an example section of my branch that implements the Methods architecture, for an example to use with the rasterizer API",1/4/16,1
lossyrob,https://github.com/lossyrob/geotrellis/tree/repreject-windows/raster/src/main/scala/geotrellis/raster/reproject,1/4/16,1
jamesmcclain,"@lossyrob Okay, many thanks",1/4/16,1
owcm,"@lossyrob  as you surmised from my questions on the landsat-tutorial  I am wanting to run everything in EMR and read and write my Singleband GeoTIff data to S3.  I prepped my data: projected and  retiled my data following the landsat-turorial pattern,   pushing the end results to S3.  One of things I would like to do on demand is perform large spatial elevation and map algebra operations on the data  I ingested in S3.   The  big picture is building a pipeline where data is ingested and then when complete the elevation/map algebra operations are run and results stored in S3 as well.   I am thinking the best way to read in the data to process and get some spark parallelism is using the  S3RDDReader. Questions I have: Is this a mature API and best approach for getting S3 records in to RDD format, any examples of usage, seems to be based on the AvroRecord spec.  If I want to limit the number of records to read in at a time  do I use the binning feature. Thanks again",1/4/16,1
lossyrob,"S3LayerReader API is pretty much set. We might put some sugar on top and add overloads, but it_Ñés pretty much there. If you ingest data into s3, you can read it out of the catalog (the place where the attribute folder is, that holds the attributes for the layers) using the S3LayerReader. You can do a queried read out of the Reader using the query API. you can call the read with the number of partitions in order to determine how many records to read at a time",1/4/16,1
lossyrob,"but once you have data ingested, things get a lot easier",1/4/16,1
lossyrob,I_Ñém not sure I have some example code to point to just yet,1/4/16,1
lossyrob,but let me see if I can try to freestyle some,1/4/16,1
lossyrob,"```scalaval layerReader = S3LayerReader(bucket, prefix)val rasterRDD =   layerReader.query(LayerId(_ÑÒingested-data_Ñù, whateverZoom))    .where(Intersects(someExtentOrPolygon))    .toRDD// Now I can do map algebra to it, assuming necessary imports (though we are refactoring to that these will be available via import geotrellis.spark._rasterRDD.localAdd(4).focalMean(Square(1)).histogram //etc```",1/4/16,1
lossyrob,"```scalaval result: RasterRDD[K] = ???// Then to write, just use the S3LayerWriterval writer = S3LayerWriter(bucket, prefix)writer.write(result, LayerId(_ÑÒsome-other-name_Ñù, whateverZoomLeve))```",1/4/16,1
lossyrob,"this is actually an example. It_Ñés a little behind in the snapshot releases, but still the general gist of things: https://github.com/WikiWatershed/mmw-geoprocessing/blob/develop/summary/src/main/scala/SummaryJob.scala#L92",1/4/16,1
pomadchin,https://gist.github.com/pomadchin/f66f3e8330451de43ff8,1/5/16,1
pomadchin,"@lossyrob @echeipesh so some questions about KeyIndex method: I_Ñéve already thought about its rpresentation in a form to have possibility to understand all keys space bounds. But now what problem we have to shoot?or just to move from `JavaSerializationJsonFormat` to `Avro`, and not to loose ZCurve? and the main problem is the `DateTime => Int` function representation?",1/5/16,1
lossyrob,Basically. We don't want Java serialization to be used to store anything long term because it's brittle. So moving to Avro or even Json is the goal,1/5/16,1
lossyrob,"We can't capture the generic Function2 of the date time -> int, so it will take some reworking of how we, and how we allow users, to define z curve key index methods",1/5/16,1
lossyrob,Brb,1/5/16,1
pomadchin,kk,1/5/16,1
pomadchin,"so one way is to provide indexing by parsing a pattern string, so it can help us to define year, month, day, hour, etc indexing",1/5/16,1
lossyrob,I_Ñém a bit worried about performanc cost there,1/5/16,1
lossyrob,to translate to string and then do some regex or datetime parsing,1/5/16,1
lossyrob,tho that may be over-worrying,1/5/16,1
lossyrob,could be settled with a benchmark,1/5/16,1
pomadchin,"`DateTimeFormat.forPattern(pattern)` hm, ok can try to implement it and some alternative version; understood the problem ",1/5/16,1
lossyrob,ok cool.,1/5/16,1
lossyrob,"and I_Ñéve been saying Avro, but JSON would be just as well",1/5/16,1
lossyrob,and would fit what we_Ñére doing with the rest of the attribute store better,1/5/16,1
pomadchin,cool,1/5/16,1
jamesmcclain,@lossyrob Howdy,1/5/16,1
lossyrob,heyo,1/5/16,1
lossyrob,so this key index keybounds task,1/5/16,1
lossyrob,let me find the code that is the crux,1/5/16,1
jamesmcclain,"Sounds interesting, okay",1/5/16,1
lossyrob,https://github.com/geotrellis/geotrellis/blob/master/spark/src/main/scala/geotrellis/spark/io/s3/S3LayerWriter.scala#L55,1/5/16,1
lossyrob,that line and the next,1/5/16,1
lossyrob,oh I only recently learned you an multi line link on github :),1/5/16,1
lossyrob,https://github.com/geotrellis/geotrellis/blob/master/spark/src/main/scala/geotrellis/spark/io/s3/S3LayerWriter.scala#L55-L56,1/5/16,1
lossyrob,you can see that we are creating a key index based on the key bounds of the incoming rdd,1/5/16,1
lossyrob,let me talk through a use case where that is bad,1/5/16,1
jamesmcclain,"Okay, I've got the code up",1/5/16,1
lossyrob,"We have some imagery, and we ingest it, write it out to Accumulo or S3 or whatever. Those images date from jan 1 2015 - jan 5 2015",1/5/16,1
jamesmcclain,Okay,1/5/16,1
lossyrob,"and say, it covers some of New Jersey",1/5/16,1
lossyrob,"we write the layer, fine",1/5/16,1
jamesmcclain,Sure,1/5/16,1
lossyrob,"we then want to update the layer with some imagery, this is some older imagery we didn_Ñét catch, so it_Ñés actually from Dec 2014 (I still haven_Ñét switched years in my head apparently), and actually covering PA",1/5/16,1
lossyrob,those tiles are outside of the index that was created for that layer,1/5/16,1
jamesmcclain,Okay,1/5/16,1
lossyrob,the key index is stored as metadata for the layer,1/5/16,1
lossyrob,"so when we get that key index, and try to index the incoming tiles, boom",1/5/16,1
lossyrob,"or negative values, or something bad",1/5/16,1
jamesmcclain,"I see, okay",1/5/16,1
lossyrob,"so, we want to let the user avoid that",1/5/16,1
lossyrob,by knowing in advance the general time range and coverage that layer will have,1/5/16,1
lossyrob,even if the incoming first RDD doesn_Ñét cover it completely,1/5/16,1
jamesmcclain,Okay,1/5/16,1
lossyrob,"so I can say, index this space with Z curve starting from 2010 and covering all the US in this layout",1/5/16,1
jamesmcclain,"Yes, that makes sense",1/5/16,1
lossyrob,"now, the keys would have to match the layout that your creating the index with",1/5/16,1
lossyrob,so there_Ñés some things to think through,1/5/16,1
lossyrob,but allowing the user to set up that key index bounds on the write would allow them to avoid the problem of updating outside those bounds,1/5/16,1
jamesmcclain,"Okay, makes sense",1/5/16,1
jamesmcclain,Do we have a canonical setup for developing S3-related code like this,1/5/16,1
lossyrob,there_Ñés some interplay between the LayoutDefinition of a layer and this that I can_Ñét quite think through immediately,1/5/16,1
jamesmcclain,"Thinking about a project that I can fork, credentials, et cetera",1/5/16,1
lossyrob,hm,1/5/16,1
lossyrob,I don_Ñét htink so. But it would be nice of you to create one for people to use moving forward :),1/5/16,1
jamesmcclain,:-) Okay,1/5/16,1
lossyrob,https://github.com/geotrellis/geotrellis-sbt-template,1/5/16,1
lossyrob,would be a start,1/5/16,1
jamesmcclain,Alright,1/5/16,1
jamesmcclain,A couple of questions: where should I write my test data? Are the standard credentials that I already have from mmw and modellab sufficient to allow me to do so?,1/5/16,1
lossyrob,perhaps. any s3 bucket will do,1/5/16,1
jamesmcclain,"Okay, sounds good",1/5/16,1
jamesmcclain,"Okay, I may have some questions later, but I think that that is enough information to get me started",1/5/16,1
lossyrob,cool cool,1/5/16,1
jamesmcclain,This task will give me a chance to look through some of the S3 stuff,1/5/16,1
lossyrob,yeah. and feel free to point out inconsistencies/API design flaws/add overloads while you go through it,1/5/16,1
owcm,"@lossyrob Hello  Back on the help you gave me with the landsat-tutorial.  I went and took my source Geotiffs re-projected them and used gdal to break them down into 512x512 tiles.   Then instead of having the tiled directory locally  to create the z/x/ index from I put all of my tiles up on S3  in a simple s3://bucket/tiledirectory  structure.   I thought I would be able to read in the tiles a group at a time, create a tiler RDD  and create the z/x/y indexing, writing the result  into another S3 directory.    Reading your code latest example to me'''val layerReader = S3LayerReader(bucket, prefix)val rasterRDD =   layerReader.query(LayerId(_ÑÒingested-data_Ñù, whateverZoom))    .where(Intersects(someExtentOrPolygon))    .toRDD'''It appears I need to be creating an index  scheme for the raw tiles I am uploading to S3 initially. Does this sound correct?  Each of my tiles has an extent of course and I believe they are at zoom level 12. Your sample code shows me how to get my tiles into a RDD using a LayerReader but the point I am missing is how to create the catalog  initially in S3.   Sorry to bother you again, is there another good example to look at.",1/5/16,1
lossyrob,No worries. Going from _ÑÒraw_Ñù to _ÑÒingested_Ñù is the process of reading in the raw tiles via something like https://github.com/geotrellis/geotrellis/blob/master/spark-etl/src/main/scala/geotrellis/spark/etl/s3/TemporalGeoTiffS3Input.scala#L14,1/5/16,1
echeipesh,"@owcm: `S3LayerReader` reads layers that are written using `S3LayerWriter`, which writes `RasterRDD[SpatialKey]` for instance. RasterRDD is an RDD that has already been tiled out to some specific tiling scheme. To get there from a bunch of GeoTiff tiles which only have extent you need to go through Ingest/Tiling process. There is an example of that here: https://github.com/geotrellis/geotrellis/blob/master/spark/src/test/scala/geotrellis/spark/ingest/IngestSpec.scala#L32-L36.  This is so common that we made an ETL subproject to handle this ingest/tile process: https://github.com/geotrellis/geotrellis/tree/master/spark-etl",1/5/16,1
lossyrob,potentially reprojecting. Deriving metadata from. Tiling to a certain layout,1/5/16,1
lossyrob,and then saving off to a geotrellis catalog,1/5/16,1
lossyrob,"The TileGeoTiff.scala part of scala-landsat-tutorial is basically doing an ingest. It sinks the tiles using a SlippMapWriter, which is a format of writing the tiles to the z/x/y coordinate scheme. However those tiles are not available as an RDD. You_Ñéd have to use a LayerWriter, like S3LayerWriter, to write them out to a separate S3 bucket, at which time they will be avialable and _ÑÒin a catalog_Ñù (the catalog is created if you write the layer to a bucket that doesn_Ñét have previous layers written to it).",1/5/16,1
lossyrob,@dwins here_Ñés an example of dipping into JTS world via jtsGeom (and a good call on using identifier for more accurate reprojects) https://github.com/geotrellis/geotrellis/blob/master/raster/src/main/scala/geotrellis/raster/reproject/ReprojectRasterExtent.scala#L32,1/6/16,1
dwins,cool,1/6/16,1
lossyrob,"If we wrapped densifier, I wouldn_Ñét have to do that. But, if there was some other corner of JTS that we didn_Ñét wrap, I would want to do a similar thing. If there_Ñés some way to do that without breaking immutability, that would be great",1/6/16,1
dwins,https://github.com/dwins/geotrellis/blob/spike-raw-jts-operations/vector-test/src/test/scala/spec/geotrellis/vector/JTSPassthroughSpec.scala has the cases I've been thinking about,1/6/16,1
lossyrob,"another thing is performance. I_Ñéll drop down into JTS objects when I want to be fast, and not make copies of everything in the name of immutability. So whatever the solution is, it would need to account for that",1/6/16,1
dwins,definitely,1/6/16,1
dwins,"so that file is doing it with .jtsGeom, and the same file in the previous commit is using the new thing i'm thinking about.",1/6/16,1
lossyrob,whoa,1/6/16,1
dwins,"basically the  idea is to introduce a VectorOp trait and wrap JTS functionality in it, with different wrappers for in-place mutations vs. computations",1/6/16,1
lossyrob,I_Ñém not immediately groking what the VectorOp stuff is doing,1/6/16,1
lossyrob,hmm interesting. I_Ñéll have to dive through the code,1/6/16,1
dwins,"The basic idea is that it has 'owned' and 'shared' methods that both perform whatever computation. owned is called when we can assume no one else has a handle to the geometry, shared otherwise",1/6/16,1
dwins,"so if the operation is considered a 'mutator' (which is a thing developers have to figure out manually) then the shared call performs a copy, but in all other cases we just use the reference directly",1/6/16,1
dwins,"we've got an all-hands meeting in 10 minutes here, but i'll be back after",1/6/16,1
lossyrob,"cool cool, thanks",1/6/16,1
lossyrob,fyi whoever this has been useful for debugging https://gist.github.com/lossyrob/ba18d3b4ac94e8b8c812,1/6/16,1
lossyrob,"There_Ñés some code design/architecture choices/notes in this PR that I_Ñéd like to start following consistently through the project (or improve and follow a modified or different consistent pattern), so worth reading if you_Ñére hacking on GeoTrellis or want to know how it works: https://github.com/geotrellis/geotrellis/pull/1268",1/7/16,1
owcm,"@lossyrob On the landsat-tutorial you perform  2 steps outside of Geotrellis with gdal. 1. Reproject the image using gdalwarp to Webmercator,  2. Tile out the R and NIR bands with gdal_retile.py.  Is there technical reason it was done this way.  I know you can reproject Geotiffs to Webmercator in Geotrellis and could you not tile a raster using standard ops using Geotrellis and feed them directly to the tiler.   Just wondered the technical reasons.",1/7/16,1
lossyrob,"Sure, I can speak to that",1/7/16,1
lossyrob,First is the retiling,1/7/16,1
lossyrob,"the thing about GDAL vs GeoTrellis is, GDAL does some operations on disk while reading internal tiles from GeoTiffs (or however the one of over 100 raster formats can do windowed reads)",1/7/16,1
lossyrob,"in Geotrellis and spark, we read the dataset in memory",1/7/16,1
lossyrob,and some operations require that we hold both the source and target array of the data in memory at the same time,1/7/16,1
lossyrob,"so, the one approach would be to tile out the read in GeoTiff right in memory and ingest from there, and skip retile.py",1/7/16,1
lossyrob,but we_Ñéve found it conveniant to take large tiles and chunk them out into smaller tile sizes in a GDAL based preprocessing step,1/7/16,1
lossyrob,we actually do this vi PySpark and rasterio for large tile in S3,1/7/16,1
lossyrob,"as for reproduction, we can do that in GeoTrellis",1/7/16,1
lossyrob,but I was doing it in GDAL for a reason that the GeoTrellis retroject was causing some artifacts to appear,1/7/16,1
lossyrob,"I wasn_Ñét sure why, until I solved it, and now we can skip the gdal reproject of the large raster, once this PR is merged in",1/7/16,1
lossyrob,https://github.com/geotrellis/geotrellis/pull/1268,1/7/16,1
lossyrob,"This will allow us to do reproject of keyed tile sets in spark without losing the accuracy you would have by having all of the pixels available to resample during repreojection of a non-tiled raster, by buffering tiles in preperation of the reprojection operation by some buffer that the adjancent, abutting tiles contribute to",1/7/16,1
lossyrob,I_Ñém going to be changing that landsat demo to use the geotrellis reproject,1/7/16,1
lossyrob,"as for the retile, you could also break that up into an RDD inside of GeoTrellis, and I wonder if there would be a memory issue there, I was just avoiding dealing with too much memory by doing the retile.py. But it would be nice to cut out some of those manual GDAL steps",1/7/16,1
owcm,So if I read in a SingleBandgeotiff and use crop it to break into to  512 x 512 tiles programmatically I would not be accomplishing the same task as the gdal_retile.py. ,1/7/16,1
lossyrob,you totally would,1/7/16,1
lossyrob,it would just be performed in memory,1/7/16,1
owcm,Actually I wrote some code to do it but was stuck on generating the new extent for each sub-tile.,1/7/16,1
owcm,Okay.,1/7/16,1
lossyrob,vs GDAL doing it chunk by chunk off the file system,1/7/16,1
lossyrob,we actually have some test code that does this,1/7/16,1
owcm,Got ,1/7/16,1
owcm,Nice,1/7/16,1
lossyrob,https://github.com/geotrellis/geotrellis/blob/master/spark/src/test/scala/geotrellis/spark/RasterRDDBuilders.scala,1/7/16,1
owcm,"Once again, thanks for your help",1/7/16,1
lossyrob,no problem,1/7/16,1
lossyrob,"actually my yet-to-be-merged in branch has a better function for that, actually takes the extent into account",1/7/16,1
lossyrob,https://github.com/lossyrob/geotrellis/blob/repreject-windows/spark/src/test/scala/geotrellis/spark/RasterRDDBuilders.scala#L26,1/7/16,1
owcm,"@lossyrob  what is the latest and greatest version of Geotrellis to use: I have been using ""0.10.0-d8968b1"" from bintrayrepo. Is that the best place to pull form",1/7/16,1
lossyrob,"that is the place to pull from, but note that a new version gets published each time we merge a PR into master",1/8/16,1
lossyrob,so it_Ñés worth checking the bintray site to see it new ones have been cooked up,1/8/16,1
lossyrob,https://bintray.com/azavea/geotrellis/geotrellis-spark/view,1/8/16,1
lossyrob,0.10.0-5ff9688 is current,1/8/16,1
lossyrob,"this will most likely have breaking API changes though, just as a warning",1/8/16,1
lossyrob,"we_Ñére slated to release 0.10 in spring, so the API will finally stop changing",1/8/16,1
lossyrob,"until then, though, we_Ñéll probably break you code pretty often :}",1/8/16,1
owcm,"Yea, I was trying to referance the initial RasterRDDBuilders trait you listed, your known merged version,and I was pulling a bintray version from early in Dec 2015, so I was thinking I needed to update",1/8/16,1
lossyrob,hmm that won_Ñét be available actually,1/8/16,1
lossyrob,becaues it_Ñés in the unit test project,1/8/16,1
lossyrob,we are going to start publishing test-kit projects,1/8/16,1
owcm,"Ahh, so you were showing it to me as an example.",1/8/16,1
lossyrob,which will help people build unit tests for GeoTrellis.,1/8/16,1
lossyrob,yeah,1/8/16,1
owcm,"Sorry, misunderstood",1/8/16,1
lossyrob,np,1/8/16,1
moradology,benchmarks on the new geotrellis-benchmarks subproject for dynamic vs macro-based nodata testing: https://gist.githubusercontent.com/moradology/7cd9bec65b885a9a358e/raw/f95f66fec6ea93d96327b0c09f8fb6a3a58a1568/nodataMacroVsDynamic.scala,1/8/16,1
jamesmcclain,"I have been working on optimizing the new rasterizer functionality, these are the current benchmarks https://gist.github.com/jamesmcclain/7788cc33ec4143462d70",1/8/16,1
jamesmcclain,"It isn't too bad, but I think that most of the speed difference is due to the fact that the approach that I took this problem involves taking differences of intervals, which automatically implicates lists because such differences are not necessarily connected",1/8/16,1
lossyrob,whats that code look like?,1/8/16,1
jamesmcclain,It seems like a lot of time could also be saved if there was an easy way to do in-place sorts without resorting to using Java arrays,1/8/16,1
jamesmcclain,I am going to push an update to my branch as soon as it finishes building on my machine (want to make sure the CI check passes),1/8/16,1
jamesmcclain,"I guess I can post a gist in a second, too",1/8/16,1
lossyrob,why not resort to java arrays for the sorts?,1/8/16,1
jamesmcclain,https://gist.github.com/jamesmcclain/0ea77e77231931da0ddf,1/8/16,1
jamesmcclain,The main area of interest is lines 78-93 and the code called from there,1/8/16,1
jamesmcclain,I should say 79-93,1/8/16,1
lossyrob,yeah that_Ñés a bunch of object sorting,1/8/16,1
jamesmcclain,"The code with the mapping and zipping doesn't seem to be the issue, either",1/8/16,1
jamesmcclain,"I tried doing that with pure arrays, and it was actually slightly slower",1/8/16,1
lossyrob,weird,1/8/16,1
lossyrob,is partial includeIntersecting?,1/8/16,1
jamesmcclain,it is equivalent to `includePartial`,1/8/16,1
jamesmcclain,so that it is true if and only if partial cells are to be reported,1/8/16,1
jamesmcclain,Otherwise only complete cells are (when it is false),1/8/16,1
lossyrob,ok so we_Ñére naming it includePartial instead of includeIntersecting,1/8/16,1
lossyrob,makes sense,1/8/16,1
jamesmcclain,If there is no objection,1/8/16,1
moradology,it appears as though all of the overhead witnessed when combine is used (in the dynamic nodata case as opposed to the macro case) comes from the fact that it's referencing a nodatavalue stored on the tile... direct comparison (e.g. x != 100) is equivalent (or damn near) to the macro case.,1/8/16,1
lossyrob,@jamesmcclain nothing jumps out at me for improvements,1/8/16,1
jamesmcclain,Okay ... like I say the performance isn't that bad;  it could be that the only avenue for improvement is a better algorithm,1/8/16,1
jamesmcclain,But I don't know the graphics literature that well,1/8/16,1
jamesmcclain,I'll ask around this weekend,1/8/16,1
lossyrob,perhaps we should do some benchmarking against other test data,1/8/16,1
lossyrob,to see if that_Ñés as bad as it gets or not,1/8/16,1
jamesmcclain,"Oh, okay -- I had only been using the mapshed polygon",1/8/16,1
jamesmcclain,I'll see what else is there,1/8/16,1
jamesmcclain,On a different topic: thinking about the keybounds issue: it is my understanding that the way to approach this is to find a scenario in which in the  a call to the `update` method on some object derived from `LayerUpdater` fails and go from there,1/8/16,1
jamesmcclain,I do not see any tests with an example of how to use the `LayerUpdater` type,1/8/16,1
pomadchin,@jamesmcclain https://github.com/geotrellis/geotrellis/blob/master/spark/src/test/scala/geotrellis/spark/io/LayerUpdateSpaceTimeTileTests.scala,1/8/16,1
pomadchin,Actually it fails here: https://github.com/geotrellis/geotrellis/blob/master/spark/src/main/scala/geotrellis/spark/io/accumulo/AccumuloLayerUpdater.scala#L35-L36,1/8/16,1
pomadchin,"I think now the workaround for solving this issue is to make possible to define keyboounds manually, not only out of the input rdd",1/8/16,1
pomadchin,https://github.com/geotrellis/geotrellis/blob/master/spark/src/main/scala/geotrellis/spark/io/accumulo/AccumuloLayerWriter.scala#L30,1/8/16,1
pomadchin,to have smth like,1/8/16,1
pomadchin,"```scalaval defaultKeyBounds: Option[KeyBounds[K]] = ??? // passed to the writer constructorval keyBounds = defaultKeyBounds.fold(  implicitly[Boundable[K]].getKeyBounds(rdd))(identity)```",1/8/16,1
pomadchin,"So if we know that in the current moment we load only part of the data we want, we can say, ok, We know that there would be more data for this layer, we_Ñéll predefine keybounds",1/8/16,1
jamesmcclain,"@pomadchin Thanks for the pointers, I am not sure how I missed those.  Those do seem to exercise precisely the functionality that I want to work on, though",1/11/16,1
jamesmcclain,Updated rasterizer benchmarks: https://gist.github.com/jamesmcclain/7788cc33ec4143462d70,1/11/16,1
Sinmarc,"Hi there, I was just working on that tutorial: https://github.com/geotrellis/geotrellis-landsat-tutorial/blob/master/src/main/scala/tutorial/MaskRedAndNearInfrared.scalaIs there any way to define the signedness of the ArrayMultiBandTile? The types, which are given within the _ÑÒconvert()_Ñù only provide the opportunity to define the bit depth. As far as I know, the current version is not able to handle unsigned rasters _Ñ_ Is that correct? It would be very nice if somebody could help me on this topic? ",1/11/16,1
pomadchin,"@lossyrob yo, https://github.com/pomadchin/geotrellis/blob/feature/keyindex-json/spark/src/test/scala/geotrellis/spark/io/accumulo/AccumuloSpaceTimeSpec.scala that_Ñés an api usage with the new KeyIndex serialization",1/11/16,1
pomadchin,":D no differ, hope there would be no additional type params in future also",1/11/16,1
lossyrob,@Sinmarc we have made a change to handle unsigned types,1/11/16,1
lossyrob,so that is in master,1/11/16,1
lossyrob,TBH I might have written that convert call before that was fully implemented. or it might be fore another reason...,1/11/16,1
lossyrob,"But we are currently working out our whole story for unsigned bytes_Ñ_the problem with UByte and UShort is that it_Ñés unclear what a default nodata should be. Our other types have default nodatas, which is a huge boon to performance (by checking against constant values instead of against dynamic user-set nodata values). @moradology is currently adding the capability for UShort and UByte to handle custom NoData values, and our Type story should be complete then. ",1/11/16,1
lossyrob,@jamesmcclain not priority but FYI to fix when you get a chance https://github.com/geotrellis/geotrellis/issues/1281,1/11/16,1
jamesmcclain,"Okay, will-do",1/11/16,1
lossyrob,Take note: scala has some issues with inferring type parameters on implicit classes that extend classes with type classes: https://github.com/geotrellis/geotrellis/pull/1282,1/11/16,1
lossyrob,Need to always explicitly state the type parameters in the extended class,1/11/16,1
lossyrob,(this was only happening when packaging in 2.11),1/11/16,1
lossyrob,Interested on other_Ñés thoughts on this vector API issue https://github.com/geotrellis/geotrellis/pull/1275#issuecomment-170629876,1/11/16,1
dwins,"I was also investigating an approach using typeclasses `{def apply[From <: jts.Geometry, To <: gt.vec.Geometry](x: From)(implicit wrapper: Wrapper[From, To]): To }` but it was a little cumbersome and ended up not really addressing the downcast question",1/11/16,1
dwins,"might make sense to have a method for downcasting on Geometry, like GeometryResult's `as[G <: Geometry : ClassTag]: Option[G]`",1/11/16,1
moradology,"as a convenience, that makes a lot of sense to me",1/11/16,1
moradology,it would also make the use of for comprehensions for potentially chained operations a breeze,1/11/16,1
echeipesh,I like the idea of `.as[G]` method on `Geometry`. It is more useful than throwing in a constructor and useful in general when type for geotrellis geometry gets upcast. One would have an option of using that or doing a full match.,1/11/16,1
lossyrob,I think that makes sense too,1/11/16,1
dwins,is that consensus then?,1/11/16,1
pomadchin,:+1: ,1/11/16,1
lossyrob,yessir,1/11/16,1
lossyrob,"@moradology I merged in upstream and made the changes you suggested, is that a +1 on travis build?",1/11/16,1
moradology,"yep, lgtm",1/11/16,1
lossyrob,cool cool,1/11/16,1
lossyrob,looks like travis is throwing a bunch of scala.reflect.internal.MissingRequirementError: object scala.runtime in compiler mirror not found. errors.,1/12/16,1
lossyrob,@echeipesh what are these for? https://github.com/geotrellis/geotrellis/tree/master/spark/src/main/scala/geotrellis/spark/mosaic I_Ñém commenting them out and they don_Ñét seem to be used,1/12/16,1
pomadchin,@lossyrob would be used by @jamesmcclain ,1/12/16,1
lossyrob,?,1/12/16,1
pomadchin,"hm out or not :0 but it's a cool stuff to merge layers, not exact tiles, I used it in a client code",1/12/16,1
lossyrob,can you give me the use case?,1/12/16,1
pomadchin,sure,1/12/16,1
pomadchin,"so it would be helpful for @jamesmcclain to update layer out of key bounds (pre set keyounds);smth like to get the existsing rdd and to merge it with the new one.(the loaded rdd had `oldKeyBounds`, the new has intersecting key bounds case)",1/12/16,1
lossyrob,"ok, got it",1/12/16,1
lossyrob,thanks,1/12/16,1
pomadchin,,1/12/16,1
Sinmarc,@lossyrob thanks for making this clear,1/12/16,1
jamesmcclain,Topics that I would like to discuss: 1) This pull request https://github.com/geotrellis/geotrellis/pull/1284 is the CI failure genuine? 2) Keybounds approach in https://github.com/jamesmcclain/geotrellis/tree/feature/jwm/keybounds,1/12/16,1
moradology,Possible celltype hierarchy: https://github.com/geotrellis/geotrellis/pull/1271/files#diff-d0830f0e5391cf535aa5bda90482698cR49,1/12/16,1
moradology,in use: https://github.com/geotrellis/geotrellis/pull/1271/files#diff-67435400a026f5e2380e10bbd09e1b28R27 and matched against: https://github.com/geotrellis/geotrellis/pull/1271/files#diff-4a2bc7c726fa4192cc13886fda2875a1R20,1/12/16,1
jamesmcclain,@pomadchin Can we chat a little bit about our respective tasks?,1/12/16,1
pomadchin,"ye sure, just afk for 30mins and will be back",1/12/16,1
jamesmcclain,"Okay, sounds great",1/12/16,1
pomadchin,ty!,1/12/16,1
echeipesh,@lossyrob re: merge layers. That was written both to support layer update functionality and for user code to perform RDD updates in the manner that pomadchin mentioned.,1/12/16,1
pomadchin,@echeipesh :+1: ,1/12/16,1
lossyrob,Ok. it just wasn_Ñét used anywhere so had me confused,1/12/16,1
lossyrob,Request for feedback of  cell type naming scheme: https://gist.github.com/lossyrob/e19365f1e092b77a22b1,1/12/16,1
lossyrob,"(the actual types have more data contained in them, so this is psuedo, but the naming is what_Ñés in question)",1/12/16,1
echeipesh,Long but explicit. Maybe _ÑÒType_Ñù suffix can be omitted.,1/12/16,1
lossyrob,"how do you mean, Type stuff?",1/12/16,1
echeipesh,"```scalasealed abstract class CellTypesealed abstract class RawCellsealed abstract class ConstantNoDataCellsealed abstract class DynamicNoDataCell[@specialized(Byte, Short, Int) T] { def noDataValue: T }case object BitCell extends RawCellcase object ByteCell extends RawCellcase object ByteConstantNoDataCell extends ConstantNoDataCellcase class ByteDynamicNoDataCell(nd: Byte) extends DynamicNoDataCell[Byte](nd)```",1/12/16,1
moradology,I like both of these options - i think it makes a lot of sense to keep them long though,1/12/16,1
moradology,especially since end users will largely have no part in this,1/12/16,1
echeipesh,It feels natural but breaks away from traditional type name. I wouldn_Ñét expect this type to  come up too often in user code though.,1/12/16,1
dwins,is it a Cell type or a Band type?,1/12/16,1
echeipesh,It_Ñés the type of all the cells in the tile. So it could be seen as a band type.,1/12/16,1
lossyrob,BandType is in GeoTiffs,1/12/16,1
lossyrob,but yeah they are all representing the type of cells the Tile contains,1/12/16,1
moradology,"then again, this isn't a cell - it is a celltype specification",1/12/16,1
lossyrob,"Yeah I think keeping the 4 characters for Type isn_Ñét much worse in terms of verbosity, it_Ñés already pretty bad",1/12/16,1
lossyrob,and yeah not sure how much the user will encounter it_Ñ_and the explicitlness for users who do encounter it probably is a big plus,1/12/16,1
dwins,i would expect Constant and Variable or Static and Dynamic. Constant/Dynamic seems odd,1/12/16,1
lossyrob,"@Sinmarc this actually deals with some of what your question was about, if you_Ñéd be interested in contributing thoughts",1/12/16,1
lossyrob,"@dwins hmm, yeah I guess it does break the usual opposite names for them",1/12/16,1
lossyrob,But Constant/Dynamic doesn_Ñét seem too at odds with me. Does it make sense to others to go with Constant/Variable or Static/Dynamic more?,1/12/16,1
echeipesh,"Constant is very descriptive, its actually a constant check in code, so that should stay.",1/12/16,1
moradology,"this is orthogonal to those distinctions, but since users and readers of the library are programmers it does make some sense",1/12/16,1
moradology,"variable is fine, too",1/12/16,1
moradology,"by my lights, that is",1/12/16,1
lossyrob,"Well, it doesn_Ñét really vary",1/12/16,1
lossyrob,it_Ñés just user set,1/12/16,1
moradology,"yeah, there's something that's just kind of gross feeling about 'variable'",1/12/16,1
echeipesh,"It_Ñés not really dynamic either. Internal/External, Standard/External ?",1/12/16,1
lossyrob,hmm Internal/External is interesting,1/12/16,1
moradology,naming is not fun.,1/12/16,1
lossyrob,"```scalaval tile = ArrayTile.empty(UShortInternalNoDataCellType, 256, 256)```",1/12/16,1
lossyrob,would you know what that meant right away?,1/12/16,1
echeipesh,I think that raises the correct question right away: What is the internal nodata value ?,1/12/16,1
dwins,internal to what? not sure i would think 'internal to the library' right away,1/12/16,1
lossyrob,"```scalaval tile = ArrayTile.empty(UShortInternalNoDataCellType, 256, 256)val tile = ArrayTile.empty(UShortExternalNoDataCellType(32), 256, 256)```",1/12/16,1
lossyrob,"yeah, that makes sense",1/12/16,1
lossyrob,is there something that says _ÑÒinternal to the library_Ñù better?,1/12/16,1
echeipesh,"```scalaval tile = ArrayTile.empty(UShortCellType with GeoTrellisNoData, 256, 256)val tile = ArrayTile.empty(UShortCellType with UserDefinedNoData, 256, 256)```",1/12/16,1
dwins,but if you're asking that question then the answer doesn't necessarily have to be a class name. UShortCellType.withNoData(32) might return a UShortExternalNoDataCellType but i don't have to know that,1/12/16,1
lossyrob,@echeipesh I was thinking about withs. Problem with the UserDefined nodata is it needs to take a parameter,1/12/16,1
lossyrob,"@dwins true, we could have some methods on the object that might be more clear. Would be nice to nail it in the type system so that there_Ñés not a multaplicity of ways to say it",1/12/16,1
lossyrob,well maybe with would work,1/12/16,1
moradology,ConfigurableNoData/GeoTrellisNoData,1/12/16,1
echeipesh,Doesn_Ñét have to be in the constructor .. let me see if it works out,1/12/16,1
lossyrob,I_Ñém not hot on the _ÑÒGeoTrellis_Ñù tag,1/12/16,1
lossyrob,but _ÑÒUserDefined_Ñù is nice,1/12/16,1
echeipesh,Standard instead of GeoTrellis ?,1/12/16,1
dwins,Fixed instead of GeoTrellis?,1/12/16,1
lossyrob,hmm Standard or Fixed is better than GeoTrellis,1/12/16,1
lossyrob,we should see if the `with` would work out in the type system before heading too far down that naming path,1/12/16,1
lossyrob,are you checking that eugene?,1/12/16,1
echeipesh,"yes, playing with that",1/12/16,1
moradology,how are you thinking about adding the nd value?,1/12/16,1
lossyrob,just have it as a val on the triat,1/12/16,1
lossyrob,but it would have to be typed then,1/12/16,1
lossyrob,`UShortCellType with UserDefinedNoData[Short]`,1/12/16,1
lossyrob,Which is cumbersome,1/12/16,1
lossyrob,and you_Ñéd want to wrap that as a exlicitly named type,1/12/16,1
lossyrob,which brings us back to the original problem,1/12/16,1
echeipesh,"No pretty way to work around the lack of constructor, something has to have it. So trying to play off the _ÑÒwith_Ñù syntax doesn_Ñét help anything.",1/12/16,1
lossyrob,cool,1/12/16,1
lossyrob,so back to the Static/Dynamic Standard/UserDefined Constant/Dynamic etc debate,1/12/16,1
echeipesh,"```scalaimport com.scalakata._sealed abstract class CellTypetrait StandardNoDataCellType extends CellTypeclass DynamicNoDataCellType[@specialized(Byte, Short, Int) T](val noDataValue: T) extends CellTypetrait ShortCellType extends CellTypecase object ShortCellType extends ShortCellType with StandardNoDataCellType {  def withNoData(nd: Short): ShortCellType with DynamicNoDataCellType[Short] =   	new DynamicNoDataCellType[Short](nd) with ShortCellType}@instrument class Playground {val ct1 = ShortCellType//ShortCellTypeval ct2 = ShortCellType.withNoData(33)//ShortCellType$$anon$1@2789e449}```",1/12/16,1
echeipesh,eh ?,1/12/16,1
lossyrob,there_Ñés reasons to not have the anonymous class (there_Ñés more to CellTypes then is in the sample code),1/12/16,1
lossyrob,but that object method could exist whatever we end up calling it,1/12/16,1
moradology,"yeah, that sugar is nice, but i'm partial to just spelling out the types explicitly rather than generating them like this",1/12/16,1
moradology,and with would be better if it were possible to apply a constructor on the trait,1/12/16,1
moradology,Constant/UserDefined,1/12/16,1
moradology,what about *that*,1/12/16,1
lossyrob,/offtopic where is `MosaicBuilder` used?,1/12/16,1
lossyrob,Does `Constant` really convey what we are talking about (to people who don_Ñét know the internals of the library?),1/12/16,1
echeipesh,Not really tbh,1/12/16,1
echeipesh,Is there a reason for CellType not to be parametric on T btw ?,1/12/16,1
lossyrob,nothing would really use that type info,1/12/16,1
moradology,ConstantNoData vs UserDefinedNoData,1/12/16,1
moradology,it seems to me that the distinction between the two makes sense of this,1/12/16,1
moradology,"though in isolation, either is vague",1/12/16,1
dwins,"while i am moving `withGeometryMethods` would it make sense to move `geotrellis.vector.op.ZonalSummaryHandler` as well? should be in its own `geotrellis.vector.zonal` package, right?",1/12/16,1
echeipesh,"```scalasealed abstract class CellType[T](val noDataValue: Option[T])trait StandardNoDatatrait UserDefinedNoDatacase object ShortCellType extends CellType[Short](None) with UserDefinedNoDatacase object ShortStandardNoDataCellType extends CellType[Short](Some(Short.MinValue)) with StandardNoDatacase class ShortUserDefinedNoDataCellType(nd: Short) extends CellType[Short](Some(nd)) with UserDefinedNoData```noDataValue would only be inspected by the tiles that require it, otherwise it_Ñés there to be documenting. The type of T would correspond to the type of `get` on the Tile.",1/12/16,1
echeipesh,Having an Option would probably require to copy the noDataValue to some constant field to avoid unboxing on every access when checking dynamic tiles.,1/12/16,1
moradology,doesn't @specialized handle that ?,1/12/16,1
echeipesh,But Option isn't,1/12/16,1
moradology,ah.,1/12/16,1
moradology,right,1/12/16,1
moradology,seems like there'd be a lot of extra machinery to pull that off - might be better to just do that where necessary,1/12/16,1
moradology,and where @specialized is possible,1/12/16,1
moradology,though the point about documentation is well taken,1/12/16,1
echeipesh,"A danger would be that a user would use the `Tile.cellType.noDataValue` in a `Tile.map` instead of the macro and just blow the performance right to heck. Where as if `noDataValue` is defined on the bottom most type or specalized in a trait, it wouldn_Ñét be that bad.",1/12/16,1
lossyrob,@dwins yeah that makes sense,1/12/16,1
dwins,cool,1/12/16,1
lossyrob,`Tile.cellType` wouldn_Ñét have a noDataValue,1/12/16,1
lossyrob,it_Ñés only used in the internal machinery of `get` and `set` (and `getDouble` and `setDouble`),1/12/16,1
lossyrob,@moradology ugh that Jpg test starts up that weird boot Java thing. I really don_Ñét get why some code does that,1/12/16,1
moradology,"Yeah, i know, it was a pain in the ass with ~test",1/12/16,1
lossyrob,whenever I use GeoTools the same thing pops up. @dwins do you know what that_Ñés about?,1/12/16,1
dwins,I believe whenever (certain parts of) Swing is engaged it kicks in. I think JAI can trigger it,1/12/16,1
dwins,i wonder if forcing swing to headless mode would prevent it.. there's a system property for that but i forgot what it is,1/12/16,1
dwins,"yeah, you could try invoking sbt  with `-J-Djava.awt.headless=true`",1/12/16,1
pomadchin,"/offtop/ @lossyrob what do you think about having a `keyIndexMethod` in a `Writer` constructor? should it be removed or what do you think? (as I understood correct, the idea is to add `write` function overloads)",1/12/16,1
lossyrob,yeah keyIndexMethods should be taken out of constructor,1/12/16,1
lossyrob,and moved as part of the parameter overloading of write,1/12/16,1
lossyrob,the base overload should take a KeyIndex,1/12/16,1
lossyrob,and overload that takes a KeyIndexMethod should create it off the keybounds of the incoming RDD,1/12/16,1
lossyrob,"or, in the case of Jame_Ñés feature, a passed in keybounds",1/12/16,1
pomadchin,"ok, cool, i got it correct after talking with James",1/12/16,1
pomadchin,cool,1/12/16,1
pomadchin,just wanted to be sure :D,1/12/16,1
lossyrob,nice,1/12/16,1
lossyrob,@dwins cool I_Ñém going to set that property on tests and see if it solves it,1/12/16,1
lossyrob,threading through partitioner or numPartitioner to methods that use spark methods that take them is hard,1/12/16,1
lossyrob,/waaa,1/12/16,1
lossyrob,those options don_Ñét seem to be working :/,1/12/16,1
lossyrob,"what do people think of this? (I don_Ñét like the default null and -1, but maybe it_Ñés acceptable in this situation?)",1/12/16,1
lossyrob,"```scalaobject Tiler {  class Options private (val resampleMethod: ResampleMethod, numPartitions: Option[Int], optPartitioner: Option[Partitioner]) {    def partitioner(rdd: RDD[_], others: RDD[_]*) =      optPartitioner.getOrElse(        numPartitions.map(new HashPartitioner(_)).getOrElse(          defaultPartitioner(rdd, others:_*)        )      )  }  object Options {    def DEFAULT = apply()    def apply(      resampleMethod: ResampleMethod = NearestNeighbor,      numPartitions: Int = -1,      partitioner: Partitioner = null    ): Options =      new Options(        resampleMethod,        if(numPartitions > 0) Some(numPartitions) else None,        if(partitioner != null) Some(partitioner) else None      )  }}```",1/12/16,1
lossyrob,"come to think of it, this might be a good use case for the Magnet pattern",1/12/16,1
lossyrob,nope_Ñ_ the numPartitions vs partitioner thing is weird. I_Ñém going to go with a simpler option,1/12/16,1
moradology,joy: ```[error] 463 errors found```,1/12/16,1
lossyrob,w00t,1/12/16,1
lossyrob,I rember one refactor I think I had something in the 1500 - 2000 errors range. so I feel your pain,1/12/16,1
lossyrob,"@dwins in your branch `LineDissolve` is still in `vector.op`, is that intentional?",1/12/16,1
dwins,no,1/12/16,1
dwins,actually i added a new Dissolve object and forgot about the old one,1/12/16,1
lossyrob,np just makin sure,1/12/16,1
lossyrob,"added `-Djava.awt.headless=true` to `.sbtopts` and if you use the `./sbt` script, no more Boot. yay.",1/12/16,1
dwins,nice!,1/12/16,1
moradology,beautiful,1/12/16,1
lossyrob,@pomadchin @jamesmcclain you_Ñéll probably be interested in this commit,1/13/16,1
lossyrob,https://github.com/geotrellis/geotrellis/commit/70eeff1b791473a023da7871a9a4d6bbe4c34b77,1/13/16,1
lossyrob,"We_Ñére moving from DateTime (which has awful equality issues when reading/writing from Avro), and representing time as an instant (Long). This will most likely make the serialization of Z curve indexes much simpler; it can be based on some offset/divisor math, instead of having to work with date times.",1/13/16,1
moradology,@lossyrob i'm curious about your thoughts here: https://github.com/geotrellis/geotrellis/pull/1271/files#diff-8e999fa8619e0b732d8dca6a1ed26ab4R108,1/13/16,1
owcm,Using geotrellis.raster.Tile gridbounds can a tile extent be determined.,1/13/16,1
lossyrob,not if that_Ñés all the information you have,1/13/16,1
lossyrob,Tile does not have extent information,1/13/16,1
lossyrob,Raster does,1/13/16,1
lossyrob,and you can get it if you are working with spark layers and have RasterMetaData,1/13/16,1
lossyrob,and the Key,1/13/16,1
owcm,Well I have the spatial key,1/13/16,1
lossyrob,do you have the metadata?,1/13/16,1
lossyrob,there_Ñés a thing on the metadata called a `mapTransform`,1/13/16,1
lossyrob,that will take a spatial key and give you an extent,1/13/16,1
owcm,"Trying to one off HadoopSlippyTileWriter and in setupWrite was trying to get the extent for the the tile. rdd: RDD[(SpatialKey, T) is passed in.  I can see how to get the keys and the till of course but the rdd does not have the metadata attribute,",1/13/16,1
lossyrob,If you know the CRS and the tile layout you can create a map transform yourself,1/13/16,1
owcm,That could be possible,1/13/16,1
lossyrob,"e.g. if it_Ñés a ZoomedTileLayout(WebMercator, 256), you can get a map transform using some methods",1/13/16,1
lossyrob,let me see if I can find an example,1/13/16,1
owcm,Basically thats what I have,1/13/16,1
lossyrob,https://github.com/azavea/usace-flood-geoprocessing/blob/develop/server/src/main/scala/com/azavea/usaceflood/server/ElevationData.scala#L32,1/13/16,1
owcm,"That should work, thanks",1/13/16,1
pomadchin,"@lossyrob cool, that makes sense; can you make a pr or I can include these changes",1/13/16,1
pomadchin,"Btw, can you add some more information about avro date time problems?",1/13/16,1
lossyrob,"well, it_Ñés part of the PR does that does other things as well.",1/13/16,1
lossyrob,"That PR should actually be OK to be merged in, once travis builds",1/13/16,1
lossyrob,"sure, we can talk about it tomorrow",1/13/16,1
pomadchin,_ª¥Ô _ª¥ç,1/13/16,1
dwins,this line looks sketchy: https://github.com/geotrellis/geotrellis/blob/master/vector/src/main/scala/geotrellis/vector/Geometry.scala#L44,1/13/16,1
moradology,"yeah, looks that way to me too",1/13/16,1
moradology,"perhaps an optional envelope is better, semantically",1/13/16,1
dwins,"in JTS an empty geometry has the special empty envelope: [0,0:-1,-1] which is an identity for the 'expandToInclude' operation, never intersects other envelopes, etc.",1/13/16,1
dwins,Option would be good too,1/13/16,1
moradology,"i've got to say, your knowledge of JTS is very useful",1/13/16,1
moradology,"i wasn't aware that there was an empty envelope, but if it plays nicely with other features in JTS, that's a strong argument in favor",1/13/16,1
moradology,(i'm definitely the greenest in this space and a fortiori JTS),1/13/16,1
dwins,"I think in scala-land I would probably make Extent a sealed trait with EmptyExtent and NonEmptyExtent implementing. but EmptyExtent would give 0,0:-1,-1 as the coordinates in serialization",1/13/16,1
dwins,"it's nice to have EmptyExtent be an Extent so that you can find the overall extent for a collection of features by ""summing"" with EmptyExtent as the zero",1/13/16,1
moradology,"yeah, seems to yield  a nice monoidal structure",1/13/16,1
dwins,exactly,1/13/16,1
dwins,https://github.com/dwins/geotrellis/commit/3b19b8c79740ead8e22ed99bada2e008b64e7cc9 how's this for geotrellis.vector.Geometry scaladoc?,1/13/16,1
jamesmcclain,Regarding the decision to change the time field in the SpaceTimeKey structure to a long: is the code for that in Grisha's branch or elsewhere?,1/13/16,1
pomadchin,@jamesmcclain it is here https://github.com/geotrellis/geotrellis/pull/1285,1/13/16,1
jamesmcclain,"Ah, many thanks",1/13/16,1
jamesmcclain,Isn't it like 1:00 a.m. where you are?,1/13/16,1
pomadchin,9 p.m. (: ,1/13/16,1
jamesmcclain,"Oh, only 9:00",1/13/16,1
jamesmcclain,"That's right, it is only 8 hours",1/13/16,1
pomadchin,"yep, 8h difference",1/13/16,1
lossyrob,@dwins those scala docs are kick ass,1/13/16,1
lossyrob,it would be nice to have them in the README at the root of the project,1/13/16,1
dwins,copied or moved?,1/13/16,1
lossyrob,and linked to the root README,1/13/16,1
lossyrob,"I_Ñéd say copied, although that adds a maintenance burden",1/13/16,1
lossyrob,but it would be good to have that information in both places,1/13/16,1
dwins,ok.,1/13/16,1
moradology,"the root vector readme - not the root GT readme, right?",1/13/16,1
dwins,the readme for vector is in `vector/src/main/scala` - should it be in `vector/` instead?,1/13/16,1
moradology,"good question - i believe i put it there just because it is a set of notes pretty close to the code, and some of the subprojects also have readmes",1/13/16,1
moradology,"i'm not sure what the One True Way is, here",1/13/16,1
dwins,oh yeah i noticed that. but it's not really where i expected to find the README,1/13/16,1
dwins,probably more important to be self-consistent within the project than to find out what other folks are doing in this case.,1/13/16,1
lossyrob,I think we need to restructure where we_Ñére putting the readme's,1/13/16,1
lossyrob,I think having a `docs/` folder would be best,1/13/16,1
lossyrob,keeps it separate and clear. Sort of like how `rasterio` has theirs. Except ours would follow the package layout.,1/13/16,1
dwins,sounds good to me. no reason to have user documentation next to the code even if the users are coders,1/13/16,1
lossyrob,@jamesmcclain here_Ñés the failing test: https://gist.github.com/lossyrob/aa7327fe3d94bedc12a7,1/13/16,1
lossyrob,seems to have to do with serialization. Will perhaps be fixed by @pomadchin_Ñés work,1/13/16,1
jamesmcclain,"Okay, thank you",1/13/16,1
pomadchin,"yep, will have a look",1/13/16,1
pomadchin,@jamesmcclain is it in your branch? `/feature/jwm/keybounds` ?,1/13/16,1
jamesmcclain,"Yeah, but it has not been updated",1/13/16,1
jamesmcclain,Give me about 10 minlutes,1/13/16,1
jamesmcclain,I am sorting out some trouble with the Hilbert index,1/13/16,1
jamesmcclain,"@pomadchin Okay, I just pushed my latest work to that branch",1/13/16,1
jamesmcclain,"I have fixed the bug that I had been experiencing, but I don't know if I introduced any new ones :-)",1/13/16,1
jamesmcclain,Running tests now to see what happens,1/13/16,1
jamesmcclain,@pomadchin Sorry it took so long to get this to you by the way,1/13/16,1
moradology,this is the tentative direction for keeping geotiff segments DRY: https://github.com/geotrellis/geotrellis/pull/1271/files#diff-ee28436093c7be45db62cbb9340c3cdeR23,1/13/16,1
lossyrob,I think it looks good,1/14/16,1
pomadchin,Noticed that sometimes there are some problems with artifacts publishing: http://dl.bintray.com/azavea/geotrellis/com/azavea/geotrellis/geotrellis-spark_2.10/,1/14/16,1
pomadchin,For example the latest version accroding to bintray is `5f2040e`: https://bintray.com/azavea/geotrellis/geotrellis-spark/view,1/14/16,1
pomadchin,it is availble only for `scala 2.11` o:,1/14/16,1
jamesmcclain,I think that sometimes the builds fail (just like the CI build sometimes fail),1/14/16,1
echeipesh,"```scalaimport com.scalakata._trait Geometryclass Polygon extends Geometryclass Feature[+G <: Geometry, D](geom: G, data: D)class ConvertableFeature[T, G <: Geometry, D](thing: T, data: D)(implicit ev: T => G) 	extends Feature[G, D](thing, data)@instrument class Playground {  implicit def intToPolygon(x: Int): Polygon = new Polygon  val f: Feature[Polygon, String] = new ConvertableFeature[Int, Polygon, String](3, ""data"")  }```",1/14/16,1
rajadain,"Hi, sorry if this is a dumb question, but is there a way in GeoTrellis to return an empty PNG (with all transparent pixels, say 256x256)?",1/14/16,1
echeipesh,You could use `ArrayTile.empty(..)` constructor to make an empty tile and render that.,1/14/16,1
rajadain,"```scalaGetTile(...) match {  case Some(tile) => process(tile).renderPng().bytes  case None => Array[Byte]()}```^ this is what we currently have",1/14/16,1
rajadain,"But unfortunately, newer versions of Leaflet render the second case as ""missing image""",1/14/16,1
rajadain,Ah thanks @echeipesh I'll try that!,1/14/16,1
echeipesh,I_Ñém not sure if you could use a `TypeByte` for that to keep them small. But I guess it can be a constant anyway.,1/14/16,1
rajadain,"This worked perfectly: `ArrayTile.empty(TypeByte, 256, 256).renderPng().bytes`",1/14/16,1
echeipesh,sweet,1/14/16,1
lossyrob,"```scalatrait Component[M, C] = PLens[M, M, C, C]trait ComponentMethods[M] extends MethodExtensions[M] {  def get[C]()(implicit componentLens: Component[M, C]): C =  def set[C](value: C)(implicit componentLens: Component[M, C]): M}def foo[M: Component[_, LayoutDefinition]: Component[_, Extent]](m: M) {  val definition = m.get[LayoutDefinition]  val extent = m.get[Extent]  val extent2: Extent = ???  m.set[Extent](extent2)}```",1/14/16,1
echeipesh,"```scalaimport com.scalakata._trait Component[M, C] {  def get(m: M): C  def set(m: M, c: C): M}trait MethodExtensions[+T] extends Serializable {  def self: T}class ComponentMethods[M](val self: M) extends MethodExtensions[M] {  def getComponent[C]()(implicit lens: Component[M, C]): C =   	lens.get(self)  def setComponent[C](value: C)(implicit lens: Component[M, C]): M =     lens.set(self, value)}@instrument class Playground {	implicit class withComponentMethods[M](m: M) extends ComponentMethods(m)      def foo[M: Component[?, Int]: Component[?, String]](m: M): M = {    val definition = m.get[Int]    val extent = m.get[String]    m.set[Int](4).set[String](""better stuff"")  }  implicit val c2 = new Component[A, String] {def get(a: A) = a.y; def set(a: A, s: String) = new A(a.x, s)}  implicit val c1 = new Component[A, Int] { def get(a: A) = a.x; def set(a: A, x: Int) = new A(x, a.y)                                             foo(new A(1, ""stuff""))                                           }```",1/14/16,1
echeipesh,http://scalakata.com,1/14/16,1
pomadchin,cool,1/14/16,1
pomadchin,im wondering how / why is it all ok with a wildcard type param,1/14/16,1
echeipesh,Wildcard makes it a typeclass with one hole,1/14/16,1
echeipesh,_Ñ_ so I guess a typeclass.,1/14/16,1
lossyrob,"```scalacase class Projected[T](obj: T, crs: CRS)    extends Composition[T, Extent](obj, crs)class Composition[T, U](obj: T, val prop: U) extend {  def get = obj}```",1/14/16,1
lossyrob,@rajadain  You probably want to return an Option[Tile] from the spray route,1/14/16,1
lossyrob,return Some(tile) if the tile is found,1/14/16,1
lossyrob,return None if not,1/14/16,1
lossyrob,that way it_Ñés reported as a `404`,1/14/16,1
jamesmcclain,"It seems to be the case that the compact hilbert curve data structure, https://github.com/geotrellis/geotrellis/blob/master/spark/src/main/scala/geotrellis/spark/io/index/hilbert/HilbertSpaceTimeKeyIndex.scala#L44-L52, seems to be limited to 62 bits of total precision.  When we move to using millis-since-the-epoch in `SpaceTimeKey`s, this may become a problem",1/14/16,1
lossyrob,"@jamesmcclain we won_Ñét ever be indexing the time as a Long, we_Ñéll index it at a resolution using division into an int",1/14/16,1
jamesmcclain,"Ah okay, good to know",1/14/16,1
lossyrob,"for example, if we want a roughly daily resolution, (instant / (1000 * 60 * 60 * 24)).toInt",1/14/16,1
lossyrob,gitter doesn_Ñét like math,1/14/16,1
lossyrob,but that_Ñés good to know...,1/14/16,1
jamesmcclain,"Okay, that makes sense, we can just index at the resolution that we need",1/14/16,1
lossyrob,I was under the impression that it has unlimited resolution,1/14/16,1
lossyrob,and used Byte arrays,1/14/16,1
jamesmcclain,There may be a way to get that,1/14/16,1
jamesmcclain,"But as presently constituted, if you go to high it complains",1/14/16,1
lossyrob,is that resolution per dimension or total resolution for holding all N dimensions/,1/14/16,1
jamesmcclain,It is the sum of the resolutions,1/14/16,1
jamesmcclain,"But I would be shocked if that was a real limitation, there has to be a way around that",1/14/16,1
jamesmcclain,"The default or ""fast"" implementation probably wants to keep the size down to a machine word",1/14/16,1
jamesmcclain,They might have a slower implementation that uses bit arrays of some type,1/14/16,1
lossyrob,sorry where is that limitation? the code you linked doesn_Ñét seem like that happens,1/14/16,1
jamesmcclain,That was just where the compact hilbert curve is constructed,1/14/16,1
jamesmcclain,"If you arrange things so that those three ints sum to more than 62, you get a runtime error",1/14/16,1
lossyrob,oh gotcha,1/14/16,1
lossyrob,you can use https://github.com/lossyrob/uzaygezen to link to usaygezen code,1/14/16,1
jamesmcclain,Okay,1/14/16,1
moradology,i figured uzaygezen was a japanese mathematician or something,1/14/16,1
jamesmcclain,I thought it might be Welsh,1/14/16,1
moradology,"turns out, it's turkish and it translates as 'space wanderer'",1/14/16,1
jamesmcclain,but apparently it is a Turkish word,1/14/16,1
jamesmcclain,Yup,1/14/16,1
lossyrob,oh nice. didn_Ñét know that,1/14/16,1
jamesmcclain,"Topics that I would like to discuss: This code https://github.com/jamesmcclain/geotrellis/blob/af682c9e74327ef28e4183f394b97f9ebe532227/spark/src/test/scala/geotrellis/spark/io/LayerUpdateSpaceTimeTileTests.scala#L59-L63 seems to cause only two key/data pairs to be written:```(SpaceTimeKey(0,0,1262304000000),FloatArrayTile([F@6ec4cb38,3,4))(SpaceTimeKey(22,23,1262304000000),FloatArrayTile([F@39fa0258,3,4))```",1/15/16,1
pomadchin,@jamesmcclain https://github.com/geotrellis/geotrellis/blob/master/spark/src/main/scala/geotrellis/spark/io/LayerUpdater.scala#L13-L17,1/15/16,1
jamesmcclain,"Ah, I see.",1/15/16,1
pomadchin,James give me a couple of minutes will check it,1/15/16,1
jamesmcclain,"Okay, thanks",1/15/16,1
jamesmcclain,"@pomadchin In the code that I pasted above, https://github.com/jamesmcclain/geotrellis/blob/af682c9e74327ef28e4183f394b97f9ebe532227/spark/src/test/scala/geotrellis/spark/io/LayerUpdateSpaceTimeTileTests.scala#L59-L63, the problem may be the way that I am constructing the `update` rdd",1/15/16,1
jamesmcclain,I do not know if that way of creating an rdd is supposed to produce an rdd over the range of the two tiles or *just* over the two tiles that are given (it appears to be doing the latter?),1/15/16,1
pomadchin,"ye, sec James Im looking",1/15/16,1
pomadchin,"btw, just flew out of my mind: https://gist.github.com/pomadchin/edb17ece2ff89883ccac",1/15/16,1
pomadchin,this test is failing also,1/15/16,1
pomadchin,What is exactly obvious that Hilbert fails not due to (de)serialization ,1/15/16,1
lossyrob,if you take out the serialization component it still fails/,1/15/16,1
lossyrob,?,1/15/16,1
pomadchin,exactly,1/15/16,1
pomadchin,"```scala  val k1 = SpaceTimeKey(0, 0, new DateTime(2000,1,1,0,0,0))  val k2 = SpaceTimeKey(0, 0, new DateTime(2015,12,31,11,59,59))  val kb = KeyBounds(k1, k2)  val kim = HilbertKeyIndexMethod(12)  val ki = kim.createIndex(kb)  ki.indexRanges(kb)```",1/15/16,1
pomadchin,added exception to the gist,1/15/16,1
jamesmcclain,"Yeah, that error appears to be from the total number of bits required being > 62",1/15/16,1
jamesmcclain,"``` java.lang.IllegalArgumentException: bitSetRange=Pow2LengthBitSetRange[start=size: 4096 bitset: {},level=4096] exceeds 62 bits for a non-negative long.```",1/15/16,1
jamesmcclain,I observed that error yesterday evening,1/15/16,1
jamesmcclain,"I suspect that if you reduce the difference between those two times enough, the test will eventually pass (or at least exhibit a different failure)",1/15/16,1
lossyrob,or if you increase 12 to something higher,1/15/16,1
lossyrob,i guess 2**12 isn_Ñét that plentiful,1/15/16,1
lossyrob,hilbert should have the byYear byDay ect methods too,1/15/16,1
lossyrob,and choose the power of two that most closely matches,1/15/16,1
pomadchin,"@lossyrob we talked with @jamesmcclain everything is cool with updater: it just replaces certain (k, v) by default",1/15/16,1
jamesmcclain,It was just a bone-headed oversight :-),1/15/16,1
lossyrob,oh ok cool,1/15/16,1
owcm,"Simple question I hope. I have created an extent using WebMercator coordinates.   I want to determine distance from SW to NE corners and from the centroid.  I get back decimal values, if I want to convert to meters is their a built in function that will give me meter constant for the latitude I am at.  ",1/15/16,1
lossyrob,"I don_Ñét think so, unless JTS has some meters distance calculations. @dwins do you know?",1/15/16,1
lossyrob,WebMercator is close enough to meters that I usually just use it at meters if I_Ñém not near the poles,1/15/16,1
lossyrob,you could also reproject to an appropriate UTM projection and use that as meters,1/15/16,1
lossyrob,"or just get some math of the internet and write the function against lat lng, so reproject hte extent to lat lng and pass it into your function",1/15/16,1
owcm,ok,1/15/16,1
dwins,I don't think there is anything like that in JTS,1/15/16,1
moradology,'near the poles' is relative here - even calculating distances as far north as great britain has caused me some serious headaches,1/15/16,1
lossyrob,is there  a way to find the UTM zone that overlaps a polygon?,1/15/16,1
owcm,"Your right WebMercator worked for me,  my mistake file was in a WGS84",1/15/16,1
jamesmcclain,"@pomadchin All of the failing tests have been corrected here: https://github.com/lossyrob/geotrellis/pull/7 , and some additional Hilbert curve fixes have been made, as well",1/15/16,1
pomadchin,@jamesmcclain cool,1/16/16,1
pomadchin,"@jamesmcclain I think you can base on #1273 , or you can base on master after that i am going just to merge our branches",1/18/16,1
pomadchin,@echeipesh @jamesmcclain @moradology https://github.com/pomadchin/geotrellis/blob/feature/keyindex-json/spark/src/main/scala/geotrellis/spark/io/SparkLayerCopier.scala,1/18/16,1
moradology,this is tricky because there's a lot of duplicated code with a few pieces changed,1/18/16,1
moradology,but the abstractions for just these tiny pieces seem weird,1/18/16,1
moradology,(as i try to think through how it might be done),1/18/16,1
pomadchin,"yes, i wonder can we deal with this overloads somehow",1/18/16,1
pomadchin,"for some backends we have this functions to be implemented: https://github.com/pomadchin/geotrellis/blob/feature/keyindex-json/spark/src/main/scala/geotrellis/spark/io/hadoop/HadoopLayerCopier.scala#L38-L43",1/18/16,1
pomadchin,and this also looks ugly,1/18/16,1
pomadchin,"why we need it: https://github.com/pomadchin/geotrellis/blob/feature/keyindex-json/spark/src/main/scala/geotrellis/spark/io/GenericLayerMover.scalahttps://github.com/pomadchin/geotrellis/blob/feature/keyindex-json/spark/src/main/scala/geotrellis/spark/io/GenericLayerReindexer.scala",1/18/16,1
moradology,"not entirely surprising that this has so much ugly error handling logic, though",1/18/16,1
pomadchin,"ye, that_Ñés another point ):",1/18/16,1
moradology,what if each of these overloads just defined a write function?,1/18/16,1
moradology,"that's where they diverge, and it at least looks like maybe the rest of the changes are superficial",1/18/16,1
moradology,TI to KeyIndex[K],1/18/16,1
moradology,but TI is a subclass of KeyIndex[K],1/18/16,1
pomadchin,hm,1/18/16,1
moradology,grabbing some lunch - will be back very shortly,1/18/16,1
pomadchin,ye! cool feedback; hope you got more thoughts,1/18/16,1
jamesmcclain,@pomadchin I rebased my keybounds work on top of #1273  : https://github.com/pomadchin/geotrellis/pull/2 .  Tests seem to be passing.,1/18/16,1
pomadchin,"yes, i see itdon_Ñét you think it is better to make a pr to the geotrellis main branch, but on top of my branch (with providing `Depend on`)? or i can merge into mine?",1/18/16,1
jamesmcclain,It is up to you,1/18/16,1
jamesmcclain,I had to do a little bit to make it consistent with your branch,1/18/16,1
pomadchin,"ok, just ping me then, and I_Ñéll merge it",1/18/16,1
jamesmcclain,But either way is okay for me,1/18/16,1
jamesmcclain,"I figured it would be convenient to have a PR against your branch so that I can easily tell when the two become inconsistent.  If you want to merge it, that is okay with me!",1/18/16,1
moradology,@pomadchin - what about a protected def that each of these overloads exploit which could deduplicate the error handling portions?,1/18/16,1
pomadchin,what do you mean by deduplicate?,1/18/16,1
pomadchin,o:,1/18/16,1
pomadchin,where we can avoid duplication by protected def o:,1/18/16,1
moradology,"well, maybe a function takes the divergent piece as an argument and returns the appropriate function",1/18/16,1
moradology,what is a celltype with a nodata policy?,1/18/16,1
moradology,i'm thinking 'cell specification',1/18/16,1
moradology,CellSpec,1/18/16,1
pomadchin,"@moradology cool, will think about it",1/18/16,1
pomadchin,and what else can be specified for the cell except nodata?,1/18/16,1
moradology,let me put a short gist up here,1/18/16,1
moradology,https://gist.github.com/moradology/0a9ea84aac3fd63d7404,1/18/16,1
moradology,type CellType in package.scala is DataType with NoDataHandling,1/18/16,1
moradology,"datatype was celltype, but this makes more sense, i think",1/18/16,1
moradology,```type CellType = DataType with NoDataHandling```,1/18/16,1
pomadchin,and can you provide gist how it would be with CellSpec?,1/18/16,1
moradology,i scrapped that thought ,1/18/16,1
moradology,just making half the class 'DataType',1/18/16,1
moradology,and a 'CellType' the conjunction of DataType and a policy for NoData turned out to be far simpler,1/18/16,1
moradology,that gist constitutes a set of changes ,1/18/16,1
moradology,that are at the root of the many shifts in the branch i've been on for a while now,1/18/16,1
pomadchin,"hm, for me gist looks ok; but dont know well this partthe only thing is a `NoNoData` type wich looks weird a little",1/18/16,1
moradology,"Agreed, though it reads pretty well in this context",1/18/16,1
moradology,a datatype with no nodata value,1/18/16,1
moradology,ByteCells with NoNoData,1/18/16,1
moradology,but yeah - i'm also uneasy about that terminology,1/18/16,1
lossyrob,"@pomadchin @jamesmcclain where are we at with updating a layer outside of keybounds? e.g. if I was ingesting landsat, detected new imagery for an area and wanted to merge in the images that were a later date then the keybounds set, it looks like the code will block that currently",1/19/16,1
jamesmcclain,It seems to be working,1/19/16,1
lossyrob,ok. would i have to set the keyboards to some large full range?,1/19/16,1
jamesmcclain,I have a pull request https://github.com/pomadchin/geotrellis/pull/2,1/19/16,1
lossyrob,"I guess so, for the index",1/19/16,1
lossyrob,ok cool,1/19/16,1
jamesmcclain,I think the current plan is to merge Grisha's branch then merge this stuff,1/19/16,1
jamesmcclain,As regards large full range: what exactly do you mean?,1/19/16,1
jamesmcclain,"The way it works right now , is that on the initial write, you can optionally specify a  keybounds other than that of the rdd",1/19/16,1
jamesmcclain,"As long as you only update within that range, everything should work",1/19/16,1
pomadchin,":+1: ye, it solves the problem (should solve) ",1/19/16,1
owcm,I am reading in some spatially adjacent rasters and want to stitch them together to create one raster and perform a local op.  Is there a best way to do this? Do I do it at the RasterSource level or at the tile level. Maybe there is a good example. Thanks in advance.,1/20/16,1
echeipesh,"@owcm: Sounds like your question is about single tiles rather than RDDs of tiles, if so you could use Tile/Raster merge methods: https://github.com/geotrellis/geotrellis/blob/d01d9709cd018f910cc80cb256cad6a865874c92/raster/src/main/scala/geotrellis/raster/merge/RasterMergeMethods.scala https://github.com/geotrellis/geotrellis/blob/d01d9709cd018f910cc80cb256cad6a865874c92/raster/src/main/scala/geotrellis/raster/merge/TileMergeMethods.scalaIt would look something like this:```scalaimport geotrellis.raster._val r1 = Raster(IntArrayTile.fill(1, 2,2): Tile, Extent(0,0,2,2))val r2 = Raster(IntArrayTile.fill(2,2,2): Tile, Extent(1,1,3,3))val target = Raster(EmptyTile(TypeInt, 3, 3): Tile,Extent(0,0,3,3))val mergeTile = target.merge(r1).merge(r2)mergeTile.asciiDraw``````scalascala> mergeTile.asciiDrawres27: String =""    ND     2     2     1     1     2     1     1    ND_ÑÒ```Now that I typed it out I notice a couple of rough corners on that API that we_Ñéll have to address:1. The method will only be available for `Raster[Tile]`, thus the cast in `r1` and `r2` definition.2. Defining the target tile can easily cause a resample if you_Ñére not careful to match resolution. ex:```scalascala> val target = Raster(EmptyTile(TypeInt, 6, 6): Tile,Extent(0,0,3,3))target: geotrellis.raster.Raster[geotrellis.raster.Tile] = Raster(IntConstantTile(-2147483648,6,6),Extent(0.0, 0.0, 3.0, 3.0))scala> target.merge(r1).merge(r2).asciiDraw()res28: String =""    ND    ND     2     2     2     2    ND    ND     2     2     2     2     1     1     1     1     2     2     1     1     1     1     2     2     1     1     1     1    ND    ND     1     1     1     1    ND    ND_ÑÒ```",1/20/16,1
owcm,"Thanks echeipesh, is the approach easier if I already have an RDD of tiles?   I my processing I do create an RDD of the tiles to accomplish another task.",1/20/16,1
echeipesh,That_Ñés actually the case we were creating this feature to support. You can use these: https://github.com/geotrellis/geotrellis/blob/master/spark/src/main/scala/geotrellis/spark/merge/TileRDDMergeMethods.scala,1/20/16,1
echeipesh,Those assume you have already gone through tiling and need to merge RDDs post fact. If you_Ñére doing this all as one still you can just union your RDDs together before the tiling step and the merge will happen automatically.,1/20/16,1
jamesmcclain,"Topics that I would like to discuss:  I have a subset method PR here: https://github.com/geotrellis/geotrellis/pull/1290 .  I have added the subset method to the GeoTiffMultiBandTile types, but is that necessary?  Also, I presume that I should be using extension methods here, especially if the answer the previous question is negative.",1/20/16,1
pomadchin,https://github.com/geotrellis/geotrellis/issues/1291,1/20/16,1
lossyrob,@owcm also check th `stitch` method out: https://github.com/geotrellis/geotrellis/blob/master/spark/src/main/scala/geotrellis/spark/SpatialRasterRDDMethods.scala#L13,1/20/16,1
lossyrob,"if it_Ñés a spatial RDD that just has tiles next to each other, calling stitch will do what you want",1/20/16,1
echeipesh,Quick poll: if you try to stitch empty array of tiles do you get None or do you get an exception ?,1/20/16,1
moradology,What about an empty tile?,1/20/16,1
echeipesh,If there are no sample tiles how many rows/cols does it have ?,1/20/16,1
moradology,0/0,1/20/16,1
echeipesh,You_Ñéd have to pick a type too,1/20/16,1
moradology,fair point. not sure what the default would be there,1/20/16,1
hectcastro,How similar is that to the divide by zero scenario?,1/20/16,1
echeipesh,Hmm. I suppose it_Ñés pretty similar. You have some operation that requies an input but the input may have an invalid state.,1/20/16,1
moradology,what happens when you stitch an array of 1?,1/20/16,1
moradology,"that's just identity, no?",1/20/16,1
echeipesh,You get back the tile,1/20/16,1
echeipesh,"Not exactly identity, since you lose the key information, but yes close enough.",1/20/16,1
moradology,"None forces the use of Some(), which is worth avoiding if it can be done in a clean way",1/20/16,1
echeipesh,"No way to avoid it here. While I could return an empty tile, lets say it doesn_Ñét really matter the type. I can_Ñét implement the corresponding stitch the returns a Raster because there is no such thing as Empty extent.",1/20/16,1
echeipesh,The base implementation can_Ñét return key bounds because there is no empty GridBounds,1/20/16,1
echeipesh,"Hmm. Actually I guess by convention GridBounds(0,0,-1,-1) would be empty.",1/20/16,1
moradology,"yeah, i was thinking about that",1/20/16,1
moradology,the resultant celltype difficulty remains,1/20/16,1
moradology,we don't have a bottom type,1/20/16,1
moradology,nor is it clear such a thing would help more than hurt in this case,1/20/16,1
echeipesh,So the non weird choices are Option or throw. I am leaning to throw since this very close to the way reduce works.,1/20/16,1
echeipesh,I am essentially reducing a set of tiles to a single tile.,1/20/16,1
moradology,"i like throw too, from the perspective of writing in the library but i wonder if it wouldn't be common to assemble a bunch of tiles (or not) and try to stitch that possibly empty set of tiles together",1/20/16,1
moradology,"I suppose in that case, they can just wrap things up in a Try.toOption",1/20/16,1
lossyrob,"Yeah, Option would seem right to me except that if we follow Spark_Ñés design pattern, and they throw against empyt RDD_Ñés, I think throwing makes sense",1/20/16,1
owcm,"@lossyrob  I borrowed heavily from your Slippy map example you supplied me earlier on. I run a number of ops and feed the results to a extension of the SlippyTileWriter   I created.   Issue is I want to use different or custom Colorramps for different op results.  I am including a code snippet below.  I  believe my limitation is programming in the scala environment but maybe you would have a pointer,   I want to pass a dynamic ColorRamp value to  the gtOut.tile.renderPng  call and not sure the best way to extend the pattern/framework you were using.  Let me know if this is outside of the scope of this board. Thanks Chris```// Pyramiding up the zoom levels, write our tiles out to the local file system.        val writeOp =          Pyramid.upLevels( new RasterRDD(tiled, rasterMetaData), layoutScheme, zoom) { (rdd, z) =>            val md = rdd.metaData            // New Approach            val writer = new SurgeIndexSlippyTileWriter[Tile](jobId) ({ (key, tile) =>              // Tile is geotrellis.raster.Tile              val extent = md.mapTransform(key)              // Make geotrellis.raster.io.geotiff.SingleBandGeoTiff              val gtOut= GeoTiff(tile, extent, WebMercator)              gtOut.tile.renderPng( ColorRamps.BlueToRed ).bytes            })            new RasterRDD( writer.setupWrite(z, rdd), md)          }        // We've set up a set of Spark ""transformations"", but need to call an ""action"" to kick off the work.        // An empty foreach call is enough to set the processing in motion.        writeOp.foreach { x => }```",1/21/16,1
schBen,"Hello everybody, I have a question regarding GeoTrellis and Spark: This tutorial https://github.com/geotrellis/geotrellis-landsat-tutorial explains how to get the NDVI of a geotiff and then uses geotrellis.spark to create a tile set of that same geotiff. Could you also use geotrellis.spark to get the NDVI, that is read the geotiff in as an RDD and then apply an NDVI calculation to it directly? Or would the NDVI calcuIation not benefit from Spark? Maybe there is already an example that I missed. I am just beginning with GeoTrellis and Spark and this would really help me.",1/21/16,1
owcm,@lossyrob Turns I moved my png generation into the operation under  Pyramid.upLevels enclosure.  When I did this  the ColorRamp I wanted to use for the current job was out of scope.  I created a project specific Pyramid class extension that allowed me to pass in the  operation I was running so I could lookup the ColorRamp to use inside the writer operation.   Scala learning experience for me.  Did not want you to waste anytime on my original question.  Thanks Chris,1/21/16,1
dwins,"hey guys, my internet is pretty flaky today, i won't be able to join the hangout. going to try and construct a PR for some of the reprojection issues today.",1/22/16,1
echeipesh,"Thank you, curious to see what they turn out to be.",1/22/16,1
echeipesh,"@schBen: You can use a layer reader, instead of a tile reader, to read an RDD instead of tile. So using exactly the same catalog as the landsat tutorial you would be able to do something like this:```scalaval reader = FileLayerReader[SpatialKey, MultiBandTile, RasterMetaData](catalogPath)val layerId: LayerId = ???val rdd: RDD[(SpatialKey, MultiBand)] with Metadata[RasterMetaData] =  reader.read(layerId) // or use .query(_Ñ_) to specify a bounding boxval ndviRdd: RDD[(SpatialKey, Tile)] =  rdd.mapValues {  tile =>     tile.convert(TypeDouble).combineDouble(0, 1) { (r, ir) =>        if(isData(r) && isData(ir)) {          (ir - r) / (ir + r)        } else {          Double.NaN        }      }  }```At the end `ndviRdd` is a sinble band tile containing NDVI values. You can map it further of course or save that. That totally benefits from being on spark because you_Ñére parallelizing the conversion. ",1/22/16,1
ameetkini,"@lossyrob , @echeipesh , hey guys!",1/25/16,1
ameetkini,I'm rising back from the dead into the ethereal realms of geotrellis :),1/25/16,1
ameetkini,"for a project, I'm looking to use geotrellis to process satellite imagery (bet you haven't heard of that use case before :) ",1/25/16,1
ameetkini,"great to see the tons of features in the past year, especially multiband support",1/25/16,1
ameetkini,"anyway, I'll start a thread with you both on some questions I have. Happy Shoveling! We're digging ourselves out of 30 inches",1/25/16,1
lossyrob,whoa blast from the past!,1/25/16,1
lossyrob,"good to hear from you again, and that you_Ñére going to be using GeoTrellis!",1/25/16,1
lossyrob,"I_Ñém actually in Victoria, BC for a locationtech code sprint, so I got to avoid all the snow :sunglasses: ",1/25/16,1
ameetkini,lucky you! ,1/25/16,1
ameetkini,"that's a great place for a code sprint, I hope you enjoy skiing",1/25/16,1
lossyrob,"not much time for skiing, trying to get GeoTrellis ready for graduation from incubation",1/25/16,1
lossyrob,"but it_Ñés a beautiful (and warmer than Philly) place, which is great",1/25/16,1
schBen,"@echeipesh Thank you for the quick reply. The NDVI operation on the RDD looks good. But I am still not sure about setting up the reader: So is catalogPath a path on my file system like ""D:/landsat/example.tif""? And what should I use as the LayerId parameters (name and zoom, my input is a single landsat dataset for now.)Finally, is there some example how to simply save the RDD as a file, let's say png? (e.g. like ndviRdd.saveAsFile(...) or is it more like ndviRdd.values.foreach(tile => tile.renderPng.write(ouputPath)), or even completely different :-) Thank you",1/26/16,1
pomadchin,"@schBen this path can be local or hdfs; `LayerId` consists of a `layerName` (juts a string) and of a zoom level. Before you can use reader, as @echeipesh pointed, you have to ingest tiles (S3, Accumulo, HDFS, Spark Files). To save a certain `Tile` as png: ```scalaimport geotrellis.raster.render._val tile: Tile = ???val somePath = ""/test.png""tile.renderPng.write(somePath)```  And possible to save the whole `RDD` as png:```scalaval rdd: RasterRDD[SpatialKey] = ???rdd.renderPng(layerId, ""s3://tile-bucket/{name}/{z}/{x}/{y}.png"")```Also may be it would be helpful for you, there is an ndvi project: https://github.com/geotrellis/geotrellis-landsat-tutorial/tree/master/src/main/scala/tutorial containing examples of ingesting and redering as png",1/26/16,1
owcm,"My question is if I have a list of and  tiles and their  extents ListBuffer[(Extent,Tile)]().  I want to make a RasterRDD from them so I can do a RasterRDD stitch operation.   Is it possible to  generate a RasterMetaData  object from just a list of tiles and their extents.  And is there a handy way to translate my list of tiles with their  extents to a  collection of tiles with spatialkeys.  Thanks in advance.  ",1/26/16,1
lossyrob,"You need to go through the tiling processed beforeyou stitch.```scalaval crs: CRS = ???val tiles: ListBuffer[(Extent,Tile)]  = ???val sourceTiles = tiles.map { case (extent, tile) => (ProjectedExtent(extent, crs), tile) }val rmd = RasterMetaData.fromRdd(sourceTiles, FloatingLayoutScheme(512))val tiled = rmd.tileToLayout(rmd)val raster: Raster = tiled.stitch```",1/26/16,1
lossyrob,"@pomadchin @jnh5y told me that GeoMesa is on Accumulo 1.6/1.7, @rfecher is that the same for GeoWave? We_Ñére going to test GeoTrellis against Accumulo instances running geowave/geomesa, want to make sure our versions are right",1/26/16,1
lossyrob,"finally figured out a shortcut to get to a branch off a PR. On the PR UI, hit _ÑéT_Ñé, then pick a random file. boo-ya",1/26/16,1
pomadchin,@lossyrob https://github.com/ngageoint/geowave/blob/master/pom.xml#L38,1/26/16,1
pomadchin,"btw, found out that there is a branch: https://github.com/ngageoint/geowave/tree/accumulo-1.7",1/26/16,1
lossyrob,sweet,1/26/16,1
lossyrob,so yeah I_Ñém submitting CQ_Ñés for accumulo 1.7 (the dependency legal check thing for locationtech),1/26/16,1
lossyrob,hoping the _ÑÒ1.7 client code works against 1.6 accumulo_Ñù narrative pans out like it seems like it,1/26/16,1
pomadchin,":D ye, what is exactly i_Ñéve checked, that chatta-demo works with 1.7 accumulo-core dep on Accumulo 1.6.2",1/26/16,1
lossyrob,sweet,1/26/16,1
echeipesh,"```scala  // If I want to do a localAdd without losing my metadata  rdd1.mapRDD { rdd =>    rdd      .join(rdd2)      .localAdd      .join(rdd3)      .localAdd  } // So far I've only saved the original metadata  .mapMetadata { md => // now I hava chance ot adjust it    md.copy(cellType = TypeInt)   }    rdd1 // spatial join will consume and transform my metadata to keybounds    .spatialJoin(rdd2).mapRDD { _.combineValues(Add.apply) }    .spatialJoin(rdd3).mapRDD { _.combineValues(Add.apply) } // this saved KeyBounds of the join through combine    .mapMetadata{ kb => // but I can combine it with metadata from above      rdd1.metadata.copy(bounds = kb)     }```There is a lack of symetry between a `.join` call and `.spatialJoin` call but at least it_Ñés clear what is happening and when with that setup.",1/26/16,1
lossyrob,what does mapRDD do?,1/26/16,1
lossyrob,is that the proposed name change of withContext?,1/26/16,1
echeipesh,new name for `withContext`,1/26/16,1
echeipesh,"One would need to change both the RDD and the metadata in one chain so lets call it what it is, a map.",1/26/16,1
echeipesh,Also we implicitly nixed the `rdd1 + rdd2` syntax for `localAdd` between two rdds.,1/26/16,1
lossyrob,`+` could do the join,1/26/16,1
lossyrob,"well, yeah. I guess that would have to default the join",1/26/16,1
lossyrob,not sure I_Ñém on board with `mapRDD`,1/26/16,1
lossyrob,just because it still just sounds like you_Ñére calling `rdd.map` somehow,1/26/16,1
echeipesh,`.withMetadata` to map over RDD and `.withRDD` to map over Metadata ?,1/26/16,1
lossyrob,hmm,1/26/16,1
echeipesh,also `+` would default the partitioner choice in addition to join choice.,1/26/16,1
lossyrob,`withRDD` would be confusing to encounter methinks,1/26/16,1
echeipesh,"```scala  rdd1.withContext { rdd =>    rdd      .join(rdd2)      .localAdd      .join(rdd3)      .localAdd  } // So far I've only saved the original metadata  .mapContext { md => // now I hava chance ot adjust it    md.copy(cellType = TypeInt)   }```Free pun with this one",1/26/16,1
lossyrob,why are we changing the cell type htere?,1/26/16,1
lossyrob,it_Ñés just an aexample of changing metadata?,1/26/16,1
echeipesh,"yes, just an example",1/26/16,1
lossyrob,hmm,1/26/16,1
lossyrob,"so I guess it should be _ÑÒwithContext_Ñù or _ÑÒwithMetadata""",1/26/16,1
lossyrob,I like _ÑÒwithContext_Ñù better,1/26/16,1
lossyrob,"but then should it be `RDD[(K, V)] with Context[M]`",1/26/16,1
lossyrob,naming is hard,1/26/16,1
echeipesh,can_Ñét use `withContext` for changing metadata if we_Ñére using it to change the RDD,1/26/16,1
lossyrob,no not for changing metadata,1/26/16,1
lossyrob,just in general,1/26/16,1
echeipesh,Probably better to standardize on `Metadata` in naming. Context generally means something closer to environment. ,1/26/16,1
lossyrob,rdd.withMetadata { rdd => _Ñ_ },1/26/16,1
echeipesh,Doesn_Ñét sound as good though,1/26/16,1
lossyrob,yeah,1/26/16,1
lossyrob,"I sort of like withContext, although it_Ñés not necessarily in line with consistent naming. is that bad?",1/26/16,1
echeipesh,"It_Ñés fine. What to call the thing that changes the metadata, _ÑÒchangeContext_Ñù _ÑÒmapContext_Ñù _ÑÒmapMetadata""",1/26/16,1
lossyrob,"I mean, `mapContext`",1/26/16,1
lossyrob,makes sense,1/26/16,1
echeipesh,On so many levels : ),1/26/16,1
lossyrob,"And we just talk about `RDD[(K, V)] with Metadata[M]` is an RDD with a context of Metadata[M]",1/26/16,1
lossyrob,or a context of M,1/26/16,1
lossyrob,I wonder if that just makes sense to us or to the broader public,1/26/16,1
lossyrob,hey we hit a 100 members in the channel :) heyo everyone!,1/26/16,1
moradology,did it just fall to 99?,1/26/16,1
lossyrob,no if you quit gitter and reopen it should say. eventually-consistent systems ftw,1/26/16,1
cuttlefish,"""eventually consistent"" ",1/26/16,1
cuttlefish,beat me to it,1/26/16,1
lossyrob,^- says the 100th member to join,1/26/16,1
cuttlefish,alleged 100th ;),1/26/16,1
moradology,"101, in fact",1/26/16,1
lossyrob,almost famous,1/26/16,1
jnh5y,#lameClaimToFame,1/27/16,1
lossyrob,Why are you shouting,1/27/16,1
lossyrob,That's a bold statement (ha),1/27/16,1
jnh5y,dern gitter made mah hashTag a markdown header...  Congrats on the first hundred!,1/27/16,1
schBen,"@echeipesh @pomadchin Okay, I managed to read in the GeoTiff into an RDD and calculate the NDVI on it. But I could not find a method like RasterRDD.renderPng(), only Tile.renderPng(). I want to write out the NDVI image as a single file with the same extent as the input Landsat GeoTiff. So is there a method that combines / merges / mosaics all of the RDD's tiles to one tile correctly by their spatial keys? So that you can then render the resulting tile? It would be best to have it as a GeoTiff, but Png would be okay.",1/27/16,1
lossyrob,one sec,1/27/16,1
jamesmcclain,Sorry for having to leave so abruptly ... what is the plan for RRP?,1/27/16,1
lossyrob,"```scalaval raster = ndviRdd.stitch()SingleBandGeoTiff(raster, crs).write(path)```",1/27/16,1
lossyrob,crs is the coordinant reference system it_Ñés in (you can get that from the original geotiff(s),1/27/16,1
lossyrob,or should be part of the metadata,1/27/16,1
jamesmcclain,"Oops, sorry about that",1/27/16,1
schBen,"@lossyrob Okay the Rdd.stitch() looks good! And I have the crs. But SingleBandGeoTiff seems to ask for a path or Array[Byte], not raster and crs?",1/27/16,1
jasonbeverage,"Hi all, I'm hoping to get some help importing some data into Accumulo.",1/27/16,1
jasonbeverage,"I've been able to run the chatta demo and the landsat demo, but those appear to only work on local files.",1/27/16,1
jasonbeverage,"From what I gather from gitter the spark-etl project is the preferred method of doing an accumulo ingest, is that correct?",1/27/16,1
lossyrob,"Sorry drop the ""singleband"" part of the last line and it should work.that or we are missing an overload.",1/27/16,1
pomadchin,"@jasonbeverage there is a PR in a chatta-demo project (not completely working, but has an ingest example: https://github.com/pomadchin/geotrellis-chatta-demo/blob/spark-version/geotrellis/src/main/scala/geotrellis/chatta/ChattaIngest.scala), which works with Accumulo: https://github.com/geotrellis/geotrellis-chatta-demo/pull/13  ",1/27/16,1
jasonbeverage,"Awesome, thanks for pointing that out!",1/27/16,1
jasonbeverage,"So is the gist that there is no standard command line tools or process for doing imports, but instead you would just use the ETL library to generate your own import code?",1/27/16,1
lossyrob,"Also seehttps://github.com/lossyrob/landsat-demo/blob/master/ingest/src/main/scala/demo/NEXIngest.scala for an alternate, in code way to do ingest",1/27/16,1
lossyrob,"Right we haven't built the command line tools yet, so it's still a bit of a manual process",1/27/16,1
jasonbeverage,"Ok great, understood.",1/27/16,1
jasonbeverage,"I looked at gt-admin thinking that was the ""standard"" way to ingest data originally before I came across the spark-etl project.",1/27/16,1
lossyrob,Yeah we need to clean that up :},1/27/16,1
jasonbeverage,"Nah, I totally understand :)  It's difficult.",1/27/16,1
lossyrob,"We_Ñére wrapping up the last bit of feature and API development, hoping to get a 0.10 release out in spring, and then tooling for ingest is high on the list. Thanks for another data point pointing towards that being necessary :)",1/27/16,1
jasonbeverage,"We develop the osgEarth open source globe rendering library, so I know the pain of open source.",1/27/16,1
jasonbeverage,"Really cool work on geotrellis, I'm excited to see what we can use it for.",1/27/16,1
lossyrob,"oh nice, that looks like a cool project",1/27/16,1
lossyrob,"would be happy to chat through use cases here as you come up with them, always a fun topic for me",1/27/16,1
jasonbeverage,Yeah that would be great.,1/27/16,1
jasonbeverage,"I will definitely have more questions, I'm still trying to get my head around everything :)",1/27/16,1
lossyrob,for sure. drop a line here anytime!,1/27/16,1
pomadchin,"Now we can ingest tiles using `SpaceTimeInputKey` (`Ingest[SpaceTimeInputKey, SpaceTimeKey]`) or using `ProjectedExtent` (`Ingest[ProjectedExtent, SpatialKey]`) as ingest keys. ```scalacase class ProjectedExtent(extent: Extent, crs: CRS)case class SpaceTimeInputKey(extent: Extent, crs: CRS, time: DateTime)````SpaceTimeInputKey` seems not a right name, may be we have to give it more suitable name, smth like: * `TemporalProjectedExtentKey`* `TemporalProjectedExtent`* `ProjectedExtentTemporalKey`@lossyrob @echeipesh @jamesmcclain @moradology  have any thoughts / possible suggestions?",1/27/16,1
echeipesh,`TemporalProjectedExtent` seems like the most consistent with our naming convention. ,1/27/16,1
echeipesh,I wonder if it_Ñés possible to use the composite types here. Could the Ingest work on `ProjectedExtent with TemporalKey` ?,1/27/16,1
pomadchin,"ye, a good point ",1/27/16,1
echeipesh,Of course one problem is that they_Ñére both case classes.,1/27/16,1
pomadchin,actually I had some experiments with that ,1/27/16,1
pomadchin,will try it now wile compiling GeoWave & GeoMesa,1/27/16,1
pomadchin,Fount out that it should be a class,1/27/16,1
pomadchin,"```scaladef hadoopSpaceTimeGeoTiffRDD(path: Path, tiffExtensions: Seq[String]): RDD[(TemporalProjectedExtent, Tile)] =    sc.newAPIHadoopRDD(      sc.hadoopConfiguration.withInputDirectory(path, tiffExtensions),      classOf[SpaceTimeGeoTiffInputFormat],      classOf[TemporalProjectedExtent],      classOf[Tile]    )```",1/27/16,1
pomadchin,"I tried to define it as ```scalaobject TemporalProjectedExtent {  type TemporalProjectedExtent = ProjectedExtent with TemporalKey  def apply(extent: Extent, crs: CRS, millis: Long): TemporalProjectedExtent =    new ProjectedExtent(extent, crs) with TemporalKey {      val instant = millis    }  def apply(extent: Extent, crs: CRS, dt: DateTime): TemporalProjectedExtent =    new ProjectedExtent(extent, crs) with TemporalKey {      val instant = dt.getMillis    }}```",1/27/16,1
echeipesh,"Yep, that makes sense. It needs to be a concrete class so it can be instantiated through java reflection with empty constructor.",1/27/16,1
pomadchin,"so ye, `TemporalKey` should be a trait",1/27/16,1
pomadchin,1,1/27/16,1
echeipesh,"wait, did `type TemporalProjectedExtent = ProjectedExtent with TemporalKey` actually work for hadoop ?",1/27/16,1
pomadchin,"nope, we need constructor for it ",1/27/16,1
pomadchin,as you said,1/27/16,1
pomadchin,"so we can define the following:```scalaclass TemporalProjectedExtent(extent: Extent, crs: CRS, date: DateTime) extends ProjectedExtent(extent, crs) with TemporalKey {  val instant = date.getMillis}```",1/27/16,1
pomadchin,but dont think that it makes sense ._.,1/27/16,1
schBen,"Hi again! How should I best create a multiband RDD like RDD[(SpatialKey, MultiBandTile)] of two singleband GeoTiffs that represent Landsat bands? I know how to do it by creating an intermediate multiband GeoTiff file and then reading it in, as in the Landsat tutorial. But I would like to skip that file and just create an RDD with spatial keys from two corresponding singleband GeoTiff files as directly as possible.",1/28/16,1
echeipesh,"You can join the two RDDs and map over the result to produce the `MultiBandTile` something like:```scalarddBandA.join(rddBandB).mapValues { case (tile1, tile2) => ArrayMultiBandTile(tile1, tile2) }```",1/28/16,1
echeipesh,https://github.com/geotrellis/geotrellis/blob/master/raster/src/main/scala/geotrellis/raster/ArrayMultiBandTile.scala#L9,1/28/16,1
echeipesh,It_Ñés a little surprising that that apply is on `ArrayMultiBandTile`rather than `MultiBandTile`. I suspect the later will gain some apply methods soon.,1/28/16,1
schBen,@echeipesh Alright. Still it's easier than I thought :-) Thank you!!,1/28/16,1
lossyrob,yeah MultiBandTile should have an apply that creates an ArrayMultiBandTile. I_Ñéll make that on the list :),1/28/16,1
jasonbeverage,Timely discussion on multiband tiles :),1/28/16,1
jasonbeverage,I managed to take the examples that you all pointed to yesterday and finally got an accumulo import (mostly) working.,1/28/16,1
jasonbeverage,I took a small downsampled RGB bluemarble tif and used the code in the pull request @pomadchin mentioned to import it.,1/28/16,1
jasonbeverage,Will that actually import all 3 bands into accumulo or just the first?,1/28/16,1
jasonbeverage,I'm guessing just the first b/c I used this code to just grab a tile and render it and it was a single channel image,1/28/16,1
jasonbeverage,"`val key = SpatialKey(x, y)        val tile = tileReader.read(LayerId(layer, zoom)).read(key)        tile.renderPng().write(""out.png"")`",1/28/16,1
jasonbeverage,that didn't work as expected :),1/28/16,1
echeipesh,"`.renderPng` is intended to work in single band tiles, that is `Tile`s. How did you construct your reader/writer and what do you mean didn_Ñét work as expected ?",1/28/16,1
pomadchin,"@jasonbeverage yes, you have to make a special ingest for multiband tiles; in order to process RGB tiles, you have to split tiles by bands and to ingest",1/28/16,1
jasonbeverage,"Right, I was working against a Tile.  I wasn't sure if that a single band or multiband.",1/28/16,1
jasonbeverage,"Ok great, that makes sense.  ",1/28/16,1
jasonbeverage,by didn't work as expected I meant that it wrote out a single band png instead of an RGB png.,1/28/16,1
jasonbeverage,So would you just ingest them as separate layers and then combine them at runtime using the MultiBandTile?,1/28/16,1
lossyrob,"You can ingest it as an rgb multiband, and then at tender time do the calculation to render the multiband files as a png",1/28/16,1
lossyrob,renderPng has a version that takes an rgb multiband told,1/28/16,1
lossyrob,Tile*,1/28/16,1
jasonbeverage,Thanks Rob.  I was using the code here to do the ingest:  https://github.com/pomadchin/geotrellis-chatta-demo/blob/spark-version/geotrellis/src/main/scala/geotrellis/chatta/ChattaIngest.scala,1/28/16,1
lossyrob,"That does a single band ingest, which you will have to tweak to work with multiband",1/28/16,1
lossyrob,https://github.com/lossyrob/landsat-demo/blob/master/ingest/src/main/scala/demo/PlanetLabsIngest.scala,1/28/16,1
lossyrob,That does a multiband rgb image ingest,1/28/16,1
jasonbeverage,"Awesome, thanks Rob.  ",1/28/16,1
jasonbeverage,"I'm still trying to get my head around how geotrellis is working, I was pretty happy to even see a single band PNG tile that looked like bluemarble :)",1/28/16,1
lossyrob,I was rendering the tiles coming out of that later with the Render.image function here https://github.com/lossyrob/landsat-demo/blob/master/server/src/main/scala/demo/Render.scala,1/28/16,1
jasonbeverage,Ok awesome.  That's probably enough for me to go off and experiment with until I have more questions :)  Thanks for your help!,1/28/16,1
lossyrob,No problem!,1/28/16,1
owcm,I was looking around though github projects to see  some examples of the geotrellis.spark.op functions in action.   Are there any good examples?  Pretty familiar with the the landsat tutorial now.  Once things have been tiled up into RDDs would it be conceivable  to use a ElevationRasterRDDMethods or LocalSpatialRasterRDDMethods on those created RDDS.  Or am I out in left field?  Thanks Chris,1/29/16,1
owcm,"  I have written in some code that reads in GeoTiffs from S3 or HDFS.  I re-project to WebMercator create a Rastersouce and then call the  ElevationRasterSourceMethodExtensions I desire for example slope or hillside. I collect all the resulting tiles and projected extents and create an RDD that I pass to a tiling routing that creates z/x/y pyramids.  Based on the Geotrellis landsat demo.  I have 2 links: One for the [Slope result] (https://drive.google.com/file/d/0B1YZbN2gCsmSRVlMai1lY0xYeGM/view)and the second to the [Hillshade result](https://drive.google.com/file/d/0B1YZbN2gCsmSRDNYZm03N3pRWGM/view)Notice in the hillside example I am getting white borders on the tile results.  Has anyone seen this before? Example code is below:``` {  ...    // Returns SingleBandGeoTiff    val gtIn = SingleBandGeoTiff(bytesToUse)    // Returns a ProjectedRaster    val projectedRaster = gtIn.projectedRaster    val ProjectedRaster(Raster(tile, extent), crs) = projectedRaster.reproject(WebMercator)    // Returns SingleBandGeoTiff    val reprojGeoTiff = SingleBandGeoTiff(tile, extent, crs, gtIn.tags, gtIn.options)    // Move from tile to RasterSource    val theRS = RasterSource( reprojGeoTiff, extent )    // Call operation to perform    val opRS = opsFunc( theRS )    // Write out product to go to RDD    val lclTile = opRS.get    val lclExtent = opRS.rasterExtent.get.extent    val projExt = new ProjectedExtent( lclExtent, crs)    val gtSlope = GeoTiff(lclTile, lclExtent, crs)    val newEntry = (projExt, lclTile)    tileList += newEntry  }}val tiles = sc.parallelize( tileList )// Pass on to tiling code...my Elevation Ops Methodsdef hillShade( inRasterSource: RasterSource ) :RasterSource = {  val outRS = ElevationRasterSourceMethodExtensions(inRasterSource).hillshade(90.0, 40.0, 1.0)  outRS}https://drive.google.com/file/d/0B1YZbN2gCsmSRVlMai1lY0xYeGM/viewdef slope( inRasterSource: RasterSource ) :RasterSource = {  val slopeRS = ElevationRasterSourceMethodExtensions( inRasterSource ).slope  slopeRS}```",1/29/16,1
lossyrob,"@owcm th ElevationRasterRDDMethods are more likely to be correct. So if you have an RDD of elevation tiles, you should be abl to call them. Also, the methods are implicitly added with a package import, so you hsould be abl to do",1/29/16,1
lossyrob,"```scalaval rdd: RasterRDD[SpatialKey] = ??? // your elevation raster addimport geotrellis.spark.op.elevation._val hillshadeRDD = rdd.hillshade(90, 40, 1)val slopeRdd = rdd.slope()GeoTiff(hillshadeRdd.stitch, hillshadeRdd.metadata.crs).write(_ÑÒ/tmp/hillshade.tiff_Ñù)GeoTiff(slopeRdd, slopRdd.metadata.crs).write(_ÑÒ/tmp/slop.tiff_Ñù)```",1/29/16,1
owcm,@lossyrob  Thanks for the response,1/29/16,1
pomadchin,https://gist.github.com/pomadchin/93f966fa1784eb21aab9,1/29/16,1
Leonhalt3141,"I'm trying to rendering PNG from my geotiff raster data.",1/29/16,1
Leonhalt3141,"```import geotiff.reader.GeoTiffReader.readSingleBandval tiff = readSingleBand(""path2filef"")tiff.toArrayTile().renderPng(ColorRamps.HeatmapBlueToYellowToRedSpectrum).write(""/Users/ken/test.png"")```but, how can I set NoData value in Geotrellis rendering API? I already set  0 for NoData value in my geotiff file.",1/29/16,1
echeipesh,"```scalaval tile = tiff.toArrayTile()val h = tile.histogramval noData = 0val breaks = ColorBreaks(h, ColorRamps.HeatmapBlueToYellowToRedSpectrum.toArray)tile.renderPng(breaks, noData, h)```This is the explicit version of what is happening through that call, so you can use that form. But if I_Ñém reading that correctly, it should already be defaulted to 0: https://github.com/geotrellis/geotrellis/blob/7b1fa201bcc6627af210b1be4884c7645c182db1/raster/src/main/scala/geotrellis/raster/render/PngRenderMethods.scalaIs that what you_Ñére seeing or is not being rendred correctly ?",1/29/16,1
echeipesh,Also I should say that renderPng api is... hairy and is under review right now :),1/29/16,1
owcm,"I moved up to a new snapshot of  geotrellis-spark specifically 0.10.0-2283ec5.  Seeing some java.lang.ArrayIndexOutOfBoundsException errors when using  the tiling tileToLayout(rasterMetaData, Bilinear) call.   Where would I go to enter a PR?",1/29/16,1
lossyrob,"Do you have a fix for it, and would like to make a Pull Request? Or do you want to enter an issue?",1/29/16,1
owcm,Want to enter an issue.   I am currently looking at it but I have not found a resolution yet.,1/29/16,1
echeipesh,Just create an issue right on the repo: https://github.com/geotrellis/geotrellis/issues,1/29/16,1
lossyrob,"awesome, thanks :)",1/29/16,1
Leonhalt3141,"@echeipesh Thanks for reply!I could not call `val h = tile.histogram`. My tiff file is float data type. Is that reason? My geotrellis version is 0.10.0-M10",1/30/16,1
Leonhalt3141,"I solved this in different way.```val a = tiff.toArray().map{ case 0 => NODATA; case x => x}val iat = IntArrayTile(a, tiff.raster.cols, tiff.raster.rows)iat.renderPng(ColorRamps.HeatmapBlueToYellowToRedSpectrum).write(""/Users/ken/test2.png"")```",1/30/16,1
lossyrob,"@Leonhalt3141 nice, glad it worked out",1/30/16,1
lossyrob,"@anandthakker hey I lost that link that you gave me (I think it was you) of the vector tile data set that map box published of OSM, do you know where that_Ñés at?",1/30/16,1
owcm,"Trying to use ElevationRasterRDDMethodExtensions as follows:```val hillShadeRdd = ElevationRasterRDDMethodExtensions(rasterCollection).hillshade(90, 40, 1.0)```",1/31/16,1
owcm,"Should read:Trying to use ElevationRasterRDDMethodExtensions as follows:```val hillShadeRdd = ElevationRasterRDDMethodExtensions(rasterCollection).hillshade(90, 40, 1.0)``` I get sbt run error ""could not find implicit value for parameter _sc: geotrellis.spark.SpatialComponent[geotrellis.vector.ProjectedExtent]""  In scala land how should the  SpatialComponent be supplied to the call for RDDMethodExtensions.  Thanks in advance.",1/31/16,1
owcm,My problem was my rasterCollection was of type  ProjectedExtent instead of SpatialKey,1/31/16,1
echeipesh,"Sounds like you figured out your problem, but an explanation for the archives: `ProjectedExtent` is a key that is read from unstructured raster source, like a _ÑÒbunch of geotiffs on S3_Ñù. In order for most of GeoTrellis functionality to work they need to be tiled using one of these methods: https://github.com/geotrellis/geotrellis/blob/master/spark/src/main/scala/geotrellis/spark/tiling/TilerMethods.scala. That will produce an RDD of keys that have a `SpatialComponent` view available. That is to say they will have been assigned column and a row in some layout. ",2/1/16,1
jasonbeverage,I'm looking at the geotrellis-ec2-cluster project to setup a cluster on AWS and I'm looking to increase the space allocated for hdfs,2/1/16,1
jasonbeverage,"I've got 1 leader and 4 followers, and I see that hadoop fs -ls is only reporting ~20GB of space available.",2/1/16,1
jasonbeverage,The followers look like they have a bunch of potential mount points at media/ephemeral{0-7},2/1/16,1
jasonbeverage,Do I need to manually create ebs volumes and attach those to the followers to give hdfs more space?,2/1/16,1
lossyrob,"@jasonbeverage  what instances? if you_Ñére using the geotrellis-ec2-cluster stuff, it should automatically mount the available ephemeral storage to the mount points you mentioned. And should show up in your hdfs ui",2/1/16,1
lossyrob,with spark you have to specifically pass in the dirs into th spark-submit command,2/1/16,1
lossyrob,"e.g. `--conf spark.executorEnv.SPARK_LOCAL_DIRS=""/media/ephemeral0,/media/ephemeral1_Ñù`",2/1/16,1
jasonbeverage,"I configured it use a c4.large, I'm thinking that is the problem.",2/1/16,1
jasonbeverage,When I first started looking at geotrellis-ec2-cluster I didn't realize there was a fixed list of instance types in template_utils.py and I was trying to use a c4.large and it wasn't working.,2/1/16,1
jasonbeverage,So I changed template_utils.py to add c4.large but I'm thinking it might not have the ephemeral storage.,2/1/16,1
lossyrob,"Gotcha. Yeah we didn't set the scripts up yo work with EBS, since it would be significantly slower than ephemeral. Not to say you couldn't tweak it to use ebs",2/1/16,1
jasonbeverage,Yeah makes sense.  It's pretty awesome as is just being able to launch a working cluster :),2/1/16,1
jasonbeverage,"here is another question, I've been tinkering around in a vagrant box, and I was able to do an ingest directly from a local file system (not using hdfs) using something like this:",2/1/16,1
jasonbeverage," spark-submit \  --class geotrellis.chatta.ChattaIngest --driver-memory=2G $JAR \  --input hadoop --format geotiff --cache NONE -I path=file:/vagrant/myfile.tif \  --output accumulo -O instance=$INSTANCE table=$TABLE user=$USER password=$PASSWORD zookeeper=$ZOOKEEPER \  --layer mylayer --pyramid --crs $CRS --layoutScheme $LAYOUT_SCHEME",2/1/16,1
jasonbeverage,"So myfile.tif isn't on hdfs, it's just local.  It worked fine, but is the reason that it worked just b/c all of my virtual machines have the same /vagrant directory mounted or will hadoop somehow allow you to work with local files as well?",2/1/16,1
jasonbeverage,"I also copied over some data to hdfs and was able to do an import from hdfs has well, just by dropping the file:/ and giving the hdfs path.",2/1/16,1
lossyrob,"yeah, thank @hectcastro and @notthatbreezy for that :)",2/1/16,1
lossyrob,HDFS works with local files,2/1/16,1
lossyrob,the `file:` or `hdfs` (or even `s3n` or `s3a`) prefixes determing the file system type,2/1/16,1
lossyrob,so it_Ñés really the Hadoop IO API,2/1/16,1
lossyrob,that can read from HDFS among other things,2/1/16,1
jasonbeverage,"Oh ok, gotcha.  So hadoop isn't tied to hdfs then.",2/1/16,1
lossyrob,"nope, it has a pretty nice abstraction over IO (distributed or not)",2/1/16,1
jasonbeverage,"Cool, using i2.xlarge for the instance type and now I have access to 1.4TB of storage space.",2/1/16,1
lossyrob,nice,2/1/16,1
jasonbeverage,A little better than 20GB,2/1/16,1
lossyrob,haha that is true,2/1/16,1
lossyrob,"just make sure spark-submit knows to use all of that space for it_Ñés shuffle data, which can get big (using the `spark.executorEnv.SPARK_LOCAL_DIRS` setting)",2/1/16,1
jasonbeverage,"Ok, so that's something I setup when creating the spark context?",2/1/16,1
jasonbeverage,Or do I pass that in using spark-submit,2/1/16,1
lossyrob,you pass it into spark-submit,2/1/16,1
jasonbeverage,"Ok, like you mentioned before, got it.",2/1/16,1
jasonbeverage,"Ok, one more question and I will leave you alone for the day :)",2/1/16,1
lossyrob,"here_Ñés an unfiltered krufty command list that I was using for a demo recently, maybe you can glean some info off if it: https://github.com/lossyrob/landsat-demo/blob/master/run-server-commands.md",2/1/16,1
lossyrob,(vieweing it as raw instead of markdown would make a little more sense),2/1/16,1
jasonbeverage,When I first tried to do an ingest on a large image (I'm just playing with the bluemarble geotiff right now to test) the ingest failed b/c it said it was too big.,2/1/16,1
jasonbeverage,So I used gdal-retile to chop it up into smaller chunks like some of the examples mentioned.,2/1/16,1
jasonbeverage,"And I didn't realize that I could just point to the directory of tiles as the ""path"" parameter.  Is that just taking those tiles and creating a virtual raster source from them?",2/1/16,1
jasonbeverage,Is it expecting to have a naming scheme with columns and rows that gdalretile dumps out or could I stick any random geotiffs in a directory and point to it as input?,2/1/16,1
kevinzau,"@pomadchin Hi, I'm working on getting OSM data using GeoWave/GeoMesa. Rob mentioned you had a Docker with both. Is it possible I could use it / where would I find it?",2/1/16,1
lossyrob,"@jasonbeverage it just does a recursive listing of the files in the directory, and takes things that match the tiff extension(s)",2/1/16,1
lossyrob,and so the input RDD is a collection of those rasters,2/1/16,1
lossyrob,"If you specify one image as the path, then the source _ÑÒtiles_Ñù is an RDD with one element, the single GeoTiff you_Ñére passing in",2/1/16,1
lossyrob,"but if it_Ñés a directory, its an RDD with as many elements as there are geotiffs in the directory",2/1/16,1
jasonbeverage,"Ok, and when it goes to ingest that list of rdds as a whole it tiles them up a single dataset right?",2/1/16,1
jasonbeverage,Kind of like how you could use gdalbuildvrt to create a virtual mosaic of a directory of files.,2/1/16,1
lossyrob,"not a list of RDDs, but an RDD of tiles (an RDD is like a list)",2/1/16,1
lossyrob,"it_Ñés a bit like VRT. but VRTs you kind of consider one image, you can pass into something that should take only one image",2/1/16,1
lossyrob,RDDs of tiles we always consider as a collection of tiles,2/1/16,1
lossyrob,that_Ñés sort of just an algorithmic difference,2/1/16,1
lossyrob,"but so if we have our source tiles in an RDD, potentially overlapping, and want to cut the tiles to another layout (say TMS Web Mercator to serve out onto a web map), then that_Ñés an operation that happens on the source RDD that converts it to an indexed tile set RDD, which you can save off into one of our backends",2/1/16,1
lossyrob,"So a source tile set is a key-value RDD where the key is something like a `ProjectedExtent` or `SpaceTimeInputKey`, where the key information has things like the Extent, the CRS, and the time.",2/1/16,1
lossyrob,"The indexed tiled set is `SpatialKey` or `SpaceTimeKey`, which has tile coordinates like `SpatialKey(123,32)`, and potentially time if your dealing with time-based tiles",2/1/16,1
jasonbeverage,"Ok, things are starting to become a little clearer now.",2/1/16,1
lossyrob,awesome,2/1/16,1
jasonbeverage,"So when you tile up an input you really are creating different ""rasters"" at different zoom levels",2/1/16,1
lossyrob,yup,2/1/16,1
jasonbeverage,"Which is why I can see that I have LayerIds like LayerId(""layer"", 0) and LayerId(""layer"", 1) etc.",2/1/16,1
jasonbeverage,"Ok, very cool.",2/1/16,1
lossyrob,exactly,2/1/16,1
lossyrob,so the zoom levels have meaning when you_Ñére using ZoomedLayoutScheme,2/1/16,1
jasonbeverage,Yeah right.,2/1/16,1
lossyrob,"There is also FloatingLayoutScheme, which just taked the native resolution and CRS, and cuts it up into tiles of a uniform size",2/1/16,1
lossyrob,then we just call it zoom 0,2/1/16,1
lossyrob,"I_Ñém not super psyched on zoom being there even if it has little meaning, but that might be a design choice from the past we will live with through 0.10 :)",2/1/16,1
jasonbeverage,"So if you wanted to provide a service using geotrellis  where you could take a dataset and view it interactively, but also wanted to be able to do some numerical calculations on the dataset at full resolution you could still use the tiled layout.",2/1/16,1
lossyrob,exactly!,2/1/16,1
jasonbeverage,and if you needed to work at the full resolution you'd just use the highest resolution zoom level.,2/1/16,1
lossyrob,so we would do the tms zoomed web mercator tiles for the visual stuff,2/1/16,1
jasonbeverage,So do you have any support for sparse tilesets?,2/1/16,1
lossyrob,"but if we want accuracy, keep it in it_Ñés native resolution and CRS, and do aggregate computations, say averages over polygons, on that set",2/1/16,1
lossyrob,it all should work with sparse data sets,2/1/16,1
lossyrob,"the tile layout will be for a full coverage of the dense space, but you_Ñéll only have keys for the tiles you have",2/1/16,1
jasonbeverage,"Ok, they will just have missing tiles then where there is no data.",2/1/16,1
jasonbeverage,Yeah ok.,2/1/16,1
jasonbeverage,The tiles as rasters is a really neat abstraction.,2/1/16,1
jasonbeverage,So you can work with tiled datasets in the same way as you could with a full raster.,2/1/16,1
lossyrob,"thanks :) it_Ñés been a couple of years in the making, I think it_Ñés converging on something really generally useful",2/1/16,1
lossyrob,yeah. and the code for working with a set that has just one tile looks the same as a giant set that you_Ñéd have to run on a cluster with many nodes. so its a nice abstraction over interfacing with small and large datasets,2/1/16,1
jasonbeverage,Yeah very cool.,2/1/16,1
jasonbeverage,"Thanks again for your help, I'm doing an ingest on an aws cluster now.  Will let you know how it goes :)",2/1/16,1
lossyrob,"cool, np, yeah let us know!",2/1/16,1
owcm,@echeipesh  Thanks for the follow-up and referencing the TilingMethods .  I  finally had the SpatialComponent breakthough this weekend,2/1/16,1
pomadchin,"@kevinzau hi, planing today to throw a link here",2/2/16,1
lossyrob,https://quay.io/organization/azavea,2/2/16,1
echeipesh,next merge is: https://github.com/geotrellis/geotrellis/pull/1297,2/2/16,1
pomadchin,@echeipesh pushed review changes,2/2/16,1
echeipesh,"Cool, going to wait for CI just in case. I_Ñéve exceeded my allownace of breaking master for the week.",2/2/16,1
echeipesh,I think https://github.com/geotrellis/geotrellis/pull/1298 can go next. It has some conflicts and I need to give it a look ove  in a bit.,2/2/16,1
pomadchin,:+1: ,2/2/16,1
moradology,hey all: trying to get a sense of people's feelings on the question of nodata and its relationship to color specification when rendering a tile as png or jpg,2/3/16,1
moradology,"currently, nodata is not really a part of the colorbreaks object. instead, a nodatacolor is fed in as a part of the call to, for instance, `.renderPng`",2/3/16,1
moradology,"it's a bit unclear to me why that makes more sense than putting it on the breaks object. yes, it is an exceptional case, in that it is always only one value rather than a true 'break', but it also seems as though we should have a single something-or-other on which we specify the colors that a rendering should use",2/3/16,1
kevinzau,@pomadchin Thanks!,2/3/16,1
lossyrob,"@moradology to clarify my position on this: As it currently stands, ColorBreaks is a composition of two arrays: the breaks and the colors. This is a funky API and should be changed. As it stands, NODATA being part of those 2 arrays seems not right to me. NODATA color being part of the _ÑÒColorBreaks_Ñù, if by _ÑÒColorBreaks_Ñù it means the thing that holds the information about how to color the raster, then yes I think it should be a part of it. Even the name, _ÑÒColor Breaks_Ñù, implies some sort of breaking the range of values up and coloring those ranges. NODATA sort of exists outside of that. This is all to say, I think your intuition that there should be a thing that tells you how to color the raster, and it should account for the color ramp, the quantile breaks, and how you_Ñéd like to color NODATA. I just don_Ñét think our current representation of ColorBreaks has the correct view of the thing that you want NODATA to be a of.",2/3/16,1
moradology,whew. awesome. i definitely misunderstood your position on this,2/3/16,1
pomadchin,"@moradology @lossyrob can we define now a custom nodata value for a tile during it_Ñés rendering as png?sometimes it needs to be to colorised and somitimes to be transparent",2/3/16,1
moradology,"yeah, we can do that now - the question is just more about where and how that should be done",2/3/16,1
lossyrob,"@moradology cool. thanks for bringing it up. I am interested in hearing what other people have to say about how the coloring/rendering API can be most useful, so if you have an opinion at all please pipe up!",2/3/16,1
moradology,"the notion of a 'break' is just an upper or lower (at this point, i think almost universally upper) bound on values which get a specified color",2/3/16,1
lossyrob,"yeah, which makes it fast in the implementation, because we_Ñére just doing a search through a sortd array that breaks on a value whenever a bound is found",2/3/16,1
jasonbeverage,So I'm trying to do an ingest directly from s3 using an s3a path and it appears to be getting hung.,2/3/16,1
jasonbeverage,I'm doing something like this:,2/3/16,1
jasonbeverage,"spark-submit \  --class geotrellis.chatta.ChattaIngest --driver-memory=2G $JAR \  --input hadoop --format geotiff --cache NONE -I path=""s3a://mybucket/mytifs"" \  --output accumulo -O instance=$INSTANCE table=$TABLE user=$USER password=$PASSWORD zookeeper=$ZOOKEEPER \  --layer mylayer --pyramid --crs $CRS --layoutScheme $LAYOUT_SCHEME",2/3/16,1
jasonbeverage,"I only have one file in there, and it finds it, but then the last line I see when I run spark-submit is like this:",2/3/16,1
jasonbeverage,16/02/03 18:08:31 INFO rdd.NewHadoopRDD: Input split: s3a://mybucket/mytifs/bluemarble_merc_small.tif:0+8428551,2/3/16,1
jasonbeverage,And it's just hung there.  Any ideas?  I have the IAM role setup and can use s3a via the hadoop fs commands and it all appears to work.,2/3/16,1
jasonbeverage,"Ok perhaps I'm wrong, it looks like it worked, it just hung for a very long time on that file before it did anything.  It's just a single 8mb tif so I expected it to just blaze through.",2/3/16,1
echeipesh,@jasonbeverage: we_Ñéve seen hadoop S3 implemintations do all manner of funky things that often manifests as unusually long delays in listing. We made our own S3 format to have more controll of the matter. You can use that spark submit but change input section to `_ÑÓinput s3 _ÑÓformat geotiff -I bucket=mybucket key=mytifs` and it should work more predictably.,2/3/16,1
jasonbeverage,Thanks @echeipesh .  Where do I set my credentials for that to work?,2/3/16,1
echeipesh,"That uses the aws s3 java client, so the usual places should work.",2/3/16,1
jasonbeverage,"Ok thanks, I'll look into that.",2/3/16,1
echeipesh,relevant docs: http://docs.aws.amazon.com/AWSSdkDocsJava/latest/DeveloperGuide/credentials.html,2/3/16,1
jasonbeverage,"Perfect, thanks.  That's what I was looking at.",2/3/16,1
jasonbeverage,I assume that I'd need to set the credentials on all of the nodes in the cluster right?  And not just on the leader.,2/3/16,1
echeipesh,Actually just the leader. We will include the resolved token in the generated hadoop slits which get shipped to the workers.,2/3/16,1
jasonbeverage,Ok great.,2/3/16,1
jasonbeverage,And that doesn't work with IAM roles out of the box right?,2/3/16,1
echeipesh,"An even better production way to do it is to setup IAM roles, this is something that S3 Input Format will check.",2/3/16,1
echeipesh,"It does, thats the other reason we wrote it. Hadoop s3 client specifically ignores IAM roles, requeiring keys in hdfs-site.xml",2/3/16,1
jasonbeverage,"Yeah ok, I have an IAM role setup, but it didn't appear to work with the --input s3",2/3/16,1
jasonbeverage,"Which I why I looked at the s3a file system, which did appear to work with iam roles.",2/3/16,1
jasonbeverage,"maybe it's not an IAM role issue, I'm getting an exception like this:",2/3/16,1
jasonbeverage,"Exception in thread ""main"" java.lang.NoClassDefFoundError: org/joda/time/DateTimeZone	at com.amazonaws.auth.InstanceProfileCredentialsProvider.loadCredentials(InstanceProfileCredentialsProvider.java:166)	at com.amazonaws.auth.InstanceProfileCredentialsProvider.getCredentials(InstanceProfileCredentialsProvider.java:72)	at com.amazonaws.auth.AWSCredentialsProviderChain.getCredentials(AWSCredentialsProviderChain.java:101)",2/3/16,1
jasonbeverage,So perhaps I don't have that dependency included in my jar.,2/3/16,1
echeipesh,Odd that should be part of the assembly,2/3/16,1
jasonbeverage,It's highly possible that I did something wrong when building my ingest assembly.  I started from @pomadchin spark version of the chatta demo and used that ingest command.,2/3/16,1
jasonbeverage,"Very likely I'm doing something wrong, I'm very new to geotrellis and scala in general :)",2/3/16,1
pomadchin,"@jasonbeverage yo! I am sure, I tried absolutely clean environment to prevent dependency issues (my local artefacts) ",2/3/16,1
pomadchin,O:,2/3/16,1
pomadchin,so this error appears in chatta-demo project?,2/3/16,1
jasonbeverage,"I'm not using your project directly, I created my own test project based on yours.",2/3/16,1
jasonbeverage,"So in theory I should be able to get the geotrellis code, run publish-local.sh to build and copy over the jars, and then use your project and run ./sbt assembly to create a working assembly right?",2/3/16,1
jasonbeverage,"I don't see org.joda.time included anywhere in your dependencies, should I have to add that manually or does it automatically get picked up from somewhere?",2/3/16,1
pomadchin,"you can try, but this err seems weird",2/3/16,1
pomadchin,"i haven_Ñét tried to make S3 ingest, and with Accumulo there was no such error",2/3/16,1
jasonbeverage,"Yeah, I only get it when I'm doing anything with s3",2/3/16,1
jasonbeverage,Going from a local file or from hdfs into Accumulo works fine.,2/3/16,1
pomadchin,I think that there _may be_ joda time versions issues,2/3/16,1
jasonbeverage,"This is a very old change, but I see here where some joda jars were removed and jets3t jars were added:  https://github.com/geotrellis/geotrellis-ec2-cluster/commit/aa8fc272908baf5317071cce0101614aec5edb0c",2/3/16,1
pomadchin,"we use `""com.github.nscala-time"" %% ""nscala-time"" % _ÑÒ2.6.0""`",2/3/16,1
pomadchin,"hm, dunno mb that_Ñés an issue where you pointed ._.",2/3/16,1
jasonbeverage,"Yeah like I said, I'm not very familiar with scala, so I'm a bit lost when issues like this come up :)  ",2/3/16,1
pomadchin,"think yes, there can be also a difference in joda time version used in geotrellis and used in aws client",2/3/16,1
jasonbeverage,That might very well be it.,2/3/16,1
jasonbeverage,So is com.github.nscala-time some replacement for joda time?,2/3/16,1
pomadchin,it_Ñés a wrapper for joda time,2/3/16,1
pomadchin,"so you can try to find out what joda time version used in aws client, exclude it from geotrellis, and to add as a dep explicitly of a necessary version",2/3/16,1
pomadchin,it seems like they use `2.8.1`,2/3/16,1
jasonbeverage,"Ok, thanks.  I'll give that a try.",2/3/16,1
lossyrob,@jasonbeverage I hit some dependency troubles along these lines at some point,2/3/16,1
jasonbeverage,And it looks like nscala-time 2.6.0 is using joda-time 2.9.1,2/3/16,1
lossyrob,https://github.com/lossyrob/geotrellis-ec2-cluster/tree/2016-01-demo,2/3/16,1
lossyrob,I_Ñéve been working off of a branch,2/3/16,1
lossyrob,not sure this is the same issue,2/3/16,1
lossyrob,"but what can happen is, Hadoop jars conflict version wise with what_Ñés in the assembly",2/3/16,1
lossyrob,and spark needs all the supplied hadoop jars to function,2/3/16,1
lossyrob,so doing things like this https://github.com/lossyrob/geotrellis-ec2-cluster/commit/04ef2a2999f12a78b5e8f1361e5a12de377c6ce3#diff-337ff348629489ac63a3a62f0b4291b1R10,2/3/16,1
lossyrob,but I thought we deleted Joda out a while ago,2/3/16,1
lossyrob,it might be the conflict with the AWS sdk,2/3/16,1
jasonbeverage,I saw that change was way back at the beginning of 2015,2/3/16,1
jasonbeverage,"I also installed awscli on the leader, I wouldn't expect that to mess with anything but you never know.",2/3/16,1
lossyrob,https://repo1.maven.org/maven2/com/amazonaws/aws-java-sdk/1.7.4/aws-java-sdk-1.7.4.pom,2/3/16,1
lossyrob,"the aws sdk that is in the hadoop client jars is way old, and conflicts with the one we are using, and the joda we are using with it",2/3/16,1
lossyrob,"so, unfortunately_Ñ_I think the solution is to build new AMI_Ñés off of my branch",2/3/16,1
jasonbeverage,Yeah no worries.,2/3/16,1
lossyrob,another path you can take is to use EMR,2/3/16,1
lossyrob,"if I_Ñém not using Accumulo, I always go with EMR",2/3/16,1
lossyrob,because of stuff like this :),2/3/16,1
jasonbeverage,At least I know now that your branch even exists and I should be looking at that :),2/3/16,1
jasonbeverage,I'm specifically looking at Accumulo right now so the geotrellis-ec2-cluster project has been very helpful.,2/3/16,1
lossyrob,"yeah, I_Ñéve been meaning to get to merging those changes. gotta catch up with that",2/3/16,1
lossyrob,oh gotcha,2/3/16,1
jasonbeverage,"I've also not done anything with EMR yet, but would definitely be interested in checking it out.   Is there a geotrellis EMR quickstart somewhere?",2/3/16,1
lossyrob,this is a project that I_Ñéve been using to do PySpark reprojection of tiles on EMR against S3. It would start an EMR cluster taht you could also use to run an uploaded scala jar: https://github.com/lossyrob/emr-reproject,2/3/16,1
lossyrob,"One of the things I am working on (in the midst of all the other things, of course) is a tutorial for grabbing and doing operations on landsat, that will show how to do that using EMR and also the cluster stuff. So this stuff is currently being worked out in documented form",2/3/16,1
jasonbeverage,"Awesome, thanks for the pointers.",2/3/16,1
jasonbeverage,"I'll try your branch and see how it goes, and then also check out your emr project.",2/3/16,1
jasonbeverage,"Also I noticed that my AMI's had aws-java-sdk-1.7.14.jar, not aws-java-sdk-1.7.4.jar",2/3/16,1
jasonbeverage,don't know if that was a typo in your roles or if something was different for me for some reason.,2/3/16,1
lossyrob,"hm, not sure. Hopefully that doesn_Ñét make that step fail to clear out those jars",2/3/16,1
jasonbeverage,Rob you're the man.  I deleted those jars and it looks like it's working now.,2/3/16,1
jasonbeverage,"No more exception, and that little 8mb tile test worked.",2/3/16,1
lossyrob,nice!,2/3/16,1
pomadchin,cool,2/3/16,1
jasonbeverage,"I'm trying the same ingest on a bunch of bluemarble tiles I made, it appears to be working but does seem to be chugging along pretty slowly.",2/3/16,1
jasonbeverage,Just based on how much output spark-submit is giving me.,2/3/16,1
jasonbeverage,I tried the same import against the tiles copied into hdfs and it seemed pretty fast.,2/3/16,1
jasonbeverage,"I've got plenty to play with now though, thanks again for your help!",2/3/16,1
lossyrob,"not sure about the speed; HDFS is always going to be faster (since it_Ñés on disk/on the worker node that is doing the computation), but I haven_Ñét found that it_Ñés *too* much slower",2/3/16,1
lossyrob,There_Ñés the _ÑÒsplitSize_Ñù argument in spark-etl that controls how many tiles are being pulled down per partition. might want to play around with that,2/3/16,1
jasonbeverage,"Ok, I wonder if that has anything to do with it.",2/3/16,1
jasonbeverage,I'm not setting it and I've got like ~4K tiles,2/3/16,1
lossyrob,"not sure what it defaults to, @echeipesh would know better",2/3/16,1
lossyrob,"but that_Ñés a point of contention for me usually_Ñ_.I want to bring down tiles in large groups because that_Ñés faster access to S3, but also I don_Ñét want my partitions to be too large or else I_Ñéll get out of memory during tiling",2/3/16,1
lossyrob,so one thing (inside a manual ingest) is pull down 100 tiles at a time and then repartition to some magic number,2/3/16,1
lossyrob,still working out how we can make that more configurable/less of guess work,2/3/16,1
jasonbeverage,"Ha, you lost me at repartition :)",2/3/16,1
jasonbeverage,I'll dig into the split size and see if I can understand what might be a good number.  ,2/3/16,1
lossyrob,ah gotcha,2/3/16,1
echeipesh,"If not specified it_Ñéll use the aws SDK default for key listing, which is 1000 keys.",2/3/16,1
lossyrob,"ok. jeesh, that_Ñéll do it",2/3/16,1
lossyrob,so you have 4 big partitions,2/3/16,1
jasonbeverage,"Yeah, and it's like 13GB of imagery.",2/3/16,1
lossyrob,"so that makes sense. bring that down a ways, maybe 50?",2/3/16,1
echeipesh,We haven_Ñét looked at that part for a while. Do you think we should do something like a sample and guess at optimal partition sizes ?,2/3/16,1
lossyrob,"yeah, or maybe guess a bit lower. Shouldn_Ñét we be using the default partition number to check it out? We know the total estimate number of elements because of the client call, or no?",2/3/16,1
lossyrob,yeah because we bin it_Ñ_so we should at least bin to default partitions (so it wouldn_Ñét be 4 in this case),2/3/16,1
echeipesh,Not sure why this is the first time I_Ñém thinking this _Ñ_ but we know the size of the keys when we list them. So we can do a really good job of keeping partition size good without any extra interactions with S3.,2/3/16,1
lossyrob,the assumption we have to make/allow control over is the size of the data elements,2/3/16,1
lossyrob,"but if we just assume some defaults that are ok, I think we_Ñéd avoid the problems that @jasonbeverage is having",2/3/16,1
echeipesh,"Yep. We_Ñére going to be off by some constant factor that accounts for compression ratio. But other than that number of bits in and out should be the same, regardless of how we tile it.",2/3/16,1
jasonbeverage,"Ok, with splitSize 50 it's at least moving along now.",2/3/16,1
lossyrob,a good thing to target is to have task completion at around 500ms.,2/3/16,1
lossyrob,"That_Ñés ideal, and I usually don_Ñét get that with ingests",2/3/16,1
lossyrob,"I do with operations on ingested data, though",2/3/16,1
lossyrob,but just a good thing to note as a target,2/3/16,1
echeipesh,"I_Ñéve heard that given, what is the reasoning behind 500ms ?",2/3/16,1
lossyrob,"Well, mainly because that_Ñés the what the databricks peoples said :)",2/3/16,1
echeipesh,"Let it be written, let it be said :)",2/3/16,1
lossyrob,"but i think it has to do with maximizing parallelization, while not having tasks too fast that the scheduling causes inefficiency",2/3/16,1
lossyrob,and it_Ñés a rule of thumb that seems to have worked on brining down execution times for me,2/3/16,1
echeipesh,balance between parallelism and scheduler thrashing makes sense to me,2/3/16,1
pomadchin,"https://github.com/pomadchin/geo-envTODO:   * write install & run examples guide   * check out what_Ñés with GeoWave on Accumulo 1.7.0  * Readmes style fix ._. @moradology   * All in all repo style fix   ",2/4/16,1
pomadchin,@kevinzau,2/4/16,1
moradology,"@pomadchin i put up a couple PRs with some minor style/word-choice changes. otherwise, it looks great. simple and to the point",2/4/16,1
pomadchin,@moradology awesome that you had a look,2/4/16,1
pomadchin,:+1:,2/4/16,1
pomadchin,O: awesome! it looks now like a readme,2/4/16,1
dwins,"```[info] CoordinateTransformTest:[info] - First[info] - FAIL_testEPSG_27700 !!! IGNORED !!![info] - EPSG_23031[info] - Amersfoort_RD_New[info] - PROJ4_SPCS_NAD27[info] - PROJ4_SPCS_NAD83[info] - LambertConformalConic[info] - Pconic[info] - Robinson[info] - StereographicAzimuthal[info] - UTM[info] - Mercator *** FAILED ***[info]   false was not true (BaseCoordinateTransformTest.scala:36)[info] - Sterea[info] - AlbersEqualArea[info] - LambertAzimuthalEqualArea[info] - SwissObliqueMercator[info] - EPSG_4326[info] - Params[info] - PROJ4[info] - PROJ4_LargeDiscrepancy[info] - XtestUndefined !!! IGNORED !!!```",2/4/16,1
dwins,"I only put in one grid file so mercator may be the only test case that loads a defined grid, I haven't reviewed the tests yet.",2/4/16,1
pomadchin,"that_Ñés an accumulo property to define class path (Accumulo ClassLoader (Apache VFS) works just like magic): https://github.com/pomadchin/geo-env/blob/master/single-node/cfg/accumulo/1.7.0/accumulo-site.xml#L36-L40",2/4/16,1
pomadchin,"in addition it is possible to add ay iterators just for a namespace: ```shhadoop fs -copyFromLocal /data/geowave/deploy/target/geowave-deploy-0.9.0-SNAPSHOT-accumulo-singlejar.jar /accumulo/classpath/geowave/# already in accumulo shellconfig -s general.vfs.context.classpath.geowave=hdfs://localhost:9000/accumulo/classpath/geowave/[^.].*.jarconfig -ns geowave -s table.classpath.context=geowave```",2/4/16,1
echeipesh,That_Ñés neat,2/4/16,1
lossyrob,"conf.set(""spark.kryo.registrationRequired"", ""true"")",2/4/16,1
echeipesh,"I_Ñéve tried that, always a bad time.",2/4/16,1
lossyrob,kryo.setRegistrationRequired(true),2/4/16,1
dwins,"```[info] CoordinateTransformTest:[info] - First[info] - FAIL_testEPSG_27700 !!! IGNORED !!![info] - EPSG_23031[info] - Amersfoort_RD_New[info] - PROJ4_SPCS_NAD27[info] - PROJ4_SPCS_NAD83[info] - LambertConformalConic[info] - Pconic[info] - Robinson[info] - StereographicAzimuthal[info] - UTM[info] - Mercator[info] - Sterea[info] - AlbersEqualArea[info] - LambertAzimuthalEqualArea[info] - SwissObliqueMercator[info] - EPSG_4326[info] - Params[info] - PROJ4[info] - PROJ4_LargeDiscrepancy[info] - XtestUndefined !!! IGNORED !!!```",2/4/16,1
dwins,:sparkles: ,2/4/16,1
lossyrob,:metal: ,2/4/16,1
dwins,found two bugs - 1: i swapped x and y. 2: missed a place where copy-vs-reference semantics changed the behavior of a method,2/4/16,1
dwins,"i posted a WIP PR for the gridshift work.  i think it still wants some refactoring and there is a test failure in geotrellis-vector, but if anyone wants to try it out it is there.",2/4/16,1
pomadchin,"found out a curious bug today, fighting with Accumulo settings (just have no idea why it workd before), so the *correct* property settings to load jars from hdfs dynamically (via vfs) are:```xml  <property>    <name>general.vfs.classpaths</name>    <value>hdfs://master1.gt:9000/accumulo/system-classpath/[^.].*.jar</value>    <description>Configuration for a system level vfs classloader. Accumulo jars can be configured here and loaded out of HDFS.</description>  </property>  <property>    <name>general.vfs.cache.dir</name>    <value>/tmp</value>    <description>Directory to use for the vfs cache. The cache will keep a soft reference to all of the classes loaded in the VM.      This should be on local disk on      each node with sufficient space. It defaults to /tmp and will use a directory with the      format ""accumulo-vfs-cache-"" + System.getProperty(""user.name"",      ""nouser"")    </description>  </property>  <property>    <name>general.dynamic.classpaths</name>        <value>      $ACCUMULO_HOME/lib/ext/[^.].*.jar,      hdfs://master1.gt:9000/accumulo/system-classpath/[^.].*.jar    </value>    <description>Classpaths that accumulo checks for updates and class files.</description>      </property>```",2/5/16,1
lossyrob,what were they before?,2/5/16,1
pomadchin,"without `general.dynamic.classpaths`by default it_Ñés set as `$ACCUMULO_HOME/lib/ext/[^.].*.jar`",2/5/16,1
lossyrob,were we loading any dynamic jars before?,2/5/16,1
lossyrob,from hdfs,2/5/16,1
lossyrob,"now that I see it, we should be making `general.vfs.cache.dir` point to the ephemeral storage on our `geotrellis-ec2-cluster` instances",2/5/16,1
jasonbeverage,Hey @lossyrob I started using your geotrellis-ec2-cluster branch to get around those AWS issues.,2/5/16,1
jasonbeverage,I noticed that you also updated spark to 1.5 and I ran into an issue with that just running the spark-shell command that is in the README just to see if spark is running.,2/5/16,1
lossyrob,rut roh,2/5/16,1
lossyrob,what are you running into?,2/5/16,1
jasonbeverage,"When I try to run that example it splits up the task into 8 jobs, and then submits them.",2/5/16,1
jasonbeverage,And then it waits for a minute and I get lots of errors like this:,2/5/16,1
jasonbeverage,Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources ,2/5/16,1
jasonbeverage,And it just keeps spitting that out until I kill it.,2/5/16,1
lossyrob,sounds like mesos isn_Ñét happy,2/5/16,1
jasonbeverage,I googled around a bit and found this issue:,2/5/16,1
jasonbeverage,http://apache-spark-user-list.1001560.n3.nabble.com/Spark-1-5-1-Dynamic-Resource-Allocation-td25275.html,2/5/16,1
lossyrob,what_Ñés port 5050 saying as far as available workers?,2/5/16,1
jasonbeverage,If you run spark-shell like this then it works:  ,2/5/16,1
jasonbeverage,spark-shell --conf spark.dynamicAllocation.minExecutors=1 --conf spark.dynamicAllocation.initialExecutors=2 --conf spark.dynamicAllocation.maxExecutors=2 --conf spark.dynamicAllocation.sustainedSchedulerBacklogTimeout=120 --conf spark.dynamicAllocation.schedulerBacklogTimeout=300 --conf spark.dynamicAllocation.executorIdleTimeout=60 --executor-memory 512m --master mesos://zk://zookeeper.service.geotrellis-spark.internal:2181/mesos,2/5/16,1
jasonbeverage,I'm still super new to spark so lots of that is gibberish to me :),2/5/16,1
jasonbeverage,But the gist I got from that bug report is that something is wrong with 1.5 with related to the # of executors.  You have to specify them up front with minExecutors < initialExecutors < maxExecutors,2/5/16,1
jasonbeverage,This is the actual bug report:  https://issues.apache.org/jira/browse/SPARK-10790,2/5/16,1
lossyrob,this is good stuff. so your saying that `spark-shell --master mesos://zk://zookeeper.service.geotrellis-spark.internal:2181/mesos` is failing to allocate any resources,2/5/16,1
jasonbeverage,Yeah exactly.,2/5/16,1
lossyrob,which shouldn_Ñét use dynamic allocation,2/5/16,1
jasonbeverage,"Once you run  distData.filter(_< 10).collect() it just keeps dumping out that ""Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources"" message",2/5/16,1
lossyrob,"well, usually I set the executors explicitly in a spark submit call",2/5/16,1
lossyrob,you have a cluster up and running?,2/5/16,1
jasonbeverage,"Yeah I should, one sec.  Let me hop on my other machine.",2/5/16,1
jasonbeverage,Yeah it's up.,2/5/16,1
lossyrob,i feel like this should be working,2/5/16,1
lossyrob,"```spark-shell \--master mesos://zk://zookeeper.service.geotrellis-spark.internal:2181/mesos \--conf spark.executorEnv.SPARK_LOCAL_DIRS=""/media/ephemeral0,/media/ephemeral1"" \--conf spark.driver.cores=4 \--conf spark.driver.memory=4g \--conf spark.executor.memory=12g \--conf spark.mesos.coarse=true```",2/5/16,1
lossyrob,sorry,2/5/16,1
lossyrob,spark-shell,2/5/16,1
lossyrob,well what instances are you using,2/5/16,1
jasonbeverage,I'm just doing a vagrant up now.,2/5/16,1
lossyrob,oh oh,2/5/16,1
lossyrob,gotcha. nevermind,2/5/16,1
lossyrob,don_Ñét try to run that :),2/5/16,1
jasonbeverage,So just make those #'s smaller probably :),2/5/16,1
lossyrob,a lot smaller,2/5/16,1
lossyrob,ha,2/5/16,1
lossyrob,but i think coarse mode is default,2/5/16,1
lossyrob,and the other stuff talks about the driver,2/5/16,1
lossyrob,so I_Ñém not sure_Ñ_I_Ñém pretty sure I was successfuly runing a shell on an AWS cluster using this branch,2/5/16,1
lossyrob,without having to do dynamic allocation,2/5/16,1
lossyrob,something to test against next time I have one fired up for sure,2/5/16,1
jasonbeverage,"Yeah like I said, very new to spark so not sure where the issues actually are :)",2/5/16,1
jasonbeverage,let me try just specifying the resources up front like you showed.,2/5/16,1
lossyrob,well way to hunt down the issue and get a workaround at least,2/5/16,1
jasonbeverage,I don't know spark but I'm good at google.  Counts for something :),2/5/16,1
lossyrob,"that issue is fixed in 1.5.2, problem is we rely on cloudera ditros of spark",2/5/16,1
lossyrob,haha,2/5/16,1
jasonbeverage,"Yeah right, I saw that.",2/5/16,1
jasonbeverage,So mesos reports 4 available cores and 2gb of memory available.,2/5/16,1
jasonbeverage,"Ok, so your command worked fine.",2/5/16,1
jasonbeverage,With reduced values of course.,2/5/16,1
lossyrob,"i wonder how many executors, or what the executor memory it wants is. Because if mesos tries to schedule a spark executor, and it wants more memory than available in that slot, it won_Ñét schedule it",2/5/16,1
lossyrob,and you_Ñéll see that behavior,2/5/16,1
jasonbeverage,Yeah that's what I'm thinking.  I wonder what the defaults are.,2/5/16,1
lossyrob,usually I specify everything explicitly based on the instances I have,2/5/16,1
lossyrob,"not sure, but that might be the trick",2/5/16,1
jasonbeverage,"Yeah don't know, I requested 16 cores (which I obviously don't have) and it still worked.",2/5/16,1
jasonbeverage,It might just be an issue of setting *something*,2/5/16,1
lossyrob,"yeah. If you request 16, and it can give you 4, it_Ñéll give you 4. But if you request 4, and each of those 4 needs more memory then mesos is willing to give 1, then it_Ñéll schedule none",2/5/16,1
lossyrob,I also get confused between mesos and yarn because I_Ñéve debugged these issues on both and can_Ñét keep straight what system needs what,2/5/16,1
lossyrob,"YARN we use on EMR, and also some docker containers that @pomadchin_Ñés working on",2/5/16,1
jasonbeverage,"Ok, so just trying a couple of things.  It looks like it's the memory",2/5/16,1
jasonbeverage,"If I don't specify a spark.driver.memory and the spark.executor.memory then I get that ""Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources"" issue.",2/5/16,1
jasonbeverage,Easy enough workaround.,2/5/16,1
lossyrob,"yeah, something is going on with the defaults. What are you setting it to? We_Ñéll have to update the README",2/5/16,1
jasonbeverage,cores=2 and 512m for both the executor and driver works.,2/5/16,1
jasonbeverage,4 cores also works.,2/5/16,1
jasonbeverage,So will cores = the number of executors then?,2/5/16,1
jasonbeverage,So 4 executors at 512 each should be the limit with the amount of memory allocated by default.,2/5/16,1
lossyrob,cores are how many cores are given to ach executor,2/5/16,1
lossyrob,"so if you have 1 node with 4 cores, and you set cores=1, then there will be 4 executors per node",2/5/16,1
jasonbeverage,And each executor will have 512mb (or whatever you specified) mb of ram available.,2/5/16,1
lossyrob,"yeah. so if you have 512mb, 1 core, then the node will want 2048mb for executor usage. If you have 2 core, it_Ñéll need 1024 for executor usage",2/5/16,1
jasonbeverage,"Oh, so if you don't specify the number of executors will it just use all that is available?",2/5/16,1
jasonbeverage,Or I'm sure there is some default for the # of executors.,2/5/16,1
lossyrob,"Well, with coarse grained mode Mesos, I would expect the # of executors to default to the number of nodes. But I_Ñém not totally sure TBH",2/5/16,1
schBen,"How do I read in Landsat GeoTiffs from the public Landsat S3 as RDDs? I use sc.hadoopGeoTiffRDD(path) to read in local Landsat Geotiff files. I can pass an s3n path to this method, but that seems to take extremely long. How should I do it instead?",2/8/16,1
lossyrob,"are you listing the individual landat paths? You can use a geotrellis-custom input format for that, an example is here: https://github.com/lossyrob/landsat-demo/blob/master/ingest/src/main/scala/demo/NEXIngest.scala#L160",2/8/16,1
lossyrob,I have to clean up that repo but soon (hopefully sooner rather than later) I_Ñéll have an example of reading landsat off of S3. But that_Ñés the InputFormat I_Ñéll be using,2/8/16,1
lossyrob,landsat is weird because you_Ñéll have to read the MTL file beforehand to get the date,2/8/16,1
lossyrob,"I_Ñém using this code to read it off the file system, have to modify it for S3 https://github.com/lossyrob/landsat-demo/blob/master/ingest/src/main/scala/demo/MTL.scala",2/8/16,1
schBen,"Okay thank you, I will have a look into that. Yes it would be really great to have an example of Landsat S3 sometime!! :-)",2/8/16,1
jamesmcclain,"@echeipesh We talked about it in standup, but can you reiterate what you idea that you had for a general strategy to require kryo registration during tests but not during normal operation?",2/8/16,1
pomadchin,"@lossyrob @jamesmcclain @moradology @echeipesh a small question: what do you thing about this env name (mb in a ~spare time)? https://github.com/pomadchin/geo-envnow it is geo envmb smth like: * GeoDocker* GeoCluster * GGG (:D)",2/8/16,1
pomadchin,sry for long (slow) mentioning (just smth weird with my gitter client),2/8/16,1
jamesmcclain,"I personally think that the word ""Docker"" should be in the name",2/8/16,1
jnh5y,"@pomadchin, hola, shout if you have any questions about doing anything with GeoMesa!",2/8/16,1
jnh5y,(we've got gitter at locationtech/geomesa),2/8/16,1
pomadchin,"@jnh5y ye, ty :D already shouted, seems like everything works now with a test cli ingest sample (from geowave docs)",2/9/16,1
pomadchin,accumulo was moved from core (since 0.8.9) that caused unexpected (for me) geowave jars behaviour,2/9/16,1
jnh5y,"@pomadchin, by the way, here's a quick PR for updating some GeoMesa links in the geo docker project: https://github.com/pomadchin/geo-env/pull/4",2/9/16,1
pomadchin,"@jnh5y merged, cool!",2/9/16,1
pomadchin,"btw guys, @jnh5y works on geomesa, so feel myself like a dumb ):",2/9/16,1
jamesmcclain,"```scala  def createLocalSparkContext(    sparkMaster: String,    appName: String,    sparkConf: SparkConf = createSparkConf,    testContext: Boolean = false  ) = {    val testRegistrator = if (testContext) ""geotrellis.spark.io.hadoop.TestRegistrator,"" ; else """"    val normalRegistrator = ""geotrellis.spark.io.hadoop.KryoRegistrator""    sparkConf      .setMaster(sparkMaster)      .setAppName(appName)      .set(""spark.serializer"", ""org.apache.spark.serializer.KryoSerializer"")      .set(""spark.kryo.registrator"", testRegistrator + normalRegistrator)    new SparkContext(sparkConf)  }```",2/9/16,1
jamesmcclain,"```[info] UnEqual Operation[info] - should check unEqual between an integer and a raster *** FAILED ***[info]   org.apache.spark.SparkException: Job aborted due to stage failure: Task serialization failed: org.apache.spark.SparkException: Failed to register classes with Kryo[info] org.apache.spark.serializer.KryoSerializer.newKryo(KryoSerializer.scala:101)...```",2/9/16,1
owcm,"Hello I am trying to read in a NaturalLandCover  Geotiff  using SingleBandGeoTiff.  I get error Message: Unsupported band type (4, 1) Trace: geotrellis.raster.io.geotiff.BandType$.apply(BandTypes.scala:31).   GDAL_INFO reports tif as Image Structure Metadata  NBITS=4.  Should I be able to use GDAL to translate to 8bit. Thanks in advance.  Interesting enough QGIS says file is defined as  Data TypeByte - Eight bit unsigned integer",2/9/16,1
lossyrob,"right, we don_Ñét support NBITS other that byte rasters with NBITS=1. So you_Ñéd haveto use gdal_translate like you are suggesting",2/9/16,1
lossyrob,or hack on the geotiff reader until it can handle those cases :),2/9/16,1
owcm,@lossyrob  thanks,2/9/16,1
lokifacio,"Hi, I'm having some trouble while assembling geotrellis. When I run `./sbt assembly` on a fresh master clone I get a bunch of the following erros:```[error] (geotrellis/*:assembly) deduplicate: different file contents found in the following:[error] /home/bigdata/.ivy2/cache/com.esotericsoftware.kryo/kryo/bundles/kryo-2.21.jar:com/esotericsoftware/minlog/Log$Logger.class[error] /home/bigdata/.ivy2/cache/com.esotericsoftware.minlog/minlog/jars/minlog-1.2.jar:com/esotericsoftware/minlog/Log$Logger.class```I've also tried removing ~/.ivy2 to start with a clean enviroment but the output was the same.Any suggestions?",2/10/16,1
pomadchin,"@lokifacio good that you noticed it, but why do you need geotrellis assembly?",2/10/16,1
lokifacio,"@pomadchin  I was trying to ingest a tiff using this [script](https://gist.github.com/lossyrob/59f8116b07d37f7f45c5). Just running  `./sbt ""project spark"" assembly` worked fine but I realized that current master branch doesn't include HadoopIngestCommand. I found it at [gt-admin](https://github.com/geotrellis/gt-admin) and following the Readme I run the `./sbt assembly` on geotrellis",2/10/16,1
lokifacio,I'm new to GIS/geotrellis so maybe I'm missing some key point,2/10/16,1
lokifacio,Which is the best way to ingest a single tiff in hdfs so I can load it afterwards as a RasterRDD?,2/10/16,1
pomadchin,"we don_Ñét have cli, but you can ingest tiles using [spark-etl](https://github.com/geotrellis/geotrellis/tree/master/spark-etl) project; * make a new poject with `geotrellis-spark` as a dep* ingest tilesthough, there is a working [example](https://github.com/pomadchin/geotrellis-chatta-demo/blob/spark-version/geotrellis/src/main/scala/geotrellis/chatta/ChattaIngest.scala)",2/10/16,1
pomadchin,"actually you don_Ñét need to make assembly, the way to use master branch locally is to publish artefacts to a local repository, to do it you can use [publish-local.sh](https://github.com/geotrellis/geotrellis/blob/master/publish-local.sh)",2/10/16,1
pomadchin,it publishes geotrellis as a `0.10.0-SNAPSHOT`,2/10/16,1
lokifacio,Thank you. I'll try using spark-etl,2/10/16,1
lokifacio,Do I need to have accumulo running on my cluster? Or having hadoop-hdfs is enough to work with geotrellis?,2/10/16,1
pomadchin,"ye, you can use just hdfs",2/10/16,1
pomadchin,there is an ingest script sample: https://github.com/pomadchin/geotrellis-chatta-demo/blob/spark-version/geotrellis/ingest.sh,2/10/16,1
pomadchin,"```--input hadoop --format geotiff --cache NONE -I path=$INPUT \  --output accumulo```just replace `accumulo` output with `hadoop` output",2/10/16,1
pomadchin,required key for `hadoop` output is `path`,2/10/16,1
pomadchin," ```bash --input hadoop --format geotiff --cache NONE -I path=$INPUT \--output hadoop -O path=$OUTPUT ```",2/10/16,1
lokifacio,"thanks, that worked using geotrellis-chatta-demo assembled jar",2/10/16,1
moradology,https://github.com/geotrellis/geotrellis/pull/1271/files#diff-b52f8c0a747c2c806808bb5ad3b2eb37R6,2/10/16,1
jasonbeverage,"Spark question for you gurus.  Specifically about the spark-version of the chatta demo from @pomadchin.I get that the ingest code is ran with spark-submit, it just runs the code and when it's done it exists.  But the code that actually serves up the imagery just works using ./sbt run without using spark-submit.  It just creates a SparkContext internally and it works.So I suppose that is my question, what is the difference between running code with spark-submit and running code using sbt run or java -cp that just creates a SparkContext?",2/10/16,1
pomadchin,"the problem in an sbt run that actually you have not a result jar, so using this mode you can_Ñét run you code ditributed; the correct way to run spark code is to build an assembly and to ingest it using spark-submit / or as a standalone jar",2/10/16,1
pomadchin,"here you can find example of chatta demo on a cluster: https://github.com/pomadchin/geo-env/blob/master/install/geotrellis/run-server.sh",2/10/16,1
jasonbeverage,"Yeah that's a fine distinction, I did something similar.  I built an assembly jar that contained some code to serve up tiles.  Just running it with java -cp worked fine, but I got some akka errors I didn't understand when I tried to run spark-submit on it.",2/10/16,1
jasonbeverage,So is there a difference between running the assembly using spark-submit or java -cp?,2/10/16,1
pomadchin,"also ye ._. there is differenceso spark-submit takes your jar and send it to other spark nodes, though when you have a fat assembly jar and start it using java -cp, it should become a driver node (as i understand), so you should worry about libs versions and be creful with data serializationthink that cool guyz in channel will correct my thoughts ",2/10/16,1
jasonbeverage,Hmm ok.,2/10/16,1
jasonbeverage,"So spark-submit will help you out with copying the jars around to the nodes if needed.  With java -cp it sounds like it might work, but you have to be careful.",2/10/16,1
pomadchin,"so as i think, the _ÑÒright way_Ñù to start geotrellis chatta server using java -cp (cuase it just stays as a process and you dont care about it); and there is no difference how to start an ingest process, cause in fact it is a job",2/10/16,1
jasonbeverage,"right, the ingest starts and finishes so it's just a one off thing.",2/10/16,1
pomadchin,(: ideally _ÑÓ docker container with server :D so you can just make `docker restart gt-server` if smth went wrong,2/10/16,1
jasonbeverage,"Yeah, or even just running under supervisor to keep it running.",2/10/16,1
jasonbeverage,"Ok, starting to click a little bit more.  Thanks for the help!  And also thanks for putting together that spark-version, it's really helpful.",2/10/16,1
pomadchin,"++, questions are appreciated :D",2/10/16,1
lokifacio,"one meta-question, which is the preferred way to post questions: gitter or the geotrellis-user group?",2/11/16,1
moradology,i think you'll get quicker responses on here in all likelihood,2/11/16,1
moradology,"i'm not saying the user group is @deprecated, but I do think this is preferred",2/11/16,1
lokifacio,"Ok, I thought it would be clearer to use the user group so I [posted](https://groups.google.com/forum/#!topic/geotrellis-user/8iRLBcaDXV4) there.... next time I'll use gitter instead :)",2/11/16,1
pomadchin,@lokifacio have you seen https://github.com/pomadchin/geotrellis-chatta-demo/blob/spark-version/geotrellis/src/main/scala/geotrellis/chatta/ChattaIngest.scala ?,2/11/16,1
jamesmcclain,"@dwins Hello, I thought that I accepted your request",2/11/16,1
dwins,"ha, nice icon for dr_jts on github",2/11/16,1
dwins,https://github.com/dr-jts,2/11/16,1
dwins,interesting: https://github.com/dr-jts/jts-ng,2/11/16,1
lokifacio,"@pomadchin yep, I managed to to do some tests  reading from my local file system a single tif file, but I couldn't find how to create an Etl instance from java. I've also been playing with ```val hc = hadoop.HadoopSparkContextMethodsWrapper(sc)val tiles = hc.hadoopGeoTiffRDD(localCatalog)```but again I'm not able to create HadoopSparkContextMethodsWrapper from java (maybe something with implicit class stuff).Next week I'll give it another try. Thanks",2/11/16,1
pomadchin,@lokifacio seems like this can be some problems with scala `object` in Java,2/11/16,1
echeipesh,@lokifacio I suspect that scala does some mangling for classes contained in a package object. From java you might need to do something like `new geotrellis.spark.io.hadoop.package$HadoopSparkContextMethodsWrapper(sc)` to get an instance of that.,2/11/16,1
pomadchin,@echeipesh think we have some java (java with scala) interacting problems ):,2/11/16,1
echeipesh,"For sure, we didn_Ñét really design API with Java interface in mind. I think if we were to tackle that problem we_Ñéd have to make a Java API wrapper",2/11/16,1
dwins,you have to be pretty careful with your api to make it nice for Java,2/11/16,1
owcm,"Looking for a suggestion on the best way to do a clustering analytic in Geotrellis.  I am using the existing raster ops to generate a slope and then create a raster that classifies cells with a certain discrete values if they fall within a certain slope range.    Next,  I want to identify groupings of cells in the Raster that have the same discrete value.  For example define a 20 x 20 cell mask  and find areas in the raster that have groupings with the same value within that 20x20  area.    Is there a focal op that assist in this type of operation.  Thanks in advance.",2/15/16,1
rshirochenko,"Hello, i`m trying to ingest landsat geotiff files. It gives me error: ERROR Executor: Exception in task 1.0 in stage 0.0 (TID 1) java.lang.OutOfMemoryError: Java heap space. To ingest spark-etl code is used. How to make ingestion for big number of landsat images? Is there solution to work with S3 storage directly?",2/15/16,1
pomadchin,"@rshirochenko increase java heap space for spark executors and driver: ```bashspark-submit _Ñ_ \  --driver-memory $DRIVER_MEMORY \  --executor-memory $EXECUTOR_MEMORY ```",2/15/16,1
rshirochenko,"Is there any docs that explains work with Raster RDD? Why in the landsat-tutorial in order to produce NDVI, it does not Spark? Will it be more efficient to work with it in a distributed way? ",2/15/16,1
lokifacio,which is the best way to save locally a tif with al the tiles of a RasteRDD? (I managed to create a GeoTiff for each tile and then write it to local filesystem) ,2/15/16,1
pomadchin,"Some (may be important) news:https://issues.apache.org/jira/browse/SPARK-6363https://issues.apache.org/jira/browse/SPARK?selectedTab=com.atlassian.jira.jira-projects-plugin:roadmap-panelhttps://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage",2/15/16,1
pomadchin,@lokifacio sure you can: https://github.com/geotrellis/geotrellis/blob/master/spark/src/main/scala/geotrellis/spark/render/package.scala#L67,2/15/16,1
echeipesh,"@owcm: I am having trouble following the second step of your process. Suppose you_Ñére running your 20x20 neighbhood and there are some cells with given value in the neighbhood, what do you want the value of the output cell to be ?",2/15/16,1
pomadchin,"@lokifacio you can even stitch tiles you want and to save it as one geotiff (used like `rdd.stitch` , implicits for this operation are here: `import geotrellis.spark._`)",2/15/16,1
echeipesh,"@rshirochenko in landsat-demo the NDVI is calcualted in the fly from Multiband tiles. _ÑÒon the fly_Ñù in this case is for a web service, that only throws out a couple tiles at a time. This was an arbitrary choice in the demo. It could have easily pre-computed the NDVI as a spark `.map` step over tiles in `RasterRDD` before saving them.",2/15/16,1
jmelching,"hey guys... i am looking for the best way to rasterize a feature collection, but don't see an easy way to do it in the 0.10.0-M1 build.  Looks like there is some code in master in github, but not many examples using it.  Any advice?",2/15/16,1
echeipesh,"@jmelching: You_Ñéd need to use the Rasterizer to update a mutable tile like so:```scalaimport geotrellis.vector._import geotrellis.raster._import geotrellis.raster.io.geotiff._import geotrellis.proj4._val features: Seq[Feature[MultiPolygon, Int]] = ???val extent = features.map(_.geom.envelope).reduce(_ combine _)val cols = 255val rows = 255val re = RasterExtent(extent, cols, rows)val tile = ArrayTile.empty(TypeInt, cols, rows)import geotrellis.raster.rasterize._for (feature <- features) {  Rasterizer.foreachCellByGeometry(feature.geom, re){ (col, row) =>    tile.set(col, row, feature.data)  }}GeoTiff(tile, extent, LatLng).write(""/tmp/features.tif"")}```",2/15/16,1
echeipesh,"That needs friendlier way to be called in the library though, seems like a sensible thing to do.",2/15/16,1
owcm,"@echeipesh  So the idea is trying to identify 20x20 cell blocks that have the same value,  if all cells in that mask have the given value I will want each cell to be set to 1 lets say in the new Raster.  If 1 or more cells  do not have the desired value I want to set the 20x20 block to NODATA in the new Raster,  Make sense? ",2/15/16,1
jmelching,@echeipesh thanks.  That works great.,2/15/16,1
echeipesh,"@owcm: Ah, yes, something like focal Count If. Doesn_Ñét look like we have anything like that. It wouldn_Ñét be hard to implement that operations by mimicing the way focal sum works: https://github.com/geotrellis/geotrellis/blob/master/raster/src/main/scala/geotrellis/raster/op/focal/Sum.scala#L42-L60and then passing it to to rdd.focal: https://github.com/geotrellis/geotrellis/blob/master/spark/src/main/scala/geotrellis/spark/op/focal/FocalOperation.scala#L41The strangeness in the focal op API is from the fact that_Ñés it_Ñés maintaining a running value, as some cells enter the focal neighborhood and some leave.",2/15/16,1
lossyrob,"Problem with Accumulo updates, any ideas? https://github.com/geotrellis/geotrellis/issues/1322",2/16/16,1
pomadchin,"probably we can note (add) there a bug (or feature), that we don_Ñét save schema for hadoop at all",2/16/16,1
pomadchin,https://github.com/geotrellis/geotrellis/blob/master/spark/src/main/scala/geotrellis/spark/io/hadoop/HadoopLayerWriter.scala#L41,2/16/16,1
lossyrob,"yeah, that_Ñés because the hadoop writer doesn_Ñét necessarily use avro",2/16/16,1
lossyrob,which I_Ñém changing with the IO refactors,2/16/16,1
pomadchin,:+1:,2/16/16,1
lossyrob,"but that_Ñés a good point, I_Ñéll have to add that with this refactor_Ñ_more on the plate ...",2/16/16,1
lossyrob,https://github.com/geotrellis/geodocker-cluster,2/16/16,1
pomadchin,yeeeeeeee,2/16/16,1
lossyrob,nice,2/16/16,1
owcm,"I have been writing out Geotiffs to a HDFS location using  the GeoTiff write method.   Try reading them back in using  SingleBandGeoTiff  and get the following stack error:```java.lang.NegativeArraySizeException	at geotrellis.raster.io.geotiff.compression.DeflateDecompressor.decompress(DeflateCompression.scala:70)	at geotrellis.raster.io.geotiff.compression.Decompressor$$anon$2.decompress(Decompressor.scala:52)	at geotrellis.raster.io.geotiff.GeoTiffSegmentCollection$class.getDecompressedBytes(GeoTiffSegmentCollection.scala:16)	at geotrellis.raster.io.geotiff.Float32GeoTiffTile.getDecompressedBytes(Float32GeoTiffTile.scala:7)	at geotrellis.raster.io.geotiff.Float32GeoTiffSegmentCollection$$anonfun$2.apply(Float32GeoTiffSegmentCollection.scala:20)	at geotrellis.raster.io.geotiff.Float32GeoTiffSegmentCollection$$anonfun$2.apply(Float32GeoTiffSegmentCollection.scala:20)	at geotrellis.raster.io.geotiff.GeoTiffSegmentCollection$class.getSegment(GeoTiffSegmentCollection.scala:26)	at geotrellis.raster.io.geotiff.Float32GeoTiffTile.getSegment(Float32GeoTiffTile.scala:7)	at geotrellis.raster.io.geotiff.Float32GeoTiffTile.mutable(Float32GeoTiffTile.scala:21)	at geotrellis.raster.io.geotiff.GeoTiffTile.toArrayTile(GeoTiffTile.scala:349)	at geotrellis.raster.io.geotiff.reader.GeoTiffReader$.readSingleBand(GeoTiffReader.scala:91)	at geotrellis.raster.io.geotiff.reader.GeoTiffReader$.readSingleBand(GeoTiffReader.scala:60)	at geotrellis.raster.io.geotiff.SingleBandGeoTiff$.apply(SingleBandGeoTiff.scala:53)```Should I be writing in and out using SingleBandGeoTiff.",2/17/16,1
lossyrob,"can you show us the full excpeption output, including the exception?",2/17/16,1
owcm,"@lossyrob  this might be more helpful:```6/02/17 02:18:31 ERROR yarn.ApplicationMaster: User class threw exception: java.lang.NegativeArraySizeExceptionjava.lang.NegativeArraySizeException	at geotrellis.raster.io.geotiff.compression.DeflateDecompressor.decompress(DeflateCompression.scala:70)	at geotrellis.raster.io.geotiff.compression.Decompressor$$anon$2.decompress(Decompressor.scala:52)	at geotrellis.raster.io.geotiff.GeoTiffSegmentCollection$class.getDecompressedBytes(GeoTiffSegmentCollection.scala:16)	at geotrellis.raster.io.geotiff.Float32GeoTiffTile.getDecompressedBytes(Float32GeoTiffTile.scala:7)	at geotrellis.raster.io.geotiff.Float32GeoTiffSegmentCollection$$anonfun$2.apply(Float32GeoTiffSegmentCollection.scala:20)	at geotrellis.raster.io.geotiff.Float32GeoTiffSegmentCollection$$anonfun$2.apply(Float32GeoTiffSegmentCollection.scala:20)	at geotrellis.raster.io.geotiff.GeoTiffSegmentCollection$class.getSegment(GeoTiffSegmentCollection.scala:26)	at geotrellis.raster.io.geotiff.Float32GeoTiffTile.getSegment(Float32GeoTiffTile.scala:7)	at geotrellis.raster.io.geotiff.Float32GeoTiffTile.mutable(Float32GeoTiffTile.scala:21)	at geotrellis.raster.io.geotiff.GeoTiffTile.toArrayTile(GeoTiffTile.scala:349)	at geotrellis.raster.io.geotiff.reader.GeoTiffReader$.readSingleBand(GeoTiffReader.scala:91)	at geotrellis.raster.io.geotiff.reader.GeoTiffReader$.readSingleBand(GeoTiffReader.scala:60)	at geotrellis.raster.io.geotiff.SingleBandGeoTiff$.apply(SingleBandGeoTiff.scala:53)	at surgesparkhandler.BuildRdd$$anonfun$tilePrepRoute$1.apply(BuildRdd.scala:106)	at surgesparkhandler.BuildRdd$$anonfun$tilePrepRoute$1.apply(BuildRdd.scala:82)	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108)	at surgesparkhandler.BuildRdd$.tilePrepRoute(BuildRdd.scala:82)	at surgesparkhandler.RequestHandler$.run$1(RequestHandler.scala:153)	at surgesparkhandler.RequestHandler$.main(RequestHandler.scala:99)	at surgesparkhandler.RequestHandler.main(RequestHandler.scala)	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)	at java.lang.reflect.Method.invoke(Method.java:606)	at org.apache.spark.deploy.yarn.ApplicationMaster$$anon$2.run(ApplicationMaster.scala:525)```",2/17/16,1
owcm,@lossyrob Interesting enough I can read the drive of hdfs and open it up fine in a tool like qgis,2/17/16,1
lossyrob,strange,2/17/16,1
lossyrob,can you write it without compression and see if that works?,2/17/16,1
lossyrob,"If so, it would be great to get a hold of the geotiff, that way I could make a unit test to prove that the deflate writing writes something that the reader can_Ñét read.",2/17/16,1
owcm,@lossyrob  I look at the GeoTiff and SingleBandGeoTiff write methods and I don't see a compression flag.   I see the option to read compressed or uncompressed when reading a Array[byte] into  SingleBandGeoTiff.,2/17/16,1
lossyrob,"ok. So you have read this in from a GeoTiff compressed, did some things to it, then wrote it out, then tried to read it back in?",2/17/16,1
owcm,"Sorry not very clear, always read in as SingleBandGeoTiff, I am only dealing with Single Band Geotiffs.  Do my operations and write out usually with  SingleBandGeoTiff write operation. Noticed in one case I wrote out using the GeoTiff object write operation.",2/17/16,1
owcm,Wondered if that could be the source of my conflict.,2/17/16,1
lossyrob,no worries about the single band. So you can read in a geotiff either uncompressed or compressed,2/17/16,1
lossyrob,do you use `SingelBandGeoTiff.compressed(_Ñ_)`,2/17/16,1
lossyrob,to read it in,2/17/16,1
lossyrob,initially,2/17/16,1
lossyrob,or set the compressed flag to true,2/17/16,1
owcm,Okay I saw the flag option.  Have usually gone with the default which I believe is false.   When I started getting these errors I tried reading as compresses and uncompressed.  If your interested I can send you the file.  Not sure how deep you wanted to get dragged in.,2/17/16,1
lossyrob,I just want to figure out if it_Ñés a geotrellis bug. Because then it needs to get fixed before the release,2/17/16,1
lossyrob,so the exception is thrown when you read it in with the default? or read it in compressed? or both?,2/17/16,1
lossyrob,and when I say read it in,2/17/16,1
lossyrob,I mean the original raster,2/17/16,1
lossyrob,"so you read in a geotiff A, and modify it, then write out geotiff B, then try to read geotiff B, and then get the exception. That_Ñés the scenario right?",2/17/16,1
owcm,"Correct, and I receive it when using either compression option.",2/17/16,1
lossyrob,"ok, that_Ñés good to know",2/17/16,1
lossyrob,"so if you don_Ñét set a compression option manually when writing, it shouldn_Ñét be trying to use the deflate compression when reading it back in",2/17/16,1
lossyrob,"but according to the stack trace, it is",2/17/16,1
lossyrob,what_Ñés the writing code look like?,2/17/16,1
owcm,"Well Geotrellis might be out of  picture.  I use the following utility to write a Array[Byte] from the Geotiff.```  def BAConvert(geoTiff: GeoTiff): Array[Byte] = {    val bos = new ByteArrayOutputStream()    try {      val dos = new DataOutputStream(bos)      try {        new GeoTiffWriter(geoTiff, dos).write()        bos.toByteArray      } finally {        dos.close      }    } finally {      bos.close    }  }```I write the Byte array to S3.   When our system restarts the file is copied by another process from S3 to HDFS.",2/17/16,1
owcm,,2/17/16,1
pomadchin,"@owcm digging a bit into the nature of this bug;how is the source tiff read? found that if we use `SingleBandGeoTiff`, decompress falg is true by default: ```scala  def readSingleBand(path: String): SingleBandGeoTiff =    readSingleBand(path, true)  def readSingleBand(path: String, decompress: Boolean): SingleBandGeoTiff =     readSingleBand(Filesystem.slurp(path), decompress)```",2/17/16,1
pomadchin,"\+ tried to read in a such way deflate compressed tiff, everything was ok (seemed to be ok, but really slow), @owcm can you provide more complete instructions to repeat this error?is it possible for you to provide us the input tiff?",2/17/16,1
owcm,@pomadchin  Yes I can supply you the input Tiff. What  is the best way to get it to you?,2/17/16,1
pomadchin,@owcm think any link like google cloud / drop box / Amazon would be _ª¥ç ,2/17/16,1
rshirochenko,How to read ingested geotiff files from hdfs?,2/17/16,1
owcm,,2/17/16,1
owcm,,2/17/16,1
owcm,"@pomadchin Here is the link to the file I am trying to read from hdfs:https://drive.google.com/file/d/0B1YZbN2gCsmSSmVKb0RHa3BsdVE/view?usp=sharingEssentially reading from HDFS with this code:```def GetHDFSData2( inPath: String ): Array[Byte] = {  val newPath = ""/"" + inPath  println( ""GetHDFSData2: "" + newPath )  val fsPath:Path =  new Path(newPath)  val conf = SparkUtils.hadoopConfiguration  val rasterAsBytes = HdfsUtils.readBytes( fsPath, conf)  println( ""GetHDFSData: file read"" )  rasterAsBytes}```I feed the resulting Array(Byte) to a val using SingleBandGeoTiff( bytesToUse ) call",2/17/16,1
pomadchin,"@owcm ok, loading",2/17/16,1
aaron-santos,"Hi, I'm trying to fully understand https://github.com/geotrellis/geotrellis/blob/master/spark/src/main/scala/geotrellis/spark/ingest/Pyramid.scala#L18 specifically the ```scala  ? => TileMergeMethods[V]: ? => TilePrototypeMethods[V]``` syntax.",2/17/16,1
lossyrob,"ah, you found the fun types",2/17/16,1
aaron-santos,Are there any docs that touch on this?,2/17/16,1
lossyrob,the `?` syntax actually comes from a compiler plugin called kind projector,2/17/16,1
lossyrob,https://github.com/non/kind-projector,2/17/16,1
lossyrob,but it translates to something pretty straightforward,2/17/16,1
aaron-santos,Perfect! That answered my next question about why it blows up when I try to use it myself,2/17/16,1
lossyrob,"When you see `V: ? => TileMergeMethods[V]`, it means that the type V must have some implicit conversion to `TileMergeMethods[V]`",2/17/16,1
lossyrob,so,2/17/16,1
lossyrob,"```scaladef foo[V: ? => TileMergeMethods[V]] = v.someTileMergeMethod(1)// translates todef foo[V](implicit ev1: V => TileMergeMethods[V]) = ...```",2/17/16,1
lossyrob,and that implicit gets satisfied by an implicit class,2/17/16,1
lossyrob,yeah that_Ñéll do it,2/17/16,1
lossyrob,https://github.com/geotrellis/geotrellis/blob/master/build.sbt#L29,2/17/16,1
lossyrob,add that to your build and you should be able to do it,2/17/16,1
aaron-santos,sweet. Thank you,2/17/16,1
lossyrob,for sure,2/17/16,1
jasonbeverage,"I'm trying out the new Multiband spark-etl stuff that was committed yesterday and having tons of issues",2/17/16,1
lossyrob,sweet,2/17/16,1
pomadchin,cool,2/17/16,1
lossyrob,lay _ÑÉem on us.,2/17/16,1
jasonbeverage,Haha :),2/17/16,1
echeipesh,great :),2/17/16,1
jasonbeverage,I think it might just be a config issue or something but I've no idea where to look.,2/17/16,1
jasonbeverage,"Exception in thread ""main"" java.lang.RuntimeException: Unable to find input module of type 'hadoop' for format `geotiff	at scala.sys.package$.error(package.scala:27)	at geotrellis.spark.etl.Etl$$anonfun$3.apply(Etl.scala:69)	at geotrellis.spark.etl.Etl$$anonfun$3.apply(Etl.scala:69)	at scala.Option.getOrElse(Option.scala:120)	at geotrellis.spark.etl.Etl.load(Etl.scala:69)	at geotrellis.spark.etl.Etl$.ingest(Etl.scala:33)",2/17/16,1
jasonbeverage,Bad paste,2/17/16,1
jasonbeverage,"I made an assembly using : ./sbt ""project spark-etl"" assembly",2/17/16,1
jasonbeverage,"And I'm using it like this:spark-submit \--master local[*] \--driver-memory 2G \--class geotrellis.spark.etl.MultibandIngest $JAR \--input hadoop --format geotiff -I path=""/data/boston-inset-mercator.tif"" \--output accumulo -O instance=$INSTANCE table=$TABLE user=$USER password=$PASSWORD zookeeper=$ZOOKEEPER \--layer boston --pyramid --crs $CRS --layoutScheme $LAYOUT_SCHEME",2/17/16,1
echeipesh,Try it with `_ÑÓformat multiband-geotiff`,2/17/16,1
jasonbeverage,"Cool, that got a bit further but then failed to write to accumulo",2/17/16,1
echeipesh,Whats the stack on that ?,2/17/16,1
jasonbeverage,"This is the interesting bit on the stack trace:  Caused by: ThriftTableOperationException(tableId:null, tableName:, op:CREATE, type:INVALID_NAME, description:Table names must only contain word characters (letters, digits, and underscores): )",2/17/16,1
jasonbeverage,"xception in thread ""main"" geotrellis.spark.io.package$LayerWriteError: Failed to write Layer(name = ""boston"", zoom = 10)	at geotrellis.spark.io.accumulo.AccumuloLayerWriter.write(AccumuloLayerWriter.scala:35)	at geotrellis.spark.io.accumulo.AccumuloLayerWriter.write(AccumuloLayerWriter.scala:15)	at geotrellis.spark.etl.OutputPlugin$class.apply(OutputPlugin.scala:17)	at geotrellis.spark.etl.accumulo.MultibandSpatialAccumuloOutput.apply(MultibandSpatialAccumuloOutput.scala:10)	at geotrellis.spark.etl.Etl.savePyramid$1(Etl.scala:114)	at geotrellis.spark.etl.Etl.save(Etl.scala:128)	at geotrellis.spark.etl.Etl$.ingest(Etl.scala:37)	at geotrellis.spark.etl.MultibandIngest$delayedInit$body.apply(MultibandIngest.scala:13)	at scala.Function0$class.apply$mcV$sp(Function0.scala:40)	at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:12)	at scala.App$$anonfun$main$1.apply(App.scala:71)	at scala.App$$anonfun$main$1.apply(App.scala:71)	at scala.collection.immutable.List.foreach(List.scala:318)	at scala.collection.generic.TraversableForwarder$class.foreach(TraversableForwarder.scala:32)	at scala.App$class.main(App.scala:71)	at geotrellis.spark.etl.MultibandIngest$.main(MultibandIngest.scala:11)	at geotrellis.spark.etl.MultibandIngest.main(MultibandIngest.scala)	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)	at java.lang.reflect.Method.invoke(Method.java:606)	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:672)	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180)	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205)	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:120)	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)Caused by: org.apache.accumulo.core.client.AccumuloException: Table names must only contain word characters (letters, digits, and underscores): 	at org.apache.accumulo.core.client.impl.TableOperationsImpl.doFateOperation(TableOperationsImpl.java:300)	at org.apache.accumulo.core.client.impl.TableOperationsImpl.doFateOperation(TableOperationsImpl.java:261)	at org.apache.accumulo.core.client.impl.TableOperationsImpl.doTableFateOperation(TableOperationsImpl.java:1427)	at org.apache.accumulo.core.client.impl.TableOperationsImpl.create(TableOperationsImpl.java:188)	at org.apache.accumulo.core.client.impl.TableOperationsImpl.create(TableOperationsImpl.java:155)	at geotrellis.spark.io.accumulo.BaseAccumuloRDDWriter$class.ensureTableExists(AccumuloRDDWriter.scala:25)	at geotrellis.spark.io.accumulo.AccumuloRDDWriter.ensureTableExists(AccumuloRDDWriter.scala:38)	at geotrellis.spark.io.accumulo.AccumuloRDDWriter.write(AccumuloRDDWriter.scala:54)	at geotrellis.spark.io.accumulo.AccumuloLayerWriter.write(AccumuloLayerWriter.scala:37)	... 25 moreCaused by: ThriftTableOperationException(tableId:null, tableName:, op:CREATE, type:INVALID_NAME, description:Table names must only contain word characters (letters, digits, and underscores): )",2/17/16,1
echeipesh,Whats the value of `$TABLE` on that execution ?,2/17/16,1
echeipesh,Looks like it might be not defined?,2/17/16,1
jasonbeverage,Yeah just saw that.  duh,2/17/16,1
jasonbeverage,I will buy you a beer at foss4g :),2/17/16,1
echeipesh,I_Ñéll remember that :),2/17/16,1
pomadchin,bet day i see,2/17/16,1
pomadchin,._._Ñé @echeipesh we have to reflect this issue in docs,2/17/16,1
echeipesh,"Yes. Looking at the actual call, that might be needlesly specific. The typetags should be enough to switch between single band and dual band usecase. We_Ñéll need to test that and opt for the less confusing config. ",2/17/16,1
jasonbeverage,"This is also probably a basic question, but what is the relationship between accumulo tables and layers?  Can I ingest multiple layers into a single table?",2/17/16,1
lossyrob,yes,2/17/16,1
lossyrob,"but the thing is, the table splits on the SFC index",2/17/16,1
lossyrob,so you_Ñéd want to store tiles from multiple layers in the same table if they occupy the same space/have the same key index range,2/17/16,1
lossyrob,"meaning, if we_Ñére indexing the tiles with the same SFC configured the same way, then it_Ñés really beneficial to have them in the same table, especially if you_Ñéll be joining across them",2/17/16,1
lossyrob,though there is a bit of functionality that is pending to really make that story complete,2/17/16,1
jasonbeverage,Ok gotcha.  So if you had tons of data collected of the same site and you wanted to analyze them together you'd store them in the same table.  But if you had lots of collections all over the world you wouldn't want to store them in the same table.,2/17/16,1
lossyrob,"yeah, you can frame it as a table per indexed space, exactly",2/17/16,1
jasonbeverage,"Makes sense.  I noticed that when reading layers I didn't have to specify a table for accumulo to look at it, it just found them all so that wasn't clear to me.",2/17/16,1
jasonbeverage,Have you had any experience running accumulo on EMR?,2/17/16,1
jasonbeverage,I saw some docs with the bootstrap scripts and it looked fairly straightforward.  But I know you  mentioned the other day that you used EMR unless you were running accumulo,2/17/16,1
jasonbeverage,So I didn't know if there were some gotchas to look out for.  I started an EMR cluster the other day and ran some geotrellis stuff from s3 and hdfs on it and it was pretty straightforward.,2/17/16,1
jasonbeverage,I think this was what I was looking at for accumulo on EMR:  https://blogs.aws.amazon.com/bigdata/post/Tx15973X6QHUM43/Running-Apache-Accumulo-on-Amazon-EMR,2/17/16,1
echeipesh,"AWS used to have bootscripts for EMR version 3, I think that is what that is.",2/17/16,1
echeipesh,"They haven_Ñét published anything like that for version 4.*, which is the latest at the moment.",2/17/16,1
jasonbeverage,"Yeah makes sense.  I was using 4. something, the one that had spark 1.5",2/17/16,1
echeipesh,"GeoWave was doing some work to deploy to EMR, they require accumulo, you should check out: https://github.com/ngageoint/geowave/tree/master/deploy/emr",2/17/16,1
lossyrob,"it_Ñés something we have wanted to check out, so if you have any luck with running an Accumulo backed GeoTrellis application on EMR, that_Ñéll be some pioneering stuff :)",2/17/16,1
jasonbeverage,"Very cool, thanks.  Right now I've been using the geotrellis-ec2-cluster project.",2/17/16,1
jasonbeverage,"rob I can't even set my $TABLE variable, don't expect too much.",2/17/16,1
jasonbeverage,:),2/17/16,1
lossyrob,hahaha,2/17/16,1
lossyrob,"I think we all have spent time banging our heads against a similar problem, so that doesn_Ñét shake my expectations too much",2/17/16,1
lossyrob,I just merged eugene_Ñés map algebra renames and broke everything. meaning people will have to update PRs to merge in master,2/17/16,1
lossyrob,@jamesmcclain https://github.com/geotrellis/geotrellis/pull/1320,2/17/16,1
lossyrob,https://github.com/geotrellis/geotrellis/pull/1319,2/17/16,1
jamesmcclain,"Okay, will-do",2/17/16,1
jamesmcclain,"I have a CI run for #1319 running right now, would you mind stopping it?",2/17/16,1
lossyrob,done,2/17/16,1
jamesmcclain,Thanks,2/17/16,1
owcm,@pomadchin  Wondered if you had any luck loading that file.,2/17/16,1
pomadchin,@owcm think will be results in 15-40mins,2/17/16,1
pomadchin,"yes exactly there is a problem, figuring out where exactly",2/17/16,1
owcm,@pomadchin  ok thanks,2/17/16,1
lossyrob,"@owcm just an update on this, it seems like a geotiff tag is being written incorrectly; we_Ñéll have to take a look tomorrow. It might take a bit to get into the work pipeline. Do you have a workaround for this by any chance?",2/17/16,1
owcm,"@lossyrob  Thanks for looking into this.  I have a work around for the short term. When you say worked into the pipeline, could that mean weeks or months down the line?",2/17/16,1
lossyrob,on the scale of weeks,2/17/16,1
owcm,"that works, thanks again ",2/17/16,1
pomadchin,yeee we got green travis badge!,2/18/16,1
lossyrob,whew,2/18/16,1
pomadchin,"again grey colored )`: though not caused by code compilation err just by a sudden travis kill https://api.travis-ci.org/jobs/110157884/log.txt?deansi=true",2/18/16,1
pomadchin,PAIN,2/18/16,1
moradology,i'm thinking through documentation indexing right now - do we want a flat structure or hierarchical so that we lean on github's support for navigating directories?,2/18/16,1
moradology,do strong opinions abound?,2/18/16,1
moradology,there will be an index in either instance,2/18/16,1
lossyrob,I think minimal folders,2/18/16,1
lossyrob,"like, raster, vector, spark",2/18/16,1
moradology,"okay, cool",2/18/16,1
moradology,that's exactly what i've got as of now,2/18/16,1
lossyrob,but not letting it go too deep (like not mimicing package structure to a T),2/18/16,1
lossyrob,awesome,2/18/16,1
echeipesh,"^ :+1: Would be good if the central page had some text on it to, rather than just an index.",2/18/16,1
moradology,"yeah, definitely",2/18/16,1
moradology,writing that one as we speak,2/18/16,1
moradology,err type,2/18/16,1
jamesmcclain,"I am working on separating  the `s3` and `accumulo` backends from the `spark`, and I would like to get opinions on how to deal with the following code: https://github.com/geotrellis/geotrellis/blob/master/spark/src/main/scala/geotrellis/spark/render/package.scala",2/18/16,1
jamesmcclain,"It is in the spark package and depends on s3, so it seems that the options are to a) remove the s3 code b) duplicate this file in the s3 project, c) move this file to the s3 project or d) use reflection or similar to see whether the s3-dependent methods are available",2/18/16,1
jamesmcclain,Or there could be another alternative that I have missed,2/18/16,1
echeipesh,You wouldn_Ñét be able to use reflection without refering to undefined types unfortunatly. ,2/18/16,1
jamesmcclain,What do you mean?  Is it not possible to inspect the rdd type to see if it has a method of the desired name (e.g. `saveToS3`)?,2/18/16,1
jamesmcclain,(Thinking about e.g. line 55 of the file that I linked to),2/18/16,1
echeipesh,thats an extension method: https://github.com/geotrellis/geotrellis/blob/00f0f342fc95da35fb489e727416f3d5c78086d8/spark/src/main/scala/geotrellis/spark/io/s3/SaveToS3Methods.scala,2/18/16,1
jamesmcclain,"Yeah, I am aware of that.   I haven't experimented, but I assume that if the correct implicit class has been imported, reflection should discover the methods",2/18/16,1
jamesmcclain,"My understanding is that under covers, methods added by implicit classes are handled the same way normal methods are",2/18/16,1
dwins,"no, an implicit class works by having the compiler insert calls to a method everywhere the extension methods are used",2/18/16,1
echeipesh,"Not sure actually, but I wouldn_Ñét expect that.",2/18/16,1
jamesmcclain,Whether they are present or not is a function of importing and linking?,2/18/16,1
dwins,x.extensionMethod ==> addExtensions(x).extensionMethod,2/18/16,1
jamesmcclain,"Ahh, I see",2/18/16,1
jamesmcclain,So under the covers they are not actually methods of the type that they appear to be?,2/18/16,1
dwins,where addExtensions is just a method that calls the appropriate constructor (actually for implicit classes it will have the same name as the class),2/18/16,1
echeipesh,"There are two options here: 1) to type class this out, 2) to use SPI https://docs.oracle.com/javase/tutorial/ext/basics/spi.html 3) change the API",2/18/16,1
jamesmcclain,"Okay, I'll explore those possibilities",2/18/16,1
echeipesh,"@lossyrob should chime in, but I would lean to change API to something like: `rdd.renderAsPng(_ÑÒs3://bucket/layer/{x}.{y}.{z.}.png_Ñù): RDD[(String, Array[Byte]` and then having having someth ing like `.saveToHadoop` or `.saveToS3` being extension methods on that.",2/18/16,1
echeipesh,"That takes aways the automatic replacement of hadoop s3 implemintation with ours, but I don_Ñét think that_Ñés a huge deal.",2/18/16,1
jamesmcclain,That seems to have the virtue of simplicity,2/18/16,1
lossyrob,I like that option.,2/18/16,1
jamesmcclain,"Okay, I'll get started on that option",2/18/16,1
echeipesh,Maybe call the methods `.writeToHadoop` isntead of save _Ñ_ saving implies it can be loaded again  :),2/18/16,1
lossyrob,"have `renderAsPng` return `RDD[(String, Pnt)]`",2/18/16,1
lossyrob,not sure about the string actually,2/18/16,1
lossyrob,just,2/18/16,1
lossyrob,"`RDD[(K, Png)]`",2/18/16,1
lossyrob,and then there can be render methods for the backends that do the path conversion,2/18/16,1
jamesmcclain,Okay,2/18/16,1
lokifacio,is there any geotrellis version which uses spark with yarn support?,2/19/16,1
lossyrob,"You should be able to use geotrellis fine with spark and yarn, we use yarn when we're using EMR. Do you mean deployment scripts like geotrellis-ec2-cluster? Of so, no we only wrote support for mesos. But if you have a cluster with yarn, you shouldn't have any problems running geotrellis.",2/19/16,1
lokifacio,I mean deploying usin spark-submit with --master yarn,2/19/16,1
lokifacio,Error: Could not load YARN classes. This copy of Spark may not have been compiled with YARN support,2/19/16,1
lokifacio,I'm using geotrellis via publish-local,2/19/16,1
lossyrob,"Geotrellis doesn't have a preference for what cluster manager your using, standalone mesos or yarn, so it should work fine like other spark apps",2/19/16,1
lokifacio,"I know, I managed to use geotrellis in our yarn cluster using our custom spark assembly",2/19/16,1
lokifacio,maybe my question is more sbt related,2/19/16,1
pomadchin,"@lokifacio be careful with spark build process, probably you forgot to include yarn package ",2/19/16,1
lossyrob,@jamesmcclain also unused: raster/src/main/conscript,2/19/16,1
jamesmcclain,"Okay, I will add it to the list",2/19/16,1
jamesmcclain,Thanks,2/19/16,1
lossyrob,also raster/src/main/ls,2/19/16,1
owcm,I am looping though a raster using  localMapDouble and if I find a cell value that matches my criteria is there a way I could change the neighboring cells to a new value.    Doing this for display purposes I am trying to show a buffer area/zone  around cells of importance.  ,2/19/16,1
ameetkini,"@lossyrob, setting up a new environment for geotrellis, wanted to chat about requirements ",2/19/16,1
ameetkini,"java and scala version, spark version, etc. ",2/19/16,1
ameetkini,"from the above chat, sounds like it's been tested with yarn, so that's great",2/19/16,1
pomadchin,"@ameetkini sry for providing this information istead of @lossyrob , but currently gt supports jdk1.7/jdk1.8 (oracle and open jdk tested), scala 2.10 and 2.11, spark 1.5.2 (and higher, probably (not tested)), accumulo 1.6.x and 1.7.x, gdal 1.11",2/19/16,1
ameetkini,"thanks @pomadchin, if we go with scala 2.11, can we have either jdk 7 or 8 or does it have to be one or the other? ",2/19/16,1
pomadchin,it doesn_Ñét matter what jdk you want to use : ),2/19/16,1
ameetkini,"ok, wasn't sure since I saw this http://www.scala-lang.org/news/2.11.1/#required-java-version ""The Scala 2.11.x series targets Java 6, with (evolving) experimental support for Java 8. In 2.11.1, Java 8 support is mostly limited to reading Java 8 bytecode and parsing Java 8 source. Stay tuned for more complete (experimental) Java 8 support. The next major release, 2.12, will most likely target Java 8 by default.""",2/19/16,1
ameetkini,"@pomadchin, is build-from-source still the preferred way to get a fairly recent release? ",2/19/16,1
pomadchin,"to be honest i use everywhere oracle jdk 8 for scala 2.10 and 2.11 :D sometimes it is possible to catch some weird compiler behaviour (some issues with type inheritence for example) but I saw it only for a couple of times, think jdk8 should be used ",2/19/16,1
pomadchin,we have http://bintray.com/azavea/geotrellis with artefacts for both scala versions (2.10 and 2.11),2/19/16,1
ameetkini,nice!,2/19/16,1
pomadchin,"yeah, artefact version corresponds to commit hash",2/19/16,1
pomadchin,but you can build it :D just launching `publish-local.sh` or `publish-local-crossversion.sh`,2/19/16,1
pomadchin,publish local will publish only scala 2.10 version artefacts as the default target is scala 2.10 due to default spark default distributions,2/19/16,1
lossyrob,@owcm you could do that witha focal operation that paints the cell a certain color (or marks it with a certain value) based on if the value of anything in that neighborhood matches the criteria,2/19/16,1
lossyrob,"there might be other ways to approach the problem in a more performant way, based on how many cells are of this specific value, how big the nieghborhood is_Ñ_potentially could pull them out as points and do a buffer operation",2/19/16,1
lossyrob,"@ameetkini the one restriction with 2.11 is that a spark installation uses 2.10, unless you build it yourself",2/19/16,1
lossyrob,"you can build it with 2.11, but the binary distributions they give are all based on 2.10 still, which is annoying",2/19/16,1
lossyrob,"we_Ñéve only really tested with YARN via EMR, not our own setup",2/19/16,1
ameetkini,"ah, right @lossyrob ",2/19/16,1
ameetkini,do you run it with standalone / mesos usually?,2/19/16,1
lossyrob,mesos,2/19/16,1
ameetkini,cool,2/19/16,1
lossyrob,if we use the `geotrellis-ec2-cluster` stuff,2/19/16,1
ameetkini,"so we have a local cluster, and have complete control over everything",2/19/16,1
ameetkini,spark-2.10 should be able to run geotrellis 2.11 or no?,2/19/16,1
lossyrob,"yeah_Ñ_so if it can run spark stuff, it should be able to run geotrellis stuff",2/19/16,1
lossyrob,I did run into bytecode problems the one time I tried that,2/19/16,1
ameetkini,"gotcha, that helps to know",2/19/16,1
lossyrob,"I forget exactly what it was, it might have been some Akka dependency stuff, or some other dependency conflict",2/19/16,1
ameetkini,so you rebuilt spark with scala 2.11?,2/19/16,1
lossyrob,no I tried to run stuff with geotrellis built with 2.11 on spark 2.10,2/19/16,1
lossyrob,We have an outstanding issue to test against spark built with 2.11,2/19/16,1
lossyrob,I don_Ñét think that_Ñés happened yet,2/19/16,1
lossyrob,@pomadchin was that something that was on your plate with the version upgrades? or was that a separate issue?,2/19/16,1
ameetkini,oh oh so you're running geotrellis with scala 2.10?,2/19/16,1
pomadchin,"@lossyrob think I will make a spark-2.11 provided images, that_Ñés not a problem, just time",2/19/16,1
lossyrob,yeah,2/19/16,1
ameetkini,gotcha,2/19/16,1
lossyrob,@pomadchin in the geodocker-cluster? that would be cool,2/19/16,1
lossyrob,"just have to work out the whole, installing spark when we build it ourself, thing",2/19/16,1
ameetkini,what java version lossyrob?,2/19/16,1
lossyrob,our geotrellis-ec2-cluster stuff uses 7,2/19/16,1
lossyrob,I use 8 on my dev machine,2/19/16,1
pomadchin,i always have `class not found` errors runing scala 2.11 built assembly on a 2.10 spark cluster ,2/19/16,1
lossyrob,"which causes problems when uploading jars to the cluster, I have to packge them by building with java 7",2/19/16,1
ameetkini,"thanks guys, have to boot the machine now, will check back in a bit",2/19/16,1
jamesmcclain,I am finishing up with the render API changes,2/19/16,1
jamesmcclain,Can someone point me to some sample code (using the old API) that I can use/turn into a unit test?,2/19/16,1
owcm,"@lossyrob I think what your approach pulling out as points and buffering sounds best.  I am assuming I can grab the point geometry with Geotrellis, any of the tutorials deal with this.  I see the ToVector object in  geotrellis.raster.op.global",2/19/16,1
lossyrob,I can maybe freestyle something,2/19/16,1
lossyrob,"hmm actually forgot we were working with focal operaitons. will have to think on it a bit",2/19/16,1
owcm,"okay,",2/19/16,1
lossyrob,oh wait that_Ñés right we_Ñére not doing focal if we convert to poitns,2/19/16,1
lossyrob,sorry I_Ñém all messed up,2/19/16,1
owcm,I was looking at tile.ToVector to see if I could pull the geometry  that way,2/19/16,1
lossyrob,"```scalaval rdd: RasterRDD[SpatialKey] = ???val points: RDD[PointFeature[Int]] =  rdd.asRasters.flatMap { case (_, raster) =>    val re = raster.rasterExtent    val points = mutable.ListBuffer[PointFeature[Int]]()    raster.tile.foreach { (col, row, z) =>      if(matchesSomeCriteria(z)) {        val (x, y) = rasterExtent.gridToMap(col, row)        points += PointFeature(Point(x, y), z)      }      points    }  }val bufferAmount = 10.0 // map unitspoints.map(_.buffer(bufferAmount))```",2/19/16,1
lossyrob,something like that.,2/19/16,1
owcm,"pretty good freestyling, that points me in the right direction",2/19/16,1
owcm,Thanks,2/19/16,1
lossyrob,no problem,2/19/16,1
pomadchin,:tada: green travis :tada:,2/20/16,1
owcm,"@lossyrob I was able to follow your lead and read the points from a tiled RasterRDD.  Working in a single raster environment I am ableto take a sequence of points and using VectorToRaster.rasterize(points,re, <some value> ) to merge them into a single raster.Something like ``` val points = Seq(  Point(re.gridToMap(100,100)).buffer(30),  Point(re.gridToMap(200,200)).buffer(30),  Point(re.gridToMap(300,300)).buffer(30),  Point(re.gridToMap(400,400)).buffer(30),  Point(re.gridToMap(500,500)).buffer(30))val rpts:Tile = VectorToRaster.rasterize(points,re, 0x55)// Set background to values of 0var rptsCleaned: Tile = rpts.mapDouble { (col: Int, row: Int, z: Double) =>  var zz = z  if (z != 0x55) {    zz = 0  }  zz}// Create a GeoTiffval ptsGT = GeoTiff( rptsCleaned, obsExtent, obsCRS )// Create a Rastersource from Geotiff Pointsval fsNormalizedOutRS = RasterSource( fsNormalizedOut, obsExtent)val r1RS = RasterSource ( pstg., obsExtent )// Or in point data with existing Raster sourceval combinedRS:RasterSource = fsNormalizedOutRS.localOr( r1RS ```This works nicely in the single raster world.I followed  your example and read all of the points of interest out of a Tiled RDD collection of type RDD[ (ProjectedExtent, Tile) ] elements. So now I would like to take these points, buffer them like in the above example,  and OR them back into my original Tiled RDD[ (ProjectedExtent, Tile) ] collection.One thought was to create a single Tile the size of the Tiled RDD extent and populated it with my points.  Then run that Tile through the X/Y/Z Tiler and creating a second RDD collection that I can OR in with my original Tiled RDD collection. Does this sound like a valid approach?",2/22/16,1
lossyrob,"hmm_Ñ_ if the point set is not going to be too large then I would just broadcast it to a task that takes the original tiles and the set of points and does something with them, rasterizes over the full tile and then creates a new tile taking into account whatever you need from the point data. Using a spatial index would speed that up a bit perhaps",2/22/16,1
lossyrob,"or from the RDD of point data assign a key to each point (or set of keys, they might overlap two tiles), and that way it_Ñés spatially partitioned, and then joining the point RDD to the tile RDD, and then creating your new tiles out of the combination of the points and tiles.",2/22/16,1
rshirochenko,,2/23/16,1
rshirochenko,"I've got an error when I`m trying to ingest to Accumulo : ```16/02/23 15:08:29 ERROR Executor: Exception in task 0.0 in stage 4.0 (TID 4)java.io.IOException: Mkdirs failed to create ""path/""```The script for ingest files: http://pastebin.com/5zB2HVqFAny suggestion how to fix it?",2/23/16,1
pomadchin,@rshirochenko is a more complete stack trace available?,2/23/16,1
rshirochenko,"@pomadchin  yes, here it is http://pastebin.com/JH0dW1is",2/23/16,1
pomadchin,@rshirochenko can only suggest you to check your hdfs write rights,2/23/16,1
pomadchin,it looks like hdfs has no sufficient space or just no rights,2/23/16,1
echeipesh,@rshirochenko also check out your fs.defaultFS value: https://github.com/geotrellis/geotrellis/blob/master/spark/src/main/scala/geotrellis/spark/io/accumulo/AccumuloWriteStrategy.scala#L32-L41,2/23/16,1
jasonbeverage,"Hey all, I just updated to the master yesterday and an ingest I ran is having issues reading back out.",2/23/16,1
jasonbeverage,"It's a single band, floating point dem.",2/23/16,1
lossyrob,oof. what_Ñés the error?,2/23/16,1
jasonbeverage,the issue is when you try to do something like layerReader.read(myLayer).read(),2/23/16,1
jasonbeverage,It fails reading the metadata in this chunk of code:  https://github.com/geotrellis/geotrellis/blob/master/spark/src/main/scala/geotrellis/spark/io/JsonAttributeStoreMethods.scala#L41,2/23/16,1
jasonbeverage,"That Try, getOrElse really masks the real issue.  I commented out the getOrElse to see the real error and its this",2/23/16,1
jasonbeverage,"```Exception in thread ""main"" java.lang.IllegalArgumentException: Cell type float32raw is not supported	at geotrellis.raster.CellType$.fromString(CellType.scala:250)	at geotrellis.raster.io.json.package$CellTypeFormat$.read(package.scala:39)	at geotrellis.raster.io.json.package$CellTypeFormat$.read(package.scala:33)	at spray.json.JsValue.convertTo(JsValue.scala:31)	at geotrellis.spark.io.json.package$RasterMetaDataFormat$.read(package.scala:77)	at geotrellis.spark.io.json.package$RasterMetaDataFormat$.read(package.scala:64)	at spray.json.JsValue.convertTo(JsValue.scala:31)	at geotrellis.spark.io.JsonAttributeStoreMethods$$anon$1.read(JsonAttributeStoreMethods.scala:97)	at spray.json.JsValue.convertTo(JsValue.scala:31)	at geotrellis.spark.io.JsonAttributeStoreMethods.readLayerAttributes(JsonAttributeStoreMethods.scala:37)```",2/23/16,1
jasonbeverage,So the cell type is being set to float32raw in the metadata.  ,2/23/16,1
jasonbeverage,"I assume it should be set to float32, I'm not sure where the ""raw"" is coming from.",2/23/16,1
pomadchin,really there is no such type (to get from string): https://github.com/geotrellis/geotrellis/blob/master/raster/src/main/scala/geotrellis/raster/CellType.scala#L196-L208,2/23/16,1
jasonbeverage,"I'm just using the SinglebandIngest class in ETL.  Keep in mind I'm not setting a  cellType on my command line, so perhaps that is the issue.",2/23/16,1
jasonbeverage,b/c previously I thought it figured out what the cell type was from the source data correctly.,2/23/16,1
lossyrob,"@moradology this is something from your PR. It looks like floattyperaw is not a valid `fromString` value, but it can be output by a valid cell type",2/23/16,1
lossyrob,@jasonbeverage yeah it should,2/23/16,1
lossyrob,"also I_Ñém not sure why it was assuming `raw` on ETL, since it should be `ConstantNoData`",2/23/16,1
lossyrob,@jasonbeverage quick fix would be to add the _ÑÒraw_Ñù cases here https://github.com/geotrellis/geotrellis/blob/master/raster/src/main/scala/geotrellis/raster/CellType.scala#L208,2/23/16,1
lossyrob,"```scala    case ""float32raw"" => FloatCellType    case ""float64raw"" => DoubleCellType```",2/23/16,1
echeipesh,"I_Ñém guessing the input tiles have no NoData tag defined, so reducing over celltype would still yield a raw.",2/23/16,1
lossyrob,right,2/23/16,1
lossyrob,also I_Ñém worried about not treating Float.NaN as a NODATA value,2/23/16,1
jasonbeverage,The tiff doesn't have a nodata defined.,2/23/16,1
echeipesh,right,2/23/16,1
lossyrob,"```scalascala> java.lang.Double.isNaN(Float.NaN)res0: Boolean = true```",2/23/16,1
lossyrob,actually it will be treated as NoData even if it_Ñés float nan,2/23/16,1
lossyrob,so yeah. that workaround should generally work. needs to be put into master,2/23/16,1
lossyrob,less a workaround more of a fix,2/23/16,1
jasonbeverage,Here is the gdalinfo of the tif:  http://pastebin.com/YKSYxe8U,2/23/16,1
jasonbeverage,I'll try that quick fix rob.  Setting the --cellType in the command line didn't appear to work either.,2/23/16,1
lossyrob,hm. that should have worked maybe_Ñ_what did you set it to?,2/23/16,1
jasonbeverage,"One other minor thing I saw that night was that the ""util"" project needs to be added to publish-local.sh and the other build scripts../sbt ""project util"" publish-local  && \",2/23/16,1
jasonbeverage,should have worked maybe :)  I did --cellType float32,2/23/16,1
lossyrob,and the metadata still reads float32raw?,2/23/16,1
pomadchin, @jasonbeverage right now was added #1333,2/23/16,1
lossyrob,@pomadchin what PR? is it merged?,2/23/16,1
jasonbeverage,Yeah that's assuming --clobber overwrote the metadata.,2/23/16,1
lossyrob,@pomadchin nice,2/23/16,1
pomadchin,@lossyrob added while loops where you pointed,2/23/16,1
jasonbeverage,"Good deal, thanks @pomadchin ",2/23/16,1
jasonbeverage,I was 2 minutes too late :),2/23/16,1
pomadchin,and a new travis icon O: (i guess the most important change),2/23/16,1
lossyrob,"@jasonbeverage thanks for leaning on this stuff, glad to have the review going on",2/23/16,1
jasonbeverage,"No problem, thanks for the quick responses here",2/23/16,1
lossyrob,@pomadchin you were in the source tile modification most recently in ETL. Can you imagine why cellType wouldn_Ñét force the set of it directly? Or is this a problem on the serialization side?,2/23/16,1
pomadchin,"@lossyrob think that the problem can be a bit deeper, though i can investigate what_Ñés going on there",2/23/16,1
pomadchin,and yes probalby it is somewhere on the serialization side,2/23/16,1
jasonbeverage,"rob that fix adding float32raw and float64raw to the CellType fromString worked, thanks!",2/23/16,1
lossyrob,@pomadchin wouldn_Ñét that be convenient ;),2/23/16,1
lossyrob,https://github.com/geotrellis/geotrellis/blob/master/spark-etl/src/main/scala/geotrellis/spark/etl/Etl.scala#L35,2/23/16,1
lossyrob,wouldn_Ñét that not take into account the EtlConfig cellType?,2/23/16,1
lossyrob,@jasonbeverage nice!,2/23/16,1
lossyrob,"if you feel like signing the eclipse CLA and making a small PR with that change, you_Ñéd get contributor credit for the fix :)",2/23/16,1
pomadchin,@lossyrob https://github.com/geotrellis/geotrellis/blob/bad3d2436a15ceba20889564ce5bc87f78ca07e3/spark-etl/src/main/scala/geotrellis/spark/etl/EtlConf.scala#L127-L138,2/23/16,1
pomadchin,clean there ):,2/23/16,1
lossyrob,oh it does use it here https://github.com/geotrellis/geotrellis/blob/master/spark-etl/src/main/scala/geotrellis/spark/etl/Etl.scala#L86,2/23/16,1
jasonbeverage,"Sure thing, I'll submit something here shortly.",2/23/16,1
lossyrob,here_Ñés the link for the CLA: https://www.eclipse.org/legal/CLA.php (the link to sign up/sign one is on the right bar),2/23/16,1
lossyrob,requires the github/eclipse emails match up,2/23/16,1
lossyrob,@jasonbeverage is the backend accumulo or S3 that didn_Ñét write the correct metadata w/ _ÑÓcellType?,2/23/16,1
jasonbeverage,"You know, scratch that @lossyrob.  I forgot I changed my ingest test script to write to hadoop last night to see if it was an accumulo only issue.",2/23/16,1
jasonbeverage,So --cellType probably works fine.,2/23/16,1
lossyrob,oh ok,2/23/16,1
echeipesh,phew _Ñ_ I was really scratching my head diving through the code looking at a likely spot for that bug.,2/23/16,1
lossyrob,haha me too,2/23/16,1
jasonbeverage,Sorry about that :)  I changed it super late last night before I just gave up :),2/23/16,1
lossyrob,better to dive through code for a couple min than leave a bug on the table,2/23/16,1
echeipesh,"speaking of weird things, does anybody have an explanation for this behavior in console:```scalaraster > console[info] Starting scala interpreter...[info] import geotrellis.raster._import geotrellis.raster.resample._import geotrellis.vector._Welcome to Scala version 2.10.5 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_74).Type in expressions to have them evaluated.Type :help for more information.scala> CellType.fromString(""float32"")res0: geotrellis.raster.CellType = float32scala> CellType.fromString(""float32"")<console>:17: error: value fromString is not a member of object geotrellis.raster.CellType              CellType.fromString(""float32_Ñù)```",2/23/16,1
pomadchin,@echeipesh think it is more a repl bug,2/23/16,1
lossyrob,so it works once and then never again?,2/23/16,1
pomadchin,"btw and should not we add `CellType.fromString(""float32raw_Ñù)` ?",2/23/16,1
echeipesh,"Yes, just works once. Restarting the console makes it kick again.",2/23/16,1
dwins,how can a method just disappear between two lines on the repl? that seems super weird to me,2/23/16,1
pomadchin,"yeah ._. sometimes repl can be a surprising thing; to check my words is possible only by compiling an app with several times function call O: sec, will check",2/23/16,1
pomadchin,"yeah works perfect :D a repl bug```scalaimport geotrellis.raster.CellTypeobject Main extends App {  println(CellType.fromString(""float32""))  println(CellType.fromString(""float32""))  println(CellType.fromString(""float32""))  println(CellType.fromString(""float32""))  println(CellType.fromString(""float32""))  println(CellType.fromString(""float32""))}```",2/23/16,1
jasonbeverage,Singed the CLA and sent in a PR with the float32raw and float64raw change @lossyrob ,2/23/16,1
lossyrob,"awesome, thanks!",2/23/16,1
lossyrob,@moradology can you review https://github.com/geotrellis/geotrellis/pull/1337,2/23/16,1
lokifacio,"I have a reference raster with the given properties:- dimensions  : 2000, 2996, 5992000  (nrow, ncol, ncell)- resolution  : 0.005, 0.005  (x, y)- extent      : -10.00039, 4.979613, 34.99771, 44.99771  (xmin, xmax, ymin, ymax)- coord. ref. : +proj=longlat +datum=WGS84 +no_defs +ellps=WGS84 +towgs84=0,0,0I'm trying to load another raster in another CRS with a different extent and want to map it to match my reference raster.I did some test using Java (now I'm using scala) and got what I expected:```Tile croppedTile = tile.crop(extent, cropExtent)Raster croppedRaster = new Raster(croppedTile, cropExtent)Raster reprojectedRaster = croppedRaster.reproject(sourceCRS, targetCRS);reprojectedRaster  = reprojectedRaster.resample(reprojectedRaster.rasterExtent().withResolution(0.005, 0.005));```Now I'm trying to do it using RasterRDD to process this on our cluster but I'm having some troubles with layout schemes/definitions (where can I find some documentation on this subject? -is it specific to geotrellis?)My current approach is something like this:```val tiffRDD = sc.hadoopGeoTiffRDD(path)val (_, input_metadata) = RasterMetaData.fromRdd(tiffRDD, FloatingLayoutScheme(250))val tiles =    RasterRDD(tiffRDD, input_metadata)val metadata = RasterMetaData(cellType, LayoutDefinition(cropExtent, TileLayout(3000,2000,50,50)), cropExtent, targetCRS) // I don't understand how to define a TileLayout, got those values after several testsval projectedTiles = tiles.reproject(targetCRS).tileToLayout(metadata, NearestNeighbor)```after some test I think that the tile size is computed somehow from the extent and the row/col definition and then multiplied by 10e4...this gives me a value of 50 pixels (when I set 1 I get tiles with 1 pixel equivalent to 50x50 on my input raster and when I set it to 50, I get 50x50 pixels of my input rasters...)I'm quite puzzled so any explanation would be very appreciated",2/23/16,1
echeipesh,"@lokifacio: LayoutScheme is really just a scheme for figuring out how to pick a TileLayout for a raster of specific extent and resolution. We have two options right now: `ZoomedLayoutScheme` that matches tile layout used by Leaflet and other slippy map clients and the one you found `FloatingLayoutScheme`. You_Ñéve told it that you want your tiles to be 250x250 at whatever their resolution is, `RasterMetaData.fromRdd(_Ñ_)` goes and collects the actual extent of the input data and figures out how many tiles of that size it would take to cover it. Tile (0,0) is going to perfectly match the upper left corner of your raster and it tiles from there. So in fact you can use `input_metadata` in place of `metadata`. You can then give this meta data to `cutToLayout` function that will retile the tiles to match the layout you_Ñéve discovered/specified. ",2/23/16,1
echeipesh,Check out the example of `Ingest.scala` to see how that workflow works: https://github.com/geotrellis/geotrellis/blob/master/spark/src/main/scala/geotrellis/spark/ingest/Ingest.scala,2/23/16,1
echeipesh,One odd thing you may see is that reprojection happens after tiling. That allows the reprojection process to sample past the tile boundaries (since the tile layout is known) and produce a more accurate reprojection. If one used reproject on source tiles before tiling them the reproject function would be sampling some NoData values around tile boundaries.,2/23/16,1
rshirochenko,,2/24/16,1
rshirochenko,"> @rshirochenko also check out your fs.defaultFS value: https://github.com/geotrellis/geotrellis/blob/master/spark/src/main/scala/geotrellis/spark/io/accumulo/AccumuloWriteStrategy.scala#L32-L41Checked it out, and there are hdfs value. Seems that it likely related to defaultFS value, is it  possible to manually setup hdfs path for accumulo ingest script?",2/24/16,1
lossyrob,"@rshirochenko the default write path is what gets written to HDFS. Accumulo then ingests to there. That_Ñés a technique of ingesting into accumulo, where you write out to HDFS and tell accumulo where the table data is, and it synchronously absorbs the data into a table you specify",2/24/16,1
lossyrob,"if you use a relative path, it defaults to the default FS value",2/24/16,1
lossyrob,but you can supply your own FS in the path,2/24/16,1
lossyrob,are you asking if you can set the HDFS path in an spark-etl script?,2/24/16,1
rshirochenko,yes,2/24/16,1
lossyrob,gotcha. let me take a look. I don_Ñét think we put that into the options,2/24/16,1
lossyrob,"no, it looks like we did not",2/24/16,1
lossyrob,are you ingesting spatial or spatiotemporal data?,2/24/16,1
lokifacio,"@echeipesh I've already checked `Ingest.scala` and I'm bit confused by this:```val (_, rasterMetaData) =      RasterMetaData.fromRdd(sourceTiles, destCRS, layoutScheme)(_.projectedExtent.extent)```if destCRS is different of the one of sourceTiles I would expect to obtain the extent definition equivalent to the targetCRS, however I get rasterMetaData with extent in the original CRS ",2/24/16,1
lokifacio,here is my output:,2/24/16,1
lokifacio,"```Original metadata extent: Extent(3120400.0, 2003800.0, 3198700.0, 2076100.0)Original metadata crs: +proj=laea +lat_0=52.0 +lon_0=10.0 +x_0=4321000.0 +y_0=3210000.0 +ellps=GRS80 +units=m Target metadata extent: Extent(3120400.0, 2003800.0, 3198700.0, 2076100.0)Target metadata crs: +proj=longlat +datum=WGS84 +no_defs ```",2/24/16,1
lossyrob,"@lokifacio you_Ñére actually pointing out a _ÑÒbug_Ñù that @echeipesh is looking into currently. The metadata gets set to th destCRS there, but is not reprojected until a later point",2/24/16,1
lossyrob,so the extent info is still in the original CRS,2/24/16,1
lokifacio,"@lossyrob oh, great I thought I wasn't understanding anything XP",2/24/16,1
lokifacio,:),2/24/16,1
rshirochenko,"the problem is that I have HDFS in defaultFS, but I have got an error when ingest  spatial  data, and seems that it`s trying to write to local file://",2/24/16,1
lossyrob,"it doesn_Ñét actually have an effect, but we were talking yesterday that it shouldn_Ñét be that way",2/24/16,1
lossyrob,good catch :),2/24/16,1
lossyrob,@rshirochenko hmm that shouldn_Ñét be the case,2/24/16,1
lossyrob,"ok, actually that make sense_Ñ_I was having that problem",2/24/16,1
lossyrob,"I actually solved it in a current branch I_Ñém working on, but that hasn_Ñét gotten pulled into master yet",2/24/16,1
lossyrob,where I set the default path explicitly to have the `hdfs://` fs prefix,2/24/16,1
lossyrob,https://github.com/geotrellis/geotrellis/blob/master/spark/src/main/scala/geotrellis/spark/io/accumulo/AccumuloLayerWriter.scala#L45,2/24/16,1
lossyrob,"are you pulling from bintray, or could you make that change and build the code from there?",2/24/16,1
lossyrob,sorry for the inconvinience...,2/24/16,1
rshirochenko,"ok, I will build the code and try, thanks)",2/24/16,1
lokifacio,"I getting weird behavior when I try to apply two consecutive cutTiles to the same RDD, the second one seems corrupted (if I comment the first cutTiles I get the expected behavior). Is this expected to work like that?",2/24/16,1
pomadchin,"@lokifacio can you provide some code example?what do you expect by cuting *same* rdd? ",2/24/16,1
lokifacio,yep,2/24/16,1
lokifacio,"@lossyrob well, I'm not sure what I'm expecting yet (that's why I was trying to  do some tests after @echeipesh suggestion of using cutTiles to perform my crop)",2/24/16,1
lokifacio,"this snippet should reproduce my example (if you want I can send you the input raster which is a QGIS crop of a bigger raster):````val tiff = sc.hadoopGeoTiffRDD(path)val (_, metadataOriginalCRS) = RasterMetaData.fromRdd(tiff, FloatingLayoutScheme(250))val tiles = ContextRDD(tiff, metadataOriginalCRS)val TARGET_CRS = CRS.fromEpsgCode(4326)val targetExtent = Reproject(metadataOriginalCRS.extent, metadataOriginalCRS.crs, TARGET_CRS)val crop = RasterMetaData(tmd.cellType, metadataOriginalCRS.layout, Extent(targetExtent.xmin, targetExtent.ymin, targetExtent.xmax, (targetExtent.ymin+ targetExtent.ymax)/2), TARGET_CRS)/* Uncomment to reproduce bugval reprojected = tiles.cutTiles(crop)Utils.write(""cutTiles.tif"", crop.extent, reprojected)*/val metadataTargetCRS = RasterMetaData(metadataOriginalCRS.cellType, metadataOriginalCRS.layout, targetExtent, TARGET_CRS)val target = tiles.cutTiles(metadataTargetCRS)Utils.write(""output.tif"", metadataTargetCRS.extent, target)```",2/24/16,1
lokifacio,you check the output and find the input raster here: https://drive.google.com/folderview?id=0B3n4l67Jylyqa1Z1TEYtV1dvY1U&usp=sharing,2/24/16,1
lossyrob,how are you constructing the expected?,2/24/16,1
lossyrob,it looks like some values are turning to nodata unexpectedly,2/24/16,1
lokifacio,expected correspond to output.tif with the commented lines,2/24/16,1
lokifacio,and corrupted with those lines uncommented,2/24/16,1
lossyrob,oh right,2/24/16,1
lossyrob,is `cutTiles.tif` on the drive?,2/24/16,1
lokifacio,"btw, I still don't know how to crop those tiles xP (that's the output of cutTiles.tif)",2/24/16,1
lokifacio,"which is the ""expected"" output but compressed to half its height",2/24/16,1
lossyrob,changing the extent in metadata won_Ñét effect things in cut tiles,2/24/16,1
lossyrob,ah but it will effect the interpolation,2/24/16,1
lossyrob,so the extent data should never really be manually set,2/24/16,1
lokifacio,I uploaded the cut-tiles.tif to drive,2/24/16,1
lossyrob,@dwins do you know of this? http://pubs.usgs.gov/pp/1395/report.pdf Evan (GDAL) is telling me proj4 is basically a C port of this book,2/24/16,1
lossyrob,@lokifacio https://github.com/geotrellis/geotrellis/blob/master/spark/src/main/scala/geotrellis/spark/tiling/LayoutDefinition.scala#L14,2/24/16,1
lossyrob,CutTiles uses this mapTransform to translate between keys and map space,2/24/16,1
lossyrob,wait I_Ñém speaking too soon. give me a minute,2/24/16,1
dwins,@lossyrob yes it came up while i was reading up on proj.4. it's one of the links on their wiki https://github.com/osgeo/proj.4/wiki#documentation-1,2/24/16,1
dwins,a couple of dead links there but some good info on the working ones,2/24/16,1
lossyrob,nice,2/24/16,1
lokifacio,so what's the correct way to reproject only some sub-extent using RasterRDD? (I suppose that the most efficient way is only resampling the area interest from the sourceCRS to my targetCRS),2/24/16,1
dwins,this is the one i found most approachable (good picture to text ratio :wink: ) http://www.progonos.com/furuti/MapProj/Normal/TOC/cartTOC.html ,2/24/16,1
lossyrob,@lokifacio ok a bit confused. When you uncomment the code do you change `val target = tiles.cutTiles(metadataTargetCRS)` to `val target = reprojected.cutTiles(metadataTargetCRS)`,2/24/16,1
lokifacio,but again not sure how to define that area properly? in which extent should I do it?,2/24/16,1
lossyrob,"so you want to take a raster layer and then crop it to some specific extent in the target CRS, yeah?",2/24/16,1
lokifacio,"nope, just uncomment those line",2/24/16,1
lokifacio,yep,2/24/16,1
lossyrob,"whoa, really?",2/24/16,1
lossyrob,so just running cut tiles somehow mutates the original tiles rdd?,2/24/16,1
lokifacio,I told you it sounded weird to me (unfortunately I'm new to scala so I tend to question myself),2/24/16,1
lokifacio,that was my impression,2/24/16,1
lossyrob,that is bizarre ,2/24/16,1
lossyrob,can I get the original tiff?,2/24/16,1
lokifacio,I already put on the drive: madrid.tif,2/24/16,1
lossyrob,"ok, thanks. let me see if I can reproduce",2/24/16,1
lokifacio,which is a crop I manually did on the [europe density ](http://www.eea.europa.eu/data-and-maps/data/population-density-disaggregated-with-corine-land-cover-2000-2) dataset using QGIS,2/24/16,1
lossyrob,are you using a specific bintray version of geotrellis or building from master,2/24/16,1
lokifacio,"actually, I can't open that dataset using geotrellis. If want to load the whole raster, I have to open/save it using QGIS...",2/24/16,1
lossyrob,what does it fail with when trying to read in geotrellis?,2/24/16,1
lokifacio,currently I'm using  18e03d9 from bintray (I read somewhere you recommended sticking to one version until API stabilizes),2/24/16,1
lokifacio,I don't remember right now if it was index out of bounds or negativearrayindex serialization (I can check it later),2/24/16,1
lossyrob,"ok, I_Ñém looking into it now",2/24/16,1
lokifacio,what I'm trying to do is obtain a raster of madrid in EPSG4326 from the europe density raster I posted before which uses EPSG3035,2/24/16,1
lokifacio,As input I have that raster and the desired extent on EPSG4326 + the number of cells it should contain,2/24/16,1
lokifacio,thanks for your help,2/24/16,1
lossyrob,what is the `Reproject` method?,2/24/16,1
lokifacio,import geotrellis.vector.reproject.Reproject,2/24/16,1
lossyrob,oh ha.,2/24/16,1
lossyrob,could also do,2/24/16,1
lossyrob,"```scala    import geotrellis.vector.reproject._    val targetExtent = metadataOriginalCRS.extent.reproject(metadataOriginalCRS.crs, TARGET_CRS)```",2/24/16,1
lossyrob,is `tmd` the original metadata (replace the `_` above with `tmd`?),2/24/16,1
lossyrob,oh wait nvm,2/24/16,1
lossyrob,what is tmd?,2/24/16,1
lokifacio,"sorry, tmd is just tiles.metadata (I replace it to put it clearer from my copy/paste but forgot that one)",2/24/16,1
lokifacio,which is metadataOriginalCRS,2/24/16,1
lossyrob,can you paste the Utils.write method,2/24/16,1
lokifacio,"yep```def write(path: String, extent: Extent, tiles: RDD[(SpatialKey, Tile)]): Unit = {    val tile = tiles.stitch    val tiff = GeoTiff(tile, extent, TARGET_CRS)    tiff.write(path)}```",2/24/16,1
lossyrob,i didn_Ñét get the artifacts when I saved off the cut tiles,2/24/16,1
lokifacio,"nop, it's on the second one",2/24/16,1
lokifacio,I always get no artifacts on cut-tiles,2/24/16,1
lossyrob,ok. I was misreading what you were saying then,2/24/16,1
lossyrob,so when you change the extent in the raster metadata,2/24/16,1
lossyrob,"you are just changing metadata, and no values",2/24/16,1
lossyrob,let me come up with a crop that_Ñéll work,2/24/16,1
lossyrob,so you have the extent you want in lat lng,2/24/16,1
lossyrob,do you have any idea of how you want to tile it? like a tile size? do you have a particular resolution that you want to fit to?,2/24/16,1
lokifacio,"btw I have no real interest on that setup, I just used to test some methods and noticed that behavior which seemed wrong to me",2/24/16,1
lossyrob,"if you open the cut tiles in QGIS you can see what_Ñés going on. you_Ñére cutting down the extent, but the content is the same, so it squashes the raster on the map",2/24/16,1
lossyrob,"i don_Ñét see as many artifacts as I see on the google drive, but those are different viewers so that doesn_Ñét shock me.",2/24/16,1
lokifacio,I would like to get a resolution of 0.005 and create tiles of 250x250 pixels for instance,2/24/16,1
lokifacio,"@lossyrob I'm not sure I've explained properly (sorry for my English if it is misleading)The issue with the artifacts is with output.tif in one of the exectuions:- when I use call twice cutTiles on the same constant tiles, the second one (output.tif) gets the artifacts (that is, it differs for the one you obtain if you comment the first call). - the first call (cut-tiles) is always the same and, as you pointed, is the expected raster but squashed",2/24/16,1
lokifacio,"> do you have any idea of how you want to tile it? like a tile size? do you have a particular resolution that you want to fit to?this is the definition of my target raster - dimensions : 2000, 2996, 5992000 (nrow, ncol, ncell)- resolution : 0.005, 0.005 (x, y)- extent : -10.00039, 4.979613, 34.99771, 44.99771 (xmin, xmax, ymin, ymax)- coord. ref. : +proj=longlat +datum=WGS84 +no_defs +ellps=WGS84 +towgs84=0,0,0well, to fit a floating layout scheme of size 250, it should be 3000 cols instead 2996",2/24/16,1
lossyrob,so your saying that the output.tif is different,2/24/16,1
lossyrob,i didn_Ñét see that,2/24/16,1
lossyrob,even if I called cut tiles,2/24/16,1
lokifacio,"yes, that's what I was saying",2/24/16,1
lossyrob,"""when I use call twice cutTiles on the same constant tiles, the second one (output.tif) gets the artifacts (that is, it differs for the one you obtain if you comment the first call)._Ñù",2/24/16,1
lossyrob,I can_Ñét reproduce this,2/24/16,1
lossyrob,@echeipesh we didn_Ñét ever implement filtering on RasterRDD did we,2/24/16,1
echeipesh,"Nope, doesn_Ñét look like we did. The closest we have is the filter by keybounds on spatialy partiioned RDD. But we don_Ñét have anything for generic case.",2/24/16,1
lossyrob,hmmm I remember that was on the list,2/24/16,1
echeipesh,James was just saying he was about to wrap up with his sprint tasks. I think he can tackle that in short order. Or are you already hacking away on it ?,2/24/16,1
lossyrob,"no I_Ñém not, just needing it",2/24/16,1
lossyrob,yeah that makes sense for him to work on that,2/24/16,1
echeipesh,Cool. I was just looking over our package structure. It isn_Ñét clear where such a method would belong. Seems like we_Ñéll need a couple of raster manipulation methods along the line of mask and crop,2/24/16,1
lossyrob,it would make sense to have a `filter` package,2/24/16,1
lossyrob,@lokifacio https://gist.github.com/lossyrob/8bb7c19b22941233abc0,2/24/16,1
lossyrob,"that_Ñés what I got, but it does not filter the layer before reprojecting like you were trying to do. We need some functionality before doing that",2/24/16,1
lossyrob,"but you can save the layer, and then do the filter of madrid on load",2/24/16,1
lokifacio,"@lossyrob I created a new project to isolate the issue and now it worked as expected on both cases, sorry for bothering you. Maybe there is something weird on my project/IDE",2/24/16,1
lossyrob,ok. yeah that would have been very alarming if what was supposed to be an immutable operation was somehow mutating the underlying RDD,2/24/16,1
lokifacio,"@lossyrob thanks, I will check it now.  Let me know when doing it before reprojecting is available :)",2/24/16,1
lokifacio,btw whats the difference between tileToLayout and cutTiles?,2/24/16,1
lossyrob,whoops. we should be using tileToLayout,2/24/16,1
lossyrob,"cutTiles actually cuts the tiles, but does not merge any overlaps",2/24/16,1
lossyrob,"tileToLayout will merge any overlapping tiles. and overlapping in the target cut tiles, so you could have a tile in the target layout that is paritally covered by two tiles in the source layout, and those need to be merged",2/24/16,1
lokifacio,"I see, thanks for the explanation",2/24/16,1
owcm,"I was trying to use a knock off geotrellis-landsat-tutorial to try out the new Tiling changes.  I want to tile up a singleband geotiff that has cells  classified as 0,1,2,3 values.I modified  the code as follows since TIFF is single band:```  def run(implicit sc: SparkContext) = {    // Read the geotiff in as a single image RDD,    // using a method implicitly added to SparkContext by    // an implicit class available via the    // ""import geotrellis.spark.io.hadoop._ "" statement.    val inputRdd = sc.hadoopGeoTiffRDD(inputPath)        // Use the ""RasterMetaData.fromRdd"" call to find the zoom    // level that the closest match to the resolution of our source image,    // and derive information such as the full bounding box and data type.    val (_, rasterMetaData) =      RasterMetaData.fromRdd(inputRdd, FloatingLayoutScheme(512))    // Use the Tiler to cut our tiles into tiles that are index by the z/x/y map coordinates.    val tiled: RDD[(SpatialKey, Tile)] = inputRdd.tileToLayout(rasterMetaData, Bilinear)     // We'll be tiling the images using a zoomed layout scheme    // in the web mercator format (which fits the slippy map tile specification).    // We'll be creating 256 x 256 tiles.    val layoutScheme = ZoomedLayoutScheme(WebMercator, tileSize = 256)     // We need to reproject the tilcd es to WebMercator    val (zoom, reprojected) = RasterRDD(tiled, rasterMetaData).reproject(WebMercator, layoutScheme, Bilinear)    // Create the writer that we will use to store the tiles in the local catalog.    val writer = FileLayerWriter[SpatialKey, Tile, RasterMetaData](outputPath, ZCurveKeyIndexMethod)     // Pyramiding up the zoom levels, write our tiles out to the local file system.    val writeOp =      Pyramid.upLevels(reprojected, layoutScheme, zoom) { (rdd, z) =>        writer.write(LayerId(""landsat"", z), rdd)      }  }```I get an assertion failure Polygon Empty, full Stack:```Exception in thread ""main"" org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 2.0 failed 1 times, most recent failure: Lost task 0.0 in stage 2.0 (TID 2, localhost): java.lang.AssertionError: assertion failed: Polygon Empty: POLYGON EMPTY[error] 	at scala.Predef$.assert(Predef.scala:179)[error] 	at geotrellis.vector.Polygon.<init>(Polygon.scala:77)[error] 	at geotrellis.vector.Polygon$.jtsToPolygon(Polygon.scala:28)[error] 	at geotrellis.vector.Polygon$.apply(Polygon.scala:68)[error] 	at geotrellis.vector.reproject.package$Reproject$.apply(package.scala:42)[error] 	at geotrellis.vector.reproject.package$ReprojectPolygon.reproject(package.scala:185)[error] 	at geotrellis.raster.reproject.ReprojectRasterExtent$.reprojectExtent(ReprojectRasterExtent.scala:37)[error] 	at geotrellis.raster.reproject.ReprojectRasterExtent$.apply(ReprojectRasterExtent.scala:42)[error] 	at geotrellis.raster.reproject.ReprojectRasterExtent$.apply(ReprojectRasterExtent.scala:102)[error] 	at geotrellis.spark.reproject.TileRDDReproject$$anonfun$4$$anonfun$apply$2.apply(TileRDDReproject.scala:149)[error] 	at geotrellis.spark.reproject.TileRDDReproject$$anonfun$4$$anonfun$apply$2.apply(TileRDDReproject.scala:146)[error] 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)[error] 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)[error] 	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)[error] 	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.insertAll(BypassMergeSortShuffleWriter.java:99)[error] 	at org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:73)[error] 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:73)[error] 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)[error] 	at org.apache.spark.scheduler.Task.run(Task.scala:88)[error] 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)[error] 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)```",2/24/16,1
owcm,Is this implying the Tif is malformed?,2/24/16,1
lossyrob,"perhaps_Ñ_It_Ñés happening because one of the tile extents, when reprojected, creates an empty extent, which shouldn_Ñét happen",2/24/16,1
lossyrob,what is the extent and CRS of the original image?,2/24/16,1
owcm,"Layer extent -76.0555887779721047,36.9022872362658561 : -76.0204016429703131,36.9521075522392124  +proj=longlat +datum=WGS84 +no_defs",2/24/16,1
owcm,pwd,2/24/16,1
lossyrob,I_Ñém wondering what something like this would print out ,2/24/16,1
lossyrob,"```scalaval results =  RasterRDD(tiled, rasterMetaData).asRasters.flatMap { case (key, raster) =>    try {      raster.extent.toPolygon.reproject(rasterMetaData.crs, WebMercator)      None    } catch {      case e: Exception =>        Some(s""$key, ${raster.extent}"")    }  }println(results.collect)```",2/24/16,1
owcm,I will give it a try,2/24/16,1
owcm,"raster.extent.toPolygon.reproject(rasterMetaData.crs, WebMercator)   reproject does not show up as a member of toPolygon . I am using ""com.azavea.geotrellis"" %% ""geotrellis-spark"" % ""0.10.0-2283ec5"".  Do I need to step it up to something newer",2/24/16,1
lossyrob,`import geotrellis.vector.reproject._`,2/24/16,1
pomadchin,@owcm \+ can you also print `rasterMetaData.crs.toProj4String` ?,2/24/16,1
owcm,Proj4 string comes back as +proj=latlong +datum=WGS84 +to_meter=1.0,2/24/16,1
pomadchin,@lossyrob parser bug appeared,2/24/16,1
pomadchin,):,2/24/16,1
lossyrob,what happened to our dang geotiff reader,2/24/16,1
pomadchin,@owcm can suggest you in case of tiles with LatLng not to read crs from tiff tags but to use `geotrellis.proj4.LatLng.scala`,2/24/16,1
owcm,When I run the code lossyrob suggested  I get the same stack trace when the results.collect is executed.,2/24/16,1
echeipesh,Looks like monocle lenses are now serializable: https://github.com/julien-truffaut/Monocle/commit/b0b918d90af9831f03e93d5007b518351390c956,2/24/16,1
lossyrob,"yeah, that_Ñés why we had the wrapper for component stuff",2/24/16,1
lossyrob,haha,2/24/16,1
lossyrob,oh,2/24/16,1
lossyrob,the _ÑÒnow_Ñù instead of _ÑÒnot_Ñù was very importent for that statement,2/24/16,1
pomadchin,@lossyrob this bug related to a wrong tag read ):,2/24/16,1
echeipesh,"lol, yeah, very critical type-o",2/24/16,1
echeipesh,going to bump it to 1.2.0 to take advantage of new goodness,2/24/16,1
lossyrob,@owcm can you try to run the geotiff through something like a `gdal_translate -of GTiff -co compress=default in.tif out.tif` and see if that fixes it,2/24/16,1
lossyrob,thinking maybe it_Ñés the tif,2/24/16,1
pomadchin,@lossyrob think it is not tiff,2/24/16,1
pomadchin,i have a similar error,2/24/16,1
lossyrob,did the geotiff reader change recently?,2/24/16,1
pomadchin,"dunno, think nope",2/24/16,1
pomadchin,also somehow we avoided this bug appearence ,2/24/16,1
owcm,@lossyrob  I will give gdal_translate a try,2/24/16,1
pomadchin,i tried to toake LC8 tiles and to translate it from utm to latlng and to read its projection; it also had this weird projection string ` +proj=latlong +datum=WGS84 +to_meter=1.0`,2/24/16,1
owcm,@lossyrob  Hmm tried the gdal_translate step and same result.,2/24/16,1
lossyrob,ok.,2/24/16,1
lossyrob,and this was running before you tried to update?,2/24/16,1
lossyrob,what version were you on?,2/24/16,1
pomadchin,"@lossyrob obviously we always avoided this case cause we never used before `RasterMetaData.fromRdd(rdd, layoutScheme)` overload for Ingest object and for Etl",2/24/16,1
lossyrob,I think that we have been ingesting lat lng before,2/24/16,1
lossyrob,and certainly reading latlng,2/24/16,1
lossyrob,there was a feature a while ago about reading the CRS directly from the EPSG,2/24/16,1
owcm,I pulled the latest geotrellis-landsat-tutorial from git and ran the out of the box landsat example and that all worked fine.   I wanted to see how the tiler would work on something I generated so I inserted by tif instead of the generated r-nir.tif.   I was thinking that would be  a valid test.,2/24/16,1
owcm,I can read the tif with tools like QGIS so I was thinking the file was readable.,2/24/16,1
pomadchin,"@lossyrob nope never used in fact :\ we always used `RasterMetaData.fromRdd(rdd, desCRS, layoutScheme)`",2/24/16,1
lossyrob,"I_Ñéve used it, I_Ñém saying",2/24/16,1
pomadchin,O: ,2/24/16,1
lossyrob,I never used ETL,2/24/16,1
lossyrob,@owcm can I get the tif,2/24/16,1
owcm,yes,2/24/16,1
owcm,let me up load.,2/24/16,1
lossyrob,thanks,2/24/16,1
owcm,"@lossyrob  Uploading:I usually project things to WebMercator to run my operations.   When I try to write out the Geotiff I get an error that writing WebMercator projection is not supported so I re-project to LatLng and write out:```                  var theTile = fsRSNormalized.get                  val fsNormalizedOut = GeoTiff(theTile, obsExtent, obsCRS)                  val fsNormalizedOutPR = fsNormalizedOut.projectedRaster                  val ProjectedRaster(fsProjTile, fsProjExtent, fsProjCRS) = fsNormalizedOutPR.reproject(LatLng)                  // Create Geotiff with new projection                  val fsNormalizedOutLatLng = SingleBandGeoTiff(fsProjTile, fsProjExtent, fsProjCRS, gtOut.tags, gtOut.options)                  fsNormalizedOutLatLng.write(""thefile"")```Maybe that is causing an issue",2/24/16,1
lossyrob,that bug about not being able to write web mercator is fixed,2/24/16,1
lossyrob,"or should be, was last time I looked",2/24/16,1
lossyrob,that should be fine though. I_Ñém suspicious that some recent code changes around the geotiff reader might be causing trouble,2/24/16,1
owcm,"@lossyrob  Links to 2 files:https://drive.google.com/file/d/0B1YZbN2gCsmSbDFVLXZTc24zLWc/view?usp=sharinghttps://drive.google.com/file/d/0B1YZbN2gCsmSTWtqMFN0T3hoTDg/view?usp=sharing  ",2/24/16,1
owcm,"@lossyrob  The  whole reason I headed down this path was I was trying to evaluate the new Tiler mechanism.   I was seeing issues with color ramp values  being maintained between zoom levels, using the SlippyMap x/y/z in the previous  landsat-tutorial version.    I knew you made some updates so I was just trying to evaluate the new version.",2/24/16,1
lossyrob,ok. I_Ñéll have to get some serious time to debug these geotiff issues.,2/24/16,1
lossyrob,"to have color ramp consistency, you need to save the breaks from one of the layers and use those breaks to color each tile",2/24/16,1
owcm,@lossyrob  thanks for the color breaks advice that fixed my issue,2/24/16,1
jasonbeverage,Does spark-etl have support for ingesting temporal geotiffs from hadoop?  I see a temporal-geotiff format for S3 but not one in the hadoop folder.,2/25/16,1
pomadchin,@jasonbeverage actually no,2/25/16,1
pomadchin,but got your suggestion o:,2/25/16,1
jasonbeverage,Yeah I actually may have written one :)  I'll submit a PR later today if it actually works.,2/25/16,1
pomadchin,cool! awesome :D,2/25/16,1
jasonbeverage,"So it looks like the ingest worked into accumulo, neat :)  Is there somewhere that you'd recommend I look for examples on how to work with temporal data?",2/25/16,1
jasonbeverage,"For example, I've created an AccumuloLayerReader[SpaceTimeKey, Tile, RasterMetaData].  Is there a way I can see all of the available times for a layer?",2/25/16,1
jasonbeverage,"Right now I'm just getting the layer with```val layer = reader.read(LayerId(""mylayer"", myzoom))```that gives me back a RDD[(geotrellis.spark.SpaceTimeKey, geotrellis.raster.Tile)]I didn't see anything on the layer.metadata object that could tell me what times are available",2/25/16,1
pomadchin,@jasonbeverage you can get it out of the `AttributeStore`,2/25/16,1
pomadchin,https://github.com/pomadchin/geotrellis-chatta-demo/blob/spark-version/geotrellis/src/main/scala/geotrellis/chatta/ChattaServiceActor.scala#L64-L68,2/25/16,1
jasonbeverage,"Yup, I can get the metadata.  Is there a way to see the list of times that are available for a layer?  I didn't see anything on the RasterMetadata object.                                        ",2/25/16,1
jasonbeverage,Or do you need to actually do a query on the layer and just collect all the unique temporal components?,2/25/16,1
jasonbeverage,Something like this seems to work:,2/25/16,1
jasonbeverage,"```layer.map{case(key,tile) => key.temporalComponent.time}.distinct.collect.foreach(println)```",2/25/16,1
lossyrob,"I_Ñéve done that, for instance to populate a drop down of NEX times here: https://github.com/lossyrob/landsat-demo/blob/master/ingest/src/main/scala/demo/NEXIngest.scala#L157",2/25/16,1
lossyrob,"I just store it as an attribute on zoom 0 of the layer, that way my spray service can just get the attribute out and return it",2/25/16,1
jasonbeverage,"Ok, so there is nothing ""official"" then for doing that.",2/25/16,1
lossyrob,nope,2/25/16,1
jasonbeverage,"So I can store anything I want on an attributestore like that, good to know.",2/25/16,1
lossyrob,"yeah. i store times and quantile breaks, for instance",2/25/16,1
lossyrob,precomputed data,2/25/16,1
jasonbeverage,so in general when you're working with SpaceTimeKeys are there any utilities that I should be looking at in geotrellis or is it more of a case by case basis where you're just making spark operators to slice and dice the data however you'd like?,2/25/16,1
jasonbeverage,I saw some stuff in LocalTemporalTileRDDMethods but honestly my spark and scala-fu aren't powerful enough to know exactly what that does :),2/25/16,1
owcm,I have reclassified Tile to have cell values of 1 or NODATA.  I was interested in pulling out the groupings of cells classified as 1 as polygons.   Is using the geotrellis.raster.op.global method toVector the way to go?,2/25/16,1
lossyrob,@jasonbeverage those are for doing summaries for ranges of time. There_Ñés some methods in the summary package for doing time based stuff. but generally I end up working with the RDDs directly to do what I need. do you have anything specific in mind?,2/25/16,1
lossyrob,"@owcm we currently don_Ñét have vectorization implemented on RDDs, but on singles yes toVector is the way to go",2/25/16,1
jasonbeverage,"Really simple use cases right now, just comparison of various imagery collects over time.  Just trying to get my head around what is even possible and how to use it correctly.",2/25/16,1
jasonbeverage,"Last question for the day, I did manage to get a temporal hadoop input working and wrote a little temporal ingest class for ETL.  It's just this:",2/25/16,1
jasonbeverage,"```package geotrellis.spark.etlimport geotrellis.spark._import geotrellis.spark.ingest._import geotrellis.spark.io.index.ZCurveKeyIndexMethodimport geotrellis.raster.Tileimport geotrellis.spark.utils.SparkUtilsimport geotrellis.vector.ProjectedExtentimport org.apache.spark.SparkConfobject TemporalSinglebandIngest extends App {  implicit val sc = SparkUtils.createSparkContext(""GeoTrellis ETL TemporalSinglebandIngest"", new SparkConf(true))  Etl.ingest[TemporalProjectedExtent, SpaceTimeKey, Tile](args, ZCurveKeyIndexMethod.byMillisecondResolution(1000L * 60 * 60 * 24))  sc.stop()}```",2/25/16,1
jasonbeverage,I copied the byMillisecondResolution stuff from your PlanetLabsIngest  you pointed me to the other day @lossyrob.  Should that millisecond resolution parameter be passed in on the command line or is that a sensible default?,2/25/16,1
lossyrob,yeah it would have to be passed in,2/25/16,1
lossyrob,"maybe just second, and you would multiply it by 1000",2/25/16,1
lossyrob,but the temporal resolution is important for performance,2/25/16,1
lossyrob,"if you hae a daily dataset, a yearly resolution would be bad",2/25/16,1
lossyrob,"and if you hav a yearly dataset, daily resolution is bad too (only using 1/365th of the index space)",2/25/16,1
jasonbeverage,so that 1000 * 60 *60 * 24 is doing daily resolution right?,2/25/16,1
lossyrob,yeah,2/25/16,1
jasonbeverage,"Ok, one more last question :)  Is there an option to ETL to ADD a dataset to an existing layer?  Doing multiple ETL ingests into the same layer name to seemed to override the existing layer.",2/25/16,1
jasonbeverage,"I was able to just pass in all my files on a single ingest and it appeared to work fine, both timestamps were picked up correctly.  But I'm wondering how you would just keep filling up that layer with daily collects for example.",2/25/16,1
lossyrob,"not through ETL, but there are _ÑÒLayerUpdaters_Ñù that would take care of this",2/25/16,1
lossyrob,"eventually would be used in ETL, but for now just accessable through code and through the LayerManager object",2/25/16,1
jasonbeverage,"Ok no worries there, just something I need to note.",2/25/16,1
lossyrob,"well, LayerManager object per backend, like AccumuloLayerManager.update",2/25/16,1
jasonbeverage,Yeah got it.,2/25/16,1
owcm,"@lossyrob  Have you had any reported issues with  MultiPolygon's when using  toVector?  I am using a single Tile, not RDD, and I get Message: ""com.vividsolutions.jts.geom.MultiPolygon cannot be cast to com.vividsolutions.jts.geom.Polygon"".  Maybe the data in my case is to complex.",2/25/16,1
lossyrob,shouldn_Ñét be too complex,2/25/16,1
lossyrob,where is that throwing from?,2/25/16,1
owcm,"@lossyrob  Here is the stack:```Message: com.vividsolutions.jts.geom.MultiPolygon cannot be cast to com.vividsolutions.jts.geom.Polygon Trace: geotrellis.vector.Polygon$.apply(Polygon.scala:68)</br>geotrellis.raster.op.global.ToVector$.apply(ToVector.scala:120)</br>geotrellis.raster.op.global.GlobalMethods$class.toVector(GlobalMethods.scala:23)</br>geotrellis.raster.op.global.package$GlobalMethodExtensions.toVector(package.scala:22)</br>geotrellis.raster.op.global.GlobalMethods$class.toVector(GlobalMethods.scala:17)</br>geotrellis.raster.op.global.package$GlobalMethodExtensions.toVector(package.scala:22)</br>tutorial.GeoTrellisService$$anonfun$ingestRoute$3$$anonfun$apply$19.apply(GeoTrellisService.scala:851)</br>tutorial.GeoTrellisService$$anonfun$ingestRoute$3$$anonfun$apply$19.apply(GeoTrellisService.scala:685)</br>spray.routing.directives.RouteDirectives$$anonfun$complete$1$$anon$3.apply(RouteDirectives.scala:49)</br>spray.routing.directives.RouteDirectives$$anonfun$complete$1$$anon$3.apply(RouteDirectives.scala:48)</br>spray.routing.directives.BasicDirectives$$anon$3$$anonfun$happly$1.apply(BasicDirectives.scala:92)</br>spray.routing.directives.BasicDirectives$$anon$3$$anonfun$happly$1.apply(BasicDirectives.scala:92)</br>spray.routing.directives.BasicDirectives$$anonfun$mapRequestContext$1$$anonfun$apply$1.apply(BasicDirectives.scala:30)</br>spray.routing.directives.BasicDirectives$$anonfun$mapRequestContext$1$$anonfun$apply$1.apply(BasicDirectives.scala:30)</br>spray.routing.directives.BasicDirectives$$anon$3$$anonfun$happly$1.apply(BasicDirectives.scala:92)</br>spray.routing.directives.BasicDirectives$$anon$3$$anonfun$happly$1.apply(BasicDirectives.scala:92)</br>spray.routing.RouteConcatenation$RouteConcatenation$$anonfun$$tilde$1$$anonfun$apply$1.apply(RouteConcatenation.scala:32)</br>spray.routing.RouteConcatenation$RouteConcatenation$$anonfun$$tilde$1$$anonfun$apply$1.apply(RouteConcatenation.scala:31)</br>spray.routing.RequestContext$$anonfun$withRejectionHandling$1.applyOrElse(RequestContext.scala:130)</br>scala.runtime.AbstractPartialFunction$mcVL$sp.apply$mcVL$sp(AbstractPartialFunction.scala:33)</br>scala.runtime.AbstractPartialFunction$mcVL$sp.apply(AbstractPartialFunction.scala:33)</br>scala.runtime.AbstractPartialFunction$mcVL$sp.apply(AbstractPartialFunction.scala:25)</br>spray.routing.RequestContext$$anon$1.handle(RequestContext.scala:84)</br>akka.spray.UnregisteredActorRefBase.$bang(UnregisteredActorRefBase.scala:72)</br>spray.routing.RequestContext$$anon$3.handle(RequestContext.scala:102)</br>akka.spray.UnregisteredActorRefBase.$bang(UnregisteredActorRefBase.scala:72)</br>spray.routing.RequestContext.reject(RequestContext.scala:202)</br>spray.routing.directives.RouteDirectives$$anon$5.apply(RouteDirectives.scala:63)</br>spray.routing.directives.RouteDirectives$$anon$5.apply(RouteDirectives.scala:62)</br>spray.routing.directives.BasicDirectives$$anon$3$$anonfun$happly$1.apply(BasicDirectives.scala:92)</br>spray.routing.directives.BasicDirectives$$anon$3$$anonfun$happly$1.apply(BasicDirectives.scala:92)</br>spray.routing.RouteConcatenation$RouteConcatenation$$anonfun$$tilde$1$$anonfun$apply$1.apply(RouteConcatenation.scala:32)</br>spray.routing.RouteConcatenation$RouteConcatenation$$anonfun$$tilde$1$$anonfun$apply$1.apply(RouteConcatenation.scala:31)</br>spray.routing.RequestContext$$anonfun$withRejectionHandling$1.applyOrElse(RequestContext.scala:130)</br>scala.runtime.AbstractPartialFunction$mcVL$sp.apply$mcVL$sp(AbstractPartialFunction.scala:33)</br>scala.runtime.AbstractPartialFunction$mcVL$sp.apply(AbstractPartialFunction.scala:33)</br>scala.runtime.AbstractPartialFunction$mcVL$sp.apply(AbstractPartialFunction.scala:25)</br>spray.routing.RequestContext$$anon$1.handle(RequestContext.scala:84)</br>akka.spray.UnregisteredActorRefBase.$bang(UnregisteredActorRefBase.scala:72)</br>spray.routing.RequestContext.reject(RequestContext.scala:202)```",2/25/16,1
lossyrob,hm that is strange. I can_Ñét trace through the code how that could be happening,2/25/16,1
lossyrob,looks like a problem with the vectorizer,2/25/16,1
lossyrob,"can I (again) get the tile? You_Ñére really flushing out bugs before release, this is great, and I_Ñém sorry you_Ñére hitting so many potholes!",2/25/16,1
owcm,"@lossyrob  uploading now,  thanks for looking at it.",2/25/16,1
jasonbeverage,@lossyrob I sent in a PR for adding the temporal geotiff support to ETL.  I'll hold off on adding the temporal ingest classes until later.,2/25/16,1
pomadchin,@jasonbeverage will take a look at this stuff,2/25/16,1
jasonbeverage,Thanks.  It looked like the multiband temporal support would need some additional methods to HadoopSparkContextMethods so I didn't mess with that.,2/25/16,1
pomadchin,"@jasonbeverage to be honest don_Ñét know why i missed this stuff; even added, as you mentioned, `TemporalGeoTiffInputFormat`",2/25/16,1
jasonbeverage,"I saw it implemented for S3, so you probably were working with S3 :)",2/25/16,1
owcm,@lossyrob  Here is the link to the file I was using ToVector on: https://drive.google.com/file/d/0B1YZbN2gCsmScThNczhtYjFIX1E/view?usp=sharing,2/25/16,1
lossyrob,thank you @owcm !,2/25/16,1
echeipesh,@jasonbeverage thank you for another PR :),2/25/16,1
lossyrob,@pomadchin is this header update here necessary? Doesn_Ñét all the correct attributes get set when the layer is written? https://github.com/geotrellis/geotrellis/blob/master/spark/src/main/scala/geotrellis/spark/io/SparkLayerCopier.scala#L31-L32,2/29/16,1
pomadchin,"Necessary, as an example of this function usage https://github.com/geotrellis/geotrellis/blob/master/spark/src/main/scala/geotrellis/spark/io/accumulo/AccumuloLayerReindexer.scala#L34",2/29/16,1
pomadchin,For accumulo we have to provide a new table written in header ,2/29/16,1
pomadchin,,2/29/16,1
pomadchin,basically we ne it for a generic layer copier,2/29/16,1
rshirochenko,"How to get tiles from layout and how to convert them to RasterRDD? Is it possible to read whole layer as RasterRDD? My code (this will return ContextRDD) :```implicit val _sc = scval accumulo_instance = AccumuloInstance(""test"",""localhost"",""root"", new PasswordToken(""zxczxc""))val attributeStore = new AccumuloAttributeStore(accumulo_instance.connector, ""attributes"")val reader = AccumuloLayerReader[SpatialKey,Tile,RasterMetaData](accumulo_instance)val layer1 = LayerId(""b1"", 12)val layer2 = LayerId(""b3"", 12)val readLayer1 = reader.read(layer1)val readLayer2 = reader.read(layer2)```",2/29/16,1
lossyrob,"@pomadchin the layer writer writes to the table, and writes the correct table in the metadata. In fact, if there was a new schema in the layer writer, that code would overwrite the correct schema with the older, incorrect one",2/29/16,1
lossyrob,I_Ñém pretty sure that code can be dropped,2/29/16,1
lossyrob,@rshirochenko that code looks correct to me,2/29/16,1
lossyrob,"it_Ñéll read the whole `layer1` and `layer2` out as `ContextRDD`, which is really just a `RasterRDD[(SpatialKey, Tile)] with Metadata[RasterMetaData]`, which is in fact type aliased to `RasterRDD[SpatialKey]`. So you have what you need I think",2/29/16,1
pomadchin,"@lossyrob probably, but how would you do cross table copy?",2/29/16,1
jasonbeverage,"If I have lots of files in different projections, can I just feed them to the ETL ingest and expect it to reproject or do I need to pre-process them into a tiled mosaic using GDAL beforehand.",2/29/16,1
jasonbeverage,I downloaded a bunch of landsat 8 scenes to play with and I'm expecting that I'll need to do some processing on it before I can ingest it as a mosaic.,2/29/16,1
lossyrob,@pomadchin the table is set on the writer,2/29/16,1
lossyrob,@jasonbeverage good question.,2/29/16,1
lossyrob,"before I redid reproject to be more accurate, the answer would be yes",2/29/16,1
lossyrob,"now with the other reprojection, it depends on the incoming tiles to be the same",2/29/16,1
lossyrob,is it one image per tile?,2/29/16,1
lossyrob,i.e. have you cut up the landsat at all?,2/29/16,1
jasonbeverage,Right now I just have the raw landsat scenes with 11 bands,2/29/16,1
lossyrob,"sorry, have you cut up each image",2/29/16,1
jasonbeverage,"Nope, just raw right now.",2/29/16,1
lossyrob,ok,2/29/16,1
jasonbeverage,I'm extracting them all now from the bz files and coming up with a game plan :),2/29/16,1
lossyrob,"so if you_Ñére trying to reproject landsat bands, and you have the RAM to do it per large image file, then you can use the other type of reproject",2/29/16,1
lossyrob,you could do something like goal_retile.py,2/29/16,1
lossyrob,gdal_retile.py,2/29/16,1
lossyrob,even do a reproject and retile preprocessing step,2/29/16,1
pomadchin,"@lossyrob you are dealing with it right now, guess you know what's going there, but I remember that I had some problems, but can't point you right now ftw was",2/29/16,1
lossyrob,"so, reprojecting a 11 band landsat image (if you want to read it in/coalesce it into a 11 band tile) is a lot of memory to hold",2/29/16,1
lossyrob,"and if you try to do that in an RDD, you can do 1 partition per multiband image, but even that might not be enough",2/29/16,1
lossyrob,"but if it is, then you can just use the alternate reproject",2/29/16,1
lossyrob,"i.e. not use ETL, but write your own ingest",2/29/16,1
lossyrob,that does the reprojection up front,2/29/16,1
lossyrob,"actually you would be able to use ETL with that, by just modifying the source tiles_Ñ_we just changed the ETL so you have a chance to modify the incoming tiles before it goes through the tiling process",2/29/16,1
lossyrob,"sorry I_Ñém sort of thinking out loud here, but I will have to do something very similar soon",2/29/16,1
jasonbeverage,"What is the ""alternate reproject""?",2/29/16,1
jasonbeverage,"Yeah no worries, I have to think out loud all the time to get anything done :)",2/29/16,1
lossyrob,"so there_Ñés two types of reprojection, one which just straight up takes the tiles and reprojects them",2/29/16,1
lossyrob,https://github.com/geotrellis/geotrellis/blob/master/spark/src/main/scala/geotrellis/spark/reproject/ProjectedExtentComponentReprojectMethods.scala,2/29/16,1
lossyrob,"that you can do on keys with a `ProjectedExtentComponent`, i.e. the keys that are coming in from the source tiles (not yet a SpatialKey or SpaceTimeKey)",2/29/16,1
lossyrob,this causes problems when you have tiles that are adjacent in the source CRS,2/29/16,1
lossyrob,"when they reproject to the dest CRS, and you don_Ñét take into account border pixels, you can end up with NoData artifacts",2/29/16,1
lossyrob,so we have this reprojection,2/29/16,1
lossyrob,https://github.com/geotrellis/geotrellis/blob/master/spark/src/main/scala/geotrellis/spark/reproject/TileRDDReproject.scala,2/29/16,1
lossyrob,which works on an indexed tile set (of SpatialKey or SpaceTimeKey),2/29/16,1
lossyrob,"and that way, you can take your source tiles, create a indexed tile set with a floating grid layout in the source CRS, then do that more accurate reprojection to the dest CRS without any nodata artifacts",2/29/16,1
lossyrob,I need to write a blog post about this...,2/29/16,1
lossyrob,"but the thing is, that relies on all the CRSs being the same",2/29/16,1
lossyrob,so currently we have the reprojection in ETL defaulting to all the projection of the incoming set being the same,2/29/16,1
lossyrob,but I think we need to have that be an optional thing,2/29/16,1
jasonbeverage,Yeah I think that would make sense.,2/29/16,1
jasonbeverage,I would think think a common use case would be just taking a bucket of files and of arbitrary resolutions and crs's and ingesting them into a mosaic.,2/29/16,1
jasonbeverage,For now I don't mind to do some pre-processing with GDAL though.,2/29/16,1
lossyrob,"I would say, try to do it by reprojecting the source tiles, and if there_Ñés memory issues there, you might have to do some preprocessing with retile",2/29/16,1
lossyrob,are you trying to ingest them into a single multiband tile?,2/29/16,1
lossyrob,or have different layers for different bands?,2/29/16,1
jasonbeverage,"I may try both.  I think having separate layer for each band could make sense so you can only pull in the bands you need to do like an ndvi, you don't need all 11 bands.",2/29/16,1
lossyrob,"Yeah, I would def suggest that if you_Ñére going to store all 11 bands, since you would be pulling a lot of data you wouldn_Ñét need otherwise, and that would cause IO pain unnecessarily ",2/29/16,1
lossyrob,"But it means you have to run an ingest per band. And also means that the one-image-per-tile thing could work. So in ETL, you could repartition to the RDD count and then do a reproject using the ProjectedComponentReproject method (`sourceTiles.reproject(destCRS, Bilinear)`)",2/29/16,1
jasonbeverage,"Ok, I'm going to need to look through those methods and see if I understand what is going on.",2/29/16,1
lossyrob,Ok cool. If you have some code that I can take a look at I can talk about where this code would go as well,2/29/16,1
jasonbeverage,Basically right now I'm just using ETL as is.,2/29/16,1
jasonbeverage,And that's been nice.,2/29/16,1
lossyrob,"The API recently changed a bit, are you using the old 2 step version?",2/29/16,1
jasonbeverage,But I've also been preprocessing everything up front using gdal to get them into tiles.,2/29/16,1
lossyrob,so in the README,2/29/16,1
lossyrob,https://github.com/geotrellis/geotrellis/tree/master/spark-etl,2/29/16,1
lossyrob,there_Ñés an example of an ETL with the newer style,2/29/16,1
lossyrob,at the point you get the tiles,2/29/16,1
jasonbeverage,I haven't updated to the master since last Thursday I think.  I'm 4 days behind.,2/29/16,1
lossyrob,"instead of doing `val tiles = etl.load[ProjectedExtent, Tile]`",2/29/16,1
lossyrob,sorry not that,2/29/16,1
lossyrob,sorry let me think about this for a second,2/29/16,1
lossyrob,"nevermind, the readme is wrong",2/29/16,1
jasonbeverage,dontreadme,2/29/16,1
jasonbeverage,:),2/29/16,1
lossyrob,lol,2/29/16,1
lossyrob,@pomadchin can you take a look at the README and update it to the current workflow,2/29/16,1
jasonbeverage,I've been using the SingleBandIngest and MultibandIngest since it was added.,2/29/16,1
jasonbeverage,Are you suggesting basically taking what is in Etl.ingest and just customizing it?,2/29/16,1
lossyrob,https://github.com/geotrellis/geotrellis/blob/master/spark/src/main/scala/geotrellis/spark/ingest/Ingest.scala,2/29/16,1
echeipesh,Ok _Ñ_ so hopefully the ETL changes will settle down after the next PR. What would happen right now is that there is a seperation between reading the source tiles and doing anything with them. Check out the actually compiling example: https://github.com/geotrellis/geotrellis/blob/master/spark-etl/src/main/scala/geotrellis/spark/etl/Etl.scala#L33-L38,2/29/16,1
jasonbeverage,@lossyrob  is that just an alternate ingest from ETL?,2/29/16,1
echeipesh,so between line 34 and 35 you could map over the tiles and reproject each one to the new CRS. At that point you have the tile and the projected extent.,2/29/16,1
echeipesh,Then at that point line 37 will be optional. Line 36 will just cut up those tiles and merge them together. This is the _ÑÒalternative_Ñù reproject that lossyrob was talking about.,2/29/16,1
jasonbeverage,"Ok, so line 36 will actually perform the mosaicing for me then?",2/29/16,1
jasonbeverage,So you have multiple tiles that overlap it will merge them into one?,2/29/16,1
lossyrob,that should read `tileToLayout`,2/29/16,1
echeipesh,"yes,  `val tiled = sourceTiles.tileToLayout[K](metadata, NearestNeighbor)`",2/29/16,1
lossyrob,tileToLayout will merge the tiles,2/29/16,1
jasonbeverage,"Ok cool, I actually didn't realize that it would mosaic for you.",2/29/16,1
jasonbeverage,I thought the tiles had to be non-overlapping.,2/29/16,1
lossyrob,no it will preform the merge,2/29/16,1
lossyrob,"the thing is, even with non-overlapping tiles",2/29/16,1
owcm,"I am trying to to use writeArray in HdfsUtils to write out an Array[Byte], ""get compile error find implicit value for evidence parameter of type geotrellis.spark.io.hadoop.formats.HadoopWritable[Byte][error]     HdfsUtils.writeArray( fsPath, conf, geotiff.toByteArray) ""   Is there an import I am missing?",2/29/16,1
lossyrob,"it will create a tile of the same key for multiple images. Say, a tile that represents the image that covers the left half, and a tile that represents the image that covers the right half",2/29/16,1
jasonbeverage,"So as it stood now if I took all my landsat files, warped them all to epsg:3857 and ran gdalretile on them to dump them out to a directory, I should be able to point ETL to all of the files and it should be able to mosaic them together?",2/29/16,1
lossyrob,"so those would have to merge, and the merge simply replaces pixels that are NoData",2/29/16,1
lossyrob,"for merging when there are both pixels in the same location (which won_Ñét happen if you_Ñére doing landsat with SpaceTimeKey but will if they are overlapping and using SpatialKey), then it will choose one at random",2/29/16,1
jasonbeverage,Ok good deal.,2/29/16,1
jasonbeverage,Do you have a recommended size for what to output using gdalretile?  I know in some of your examples you're using 512x512 pixels.,2/29/16,1
lossyrob,"I would go a big bigger, 512x2 or 512 x 4",2/29/16,1
jasonbeverage,That seems pretty small.,2/29/16,1
jasonbeverage,"Yeah ok, that's what I figured.",2/29/16,1
lossyrob,small is good unless youre using S3,2/29/16,1
jasonbeverage,"Yeah right, then you get tons of network reads.",2/29/16,1
jasonbeverage,"Ok, good deal, lots to go on here.  Enough to keep me quiet for a day or two :)",2/29/16,1
lossyrob,"@owcm that utility function was written to work with writing out things like tiles, but not writing generally to HDFS.",2/29/16,1
owcm,"Okay, thanks",2/29/16,1
lossyrob,"you would have to use the hadoop API to write out the bytes (convert it to a BytesWritable, then write it)",2/29/16,1
lossyrob,you can use the contents of that function to figure out the logic for it though,2/29/16,1
pomadchin,@lossyrob what you pointed? sry missed smth in the dialog,2/29/16,1
lossyrob,the README in the spark-etl is broken,2/29/16,1
lossyrob,nevermind tho @echeipesh is taking a look at it,2/29/16,1
pomadchin,"oops, yes, as we introduced reporojection with buffered tiles :\",2/29/16,1
pomadchin,"@lossyrob also looked through that `headerUpdate` function; absolutely useless, you are right and even in the current api we have to remove it; probably can make it during the different fixes pr ",2/29/16,1
pomadchin,"it is a deprecated function which i needed in a wrong api design (in dirty versions of layer management stuff, and in clean version, which is in a master branch, it is useless)",2/29/16,1
lossyrob,"cool, wanted to make sure, I_Ñéve removed it in my io refactor codebase",2/29/16,1
pomadchin,"cool, thank a lot, Rob!",2/29/16,1
lokifacio,"hihas it changed the way `RasterMetaData.fromRDD(tiff, f, FloatingLayout(250))` returned the raster extent?I did some test few weeks ago (don't remember which version I was using) and I would say it will return an expanded extent to fit the given layout. Today I am doing another test (with 0.10.0-df9500b) and I get the original tiff extent which is not a multiple of tile size.I 'm just reading a tiff using `val tiff = sc.hadoopGeoTiffRDD(path)`.",3/1/16,1
lokifacio,should I correct the extent myself?,3/1/16,1
lossyrob,The extent from the RasterMetaData now (for a while I think) holds the extent of the original raster,3/1/16,1
lossyrob,"which is used in reprojection logic, and I think maybe another place",3/1/16,1
lossyrob,you can get the snapped extent though pretty easily ,3/1/16,1
lossyrob,"```scalaval rmd = RasterMetaData.fromRDD(tiff, f, FloatingLayout(250))val gridBounds = rmd.mapTransform(rmd.extent)val gridBoundsExtent = rmd.mapTransform(gridBounds)```",3/1/16,1
jasonbeverage,"I'm having issues with this landsat ingest, maybe someone can shed some light on what is going wrong.",3/1/16,1
jasonbeverage,I downloaded a bunch of landsat scenes (about 40 of them) and extracted and processed them using gdal.,3/1/16,1
jasonbeverage,"The scenes span the last 3 months, and I've added the TIFFTAG_DATETIME to them using gdal_edit.py so they are recognized as temporal datasets.  ",3/1/16,1
jasonbeverage,"I reprojected all of the bands and chopped them up using gdalwarp and gdalretile (2048x2048).  And I've separated each band into a directory on S3.  So I have a directory for each band (b1, b2, b3, etc) that contains the tiled up data for all of the datasets.",3/1/16,1
jasonbeverage,And they are all warped to epsg:3857,3/1/16,1
jasonbeverage,"I'm trying to do a temporal ingest on band 1 (about 500 files) and it starts but eventually I start getting failures like this```16/03/01 20:05:26 WARN TaskSetManager: Lost task 0.0 in stage 4.0 (TID 500, ip-172-31-6-115.ec2.internal): java.lang.ArrayIndexOutOfBoundsException: 66306	at geotrellis.raster.UShortRawArrayTile.update(UShortArrayTile.scala:32)	at geotrellis.raster.MutableArrayTile$class.set(MutableArrayTile.scala:34)	at geotrellis.raster.UShortArrayTile.set(UShortArrayTile.scala:11)	at geotrellis.raster.MutableArrayTile$class.update(MutableArrayTile.scala:49)	at geotrellis.raster.UShortArrayTile.update(UShortArrayTile.scala:11)	at geotrellis.raster.stitch.Stitcher$TileStitcher$$anonfun$stitch$2.apply(Stitcher.scala:14)	at geotrellis.raster.stitch.Stitcher$TileStitcher$$anonfun$stitch$2.apply(Stitcher.scala:13)	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)	at scala.collection.immutable.List.foreach(List.scala:318)	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)	at geotrellis.raster.stitch.Stitcher$TileStitcher$.stitch(Stitcher.scala:13)	at geotrellis.raster.stitch.Stitcher$TileStitcher$.stitch(Stitcher.scala:10)	at geotrellis.spark.buffer.BufferTiles$$anonfun$bufferWithNeighbors$1$$anonfun$apply$2.apply(BufferTiles.scala:118)	at geotrellis.spark.buffer.BufferTiles$$anonfun$bufferWithNeighbors$1$$anonfun$apply$2.apply(BufferTiles.scala:83)	at scala.Option.map(Option.scala:145)	at geotrellis.spark.buffer.BufferTiles$$anonfun$bufferWithNeighbors$1.apply(BufferTiles.scala:83)	at geotrellis.spark.buffer.BufferTiles$$anonfun$bufferWithNeighbors$1.apply(BufferTiles.scala:82)	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$flatMapValues$1$$anonfun$apply$43$$anonfun$apply$44.apply(PairRDDFunctions.scala:712)	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$flatMapValues$1$$anonfun$apply$43$$anonfun$apply$44.apply(PairRDDFunctions.scala:711)	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)	at scala.collection.Iterator$class.foreach(Iterator.scala:727)	at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)	at scala.collection.TraversableOnce$class.reduceLeft(TraversableOnce.scala:172)	at scala.collection.AbstractIterator.reduceLeft(Iterator.scala:1157)	at org.apache.spark.rdd.RDD$$anonfun$reduce$1$$anonfun$14.apply(RDD.scala:989)	at org.apache.spark.rdd.RDD$$anonfun$reduce$1$$anonfun$14.apply(RDD.scala:987)	at org.apache.spark.SparkContext$$anonfun$36.apply(SparkContext.scala:1932)	at org.apache.spark.SparkContext$$anonfun$36.apply(SparkContext.scala:1932)	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)	at org.apache.spark.scheduler.Task.run(Task.scala:88)	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)	at java.lang.Thread.run(Thread.java:745)```",3/1/16,1
jasonbeverage,"Those tasks continue to to fail, and then finally the spark-submit dies with:```Driver stacktrace:	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1280)	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1268)	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1267)	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1267)	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:697)	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:697)	at scala.Option.foreach(Option.scala:236)	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:697)	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1493)	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1455)	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1444)	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:567)	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1813)	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1933)	at org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1003)	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:147)	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:108)	at org.apache.spark.rdd.RDD.withScope(RDD.scala:306)	at org.apache.spark.rdd.RDD.reduce(RDD.scala:985)	at geotrellis.spark.RasterMetaData$.envelopeExtent(RasterMetaData.scala:70)	at geotrellis.spark.RasterMetaData$.fromRdd(RasterMetaData.scala:95)	at geotrellis.spark.reproject.TileRDDReproject$.apply(TileRDDReproject.scala:109)	at geotrellis.spark.reproject.TileRDDReproject$.apply(TileRDDReproject.scala:172)	at geotrellis.spark.reproject.TileRDDReprojectMethods.reproject(TileRDDReprojectMethods.scala:27)	at geotrellis.spark.reproject.TileRDDReprojectMethods.reproject(TileRDDReprojectMethods.scala:30)	at geotrellis.spark.etl.Etl.reproject(Etl.scala:79)	at geotrellis.spark.etl.Etl$.ingest(Etl.scala:37)	at geotrellis.spark.etl.SinglebandIngest$delayedInit$body.apply(SinglebandIngest.scala:13)	at scala.Function0$class.apply$mcV$sp(Function0.scala:40)	at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:12)	at scala.App$$anonfun$main$1.apply(App.scala:71)	at scala.App$$anonfun$main$1.apply(App.scala:71)	at scala.collection.immutable.List.foreach(List.scala:318)	at scala.collection.generic.TraversableForwarder$class.foreach(TraversableForwarder.scala:32)	at scala.App$class.main(App.scala:71)	at geotrellis.spark.etl.SinglebandIngest$.main(SinglebandIngest.scala:11)	at geotrellis.spark.etl.SinglebandIngest.main(SinglebandIngest.scala)	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)	at java.lang.reflect.Method.invoke(Method.java:606)	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:672)	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180)	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205)	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:120)	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)```",3/1/16,1
jasonbeverage,"This happens if I do a temporal ingest or if I just do a single band ingest as well, so I don't think it's a temporal problem.  Anyone have any ideas?",3/1/16,1
lossyrob,how far back of a version are you using? I seem to remember fixing something similar at some point in the recent past,3/1/16,1
jasonbeverage,I updated to the master like last thursday,3/1/16,1
jasonbeverage,So I'm not super far behind.,3/1/16,1
jasonbeverage,"This is going s3 to s3 on an EMR cluster, but same thing happens going s3 to accumulo",3/1/16,1
jasonbeverage,"I'm also using a splitSize of 10 for the S3 input.  But now I'm just trying 1 to see if that gets any further.  I should have plenty of resources, it's only like 4GB of data and I'm running on a 3 node i2.xlarge cluster.",3/1/16,1
jasonbeverage,Yeah no dice with splitSize of 1 either,3/1/16,1
lossyrob,"and it doesn_Ñét happen consistently, only with some images",3/1/16,1
jasonbeverage,"I've done a temporal ingest before, not with this many images but it's worked previously.",3/1/16,1
jasonbeverage,"I'll grab the master and see if I have any better luck, I was just curious if  you could think of anything obvious that I might be doing wrong.",3/1/16,1
viswaug,wow _Ñ_ i was just on freenode IRC and never realized a gitter channel existed :),3/1/16,1
lossyrob,Sorry about that @viswaug! I have been meaning to get to the subject of the room to point to the gitter.,3/1/16,1
viswaug,noob questions here _Ñ_ i am not a scala/java guy (am willing to pick up scala / i have tinkered with clojure in the past)_Ñ_ is using geotrellis still a good idea for a raster statistics service i need to setup in a couple of months?,3/1/16,1
viswaug,@lossyrob np. just feel stupid that is all _Ñ_ computers make me feel that way every day _Ñ_ so NBD,3/1/16,1
lossyrob,"well, *I* think it_Ñés a good idea :)",3/1/16,1
viswaug,"before I found geotrellis, i was thinking about using GDAL on AWS lambda to accomplish my tasks",3/1/16,1
viswaug,?,3/1/16,1
viswaug,does geotrellis always need me to have a cluster idling on AWS,3/1/16,1
lossyrob,not necessarily,3/1/16,1
lossyrob,"GeoTrellis is a Scala library for geospatial types in general, with a lot of raster operations in particular",3/1/16,1
lossyrob,it also has a package (it_Ñés own jar) for doing raster processing with spark,3/1/16,1
viswaug,i mean it needs a cluster of machines to be available to run the tasks all the time right?,3/1/16,1
lossyrob,"you can do that on a single server to take advantage of multiple cores, or run batch jobs on EMR, or run a cluster for doing processing",3/1/16,1
lossyrob,"It doens_Ñét need a cluster of machines, unless you have a spark application that reads from accumulo that needs a cluster of machines. You can use it in many different ways",3/1/16,1
viswaug,"i was thinking in AWS lambda, i could split up the work into chuncks and make HTTP requests to other AWS lambda enpoints to do the work",3/1/16,1
lossyrob,"I don_Ñét know anything about the use case, so that seems like it might work :)",3/1/16,1
viswaug,"this way, i pay for only the compute i need when i need it",3/1/16,1
viswaug,"also, does geotrellis need the rasters to be preprocessed to ARGs ?",3/1/16,1
lossyrob,"well, there_Ñés a new version of GeoTrellis coming out, that has a GeoTiff reader and writer, so no",3/1/16,1
viswaug,my use case is the exact same as the chattanooga TN example on the geotrellis homepage,3/1/16,1
lossyrob,"but if you want to store distributed large rasters, we have support for storing indexed tiled sets in S3, Accumulo, HDFS, or a local file system",3/1/16,1
lossyrob,in which case it_Ñés stored in a binary format that is our own specification (just avro serialized data types),3/1/16,1
viswaug,very neat _Ñ_ i will look for the docs on preprocessing my geotiffs ,3/1/16,1
lossyrob,"like I said, we_Ñére in a bit of a weird state since we are pre-release",3/1/16,1
lossyrob,"so working out bugs, working out docs",3/1/16,1
viswaug,great,3/1/16,1
viswaug,thank you,3/1/16,1
lossyrob,right now the best docs to read are being pulled together as part of this pull request ,3/1/16,1
lossyrob,https://github.com/geotrellis/geotrellis/pull/1330,3/1/16,1
lossyrob,"the documentation for 0.9 on the website is going to give you the rough idea of things, but also the versions are very different from eachother",3/1/16,1
viswaug,thank you for the links _Ñ_ appreciate it,3/1/16,1
lossyrob,"no problem, let me know how it works out!",3/1/16,1
viswaug,sure thing,3/1/16,1
jasonbeverage,@lossyrob fyi I tried with the master geotrellis and still get the same failure.,3/1/16,1
lossyrob,that_Ñés a nasty bug then,3/1/16,1
jasonbeverage,Do you have any idea what it could be though?  Am I running out of memory somewhere?,3/1/16,1
lossyrob,no it_Ñés not a memory thing,3/1/16,1
lossyrob,where it_Ñés throwing is in the stitcher,3/1/16,1
lossyrob,and it_Ñés trying to update a tile out of bounds of where it should be,3/1/16,1
jasonbeverage,Hmm ok.,3/1/16,1
lossyrob,https://github.com/geotrellis/geotrellis/blob/master/raster/src/main/scala/geotrellis/raster/stitch/Stitcher.scala#L11,3/1/16,1
jasonbeverage,"So like I said, basically the input is a folder full of different layers that have all been warped to mercator and then gdalretiled.",3/1/16,1
jasonbeverage,So they will be overlapping.,3/1/16,1
lossyrob,"they will be overlapping, but at the point it_Ñés throwing they shouldn_Ñét be, since it_Ñés already gone through a `tileToLayout` call",3/1/16,1
jasonbeverage,And is it fine that the input files are overlapping and not necessarily in the same tiling scheme?,3/1/16,1
jasonbeverage,Like I took each individual mercator landsat tile and ran gdalretile on it.,3/1/16,1
lossyrob,yeah,3/1/16,1
lossyrob,that is the right way to go about it,3/1/16,1
lossyrob,I think there is just a bug in buffered tiles,3/1/16,1
lossyrob,I_Ñém going to poke around at it,3/1/16,1
jasonbeverage,"Awesome, thanks.  I'll look around as much as I can and see if I can find any more info that might help out.",3/1/16,1
jasonbeverage,Rob I'm going to say you're right.  I just tried a much simpler ingest of a single layer that was gdalretiled that would have worked before and it's failing in the exact same way.,3/1/16,1
lossyrob,"yeah, I just talked it out with @jamesmcclain and I_Ñém pretty sure I_Ñém not handling some corner cases there",3/1/16,1
lossyrob,so he_Ñés currently trying to fix my dumb code :),3/1/16,1
jasonbeverage,I can stick the data that's failing somewhere you can get to it if it would help.,3/1/16,1
lossyrob,"If it_Ñés not a lot of data, then that might help. @jamesmcclain your call",3/1/16,1
jasonbeverage,It's ~4gb,3/1/16,1
lossyrob,ah. in that case I think it might be better to make sure we can_Ñét flush it out with a unit test first,3/1/16,1
jasonbeverage,"Ok, just let me know.  Happy to upload it if it helps.",3/1/16,1
owcm,Created issue: https://github.com/geotrellis/geotrellis/issues/1350 for Tile ToVector issue I reported back on Feb 25th.,3/1/16,1
lossyrob,thanks @owcm !,3/1/16,1
lossyrob,"I updated the issue to format the stack trace, makes it much easier to read",3/1/16,1
owcm,"Okay, next time I will know better",3/1/16,1
jamesmcclain,@lossyrob @jasonbeverage There is a pull request up that (should) fix the problem: https://github.com/geotrellis/geotrellis/pull/1352,3/2/16,1
jamesmcclain,"If you have a chance, please give it a try",3/2/16,1
owcm,"Trying to read in a small shape file of points:  val features = ShapeFileReader.readPointFeatures(""Community_Centers.shp"").  Get stack trace:```Uncaught error from thread [default-akka.actor.default-dispatcher-4] shutting down JVM since 'akka.jvm-exit-on-fatal-error' is enabled for ActorSystem[default]java.lang.NoSuchMethodError: geotrellis.vector.PointFeature.<init>(Lgeotrellis/vector/Point;Ljava/lang/Object;)V	at geotrellis.geotools.ShapeFileReader$$anonfun$readPointFeatures$1$$anonfun$apply$1.apply(ShapeFileReader.scala:65)	at geotrellis.geotools.ShapeFileReader$$anonfun$readPointFeatures$1$$anonfun$apply$1.apply(ShapeFileReader.scala:65)	at scala.Option.map(Option.scala:145)	at geotrellis.geotools.ShapeFileReader$$anonfun$readPointFeatures$1.apply(ShapeFileReader.scala:65)	at geotrellis.geotools.ShapeFileReader$$anonfun$readPointFeatures$1.apply(ShapeFileReader.scala:65)	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)	at scala.collection.immutable.List.foreach(List.scala:318)	at scala.collection.TraversableLike$class.map(TraversableLike.scala:244)	at scala.collection.AbstractTraversable.map(Traversable.scala:105)	at geotrellis.geotools.ShapeFileReader$.readPointFeatures(ShapeFileReader.scala:65)	at tutorial.GeoTrellisService$$anonfun$testRoute$1.apply(GeoTrellisService.scala:495)	at tutorial.GeoTrellisService$$anonfun$testRoute$1.apply(GeoTrellisService.scala:483)```Should be straight forward or am I missing something.",3/2/16,1
lossyrob,looks like a versioning issue,3/2/16,1
owcm,ahh,3/2/16,1
lossyrob,it_Ñés trying to use a Point class that is different from the version loading into the jvm,3/2/16,1
owcm,"I am on 0.10.0-M1 for most of my modules except geotrellis-raster 0.10.0-d8968b1,  maybe I should step up my geotrellis-geotools version",3/2/16,1
lossyrob,yeah,3/2/16,1
lossyrob,"all the geotrellis versions need to match, since they depend on eachother",3/2/16,1
owcm,"okay",3/2/16,1
owcm,"@lossyrob Thanks that worked, pretty painless to upgrade the versions",3/2/16,1
lossyrob,nice!,3/2/16,1
lossyrob,"I usually put the version in a variable, that way if I change it in one place it changes for all the dependencies",3/2/16,1
lossyrob,a la https://github.com/lossyrob/landsat-demo/blob/master/project/Version.scala#L2,3/2/16,1
owcm,"Good suggestion",3/2/16,1
lossyrob,@dwins the proj4 changes look cool,3/2/16,1
lossyrob,"so you run `test:run` and it will look at our epsg db, a list of known codes, and write out a CSV of _ÑÒvalid_Ñù CRSs",3/2/16,1
lossyrob,so creates the file that the test will use to run against,3/2/16,1
lossyrob,is that right?,3/2/16,1
lossyrob,"and to run it, you_Ñéll have to have `cs2cs` installed (and currently in a specific place on the file system, which should change)",3/2/16,1
dwins,@lossyrob yes to all of that,3/2/16,1
dwins,"the hard-coded path is just so that it will be using the same build that i have been running a debugger against, just taking `cs2cs` from the PATH should be fine",3/2/16,1
lossyrob,nice,3/2/16,1
lossyrob,I think that placement of the generation code makes sense,3/2/16,1
dwins,ok so i'll just move it out of the `dwins` package and call it good,3/2/16,1
lossyrob,"yeah, maybe a name other than `Main`, like `GenerateTestCases` would be better",3/2/16,1
dwins,good idea,3/2/16,1
owcm,"I am reading in Road data from a shapefile, using ShapeFileReader.readMultiLineFeatures.  I want to create a Raster of these LineFeatures.   Is there an approach to Rasterize a Seq of MultiLineFeatures or does one break them out as individual geometries and use the Rasterizer. Thanks in advance.",3/2/16,1
lossyrob,rasterizer can work over MultiLines as well,3/2/16,1
lossyrob,should be able to pass that in as the geometry arg,3/2/16,1
owcm,"cool, okay I will take a better look, thanks",3/2/16,1
owcm,k,3/2/16,1
lossyrob,http://geotrellis.github.io/scaladocs/latest/#package actually points to something useful now. Now to get travis building it automatically...,3/2/16,1
owcm,When reading in Shapefiles is there away to reproject the data similiar to how Geotiffs can be reprojected? ,3/2/16,1
lossyrob,there_Ñés vector reprojection logic,3/2/16,1
lossyrob,looking for examples,3/2/16,1
lossyrob,but basicall `import geotrellis.vector.reproject._`,3/2/16,1
lossyrob,"and `geometry.reproject(oldCrs, newCrs)`",3/2/16,1
lossyrob,"minimal test, since the heavy testing happens in proj4...",3/2/16,1
lossyrob,https://github.com/geotrellis/geotrellis/blob/master/vector-test/src/test/scala/spec/geotrellis/vector/reproject/packageSpec.scala,3/2/16,1
lossyrob,you can call `reproject` on Feature data as well and it will keep it_Ñés data,3/2/16,1
lossyrob,so reprojecting a Multiline should just be,3/2/16,1
lossyrob,"```scalaval mlf: MultiLineFeature[Foo] == ???val reprojected: MultiLineFeature[Foo]] = mlf.reproject(oldCRS, newCRS)```",3/2/16,1
lossyrob,"one hitch might be, we don_Ñét read in CRS_Ñés from shapefiles, so you_Ñéll have to know the source CRS",3/2/16,1
lokifacio,"how do you read back tiff tiles stored in hadoop using  `   tiles.renderGeoTiff(""tile-bucket/{z}/{x}/{y}.tiff"")`?. I can read one coord tiffs using `sc.hadoopGeoTiffRDD(""tile-bucket/<zoom>/<coord>/"")`",3/3/16,1
lossyrob,you_Ñéd have to use the SlippyMap reader,3/3/16,1
lossyrob,"generally we save them off to TIFFs if we are going to read them through web services, and not use them in spark anymore",3/3/16,1
lossyrob,"if we are going to do data processing on the layers, then I_Ñéd write them out into one of the backend Avro formats (using the LayerWriters)",3/3/16,1
lossyrob,"@moradology I see a fromQuantileBreaks for a histogram int, can we get colors from a double histogram_Ñés quantile breaks? https://github.com/geotrellis/geotrellis/blob/master/raster/src/main/scala/geotrellis/raster/render/color/ColorClassifier.scala#L432",3/3/16,1
lokifacio,"thanks @lossyrob, I found them later and I'm  using HadoopLayerReader/Writer",3/4/16,1
schBen,a,3/4/16,1
schBen,"Hi! How do I compress a GeoTiff in GeoTrellis? I have a GeoTiff Array[Byte] from GeoTiffWriter.write(data). I want to use deflate compression and found DeflateCompression.scala. I have tried ```val compressor = DeflateCompression.createCompressor(1)val geotiffCompressed: Array[Byte] = compressor.compress(geotiffBytes, 0)```There is an output but it seems to be broken. Am I missing something?",3/4/16,1
SDeathLeaDer,,3/4/16,1
SDeathLeaDer,,3/4/16,1
SDeathLeaDer,,3/4/16,1
lossyrob,"@schBen there should be some API to make this easier (I_Ñéll put this in, thanks for the use case)",3/4/16,1
lossyrob,but really you just want to change the geotiff options to a different compression before writing,3/4/16,1
lossyrob,"```scalaval geoTiff: SingleBandGeoTiff = ???val compressed = geoTiff.copy(options = GeoTiffOptions(DeflateCompression))val geotiffCompressed: Array[Byte] = GeoTiffWriter.write(compressed)```",3/4/16,1
pomadchin,@lossyrob @jamesmcclain Spark 1.5.x / 1.6.x uses kryo `2.21` by default; that was the only reason we excluded newer version from our deps. https://github.com/geotrellis/geotrellis/issues/1305,3/4/16,1
jamesmcclain,"Okay, that pull request contains an upgrade of the Kryo version.   Are you saying that that needs to be reverted?",3/4/16,1
pomadchin,"yes, kryo version should be 2.21",3/4/16,1
lossyrob,"yeah if it_Ñés caused issues, we should revert",3/4/16,1
lossyrob,@pomadchin thanks for the catch. any others you see as potential problems?,3/4/16,1
jamesmcclain,"Okay, preparing pull request now",3/4/16,1
pomadchin,"yes, there was also a postresql driver upgrade: from `9.1-901.jdbc4` `9.2-1002.jdbc4`, was that tested? I know that there are some problems and bugs in some posters drivers versions (the same code but with different driver versions just can throw you runtime errors, had experince with that);",3/4/16,1
jamesmcclain,"It was not thoroughly tested, might as well revert it",3/4/16,1
jamesmcclain,can the kryo serialization update stay?,3/4/16,1
jamesmcclain,"""kryo-serializers"", I should say",3/4/16,1
jamesmcclain,From 0.27 to 0.37,3/4/16,1
pomadchin,"that_Ñés why i could not used newer postgres driver version than`9.1-901.jdbc4`yes, i think if it compiles with 0.37 ",3/4/16,1
jamesmcclain,Okay,3/4/16,1
pomadchin,btw i'll test this libs compatibility during my tasks,3/4/16,1
pomadchin,but think everything should be fine,3/4/16,1
jamesmcclain,"Okay, thanks!",3/4/16,1
pomadchin,"Sry, i had to catch it before merge ",3/4/16,1
pomadchin,):,3/4/16,1
jamesmcclain,Better now than never,3/4/16,1
jamesmcclain,"I reverted those changes, building locally on my machine before putting up PR",3/4/16,1
lossyrob,did we revert the pgsql?,3/4/16,1
jamesmcclain,Yes,3/4/16,1
jamesmcclain,The branch is here: https://github.com/jamesmcclain/geotrellis/tree/fix/revert-kryo-upgrade,3/4/16,1
pomadchin,btw James: http://mvnrepository.com/artifact/org.postgresql/postgresql,3/4/16,1
pomadchin,`postgresql` moved to an `org.postgresql`,3/4/16,1
jamesmcclain,"Ah, okay",3/4/16,1
jamesmcclain,Good to know,3/4/16,1
pomadchin,so think that there are two ways: to revert or to investigate how new driver version works ):,3/4/16,1
jamesmcclain,I think we should revert now and investigate later,3/4/16,1
lossyrob,"yeah, these should be pretty soft dependency upgrades. If there_Ñés potential to add last minute bugs, let_Ñés prefer not to upgrade",3/4/16,1
lossyrob,really the slick stuff should be tested as part of integration tests,3/4/16,1
dwins,@lossyrob ok looks like we have a few places in proj4j where we are counting on reference equality rather than structural equality to choose some branches. so serializing and deserializing changes how some code paths work out,3/4/16,1
dwins,pretty mechanical fix i suppose,3/4/16,1
lossyrob,"huh, I hope that works out, that would be a pretty easy fix",3/4/16,1
dwins,well there's a lot of stuff like `if src.setting != tgt.setting { doSomeMath }` to avoid calculations entirely when something should be an identity. aside from speeding things up you avoid accumulating rounding errors in these cases so I think it is a good candidate for the slight discrepancies we're seeing,3/4/16,1
lossyrob,that does make sense. It_Ñés off by so little that it must be a rounding error,3/4/16,1
dwins,i'll review the rest of the code but https://github.com/dwins/geotrellis/commit/4cc9443c2d7255c8cbf84f94e65ca08ff0a631ae makes TileRDDReprojectSpec pass again,3/4/16,1
lossyrob,awesome,3/4/16,1
lossyrob,@owcm what_Ñés the status of this bug? Is this fixed in a newer version or still a problem? https://github.com/geotrellis/geotrellis/issues/1304,3/5/16,1
owcm,"@lossyrob Missed your message earlier. Will try out this weekend.  I am on  version 0.10.0-d8968b1, should I referance even a later version?",3/6/16,1
owcm,"@lossyrob updated geotrellis/geotrellis#1304, I am receiving the same error.",3/6/16,1
owcm,"@lossyrob update, I ran a couple different scenarios.  If the tiles I ingest are spatially adjacent things work out fine.  If they are not adjacent then I get the error.   I would not have a problem closing the ticket since it is working for the use case I need.  Used version 0.10.0-2283ec5",3/7/16,1
lossyrob,"no, we need to solve the bug",3/7/16,1
lossyrob,is there a repeatable pattern about the setup of the tiles? can you come up with a test case that fails with a reasonable amount of data to give to us?,3/7/16,1
owcm,Yes that was my next question,3/7/16,1
owcm,I can recreate with three tiles,3/7/16,1
owcm,and send the code I used,3/7/16,1
lossyrob,that would be great,3/7/16,1
lossyrob,if you could put the links in the ticket,3/7/16,1
owcm,sure thing,3/7/16,1
lossyrob,thanks!,3/7/16,1
owcm,"@lossyrob Okay ticket updated geotrellis/geotrellis#1304, with link to test project.  You think there is any chance geotrellis/geotrellis#1350 will be on the docket this week? Thanks Chris",3/7/16,1
rshirochenko,"Whats the purpose of movement accumulo folder from geotrellis/spark/io/accumulo  to geotrellis/accumulo?",3/7/16,1
jamesmcclain,We have removed the accumulo and s3 dependencies from the `geotrellis-spark` project,3/7/16,1
pomadchin,"@rshirochenko now it is a separate sub project (both accumulo and s3), so if you don_Ñét want to use accumulo / s3 just don_Ñét include them into your project dependencies;and to use accumulo you have to include a separate `""com.azavea.geotrellis"" %% ""geotrellis-accumulo""  % ""0.10.0-SNAPSHOT_Ñù` dependency ",3/7/16,1
jamesmcclain,"accumulo and s3 can still be used, but they are no longer required for those who do not need them",3/7/16,1
lossyrob,"@owcm there is an open PR that references the issue, should close when merge https://github.com/geotrellis/geotrellis/pull/1368",3/7/16,1
owcm,"Using the built in  Geotrellis  Pyramid.upLevels method from geotrellis-landsat-tutorial.  If the resolution of my data is 1 meter or 0.5  resolution I get zoom layers of 17 or 18.   Is there way to force lower level zoom levels 19,20,21 ?",3/8/16,1
lossyrob,"you would have to upsample the layer to zoom 21 first, and then pyramid. are you sure you want to do that? that_Ñés going to explode the storage size",3/8/16,1
pomadchin,@lossyrob @owcm mb resampling would be enough?,3/8/16,1
owcm,@pomadchin mb means?,3/8/16,1
pomadchin,*may be to resample tiles on demand would be enough for you? or you want to store these zoom levels in your storage?,3/8/16,1
owcm,@pomadchin   that is a good point resampling on fly,3/8/16,1
pomadchin,@owcm do you need an example?,3/8/16,1
owcm,"@pomadchin  No I think I can figure that out.   But I had another question, off topic, home may be you could answer. I wanted to make sure I understand the current state of the ViewShed Functionality in the system.  It seems ViewShed currently is written at the single tile level.  Not supported for RasterRDD's currently.  ",3/8/16,1
owcm,"If I wanted to run a view shed over all the rasters in my RasterRDD I guess I would have to create a master tile from them.  Run my viewshed, create a new RasterRDD and then tile that result.  If I that makes senses.",3/8/16,1
lossyrob,yeah veiwshed is not yet parallelized,3/8/16,1
owcm,"@lossyrob  Is there a plan to parallelize  in the near future?  Is there even interest in having it done?  It's a feature that our team can use.  Asking your advice: If one (thinking me, geotellis newbie) were to follow the patterns for the focal and elevation  parallelized methods , do you think it would be a manageable task. \",3/8/16,1
rshirochenko,Is there any method to list all layers with zooms in accumulo?,3/8/16,1
lossyrob,"@owcm veiwshed is actually a very challenging one to parralellize. There are papers I_Ñéve read about it, but the implimentation in Spark will be non-trivial. It_Ñés something that we_Ñéve wanted, but it_Ñés not currently in the near term feature list for the next release",3/8/16,1
lossyrob,"@rshirochenko you can do that with an AccumuloAttributeStore. You might already have one if you have an AccumuloLayerReader. If not, and have an AccumuloInstance,",3/8/16,1
lossyrob,"```scalaval attributeStore = AccumuloAttributeStore(instance)val layers: Seq[LayerId] = attributeStore.layerIds```",3/8/16,1
lossyrob,https://github.com/geotrellis/geotrellis/milestones/0.10,3/8/16,1
jasonbeverage,"Hey all, quick EMR question for you experts.  I've had good success running geotrellis jobs on EMR, it's great to just fire up a cluster and have it work and have it all configured for you.  One thing I've noticed though is that no matter what it doesn't seem like all the available cores are actually used.",3/8/16,1
jasonbeverage,"For example, I'm running a cluster now with 32 Vcores (2 followers) and VCores used is only 3",3/8/16,1
echeipesh,"yes, it_Ñés frustratingly tricky to configure the YARN correctly. Opinions on this differ but I get best results with to set num-executors to the number of workers and set executor cores to the number of cores on each machine.",3/8/16,1
echeipesh,You should also double check the memory allocation.,3/8/16,1
jasonbeverage,Memory usage is 43.88 GB of 45GB though.,3/8/16,1
echeipesh,That sounds good at least :),3/8/16,1
jasonbeverage,So it at least thinks it's using all the memory,3/8/16,1
jasonbeverage,"```spark.executor.instances         2spark.executor.cores             16```",3/8/16,1
echeipesh,`--num-executors 6 --executor-cores 4` is the args I use for spark-submit,3/8/16,1
jasonbeverage,That makes sense b/c I have 2 follower nodes and they have 16 cores each.,3/8/16,1
lossyrob,https://github.com/hotosm/oam-server-api/blob/master/lib/tiler.js#L116,3/8/16,1
lossyrob,https://github.com/hotosm/oam-server-api/blob/master/lib/tiler.js#L50,3/8/16,1
lossyrob,executors would be the number of nodes * (cores per node / cores per executor),3/8/16,1
lossyrob,then it_Ñés a matter of playing with the executor memory so that you can fit all the executors on one machine,3/8/16,1
lossyrob,which is somewhat less that (memory per node * number of nodes) / number of executors,3/8/16,1
lossyrob,"if executors aren_Ñét being allocated, it_Ñés probably because YARN is seeing that it can_Ñét fit executors on a node because of memory requirements",3/8/16,1
lossyrob,@pomadchin have you been using `mergeUpdate`?,3/8/16,1
jasonbeverage,Cool thanks rob.,3/8/16,1
pomadchin,"@lossyrob yes, but usually I make same thing manually",3/8/16,1
lossyrob,"Also just merged https://github.com/geotrellis/geotrellis/pull/1371, @pomadchin you were asking for that yesterday",3/8/16,1
jasonbeverage,"I was hoping that EMR's ""maximizeResourceAllocation"" setting was just going to work with no tinkering :)",3/8/16,1
pomadchin,@lossyrob cool; btw you looked through implicits reorgonization? o:,3/8/16,1
jasonbeverage,Cool on the merge rob.  Thanks.,3/8/16,1
lossyrob,"@jasonbeverage yeah I had some unexpected results with that, so went to manual mode and tinkered with numbers until the spark UI showed me the right thing",3/8/16,1
lossyrob,@jasonbeverage thank you!,3/8/16,1
lossyrob,@pomadchin I_Ñém thinking about how this merge update should work,3/8/16,1
lossyrob,"There_Ñés a merge that merges two RDD[(K, V)]",3/8/16,1
lossyrob,but no mention of the M,3/8/16,1
lossyrob,"we need to merge the M_Ñés as well, especially now that we support key indexes with larger key bounds then the initially written data",3/8/16,1
lossyrob,"so if I update a layer outside of the original extent and key bounds, I need to update the metadata accordingly",3/8/16,1
lossyrob,we weren_Ñét updating the metadata on our updates,3/8/16,1
pomadchin,"yes, we were planing to refactor updater later",3/8/16,1
lossyrob,cool. later is now now :),3/8/16,1
pomadchin,:D :D ,3/8/16,1
lossyrob,@pomadchin  did you write this? https://github.com/geotrellis/geotrellis/blob/master/spark/src/main/scala/geotrellis/spark/merge/TileRDDMerge.scala,3/8/16,1
pomadchin,do you want to include it into your api refactor pr?,3/8/16,1
pomadchin,@lossyrob Eugene,3/8/16,1
lossyrob,"yeah I am currently messing with the Updaters,  I thought I had them sorted out in that PR but def didnt",3/8/16,1
lossyrob,"@echeipesh I_Ñém thinking that the TileRDDMerge should be on `RDD[(K, V)] with Metadata[M]` instead of just `RDD[(K, V)]`",3/8/16,1
pomadchin,@lossyrob that means that Metadata should provide merge method also,3/8/16,1
lossyrob,yeah,3/8/16,1
pomadchin,actually RasterMetadata type has such function,3/8/16,1
lossyrob,eh I have to push code around to see how it should go,3/8/16,1
echeipesh,"That makes sense, would also give a chance to merge offset layouts.",3/8/16,1
jasonbeverage,Thanks for the tips on the spark settings rob.  It's almost like the spark.executor.cores is being ignored or misused on yarn.,3/8/16,1
pomadchin,@lossyrob @echeipesh old functions to merge rdd_Ñés without metadata would be removed? ,3/8/16,1
jasonbeverage,I changed it so I only had 1 core per executor but an executor per CPU and then just reduced the memory per executor.  Now it's showing lots more vcores in use.,3/8/16,1
lossyrob,"@jasonbeverage nice, that was pretty much the path I took as well",3/8/16,1
lossyrob,@pomadchin I guess I might just be trying to avoid a conflict between RDD_Ñés with or without metadata,3/8/16,1
lossyrob,I_Ñém starting to think that the merge function should just be optionally passed into the updater,3/8/16,1
lossyrob,"and that could just be (_ merge _), which would pick up the MergeMethods implicit at the call site",3/8/16,1
lossyrob,ugh,3/8/16,1
lossyrob,`_ merge _`,3/8/16,1
owcm,"Just a general question, I know you guys are moving towards a major release, and are doing a lot of refactoring.  After the release do you see things stabilizing in terms of packages, classes, traits, etc.   ",3/8/16,1
lossyrob,"yes, the goal is that the API is set at 0.10.0",3/8/16,1
owcm,"Cool beans, thanks",3/8/16,1
lossyrob,"then the next couple of bugfix releases fix any issues, but don_Ñét change APIs*",3/8/16,1
lossyrob,*(we_Ñéll see about that. that_Ñés the hope),3/8/16,1
owcm,Cool,3/8/16,1
owcm,"@lossyrob I referenced the 0.10.0-7b20123 March 8th from bintray, and ToVector seems to be working for me now. geotrellis/issues/1350",3/8/16,1
lossyrob,awesome. I closed the issue as fixed,3/8/16,1
lossyrob,"@pomadchin can you see anything wrong with this interface, like would this not support a case you have seen",3/8/16,1
lossyrob,"```scala  def update[    K: AvroRecordCodec: Boundable: JsonFormat: ClassTag,    V: AvroRecordCodec: ClassTag,    M: JsonFormat: Component[?, Bounds[K]]: Mergable  ](id: ID, rdd: RDD[(K, V)] with Metadata[M], mergeFunc: (V, V) => V): Unit =```",3/8/16,1
pomadchin,probably that what we wanted; looks brilliant,3/8/16,1
lossyrob,cool cool,3/8/16,1
lossyrob,@echeipesh it doesn_Ñét look like the `LayerReader`_Ñés `defaultNumPartitions` is used anywhere,3/8/16,1
pomadchin,it would be a generic function which can work with keys out of old layer bounds?,3/8/16,1
lossyrob,just defined in all the readers,3/8/16,1
lossyrob,@pomadchin here is the usage,3/8/16,1
lossyrob,"```scala    val updatedRdd: RDD[(K, V)] =      existingTiles        .leftOuterJoin(rdd)        .mapValues { case (layerTile, updateTile) =>          updateTile match {            case Some(tile) =>              mergeFunc(layerTile, tile)            case None =>              layerTile          }      }```",3/8/16,1
lossyrob,"so if there_Ñés no overlap, it leaves the exsiting tiles alone, but if there_Ñés an update, do the merge",3/8/16,1
lossyrob,default is to just return the update tile unchanged,3/8/16,1
pomadchin,:+1: ,3/8/16,1
lossyrob,"@pomadchin I_Ñém at the code sprint with @rfecher, he_Ñés having trouble running geodocker container",3/8/16,1
lossyrob,we_Ñére getting out of memory errors,3/8/16,1
pomadchin,that_Ñéa real problem; caused by Accumulo; ,3/8/16,1
rfecher,when I run install.sh and it gets to compiling geowave from source it kills the process,3/8/16,1
pomadchin,oh gothca,3/8/16,1
lossyrob,"you got around that though, right? and had run the geowave example code",3/8/16,1
pomadchin,"yes, i just had no such problemsprobably that caused by my fat machine :\",3/8/16,1
pomadchin,sec I remember smth was in geowave docs,3/8/16,1
pomadchin,"`export MAVEN_OPTS=""-Xmx512m -XX:MaxPermSize=192m_Ñù`",3/8/16,1
pomadchin,they use this maven opts before build,3/8/16,1
pomadchin,probably it can help,3/8/16,1
lossyrob,yeah he messed with that,3/8/16,1
lossyrob,before it was killing the process,3/8/16,1
lossyrob,but he reduced the mem and then was getting a JVM memory error,3/8/16,1
pomadchin,how many nodes you have launched?,3/8/16,1
pomadchin,2 / 3 ?,3/8/16,1
lossyrob,3,3/8/16,1
pomadchin,and ram amount?,3/8/16,1
lossyrob,2 gigs in the docker machine,3/8/16,1
lossyrob,how much in yours?,3/8/16,1
pomadchin,probably Accumulo takes all mem ,3/8/16,1
lossyrob,and how many nodes are you running?,3/8/16,1
pomadchin,on my local machine i have ~4 gigs and max two nodes,3/8/16,1
lossyrob,including master?,3/8/16,1
pomadchin,but i tested multinode cluster in a google cloud with 50+ gb ram,3/8/16,1
pomadchin,"yea, including master",3/8/16,1
lossyrob,oh nice so you were running geotrellis in docker containers on google cloud,3/8/16,1
pomadchin,"yes, problem is in Accumulo; for Accumulo I seted up not the smallest memory flags",3/8/16,1
lossyrob,you set the memory flags to the smallest/,3/8/16,1
lossyrob,?,3/8/16,1
pomadchin,only accumulo requires in this configuration ~1/2 gbs; :\ guess we have to tune it a bit ,3/8/16,1
pomadchin,10 mins I can make smallest flags version,3/8/16,1
lossyrob,well let_Ñés see if this works_Ñ_change the VM memory and trying 2 nodes,3/8/16,1
pomadchin,there were problems with the smallest configuration on Accumulo 1.7.0 probably that was fixed in Accumuo 1.7.1,3/8/16,1
lossyrob,but it would probably be a good thing to have_Ñ_maybe an issue in geodocker-cluster,3/8/16,1
lossyrob,@echeipesh nvm about defaultNumPartitions,3/8/16,1
pomadchin,ok,3/8/16,1
pomadchin,https://github.com/geotrellis/geodocker-cluster/issues/18,3/8/16,1
lossyrob,nice,3/8/16,1
lossyrob,actually I guess that would be necessary for integration tests yeah,3/8/16,1
lossyrob,"I_Ñéd want to set up a machine in the office to run the tests against the docker cluster, wouldn_Ñét want it to take up to much mem",3/8/16,1
pomadchin,"yeas, gotha",3/8/16,1
lossyrob,or maybe that_Ñés what we could use the LocationTech build box for,3/8/16,1
pomadchin,preparing a quick fix; will start build process for this things this evening,3/8/16,1
lossyrob,looks like Rich_Ñés thing is working,3/8/16,1
pomadchin,wooh(,3/8/16,1
rfecher,"yep, increased my VM memory to 4G and its running fine now",3/8/16,1
pomadchin,there could be an issue with geotrellis build; due to outdated chattademo ,3/8/16,1
pomadchin,it is not hard to figure out revision of geotrellis to make it work,3/8/16,1
pomadchin,@lossyrob guess it is another thing during this sprint that has to be done (update demo),3/8/16,1
lossyrob,I would say that can be a next sprint thing,3/8/16,1
pomadchin,@rfecher @lossyrob https://github.com/geotrellis/geodocker-cluster/pull/19/files,3/8/16,1
lokifacio,"how do you get RasterMetaData form RDD[(SpatialKey, Tile)]? when using RasterMetaData.fromRDD I'm getting```Error:(139, 42) could not find implicit value for evidence parameter of type geotrellis.spark.ingest.ProjectedExtentComponent[geotrellis.spark.SpatialKey]    val metadata = RasterMetaData.fromRdd(rdd, layout)                                         ^```",3/9/16,1
pomadchin,@lokifacio `import geotrellis.spark.ingest._` implicits are in the ingest package,3/9/16,1
lokifacio,I'd already imported it,3/9/16,1
pomadchin,oh yes,3/9/16,1
pomadchin,`geotrellis.spark._`,3/9/16,1
pomadchin,https://github.com/geotrellis/geotrellis/blob/master/spark/src/main/scala/geotrellis/spark/reproject/Implicits.scala#L19,3/9/16,1
lokifacio,"I can use it with RDD[(ProjectedExtent,  Tile)] but not with RDD[(**SpatialKey**, Tile)]",3/9/16,1
pomadchin,"@lokifacio oh sry; that means that your tiles are ingested, and you can read raster metadata out of the layer attribute store",3/9/16,1
pomadchin,https://github.com/geotrellis/geotrellis/blob/master/accumulo/src/main/scala/geotrellis/spark/io/accumulo/AccumuloAttributeStore.scala,3/9/16,1
lokifacio,"well, I'm not using Accumulo...and not, my tiles are not yet ingested (although I do it at the end of my pipeline)Indeed, I really don't need it, I'm trying to check the metadata after applying a localMask to a RasterRDD[SpatialKey]",3/9/16,1
lokifacio,I guess it should be the same as my input raster,3/9/16,1
pomadchin,"oh, `RasterRDD` changes everything; you can just call `metadata` function: ```scalaval rdd: RasterRDD[K] = _Ñ_val metadata = rdd.metadata```",3/9/16,1
lokifacio,"yep, I know",3/9/16,1
echeipesh,"@lokifacio Sounds like what happened is that your ingested some tiles, then you used a layer reader to get back a layer, maybe used them in an operation and now you_Ñére looking for their metdata ?",3/9/16,1
lokifacio,"I have:```val masked = rdd.localMask(mask, NODATA, NODATA) ```which hasn't any metadata",3/9/16,1
echeipesh,"You have to be a little careful when wanting to preserve metadata through the operations, check out this README and let us know if there are any clarifications that are needed: https://github.com/geotrellis/geotrellis/blob/master/docs/spark/map-algebra.md",3/9/16,1
lokifacio,@echeipesh more or less,3/9/16,1
echeipesh,"basically you should be able to do `rdd.withContext { _.localMask(mask, NODATA, NODATA) }` to keep the metadata around.",3/9/16,1
lokifacio,"@echeipesh  thanks for the doc, really appreciated after being diving through code for some weeks...what I usually do after my operations is creating a new ContextRDD(rdd, metadata)",3/9/16,1
lokifacio,not sure if the outcome is the same,3/9/16,1
lokifacio,"I have to leave now, but I'll check that doc tomorrow and let you know if I have any questions",3/9/16,1
owcm,"I have list of Polygons List[vector.PolygonFeature[Int]],  I want create a raster populated with that list of polygons .  I see PolygonRasterizer but it seems to work on single Polygon objects.    Do I need to iterate through my polygon list and create individual rasters and then combine the results to achieve my final result or is there another approach?",3/9/16,1
echeipesh,"@owcm The approach would be to create a single mutable tile and then iterate over each of your poligons using `PolygonRasterizer.foreachCellByPolygon` setting the correspondingy value to cell given to the f(col, row) function.",3/9/16,1
owcm,"@echeipesh  okay this worked for me:```       val tileForAll = DoubleArrayTile.empty( lclTile.cols, lclTile.rows)        fileteredPolygons.foreach( f=> {          // Now lets create a raster wiith just our polygons          val r = PolygonRasterizer.foreachCellByPolygon( f.geom, reprojGeoTiff.rasterExtent) { (x:Int, y:Int) =>            tileForAll.setDouble(x, y, 1.0)//          }        }) ```",3/9/16,1
echeipesh,"That looks correct, you could grab the data from the feature in the closure as well of course.",3/9/16,1
owcm,I have been experimenting with a simple viewshed call,3/9/16,1
lossyrob,Any luck?,3/9/16,1
owcm,"Question should have read:I have been experimenting with a simple viewshed call```val vsExtTile = Viewshed( lclTile, lclTile.cols/2, lclTile.rows/2 )```Running on tile that is 1500x1500 cells, 1 meter resolution.   Takes on the order of 15 to 20 seconds to run on my local setup.  ApproxViewshed takes less then a second but results are to generalized. Local  spark setup is:```  val conf =      new SparkConf()        .setMaster(""local[*]"")        .setAppName(""Spark Tiler"")        .set(""spark.serializer"", ""org.apache.spark.serializer.KryoSerializer"")        .set(""spark.kryo.registrator"", ""geotrellis.spark.io.kryo.KryoRegistrator"")```iMac specs are:Processor Name: Intel Core i7      Processor Speed: 2.3 GHz      Number of Processors: 1Wonder if others had any benchmark numbers or  experiences with different tile sizes ",3/9/16,1
lossyrob,"The exact viewshed has not been optimized at all, there are perhaps better algorithms for it",3/9/16,1
lossyrob,"but that is what a student being mentored under Facebook Open Academy had come up with. I hadn_Ñét run it on that big of tiles TBH, so I_Ñém not too suprised it_Ñés taking that long",3/9/16,1
lossyrob,it_Ñés something we would have to throw someone like @jamesmcclain at to get up to speed :),3/9/16,1
jamesmcclain,Sounds pretty interesting,3/9/16,1
dwins,"eh, just hit the turbo button https://en.wikipedia.org/wiki/Turbo_button",3/9/16,1
jamesmcclain,I miss the turbo button,3/9/16,1
owcm,Hmm okay,3/9/16,1
lossyrob,"@jamesmcclain @pomadchin de.javakaffee:kryo-serializers uses kryo 3.0.3, was that some thing we discussed? Will that cause conflict?",3/9/16,1
lossyrob,it_Ñés weird because they have different orgs,3/9/16,1
lossyrob,"com.esotericsoftware	kryo	3.0.3",3/9/16,1
lossyrob,vs,3/9/16,1
lossyrob,"com.esotericsoftware.kryo	kryo	2.21",3/9/16,1
jamesmcclain,Didn't the kryo upgrade get reverted?,3/9/16,1
lossyrob,"it did, to 2.21",3/9/16,1
lossyrob,but theres a transitive dependency to 3.0.3,3/9/16,1
lossyrob,so the javakaffe kryo serializers depends on 3.0.3,3/9/16,1
lossyrob,so they are both on the classpath,3/9/16,1
lossyrob,so says the output of dependencyGraph,3/9/16,1
jamesmcclain,I think the kryo serializer upgrade was reverted also,3/9/16,1
lossyrob,"```console+-de.javakaffee:kryo-serializers:0.37[info]   | | +-com.esotericsoftware:kryo:3.0.3[info]   | | | +-com.esotericsoftware:minlog:1.3.0[info]   | | | +-com.esotericsoftware:reflectasm:1.10.1[info]   | | | | +-org.ow2.asm:asm:5.0.3[info]   | | | | [info]   | | | +-org.objenesis:objenesis:1.2 (evicted by: 2.1)[info]   | | | +-org.objenesis:objenesis:2.1[info]   | | | [info]   | | +-com.google.protobuf:protobuf-java:2.6.1```",3/9/16,1
jamesmcclain,So things should be the same?,3/9/16,1
jamesmcclain,"No that is incorrect, 0.37 is still there",3/9/16,1
jamesmcclain,So that could be an issue,3/9/16,1
pomadchin,"we have to replace `exclude(""com.esotericsoftware.kryo"", _ÑÒkryo_Ñù)` to `exclude(""com.esotericsoftware"", ""kryo_Ñù),`",3/9/16,1
pomadchin,didn_Ñét notice that organisation moved; yes that can cause problems,3/9/16,1
echeipesh,"@lossyrob Why is `FileLayerMover` the only mover that accepts two attribute stores, is that the mechanism to encode moving layers between directories? https://github.com/geotrellis/geotrellis/blob/master/spark/src/main/scala/geotrellis/spark/io/file/FileLayerMover.scala",3/9/16,1
lossyrob,we should change the javakaffee to be version `0.25`,3/9/16,1
lossyrob,to match kryo verisons,3/9/16,1
lossyrob,"I_Ñém going to include that in my IO branch, since it_Ñés a small change and that_Ñés what I_Ñém currently working with",3/9/16,1
jamesmcclain,Okay,3/9/16,1
pomadchin,@lossyrob `0.24`,3/9/16,1
pomadchin,http://mvnrepository.com/artifact/de.javakaffee/kryo-serializers/0.24,3/9/16,1
lossyrob,right,3/9/16,1
pomadchin,but it has no yoda time serialisers,3/9/16,1
lossyrob,crap,3/9/16,1
pomadchin,tht_Ñé why i just excluded and imported the older kryo version,3/9/16,1
jamesmcclain,I thought we were running 0.27 before: https://github.com/jamesmcclain/geotrellis/commit/4adf6ae0fb31deeb2f210d280d91a1bf19c86e53,3/9/16,1
lossyrob,0.24 is the last one that lists the desired kryo version,3/9/16,1
pomadchin,"I wanted to keep all javakaffee serialisers, but to use old kryo lib with it; with 0.2.7 that worked and was tested",3/9/16,1
lossyrob,I_Ñém worried about deep bugs that will spring up,3/9/16,1
lossyrob,was tested just with our unit tests?,3/9/16,1
pomadchin,"no, the reason why we decided to use older kryo version was that runing chatta demo on a real cluster caused serialisation problems",3/9/16,1
pomadchin,so was tested on a chatta demo launched on a real cluster ,3/9/16,1
lossyrob,are we just matching the kryo version of a spark dependency?,3/9/16,1
pomadchin,we just _have to_ match spark kryo dependency version (2.21),3/9/16,1
lossyrob,for what spark version,3/9/16,1
lossyrob,and what happens when we use a spark with a different version,3/9/16,1
pomadchin,"they should be always the same; yes, of cource you can build your own spark dist with a different kryo version, but I am not sure that it is a popular solution",3/9/16,1
pomadchin,not sure since what time they are using kryo 2.21 by default; but think it is 1.2.x - 1.6.x ,3/9/16,1
pomadchin,probably even earlier spark versions use kryo 2.21 by default,3/9/16,1
pomadchin,"so if there would be kryo version mismatch, your client node would not be able to (de)serialize data correct (https://github.com/pomadchin/geotrellis-chatta-demo/blob/spark-version/geotrellis/src/main/scala/geotrellis/chatta/ChattaServiceActor.scala#L28-L36) after connecting to the runing spark cluster ",3/9/16,1
lossyrob,"@pomadchin chatta demo should be run with spark submit, why are you doing the `setJars` and `setMaster` config?",3/10/16,1
pomadchin,"@lossyrob to be able to run it as client executor; just tested that case; I am not sure that that_Ñés a good idea to launch spray server using spark-submit ",3/10/16,1
lossyrob,I_Ñém not sure it_Ñés a good idea to launch a spark job *not* through spark submit,3/10/16,1
pomadchin,why not?,3/10/16,1
lossyrob,because it doesn_Ñét let the client configure it at all,3/10/16,1
lossyrob,we shouldn_Ñét hard code spark configuration,3/10/16,1
lossyrob,"also it lets spark give the dependencies, based on the spark installation on the client side",3/10/16,1
lossyrob,that_Ñés why we have spark as provded,3/10/16,1
lossyrob,so it doens_Ñét get bundled up in the assembly,3/10/16,1
lossyrob,because the spark-submit sets the classpath with the correct dependencies for that version of spark,3/10/16,1
pomadchin,"i can agree with you, that to it, it is _easier_ to use spark-submit; and that_Ñés a good idea to use it for jobs (like spark submit); but that_Ñés not too convinient to run server as a pending job (that_Ñés my opinion).for example you can start server on a separate machine on a separate virtual machine and you can relaunch it without killing it as a spark job, but as a separate process (or even whole virtual machine). yes you have to control all dependecies yourself, that_Ñés defintely cons.",3/10/16,1
lossyrob,running the server as a spark-submit job is not as convinient as what?,3/10/16,1
pomadchin,as runing it as a separate executor,3/10/16,1
lossyrob,how do you mean executor,3/10/16,1
pomadchin,"I mean as a client, as in chatta demo",3/10/16,1
lossyrob,how do you start it as a client,3/10/16,1
lossyrob,through the build tool?,3/10/16,1
lossyrob,"I_Ñém looking through `spark-core`_Ñés `pom.xml`, and not finding kryo as a dependency. Where is it that spark 1.6 uses Kryo 2.21?",3/10/16,1
pomadchin,"through the build tool it is not possible, due to the fact we have no build assembly, and it would be not possible to spread this assembly between spark nodes.but we can build assmebly and just to run:`java -jar <fat assembly jar with all deps> <args like spark master, memory, etc>`",3/10/16,1
lossyrob,so what is the difference between doing that `java` call and calling it through `spark-submit`?,3/10/16,1
pomadchin,"you can put your jar on a seprate machine without spark installed (i mean spark distribution), and connect to spark server",3/10/16,1
pomadchin,your app will rise a separate executor (that would be your driver),3/10/16,1
lossyrob,ok. yeah I think that_Ñés just shortcutting how spark is supposed to work.,3/10/16,1
lossyrob,You_Ñére basically bundling a spark installation into the fat jar,3/10/16,1
lossyrob,are there other people doing that?,3/10/16,1
lossyrob,"I haven_Ñét heard of that, maybe I just haven_Ñét found that that_Ñés a valid use case",3/10/16,1
pomadchin,"thinks that_Ñés  good use case: put your server into docker container and rise it on any machine, just by runing smth like `docker run -d --restart <image>`",3/10/16,1
pomadchin,"and you should not even have installed spark distribution anywhere.the cons is that you_Ñéve said: you have to provide _same_ dependecies ",3/10/16,1
lossyrob,yeah. I think the docker image should just have a spark installation,3/10/16,1
lossyrob,because if <image> has a spark installation (the spark version you are targeting) then it_Ñés the same thing to just run `spark-submit`,3/10/16,1
lossyrob,"and if you_Ñére running against a cluster, the `spark-submit` should be of the same version of spark as is installed on the worker nodes, and administered through the spark installation",3/10/16,1
pomadchin,"not only the same spark, but the same spark with same dependencies. that_Ñés asbolutely similar to provideing all deps during your application building",3/10/16,1
lossyrob,but then we are managing dependencies,3/10/16,1
lossyrob,"like I said, it_Ñés like bundling a spark installation into a fat jar",3/10/16,1
lossyrob,which I think opens up a lot of room for bad things to happen,3/10/16,1
pomadchin,what bad things?,3/10/16,1
lossyrob,"so, my question is, did we decide not to upgrade kryo because of one of these spark-installed-into-a-fat-jar situations",3/10/16,1
lossyrob,"because then we have to keep track of what dependencies we are pulling in and make sure they match what a spark installation would install, and maintaining that is a burden. and we can just do the normal thing and have spark keep track of itself",3/10/16,1
lossyrob,"because if we pull in dependencies that don_Ñét actually match what a spark installation would have, then we have a different build then what we want to be working with",3/10/16,1
pomadchin,"as already said, i see your point of view, but why there is an api to provide possibility to start your server as a separate node? in order not to have heavy _same spark distr with same libs which you have to provide yourself (kryo is the only one, but it already bundled)_",3/10/16,1
pomadchin,and to make possible to control your application without touching spark api (i mean spark cli),3/10/16,1
lossyrob,because I think it works pretty well in a development environment,3/10/16,1
lossyrob,like through a build tool,3/10/16,1
lossyrob,like our unit tests,3/10/16,1
pomadchin,wow wow,3/10/16,1
lossyrob,"but if your actually running it on a cluster, I don_Ñét think it_Ñés meant for that",3/10/16,1
pomadchin,you cant run app in a distributed fashion without using assembly,3/10/16,1
lossyrob,right,3/10/16,1
lossyrob,"and I don_Ñét think you should run in a distributed fashion without a spark installation, is what I_Ñém saying",3/10/16,1
lossyrob,"perhaps I_Ñém wrong, but can you point me towards something that shows that as a case, or something that corroborates that?",3/10/16,1
lossyrob,"My worry is that if we have a dependency weirdness thing going on with Kryo, and we were seeing that weirdness in an environment that wasn_Ñét using `spark-submit`, I just want to make sure the weirdness isn_Ñét being cause by using spark in an unorthadox manner",3/10/16,1
pomadchin,"i just see no difference in using this approaches, except that fact that you alway have to install spark (and I guess hadoop libs, in some cases) on your machine",3/10/16,1
lossyrob,https://spark.apache.org/docs/latest/submitting-applications.html,3/10/16,1
lossyrob,https://spark.apache.org/docs/latest/cluster-overview.html,3/10/16,1
lossyrob,I just don_Ñét see anywhere in the docs where it mentions that use case.,3/10/16,1
lossyrob,"not saying the docs are the final say (see our docs for that), but I just want to know",3/10/16,1
lossyrob,so maybe talking about the kryo weirdness a bit more would give me a perspective on how bundling spark into the client jar is not affecting that,3/10/16,1
pomadchin,"ok, so",3/10/16,1
pomadchin,https://issues.apache.org/jira/browse/SPARK-11416,3/10/16,1
pomadchin,1 they are not going to cahnge kyro version untill major release ,3/10/16,1
pomadchin,"2 yes, if want not to let runing app not using spark-submit, we can update kryo version until whatever we want",3/10/16,1
lossyrob,where does spark set the Kryo version? I don_Ñét see it as a dependency on spark-core,3/10/16,1
lossyrob,which seems weird because that_Ñés where KryoSerializer is,3/10/16,1
lossyrob,or do I just not know how to read poms,3/10/16,1
pomadchin,@lossyrob it depends on twitter chill,3/10/16,1
pomadchin,https://github.com/apache/spark/blob/v1.5.2/pom.xml#L147,3/10/16,1
pomadchin,https://github.com/twitter/chill/blob/0.5.0/project/Build.scala#L13,3/10/16,1
lossyrob,gotcha,3/10/16,1
lossyrob,"ok. so we_Ñére depending on javakaffee not doing anything weird, wanting a version 3 and getting a version 2",3/10/16,1
lossyrob,I_Ñém going to bump it down to the last version 2 kryo,3/10/16,1
lossyrob,hopefully that will have the date time stuff we need,3/10/16,1
lossyrob,and it_Ñés only minor versions away,3/10/16,1
lossyrob,"as far as the spark-submit stuff, I hear that the docker deployment gets more difficult if your using spark-submit, but I_Ñém very much in favor of using it if we_Ñére going to think about deploying stuff in a distributed environment. It allows the running to be more configurable, and is the documented way of how to run things in a distributed mode. I_Ñém willing to be swayed if you provide me evidence that it_Ñés something people do, but I_Ñém not swayed by that fact that it _should be_ equivalent.",3/10/16,1
pomadchin,"it _is_ equivalent, and you _can provide same possibility to tune your app, because it has absolutely same configurable spark args_ ",3/10/16,1
lossyrob,this: https://github.com/apache/spark/blob/master/bin/spark-class,3/10/16,1
lossyrob,is the same thing as adding those couple of lines of config in your scala code?,3/10/16,1
pomadchin,"you can also provide args to setup executors cores amount, memory, etc",3/10/16,1
lossyrob,(which calls out to more things that aren_Ñét just a straight java call into your main funciton),3/10/16,1
pomadchin,what do you mean not just a straight call?,3/10/16,1
lossyrob,https://github.com/apache/spark/blob/master/launcher/src/main/java/org/apache/spark/launcher/Main.java,3/10/16,1
lossyrob,there_Ñés a lot of code/config/setup executed between calling `spark-submit` and executing the chatta demo `Main.main` function,3/10/16,1
lossyrob,"when you run `java -cp _Ñ_`, you_Ñére just entering that Main funciton",3/10/16,1
pomadchin,"```scalanew SparkConf()      .setMaster(argHolder.master)      .setAppName(_ÑÒServer"")      .set(""spark.serializer"", ""org.apache.spark.serializer.KryoSerializer"")      .set(""spark.kryo.registrator"", ""geotrellis.spark.io.hadoop.KryoRegistrator"")      .set(""spark.scheduler.mode"", argHolder.schedulerMode)      .set(""spark.driver.cores"", argHolder.driverCores)      .set(""spark.executor.cores"", argHolder.executorCores)      .set(""spark.cores.max"", argHolder.coresMax)      .set(""spark.executor.memory"", argHolder.executorMemory)      .set(""spark.driver.memory"", argHolder.driverMemory)      .setJars(SparkContext.jarOfObject(this).toList)```",3/10/16,1
lossyrob,so _ÑÒequivalent_Ñù doesn_Ñét seem like the correct word here,3/10/16,1
lossyrob,I get that you can set configuration in code. I do.,3/10/16,1
lossyrob,"There is a lot more happening in that code path, if you start by looking at what the `spark-submit` script does, and follow that down to where your Main function would be executed. And you can say that none of that really does anything, but I would have to read through each line of code to be convinced",3/10/16,1
pomadchin,"I am not sure, I can_Ñét say you exactly what _really_ differs. but yes, on machine with an app launched this way, would be arised driver node, and that would be availbe on the default port (4040/4041/_Ñ_/404n).this approach makes sense, and this letting you to conigure you app to suite cluster, and providing possibility not to put some _special_ deps inside spark nodes classpath (they would be included into fat jar and distributed between nodes)",3/10/16,1
pomadchin,"but, probably you are right, and nobody uses this spark api (except me / mine experience). ",3/10/16,1
lossyrob,"yeah, I believe you that it works. I just worry about the spark-submit stuff that I don_Ñét know about what it_Ñés doing. Maybe a good mailing list question?",3/10/16,1
lossyrob,spark mailing list,3/10/16,1
pomadchin,probably yes O:,3/10/16,1
lossyrob,when did you say the DateTime stuff appeared in `kryo-serializers`? Is `0.28` late enough?,3/10/16,1
lossyrob,"if you don_Ñét know offhand don_Ñét worry about it, I thought that you had said it before, I can just compile and see if it works :)",3/10/16,1
pomadchin,since 0.27  kaffe,3/10/16,1
lossyrob,thx,3/10/16,1
pomadchin,"ye, any newer version (mean 3.x) would also be compiling (James tried already); probably this wont even break lunching apps using `spark-submit`",3/10/16,1
pomadchin,"@lossyrob googled that question a bit: nobody uses spark-submit in production (yarn and mesos are our friends); probably it is a good idea to launch spark server as a yarn / mesos job (thought it is also possible to run it using internal api), and we dont need this api,mb this api is for experienced (or just a deprecated thing) users, so probably they can solve these problems manually, by manual deps excluding",3/10/16,1
lossyrob,"sorry, ""nobody uses spark-submit in production_Ñù did you mean nobody _doesn_Ñét_ use spark-submit?",3/10/16,1
lossyrob,"people launch to YARN or mesos through spark-submit, was my understanding",3/10/16,1
lossyrob,by setting the _ÑÓmaster to a yarn or mesos master,3/10/16,1
lossyrob,"that_Ñés how EMR does it (well, they have a modified version of spark-submit, but still the same sort of workflow)",3/10/16,1
lossyrob,and they submit to YARN,3/10/16,1
pomadchin,ah yes; so _seems_ like api equals ,3/10/16,1
pomadchin,yes you right,3/10/16,1
pomadchin,"i meant standalone, and searched can we start spark context with yarn / mesos usage without calling cli spark-submit.",3/10/16,1
lossyrob,oh gotcha,3/10/16,1
pomadchin,can; but as you sad probably it is not popular or whatever,3/10/16,1
rshirochenko,"Hi, trying to ingest landsat images as MultiBandRaster in Accumulo, found some example code here ( https://github.com/lossyrob/landsat-demo/blob/master/ingest/src/main/scala/demo/LandsatIngest.scala). What is SpaceInputTimeKey here https://github.com/lossyrob/landsat-demo/blob/master/ingest/src/main/scala/demo/LandsatIngest.scala#L131? I can not find it anywhere. Is it possible to do it(ingest landsat images) in another way with Etl ingest?",3/10/16,1
lossyrob,"@rshirochenko SpaceTimeInputKey actually changd names (well, got refactored out)",3/10/16,1
lossyrob,It really is a `ProjectedExtent`,3/10/16,1
lossyrob,@rshirochenko https://github.com/lossyrob/landsat-demo/tree/update-geotrellis-version,3/10/16,1
lossyrob,is the current work to update it,3/10/16,1
lossyrob,"it will be updated next week, but right now it_Ñés a bit broken",3/10/16,1
lossyrob,"we_Ñére doing some heavy API changes, and releaseing next week, so might be better to wait to upgrade until then",3/10/16,1
rshirochenko,"Ok, thanks) Also, if it make sense now, trying to read some data from S3 NASA NEX dataset. Here is my code and error text. Any suggestion how to fix it?``` val conf =      new SparkConf()        .setIfMissing(""spark.master"", ""local[*]"")        .setAppName(""Dem"")	.set( ""spark.serializer"", ""org.apache.spark.serializer.KryoSerializer"" )        .set(""spark.kryo.registrator"", ""geotrellis.spark.io.hadoop.KryoRegistrator"")        .set(""spark.executor.memory"", ""8g"")	.set(""driver-memory"",""4g"") implicit val sc = new SparkContext(conf)def s3SourceTiles(bucket: String, prefix: String)(implicit sc: SparkContext): RDD[(TemporalProjectedExtent, Tile)] = {    val confHadoop = sc.hadoopConfiguration    S3InputFormat.setBucket(confHadoop, bucket)    S3InputFormat.setPrefix(confHadoop, prefix)    //S3InputFormat.setMaxKeys(confHadoop, 100)    TemporalGeoTiffS3InputFormat.setTimeTag(confHadoop, ""ISO_TIME"")    TemporalGeoTiffS3InputFormat.setTimeFormat(confHadoop, ""yyyy-MM-dd'T'HH:mm:ss"")    val tiles = sc.newAPIHadoopRDD(confHadoop, classOf[TemporalGeoTiffS3InputFormat], classOf[TemporalProjectedExtent], classOf[Tile])    tiles.repartition(4000)}val s3tiles = s3SourceTiles(""nasanex"",""NEX-DCP30/NEX-quartile/NEX-qaurtile"")6/03/10 17:53:14 INFO BlockManagerMaster: Registered BlockManager16/03/10 17:53:14 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 154.4 KB, free 154.4 KB)Exception in thread ""main"" org.apache.spark.SparkException: Failed to register classes with Kryo	at org.apache.spark.serializer.KryoSerializer.newKryo(KryoSerializer.scala:128)	at org.apache.spark.serializer.KryoSerializerInstance.borrowKryo(KryoSerializer.scala:273)	at org.apache.spark.serializer.KryoSerializerInstance.<init>(KryoSerializer.scala:258)	at org.apache.spark.serializer.KryoSerializer.newInstance(KryoSerializer.scala:174)	at org.apache.spark.broadcast.TorrentBroadcast$.blockifyObject(TorrentBroadcast.scala:201)	at org.apache.spark.broadcast.TorrentBroadcast.writeBlocks(TorrentBroadcast.scala:102)	at org.apache.spark.broadcast.TorrentBroadcast.<init>(TorrentBroadcast.scala:85)	at org.apache.spark.broadcast.TorrentBroadcastFactory.newBroadcast(TorrentBroadcastFactory.scala:34)	at org.apache.spark.broadcast.BroadcastManager.newBroadcast(BroadcastManager.scala:63)	at org.apache.spark.SparkContext.broadcast(SparkContext.scala:1326)	at org.apache.spark.rdd.NewHadoopRDD.<init>(NewHadoopRDD.scala:74)	at org.apache.spark.SparkContext$$anonfun$newAPIHadoopRDD$1.apply(SparkContext.scala:1134)	at org.apache.spark.SparkContext$$anonfun$newAPIHadoopRDD$1.apply(SparkContext.scala:1129)	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)	at org.apache.spark.SparkContext.withScope(SparkContext.scala:714)	at org.apache.spark.SparkContext.newAPIHadoopRDD(SparkContext.scala:1129)	at Main$.s3SourceTiles(Main.scala:54)	at Main$.main(Main.scala:73)	at Main.main(Main.scala)	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)	at java.lang.reflect.Method.invoke(Method.java:606)	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:731)	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181)	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206)	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121)	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)Caused by: java.lang.ClassNotFoundException: geotrellis.spark.io.hadoop.KryoRegistrator	at java.net.URLClassLoader$1.run(URLClassLoader.java:366)	at java.net.URLClassLoader$1.run(URLClassLoader.java:355)	at java.security.AccessController.doPrivileged(Native Method)	at java.net.URLClassLoader.findClass(URLClassLoader.java:354)	at java.lang.ClassLoader.loadClass(ClassLoader.java:425)	at java.lang.ClassLoader.loadClass(ClassLoader.java:358)	at java.lang.Class.forName0(Native Method)	at java.lang.Class.forName(Class.java:278)	at org.apache.spark.serializer.KryoSerializer$$anonfun$newKryo$5.apply(KryoSerializer.scala:123)	at org.apache.spark.serializer.KryoSerializer$$anonfun$newKryo$5.apply(KryoSerializer.scala:123)	at scala.Option.map(Option.scala:145)	at org.apache.spark.serializer.KryoSerializer.newKryo(KryoSerializer.scala:123)	... 28 more```",3/10/16,1
lossyrob,"well NASA NEX is not geotiff, it_Ñés NetCDF",3/10/16,1
lossyrob,so you wouldn_Ñét be able to use the TemporalGeoTiffS3InputFormat for that,3/10/16,1
lossyrob,also,3/10/16,1
lossyrob,"`.set(""spark.kryo.registrator"", ""geotrellis.spark.io.hadoop.KryoRegistrator_Ñù)` needs to be `.set(""spark.kryo.registrator"", _ÑÒgeotrellis.spark.io.kryo.KryoRegistrator_Ñù)`",3/10/16,1
rshirochenko,thx,3/10/16,1
lossyrob,what we did with NetCDF is to do a priminary step to export it to single band geotiffs with the time element set,3/10/16,1
lossyrob,"the problem with ingesting netCDF files is that they are very large, and the distributed ingest works better with smaller files",3/10/16,1
lossyrob,"however at Azavea we_Ñére going to be working with the NEX dataset again soon, so the support should get better",3/10/16,1
rshirochenko,what size of file is ok for the distributed ingest?,3/10/16,1
lossyrob,"if you_Ñéd like to talk about how to do the preprocessing, I_Ñéd be happy to discuss. Or if you have a specific subset of the data in mind, I could maybe place some of the geotiffs we have already cut out into availability on s3",3/10/16,1
lossyrob,"ideally tiles of about 1024x1024, though I haven_Ñét found the exact right sweet spot",3/10/16,1
lossyrob,we can actually read netCDF with the input format found here (https://github.com/geotrellis/geotrellis-gdal/blob/master/gdal/src/main/scala/geotrellis/gdal/io/hadoop/NetCdfInputFormat.scala),3/10/16,1
lossyrob,it requires gdal java bindings,3/10/16,1
lossyrob,if you_Ñéd like to use that we can talk about that (which is a bit of a pain),3/10/16,1
rshirochenko,i think i firstly will work with geotiff dataset then),3/10/16,1
lossyrob,"ok yeah, if your not tied to netcdf it would better to go geotiff (which we support natively)",3/10/16,1
jamesmcclain,Hello,3/10/16,1
lossyrob,@jamesmcclain @dwins here_Ñés the 0.10 open issue list https://github.com/geotrellis/geotrellis/milestones/0.10,3/10/16,1
jamesmcclain,Would be happy to work on any of them,3/10/16,1
jamesmcclain,Just let me know,3/10/16,1
lossyrob,is there anything on that list that you feel like you could knock out pretty quick?,3/10/16,1
jamesmcclain,I will need a few moments to look carefully,3/10/16,1
dwins,https://github.com/geotrellis/geotrellis/issues/1101 this is a reprojection issue and might already be fixed,3/10/16,1
dwins,i'll investigate,3/10/16,1
lossyrob,great thanks,3/10/16,1
jamesmcclain,"https://github.com/geotrellis/geotrellis/issues/1291 is handled by the IO refactor, I think?",3/10/16,1
lossyrob,yes that will be closed iwth the IO refactor,3/10/16,1
lossyrob,I_Ñéll paste the ones that I think might be good,3/10/16,1
lossyrob,https://github.com/geotrellis/geotrellis/issues/1339,3/10/16,1
lossyrob,https://github.com/geotrellis/geotrellis/issues/1249,3/10/16,1
dwins,i will look at 1249 as well,3/10/16,1
jamesmcclain,Okay,3/10/16,1
jamesmcclain,#1339 Looks like it requires some setup?,3/10/16,1
lossyrob,https://github.com/geotrellis/geotrellis/issues/1222,3/10/16,1
lossyrob,"1222 would be quicker I think. I_Ñém suprised it_Ñés even a thing, am I just missing something?",3/10/16,1
lossyrob,Do we not have a spacetime input format for geotiffs in hodoop?,3/10/16,1
lossyrob,I_Ñém almost sure we do,3/10/16,1
jamesmcclain,Okay,3/10/16,1
lossyrob,no we have that,3/10/16,1
lossyrob,that bug is invalid,3/10/16,1
jamesmcclain,"Okay, it looks like #1339 then?",3/10/16,1
lossyrob,https://github.com/geotrellis/geotrellis/issues/1112,3/10/16,1
lossyrob,"you did subset, want to do a reorder",3/10/16,1
lossyrob,so that we can reorder bands on a multi band tile,3/10/16,1
jamesmcclain,"Sure, that sounds good",3/10/16,1
lossyrob,this is another one,3/10/16,1
lossyrob,v,3/10/16,1
lossyrob,https://github.com/geotrellis/geotrellis/issues/1057,3/10/16,1
jamesmcclain,Would you mind adding a little bit of text to that issue so that I am sure what needs to happen?,3/10/16,1
jamesmcclain,(Talking about #1057),3/10/16,1
jamesmcclain,"Those two issues are enough to get me started, I think",3/10/16,1
jamesmcclain,I can come back if I need more,3/10/16,1
lossyrob,@jamesmcclain https://github.com/geotrellis/geotrellis/pull/1367 needs to be updated to master,3/10/16,1
lossyrob,"I put some text in 1057, let me know if it needs more clarity",3/10/16,1
jamesmcclain,"Okay, will rebase",3/10/16,1
jamesmcclain,"Okay, thanks for the text, I will read it and get back to you if I have more questions",3/10/16,1
lokifacio,"hey, I'm not sure if this is related to https://github.com/geotrellis/geotrellis/issues/1101, but when I reproject some RDD[(ProjectedExtent, Tile)] which is in EPSG3035 to EPSG4326, I need to fix the extent myself after reprojection:```      val reprojected = tiles.reproject(EPSG4326)      val (_, metadata) = RasterMetaData.fromRdd(reprojected, FloatingLayoutSchema(250))      val RasterMetaData(cellType, layout, extent, crs) = metadata      // NOTE: As for geotrellis df9500 the extent returned after reprojection is wrong so we need to fix it      val fixedExtent = extent.reproject(crs,  EPSG4326) // this is very strange because crs is already 4326 but if use the original crs (3035) I don't get the expected extent      val fixedLayoutExtent = layout.extent.reproject(crs, EPSG4326)      val fixedLayout = layout.copy(extent = fixedLayoutExtent)      val fixedMetadata = RasterMetaData(cellType, fixedLayout, fixedExtent, crs)      ContextRDD(reprojected.tileToLayout(metadata), fixedMetadata)```",3/10/16,1
dwins,"@lokifacio i've been working on improving reprojection for the last few sprints and the changes were just merged to master. if you are building from source, try updating your clone and take a look at the results",3/10/16,1
lokifacio,@dwins I just updated this morning to 0.10.0-ae447e2 ,3/10/16,1
dwins,"when i say ""just"" i mean around 10 minutes ago :)",3/10/16,1
lokifacio,lol,3/10/16,1
lokifacio,"well, I'll check tomorrow when bintray gets updated",3/10/16,1
dwins,sounds good,3/10/16,1
dwins,i can confirm that 3305 uses some of the parameters affected by my changes,3/10/16,1
dwins,oh you said 3035. that one does too :P,3/10/16,1
lokifacio,"cool, I'll let you know tomorrow :)",3/10/16,1
lokifacio,"btw, is someone using IntelliJ to develop geotrellis? I tried to compile master to hunt a bug by creating an IntelliJ project but scala failed to find some definitions...",3/10/16,1
lokifacio,unfortunately I have not spare time to go deeper on that issues :(,3/10/16,1
dwins,@dwins is on vim,3/10/16,1
lossyrob,@pomadchin I think uses IntelliJ,3/10/16,1
pomadchin,"@lokifacio yes, just throw here some lines, i_Ñéll try to help you",3/10/16,1
lossyrob,@pomadchin this is to geotrellis master and not the IO branch https://github.com/geotrellis/geotrellis/pull/1376,3/10/16,1
dwins,lot of my commits on that PR,3/10/16,1
dwins,isn't the proj stuff already merged?,3/10/16,1
lossyrob,yeah,3/10/16,1
lossyrob,that PR should be made against lossyrob/writer-updates branch,3/10/16,1
pomadchin,"@lossyrob mm ok then, will make a pr against your io branch then",3/10/16,1
pomadchin,it is against master brnached merged with your branch i guess,3/10/16,1
pomadchin,@lossyrob @dwins fixed that weird commits ,3/10/16,1
lossyrob,@pomadchin in regard to these renames,3/10/16,1
lossyrob,"```scalaRasterRDD[K] -> TileRDD[K] MultiBandRasterRDD[K] -> MultiBandTileRDD[K]```",3/10/16,1
lossyrob,You said a raster is a tile with metadata,3/10/16,1
pomadchin,with extent,3/10/16,1
lossyrob,"which a RDD[(K, Tile)] does not have tiles with metadata, it has tiles",3/10/16,1
lossyrob,i see what your saying though,3/10/16,1
lossyrob,"do we want to name the RDD type based on the element type, or something else?",3/10/16,1
lossyrob,because RasterRDD isn_Ñét describing the element type,3/10/16,1
lossyrob,"let_Ñés think if we have a `RDD[(K, Point)]`",3/10/16,1
lossyrob,is that a `FeatureRDD`?,3/10/16,1
pomadchin,"but `type RasterRDD[K] = RDD[(K, Tile)] with Metadata[RasterMetaData]` and not just `RDD[(K, Tile)]`",3/10/16,1
echeipesh,"`type RasterRDD[K, V <: CellGrid] = RDD[(K, V)] with Metadata[RasterMetaData[K]]`",3/10/16,1
echeipesh, ?,3/10/16,1
echeipesh,"That_Ñés along the same lines as `case class Raster[+T <: CellGrid](tile: T, extent: Extent)`",3/10/16,1
lossyrob,hm I can see that,3/10/16,1
lossyrob,it_Ñés almost like a `TileLayerRDD`,3/10/16,1
lossyrob,where the layer-ness comes from the Metadata[M],3/10/16,1
lossyrob,I_Ñém maybe not into the name RasterMetaData either_Ñ_,3/10/16,1
echeipesh,Sure _Ñ_ that seems too verbose. ,3/10/16,1
lossyrob,does that really describe what it is?,3/10/16,1
echeipesh,It_Ñés more of a `TiledRasterMetadata`,3/10/16,1
lossyrob,`TileLayerMetadata`,3/10/16,1
echeipesh,"it has tile layout, thus tiled, has extent/cellsize, thus raster.",3/10/16,1
lossyrob,"I think there_Ñés things you can do with RDD_Ñés of rasters, like how we do `asRasters` to get to it for zonal summaries",3/10/16,1
lossyrob,"that_Ñés where I started thinking, hm is this a RasterRDD when it doesn_Ñét contain rasters",3/10/16,1
echeipesh,"`TileLayerRDD[GridKey, MultiBandTile]` or `LayerRDD[GridKey, MultiBandTile]` ",3/10/16,1
lossyrob,I_Ñém leaning towards,3/10/16,1
echeipesh,It would make some sense if `S3LayerReader` produced `_LayerRDD`.,3/10/16,1
lossyrob,"```scalatype TileLayerRDD[K] = RDD[(K, Tile)] with Metadata[TileLayerMetadata]```",3/10/16,1
lossyrob,I think we_Ñéll want to not nail down layers to just tiles,3/10/16,1
pomadchin,I think TileLayerRDD makes sense ,3/10/16,1
pomadchin,"Don_Ñét we want to have just `LayerRDD[K, V <: CellGrid]` as suggested @echeipesh ? or it would be too complex",3/10/16,1
echeipesh,"Well, one problem with that is that it would pin the metadata of LayerRDD to TileLayerMetadata",3/10/16,1
lossyrob,I_Ñém actually thinking of trying to get rid of the `<:` in CellGrid,3/10/16,1
pomadchin,gotcha,3/10/16,1
lossyrob,i_Ñém not sure that iwll work,3/10/16,1
lokifacio,I think `TileLayerRDD`/`TileLayerMetadata` make more sense,3/11/16,1
jasonbeverage,"I've got some deployment questions for you all.  I've been using the geotrellis-ec2-cluster project for awhile for doing vagrant and ec2 based deployments, and I'm beginning to wonder if some of the issues I've been running into to are version mismatches.  I've been using your branch @lossyrob with Spark using version 1.5.0+cdh5.5.*.",3/11/16,1
jasonbeverage,Geotrellis looks like it's being built against spark version 1.5.2 but I didn't think it was available on Cloudera yet.,3/11/16,1
jasonbeverage,"geotrellis-ec2-cluster is also using Accumulo 1.6 I think, and geotrellis is built against 1.7.0",3/11/16,1
jasonbeverage,So I suppose what I'm asking is if geotrellis-ec2-cluster is what I should be using for deployment on AWS or is there something else I need to be looking at?,3/11/16,1
echeipesh,"Yes, the published artifacts are built against vanilla spark. Although barring version mismatch between CDH and vanilla you can usually relly on spark not to break API in patch versions. Also @pomadchin found that 1.7.0 client is compatiable with 1.6.0 accumulo cluster.",3/11/16,1
jasonbeverage,"Ok, so in theory it should be good to go then.",3/11/16,1
pomadchin,@jasonbeverage tested with chatta demo compitability (when it was up to master and it was possible to build it),3/11/16,1
echeipesh,Barring version mismatch you should be GTG :)  But lately we_Ñéve been leaning on EMR for our internal deployments of GeoTrellis. We will have to figure out how to get Accumulo on that soon too.,3/11/16,1
jasonbeverage,"The reason I even thought it was an issue is that I'm building an assembly locally, uploading it to my instances and then trying to run my tests.",3/11/16,1
jasonbeverage,"The spark jobs like ETL ingest all start and run, although jobs will fail very often with ""Lost executor"" issues.",3/11/16,1
jasonbeverage,And when I try to run some of spray tests I get an exception like this:,3/11/16,1
jasonbeverage,"Exception in thread ""main"" akka.ConfigurationException: Akka JAR version [2.3.9] does not match the provided config version [2.3.4]",3/11/16,1
pomadchin,have you a more complete stack trace? ,3/11/16,1
pomadchin,"because I remember similar issues (but long ago)issues were caused in my case by a bad `.ivy2` cache",3/11/16,1
jasonbeverage,"```Exception in thread ""main"" akka.ConfigurationException: Akka JAR version [2.3.9] does not match the provided config version [2.3.4]at akka.actor.ActorSystem$Settings.<init>(ActorSystem.scala:210)at akka.actor.ActorSystemImpl.<init>(ActorSystem.scala:505)at akka.actor.ActorSystem$.apply(ActorSystem.scala:142)at akka.actor.ActorSystem$.apply(ActorSystem.scala:109)at test.ServeNDVIAccumulo$.main(ServeNDVIAccumulo.scala:28)at test.ServeNDVIAccumulo.main(ServeNDVIAccumulo.scala)```",3/11/16,1
jasonbeverage,So maybe try deleting my ivy2 cache and rebuilding the assembly?,3/11/16,1
pomadchin,probably it may help; and your project is based on @lossyrob ndvi demo project?,3/11/16,1
jasonbeverage,@echeipesh One of my coworkers was able to get Accumulo running on EMR using some scripts from GeoWave,3/11/16,1
echeipesh,"And all the executors fail, you never even get a partial complete of the tasks ?",3/11/16,1
echeipesh,"Awesome, that was my plan to look at those as well.",3/11/16,1
jasonbeverage,"He just got it running last night so he could see the status page, so that was at least a partial win :)",3/11/16,1
jasonbeverage,"Yeah, it's based on that project @pomadchin , just slightly modified to use accumulo and my own ingested data.",3/11/16,1
pomadchin,https://github.com/geotrellis/geotrellis-landsat-tutorial ?,3/11/16,1
jasonbeverage,"@echeipesh What I've been seeing is just general ingest instability that I don't remember seeing before.  I've got a test that does an s3 to s3 ingest test using ETL with a very low splitSize (so in theory it's not taking up tons of ram) and the first job gets to the last task, then hangs for a bit, then finally the executors start to fail with ""Lost Executor"" on MESOS and if I run on EMR I get failures like ""Lost executor .... remote Rpc client disassociated""",3/11/16,1
owcm,"@ pomadchin  Back on March 7th I was asking about generating lower zoom layers for my 1m and 0.5 resolution data, support zoom layers 19,20,21. You mentioned resample tiles on demand and offered to supply an example.  Decided to go it on my own, was trying that approach and to resample the tile I used the approach:```val resampledTile = vsExtTile.resample(extent, (vsExtTile.cols * resolutionFactor.toInt), (vsExtTile.rows * resolutionFactor.toInt), NearestNeighbor )```Where resolutionFactor is basically the multiplier I derived from the tiles current resolution and the desired resolution. Using the original extent of the source Tile. Of course the resulting tile needs to be cut into quads to support the X/Y/Z tilting. Question is if the resampling call I making is the right call to make. ",3/11/16,1
jasonbeverage,"Also FYI, I'm not using the master geotrellis either.  I'm using the git version right after I submitted the fix for the temporal multiband hadoop stuff.",3/11/16,1
jasonbeverage,Right @pomadchin ,3/11/16,1
echeipesh,"@jasonbeverage Sounds like a dependency conflict, if I had to make a blind guess it would be between the version of akka that spray brings in and the version that sparks brings in. Would be a good thing to try using the sbt dependencyGraph plugin and see if you can match up what is getting pulled from where :(",3/11/16,1
jasonbeverage,"Ok, I'll look into that plugin @echeipesh .",3/11/16,1
jasonbeverage,So akka will come in from spark or is it part of scala?,3/11/16,1
echeipesh,Spark relies on akka for communication between driver/executor. Usually the conflict goes the other way around because spark assembly will be loaded first through `spark-submit` and that will get the spark version of akka into the class loadeder.,3/11/16,1
jasonbeverage,"ok, and I assume akka 2.3.9 is what spark 1.5.2 uses b/c that is what geotrellis is built against.  And when I try to run on a 1.5.0 system it sees akka 2.3.4 and is happy that it's not 2.3.9",3/11/16,1
jasonbeverage,If that is truly an issue then if I want to run on a 1.5 system then I should be able to build geotrellis against spark 1.5.0 I suppose.,3/11/16,1
pomadchin,"yes, there can be a potential problem; seems like geotrellis should work correct (and was tested) only with spark 1.5.2 + versions",3/11/16,1
pomadchin,"you can try to build your own version, yes ._.'",3/11/16,1
jasonbeverage,"An offhand question, do you know of any issues with geotrellis and LZW compressed data?",3/11/16,1
echeipesh,"LZW geotiffs, we should have tests against reading those.",3/11/16,1
jasonbeverage,The reason I ask is that I have one dataset that isn't LZW compressed and it ingests from s3 to s3 just fine.  I have other datasets that are lzw compressed and those the ones that are exhibiting the hanging and then eventually failing from s3 to s3.,3/11/16,1
jasonbeverage,I'll do some more testing but I just wanted to see if there was any caveats anyone could think of.,3/11/16,1
echeipesh,"Also, to think out loud: geotrellis is built against spark 1.5.2 but the dependency is _ÑÒprovided_Ñù, that is it_Ñés transitive. So we exepct the actual classes and their dependencies to already be available in the class path. Usually that is setup for us by `spark-submit` unless the project using geotrellis explicitly depends on `spark-core`.",3/11/16,1
jasonbeverage,"I have spark-core in my build.sbt for my project, but I have it marked as provided.",3/11/16,1
jasonbeverage,"I'm also using spark-mllib too, I wonder if that is pulling something in.",3/11/16,1
jasonbeverage,Should I also include all of the akka deps in my build.sbt but mark them as provided too?,3/11/16,1
echeipesh,I don_Ñét think that would change anything. Really the only tool you have is changing the version of the dependencies to line up their transitive dependencies.,3/11/16,1
jasonbeverage,"Ok great, thanks for the help!",3/11/16,1
jasonbeverage,So on EMR you're using emr-4.2.0 since it has spark 1.5.2 correct?,3/11/16,1
lossyrob,@jasonbeverage yeah we_Ñére using that EMR,3/11/16,1
echeipesh,"yep, 4.2.0",3/11/16,1
lossyrob,"@lokifacio thanks for the input on the renaming, great to have input on those decisions",3/11/16,1
aaron-santos,"Hi, I'm trying to use `Rasterizer.rasterizeWithValue`to generate a basetile, but I'm having a hard time generating the right `RasterExtent`.",3/11/16,1
aaron-santos,"It looks like I can plug my geometry's extent like so `RasterExtent(geometry.envelope, ...`",3/11/16,1
lossyrob,"yeah, depending on what version your using the API can be a little nicer, but to use the Rasterizer, you_Ñéd want to take the envelope and then figure out the # of cols and rows",3/11/16,1
aaron-santos,"But I'm at a bit of a loss when it comes to the `cellwidth`, `cellheight`, `cols`, `rows` arguments",3/11/16,1
lossyrob,and then do something like,3/11/16,1
lossyrob,"```scalaval geom: Polygon = ???val re: RasterExtent = RasterExtent(geom.envelope, 256, 256) // 256 cols x 256 rowsRasterizer.rasterizeWithValue(re, value)```",3/11/16,1
lossyrob,or however that rasterizer call goes,3/11/16,1
lossyrob,the object apply will compute the correct cellwidth and cellheight,3/11/16,1
aaron-santos,"oh, nice!",3/11/16,1
lossyrob,"they are there explicitly so in case you care more about the resolution (30 meter by 30 meter), you can have it so it might not be exact where (extent.xmax- extent.xmin)/cellwidht == cols",3/11/16,1
lossyrob,"but generally raster extent is thought of as an extent + a (col, row) grid",3/11/16,1
timothymschier,"to get Accumulo up on EMR, I took the GeoWave bootstrap scripts and commented out the GeoWave parts: https://gist.github.com/timothymschier/0794a196af044055552d",3/11/16,1
timothymschier,"then when configuring the EMR cluster, set the ""bootstrap jar"" value (yeah website calls it jar) point to bootstrap-bda8.sh (which was uploaded to S3)",3/11/16,1
aaron-santos,Thank! I'll try that out,3/11/16,1
timothymschier,"pretty straightforward... got the Accumulo status page up, but wrestling with the ingest issues along with @jasonbeverage ",3/11/16,1
pomadchin,@timothymschier you mean akka version issues? that caused by an old spark cluster version,3/11/16,1
timothymschier,yeah those...,3/11/16,1
pomadchin,"yes, the problem that we depend on Spark 1.5.2 core, that means you can use Spark 1.5.2 + version",3/11/16,1
timothymschier,silly qn - looks like ingest is working for landsat imagery but not for others so probably not a config / deps version issue,3/11/16,1
timothymschier,what data types does gt support in ETL ingest?,3/11/16,1
timothymschier,landsat is UINT16; one that fails is float32,3/11/16,1
timothymschier,(GeoTiff source data btw),3/11/16,1
lossyrob,hmm it should work with float32,3/11/16,1
lossyrob,what is the error?,3/11/16,1
pomadchin,and probably the input example tiff,3/11/16,1
lossyrob,@dwins weird difference between proj4 and GDAL_Ñés gcs code definitions,3/12/16,1
lossyrob,"4307,Nord Sahara 1959,6307,Nord Sahara 1959,6307,9122,7012,8901,1,0,6422,5660,1,9606,-209.3622,-87.8162,404.6198,0.0046,3.4784,0.5805,-1.4547",3/12/16,1
lossyrob,"=  `'+proj=longlat +ellps=clrk80 +towgs84=-209.3622,-87.8162,404.6198,0.0046,3.4784,0.5805,-1.4547 +no_defs '`",3/12/16,1
lossyrob,that_Ñés in GDAL,3/12/16,1
lossyrob,"```console# Nord Sahara 1959<4307> +proj=longlat +ellps=clrk80 +towgs84=-186,-93,310,0,0,0,0 +no_defs  <>```",3/12/16,1
lossyrob,that_Ñés in the proj4 `epsg` file that we have for `proj4j`,3/12/16,1
timothymschier,I've made public a sample file up at s3://bda8.data/test_5_2_input/test_5_2_input_01_01.tif  (hope that link works?),3/12/16,1
timothymschier,"am spooling up a cluster to get the exact error message, but essentially it spends ages sitting on RasterMetaData.scala (forget the line) and then fails on the last task in a job (84/84 or 260/260)",3/12/16,1
lossyrob,that sounds like when it_Ñés collecting the metadata from the tiles,3/12/16,1
timothymschier,yeah could well be,3/12/16,1
timothymschier,"for the landsat imagery it scoots through that straight onto full ingest, for this other imagery it gets stuck there",3/12/16,1
lossyrob,with any error or just fails silently,3/12/16,1
timothymschier,with the loss of executor error,3/12/16,1
lossyrob,hm. It_Ñéll be a good pre-release test to check it out,3/12/16,1
lossyrob,have you dug into the log files on that executor to see if it tells you anything,3/12/16,1
timothymschier,no actually,3/12/16,1
timothymschier,which log would be the best to look at?,3/12/16,1
timothymschier,"ok job that it sits on before failing is ""reduce at RasterMetaData.scala:128""",3/12/16,1
lossyrob,the stderr/stdout of the executor that has the failing tasks,3/12/16,1
lossyrob,int the Spark UI,3/12/16,1
lossyrob,you can go to the executors,3/12/16,1
lossyrob,and get to the log output that way,3/12/16,1
lossyrob,@timothymschier are there more than one CRS in the dataset you are trying to ingest?,3/12/16,1
timothymschier,they _should_ all be the same...,3/12/16,1
timothymschier,so I tried ingest with that one single file and it worked ok,3/12/16,1
timothymschier,trying again with the full set now,3/12/16,1
timothymschier,(596 total tiles),3/12/16,1
timothymschier,"seems to be doing the same thing now - 89/90 jobs complete fine, now sitting for ages on the last one",3/12/16,1
lossyrob,"if you could find the executor that is hanging and then go int the UI to get the stdout/stderr, that might tell us something",3/12/16,1
timothymschier,"yeah I found that, ta for the tip... will let you know when it falls over",3/12/16,1
lossyrob,ok cool,3/12/16,1
timothymschier,ok starting to get failures...,3/12/16,1
timothymschier,,3/12/16,1
timothymschier,"```16/03/12 04:34:33 INFO executor.Executor: Finished task 86.0 in stage 0.0 (TID 86). 2614 bytes result sent to driver16/03/12 04:34:33 INFO executor.Executor: Finished task 87.0 in stage 0.0 (TID 87). 2614 bytes result sent to driver16/03/12 04:34:33 INFO executor.Executor: Finished task 88.0 in stage 0.0 (TID 88). 2614 bytes result sent to driver16/03/12 04:38:21 ERROR executor.Executor: Exception in task 89.0 in stage 0.0 (TID 89)java.lang.NegativeArraySizeException	at org.apache.commons.io.output.ByteArrayOutputStream.toByteArray(ByteArrayOutputStream.java:322)	at org.apache.commons.io.IOUtils.toByteArray(IOUtils.java:463)	at geotrellis.spark.io.s3.S3RecordReader.nextKeyValue(S3RecordReader.scala:39)	at org.apache.spark.rdd.NewHadoopRDD$$anon$1.hasNext(NewHadoopRDD.scala:163)	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)	at org.apache.spark.rdd.RDD$$anonfun$reduce$1$$anonfun$14.apply(RDD.scala:992)	at org.apache.spark.rdd.RDD$$anonfun$reduce$1$$anonfun$14.apply(RDD.scala:991)	at org.apache.spark.SparkContext$$anonfun$36.apply(SparkContext.scala:1943)	at org.apache.spark.SparkContext$$anonfun$36.apply(SparkContext.scala:1943)	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)	at org.apache.spark.scheduler.Task.run(Task.scala:88)	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)	at java.lang.Thread.run(Thread.java:745)16/03/12 04:38:21 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 9016/03/12 04:38:21 INFO executor.Executor: Running task 89.1 in stage 0.0 (TID 90)16/03/12 04:38:21 INFO rdd.NewHadoopRDD: Input split: geotrellis.spark.io.s3.S3InputSplit@533633d16/03/12 04:42:31 ERROR executor.Executor: Exception in task 89.1 in stage 0.0 (TID 90)java.lang.NegativeArraySizeException	at org.apache.commons.io.output.ByteArrayOutputStream.toByteArray(ByteArrayOutputStream.java:322)	at org.apache.commons.io.IOUtils.toByteArray(IOUtils.java:463)	at geotrellis.spark.io.s3.S3RecordReader.nextKeyValue(S3RecordReader.scala:39)	at org.apache.spark.rdd.NewHadoopRDD$$anon$1.hasNext(NewHadoopRDD.scala:163)	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)	at org.apache.spark.rdd.RDD$$anonfun$reduce$1$$anonfun$14.apply(RDD.scala:992)	at org.apache.spark.rdd.RDD$$anonfun$reduce$1$$anonfun$14.apply(RDD.scala:991)	at org.apache.spark.SparkContext$$anonfun$36.apply(SparkContext.scala:1943)	at org.apache.spark.SparkContext$$anonfun$36.apply(SparkContext.scala:1943)	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)	at org.apache.spark.scheduler.Task.run(Task.scala:88)	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)	at java.lang.Thread.run(Thread.java:745)16/03/12 04:42:31 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 9116/03/12 04:42:31 INFO executor.Executor: Running task 89.2 in stage 0.0 (TID 91)16/03/12 04:42:31 INFO rdd.NewHadoopRDD: Input split: geotrellis.spark.io.s3.S3InputSplit@315b0d60```",3/12/16,1
timothymschier,funny... not seen that NegativeArraySizeException before,3/12/16,1
lossyrob,interesting,3/12/16,1
lossyrob,how many images are you trying to ingest?,3/12/16,1
timothymschier,596,3/12/16,1
timothymschier,one 11GB tiff was retiled ,3/12/16,1
lossyrob,how are you running ingest? is it spark-etl? can you give me the parameters you are using?,3/12/16,1
timothymschier,"```spark-submit --class geotrellis.spark.etl.SinglebandIngest --conf spark.executor.instances=2 --conf spark.executor.cores=8 --conf spark.executor.memory=20G --conf spark.serializer=org.apache.spark.serializer.KryoSerializer --conf spark.kryo.registrator=geotrellis.spark.io.kryo.KryoRegistrator --conf spark.driver.cores=6 --conf spark.driver.memory=20G --conf spark.executorEnv.SPARK_LOCAL_DIRS=""/media/ephemeral0,/media/ephemeral1"" bda-geotrellis-test-assembly-0.1.0.jar --format geotiff --input s3 -I splitSize=10 bucket=bda8.data key=test_5_2_input --output s3 -O bucket=bda8.data key=kmeans_ingest --layer kmeans --pyramid --layoutScheme tms --crs EPSG:3857 --tileSize 256 --clobber```",3/12/16,1
lossyrob,can you try bumping up the splitSize to 50,3/12/16,1
timothymschier,"roger that, kicked off again",3/12/16,1
lossyrob,might be a corrupt object_Ñ_best way to figure it out I would think is to wrap the failing code in a try/catch and print out the bucket/key combo to see what file it is,3/12/16,1
lossyrob,https://github.com/geotrellis/geotrellis/blob/master/s3/src/main/scala/geotrellis/spark/io/s3/S3RecordReader.scala#L43,3/12/16,1
timothymschier,"ok, can give that a shot...",3/12/16,1
timothymschier,is it worth trying that on latest master? we have a fork from ~10 days ago,3/12/16,1
lossyrob,could be,3/12/16,1
lossyrob,"but this is a pretty strange error, in the s3 client, so I_Ñém not sure it_Ñés that",3/12/16,1
timothymschier,ok ... even that file I notice has changed since our fork,3/12/16,1
timothymschier,ok how about a quicker test (while I'm doing the code change and re-building) - just trying it with the latest bintray rather than this older fork,3/12/16,1
lossyrob,I just can_Ñét fathom how it_Ñés getting a negative number in that array size in ByteArrayOutputStream,3/12/16,1
lossyrob,"hopefully will work, though if API changed then you_Ñéll have to update the code",3/12/16,1
timothymschier,I think we're just calling the native spark-etl for the ingest though,3/12/16,1
timothymschier,so hope no code recompile,3/12/16,1
timothymschier,in fact I was just going to ask a silly qn - is there a shaded jar URL somewhere that I could download and use?,3/12/16,1
lossyrob,all the dependencies have updated too,3/12/16,1
lossyrob,so there_Ñéd be about 4 or 5 off of spark-etl that you_Ñéd have to update,3/12/16,1
timothymschier,getting the same `NegativeArraySizeException` in the run with the splitSize at 50,3/12/16,1
timothymschier,what if I start with a clean slate and just want to try ingest using a single spark-submit call?,3/12/16,1
lossyrob,this is where it_Ñés throwing,3/12/16,1
lossyrob,https://github.com/apache/commons-io/blob/2.4/src/main/java/org/apache/commons/io/output/ByteArrayOutputStream.java#L322,3/12/16,1
timothymschier,(i.e. just to see if it works for our data on latest),3/12/16,1
lossyrob,that_Ñés a pretty deep error. I don_Ñét see how that should every be the case,3/12/16,1
timothymschier,what fun!,3/12/16,1
lossyrob,my strategy would be to find the file and then try to read it directly with the S3 client to see if you get the error reading the file just through a single call,3/12/16,1
lossyrob,but I have to knock off for the night,3/12/16,1
lossyrob,so I_Ñéll check in tomorrow to see you had any luck,3/12/16,1
timothymschier,"no worries, ta for your help :)",3/12/16,1
lossyrob,"np, cheers",3/12/16,1
timothymschier,hey folks I'm having trouble building the master branch,3/13/16,1
timothymschier,I did a fresh clone from https://github.com/geotrellis/geotrellis,3/13/16,1
timothymschier,"```rm -rf ~/.ivy2/cache```to clean out issues with the cache",3/13/16,1
timothymschier,"```./sbt assembly```",3/13/16,1
timothymschier,"and I get errors:```[error] (geotrellis/*:assembly) deduplicate: different file contents found in the following:[error] /home/hadoop/.ivy2/cache/com.esotericsoftware/kryo/bundles/kryo-3.0.3.jar:com/esotericsoftware/kryo/Kryo$DefaultSerializerEntry.class[error] /home/hadoop/.ivy2/cache/com.esotericsoftware.kryo/kryo/bundles/kryo-2.21.jar:com/esotericsoftware/kryo/Kryo$DefaultSerializerEntry.class```",3/13/16,1
timothymschier,and many more of the same - issues withe kryo version numbers,3/13/16,1
timothymschier,"Also, for those interested just finished running ingest on each of the 896 files independently. No failures. Re-ran the ingest with all the files together, got the same old failure we'd been getting previously (```ExecutorLostFailure```, the one likely to be related to dependency versions; the ```NegativeArraySizeException``` seems to have disappeared?!?). So keen to see if we can do a clean build of a newer GeoTrellis and try the latest Spark ETL.",3/13/16,1
timothymschier,"Next thing I've tried is modifying our own project - updated reference in ```build.sbt``` from dependencies of ```""com.azavea.geotrellis"" %% ""geotrellis-spark""     % ""0.10.0-SNAPSHOT""``` to```""com.azavea.geotrellis"" %% ""geotrellis-spark""     % ""0.10.0-aee77fa""``` (and same for geotrellis-spark-etl).Did a ```./sbt assembly```, copied the JAR, ran the ingest again with:```spark-submit --class geotrellis.spark.etl.SinglebandIngest bda-geotrellis-test-assembly-0.1.1.jar --format geotiff --input s3 -I splitSize=50 bucket=bda8.data key=test_5_2_input --output s3 -O bucket=bda8.data key=kmeans_ingest --layer kmeans --pyramid --layoutScheme tms --crs EPSG:3857 --tileSize 256 --clobber ```and still getting the ExecutorLostFailure happening.",3/13/16,1
timothymschier,"to me it seems like this should be using a latest-built version of GeoTrellis, so shouldn't have the dependency issues?",3/13/16,1
timothymschier,"hmm... wait. I'm getting the stderr output saying there were exceptions, but the EMR Spark Application History is saying that the job ran successfully :S... it does seem to be not doing the old behaviour of failing on the last executor; this time it's on job 0.0. Perhaps it's a Spark shutdown issue? Do others see this sort of thing?",3/13/16,1
lossyrob,"@timothymschier I_Ñéve seen something similar before, but it_Ñés usually either a dependency issue (my fear is it_Ñés a AWS S3 sdk version issue with your S3 region, we are using an older SDK version for compatibility reasons), or something wrong with the setup.",3/13/16,1
lossyrob,"have geotrellis builds gotten slower, or am I imagining that? I feel like in the last week compiling has taken longer",3/13/16,1
timothymschier,"Ah interesting, hopefully something that straightforward. We're in US NEast (Virginia) region. I can try copying it over to HDFS before ingest.",3/13/16,1
lossyrob,that might be a good workaround actually,3/13/16,1
lossyrob,"@timothymschier pretty sure you were reading a very large file off of S3. The negative array index would happen when the # of bytes becomes greater than Int.MaxValue, causing the index inside of ByteArrayOutputStream it to overflow and go negative.",3/14/16,1
timothymschier,"@lossyrob  looks like that could be it. Easy trap though, because the large tif isn't actually inside the same folder: bda8.data/test_5_2_input.tif vs bda8.data/test_5_2_input/*.tif",3/14/16,1
timothymschier,I've had success doing an S3->S3 ingest from bda8.public bucket. S3->Accumulo had a failure; just re-running now.,3/14/16,1
lossyrob,"s3 works by key prefixes, it_Ñés not really a folder structure, so a key prefix `test_5_2_input` would include both",3/14/16,1
timothymschier,ah indeed,3/14/16,1
timothymschier,so it would also include ```test_5_2_input_old.tif``` which I just renamed it to *headslap*,3/14/16,1
lossyrob,i believe so. may could check with a `aws s3 ls` client listing before ingest,3/14/16,1
timothymschier,yeah looks like that was the issue - us not understanding S3... at least now we can get on with the rest of our eval now that we have ingested data :),3/14/16,1
lossyrob,this worked out nicely: https://github.com/geotrellis/geotrellis/commit/eaf719a0cfa73e6bda78f59958ae5ef04302f363,3/14/16,1
lossyrob,"kudos to @echeipesh, _Ñétis great when good API design pays off down the road.",3/14/16,1
jasonbeverage,@lossyrob Thanks for the help tracking down that s3 ingest error.  It didn't click until @timothymschier started emailing you :),3/14/16,1
jasonbeverage,"We have a bunch of big tiffs we're ingesting, and I wrote a script to warp them and run gdal_retile.py on them, and they dump out directories with the same name as the source tiff, which I then uploaded back to s3.",3/14/16,1
jasonbeverage,So you had input.tif you had a directory along side of it called input with all the tiles in it.,3/14/16,1
lossyrob,@jasonbeverage glad it worked out.,3/14/16,1
moradology,do we want all implicit extension methods to be listed on the objects they extend in our scaladocs?,3/14/16,1
moradology,"it *could* get messy, but it might help to clarify what's possible",3/14/16,1
lossyrob,"Hm, yeah that could get messy",3/14/16,1
lossyrob,Well if we have them on the traits,3/14/16,1
lossyrob,shouldn_Ñét the objects inheret those scaladocs,3/14/16,1
jamesmcclain,[patch.diff](https://files.gitter.im/geotrellis/geotrellis/tleM/patch.diff),3/14/16,1
jamesmcclain,^^ @lossyrob ,3/14/16,1
lossyrob,thx,3/14/16,1
jamesmcclain,You are welcome,3/14/16,1
jamesmcclain,[params.diff](https://files.gitter.im/geotrellis/geotrellis/Qxm8/params.diff),3/14/16,1
jamesmcclain,^^ Here are the parameter name changes that I made,3/14/16,1
lossyrob,ok cool I_Ñéll make taht change too,3/14/16,1
jamesmcclain,"That patch includes sorting of an import block, as well",3/14/16,1
jamesmcclain,I am writing up the README for the util package.  I have a question: what is the motivation for using extension methods throughout the library rather than making direct additions to the types themselves,3/14/16,1
lossyrob,"It allows for code organization, so that all methods don_Ñét end up in one big type file",3/14/16,1
lossyrob,it allows for methods to be written in an abstract way for multiple types that can utilize it,3/14/16,1
lossyrob,e.g. implicit method additions that work over MultibandTile and Tile,3/14/16,1
lossyrob,it provides a framework for adding functionality that does not require modifying core types,3/14/16,1
lossyrob,"it keeps the core types simple, with those types only defining what necessarily makes up that type",3/14/16,1
lossyrob,"it allows other functionality to require that a type T has methods defined by that extentsion, i.e. the `[T: ? => CropMethods[T]]` thing, so that that funcitonality does not have to be written against core types",3/14/16,1
lossyrob,this allows for later adding types that will still work with those functions,3/14/16,1
lossyrob,"e.g. client code an define their own types that give the proper method extensions, and that funcitonality will still work",3/14/16,1
jamesmcclain,"Okay thanks, I will add those points",3/14/16,1
bneff,"Hey Rob, if I'm using ZoomedLayoutScheme, is there a way I can determine the extent of a tile so I can store that in another service and have it match what is in my catalog?",3/14/16,1
lossyrob,"yeah, there_Ñés two ways you can do it_Ñ_",3/14/16,1
lossyrob,"if you have the layer metadata, you can do it generally by pulling out the metadata.mapTransform and applying it to the SpatialKey",3/14/16,1
lossyrob,"but if you know it_Ñés ZoomedLayoutScheme, it_Ñés a bit more straightforward",3/14/16,1
lossyrob,hold on I think there might be an example floating around,3/14/16,1
lossyrob,@rajadain where did that code live where you were doing this exact thing?,3/14/16,1
lossyrob,oops that_Ñés in a private repo,3/14/16,1
lossyrob,let me share the code,3/14/16,1
bneff,@lossyrob thank you!,3/14/16,1
lossyrob,still searching for it_Ñ_.,3/14/16,1
lossyrob,oh actually the repo is open my bad,3/14/16,1
lossyrob,https://github.com/azavea/usace-flood-geoprocessing/blob/develop/server/src/main/scala/com/azavea/usaceflood/server/ElevationData.scala#L23,3/14/16,1
lossyrob,"so as long as you know the tileSize, and have the zoom and the SpatialKey, that same logic will give you the Exent your looking for",3/14/16,1
bneff,"I'm slow, but I'm assuming I could also use this to go from Extent to Spatial Key?",3/14/16,1
lossyrob,"Yeah, but not exactly",3/14/16,1
lossyrob,because an extent might intersect with more than one SpatialKey,3/14/16,1
lossyrob,so what you get back is a GridBounds,3/14/16,1
lossyrob,"`GridBounds(colMin, rowMin, colMax, rowMax)`",3/14/16,1
lossyrob,which describes the tile keys that intersect the extent,3/14/16,1
bneff,@lossyrob thats perfect for us! Thanks again!,3/14/16,1
lossyrob,"and that_Ñés on `MapKeyTransform`, so it_Ñés just a matter of `val gridBounds = transform(extent)`",3/14/16,1
lossyrob,no problem!,3/14/16,1
timothymschier,"hey guys, just a quick performance qn - I'm seeing ingest from S3 to S3 as being ~50% faster than ingest from S3 to Accumulo; running on the same cluster. Is that expected? Is it maybe a sign of an underpowered cluster?",3/15/16,1
lossyrob,"This is interesting, I_Ñéve seen some performance problems with accumulo ingests, @jamesmcclain is looking into optmizing accumulo ingest over th next couple of weeks, so we might have a better answer for you then. But basically we do a bulk ingest of data, which copies it to HDFS and then creates the accumulo RFiles for the table after that step; though it seems completely counter intuitive that an S3 ingest would be faster.",3/15/16,1
timothymschier,OK thanks @lossyrob I'll keep an eye out for the work from @jamesmcclain . In the meantime I'm doing the performance profiling this week so I'll have to go with the S3 numbers. Seem to be ~9MB/s for RGB temporal ingests on S3-S3 vs ~6MB for S3-Accumulo. This was using the geotrellis-ec2-cluster AMIs too (both Accumulo and S3) on m3.2xlarge cluster with 2 followers.,3/15/16,1
Prasannads,"Hi guys, can you please share me a code snippet on how to store a TIFF file from local path into HDFS as RasterRDD ? i am using the bintray version ""6c567b2""",3/15/16,1
pomadchin,"@Prasannads hi; you mean to ingest into hdfs, and after that to deal with tiff data operating with RasterRDD?",3/15/16,1
Prasannads,"@pomadchin  yes, my use case is to ingest tiffs into HDFS and do some local map algebra operations on top of it .",3/15/16,1
pomadchin,"To ingest, it is possible to use `spark-etl` project: https://github.com/geotrellis/geotrellis/blob/master/docs/spark-etl/spark-etl-intro.md",3/15/16,1
lokifacio,@Prasannads I'm updating right the code I gave you on the mailing group (latest commits have renamed some classes),3/15/16,1
lokifacio,RasterRDD is now TileLayerRDD,3/15/16,1
pomadchin,@lokifacio @Prasannads working on updating https://github.com/pomadchin/geotrellis-chatta-demo probably that would be a useful usage example,3/15/16,1
lokifacio,"@pomadchin cool. I see that TileLayerMetadata now requires an extra bounds parameter, what is it meant for?",3/15/16,1
Prasannads,@lokifacio  ok. Seems RasterMetadata is now TileLayerMetadata .,3/15/16,1
lokifacio,yep,3/15/16,1
pomadchin,now we store layer KeyBounds in TileLayerMetadata,3/15/16,1
pomadchin,"```scalacase class TileLayerMetadata[K: SpatialComponent](  cellType: CellType,  layout: LayoutDefinition,  extent: Extent,  crs: CRS,  bounds: Bounds[K]) ```",3/15/16,1
Prasannads,@lokifacio  Can you please share the code once you have done updating it ? i think HadoopLayerWriter does not require TileLayerMetadata?,3/15/16,1
lokifacio,"@Prasannads, just a quick glance: it seems now you need to construct a new object instead of using a method of the object companion (similar to Java statics)",3/15/16,1
lokifacio,and then call read/write,3/15/16,1
lokifacio,but now sure on how to deal with generic parameters yet,3/15/16,1
lokifacio,"@pomadchin what's the proper way to call read to retrieve `TileLayerRDD[SpatialKey]````def readTiles(region: Region, name: String): TileLayerRDD[SpatialKey] = {    // Previous API: HadoopLayerReader.spatial(regionPath(region))(sc).read(layer(name))    val reader = HadoopLayerReader(regionPath(region))(sc)    reader.read(layer(name))  }```",3/15/16,1
pomadchin,"```scalareader.read[SpatialKey, Tile, TileLayerMetadata](layer(name))```",3/15/16,1
lokifacio,"thx, I wasn't sure about the third type",3/15/16,1
pomadchin,"```scaladef read[    K: AvroRecordCodec: Boundable: JsonFormat: ClassTag,    V: AvroRecordCodec: ClassTag,    M: JsonFormat: Component[?, Bounds[K]]  ](id: ID)```",3/15/16,1
pomadchin,"Yes, it_Ñés metadata :)",3/15/16,1
lokifacio,where can I find doc about SpatialKeys/Bounds? It is one of the parts I understand less,3/15/16,1
pomadchin,"think it (docs) is not ready yet, but if you have certain questions just post them here",3/15/16,1
lokifacio,"@pomadchin actually it is `reader.read[SpatialKey, Tile, TileLayerMetadata[SpatialKey]]`> ```scalareader.read[SpatialKey, Tile, TileLayerMetadata](layer(name))```",3/15/16,1
pomadchin,"@lokifacio ty, yes; +1 for imporatant notice",3/15/16,1
cdelgadob,"Hi all, I'm new to geotrellis, and generally to all GIS world. I'm here because I'm facing a GIS-related problem in a Hadoop-Spark environment. We have a lot of points (coordinates (lat, lon) in Mercator projection, same as Google's, right?) and I need to calculate which points are less than 100m to each other. Ideally I would need the info like this: (origin, dest,distance). Each point has a unique identifier.",3/15/16,1
cdelgadob,"Looking around I think that a quadtree would be a great solution for this, but I can't find a way to start implementing this, can you guys help me a bit?",3/15/16,1
cdelgadob,"So my starting point is a Spark Dataframe of (PointId, lat, lon), which comes from a Hive query, and the resulting data will be written in another Hive table",3/15/16,1
lokifacio,"hey, are you planning to do more heavy changes on the API? should I wait until 0.10 is released out? Or it is ""safe"" for me to resolve all API conflicts now? (I don't have so much spare time to do this twice a week xD)",3/15/16,1
lokifacio,"@cdelgadob I used a JSI rtree as a broadcast variable (I needed to use ""net.sf.jsi"" % ""jsi"" % ""1.1.0-SNAPSHOT"" to use serialization) for a similar job (this is independent of geotrellis/spark). Maybe geotrellis has another rtree/quadtree implementation",3/15/16,1
cdelgadob,"@lokifacio Thanks for the tip, I'll take a look. Just one thing: Is it capable of distributing the operations across a Hadoop cluster?",3/15/16,1
Prasannads,@pomadchin   will I be able to read the data using HadoopLayerReader which was ingested through old API HadoopIngestCommand.scala ?,3/15/16,1
pomadchin,"@Prasannads nope; btw, what revision is it? there would be metadata migration tool (not published yet)",3/15/16,1
pomadchin,or it is gt-admin?,3/15/16,1
Prasannads,@pomadchin  yes that is gt-admin,3/15/16,1
pomadchin,current version?,3/15/16,1
pomadchin,it is really very very deprecated ,3/15/16,1
pomadchin,: \,3/15/16,1
Prasannads,ok :( we have some of the tiffs ingested through that way.,3/15/16,1
pomadchin,not sure what versions migrations are we going to support; ,3/15/16,1
pomadchin,really big data amount?,3/15/16,1
Prasannads,not big though.,3/15/16,1
pomadchin,"yes, and strongly recommend you to use another index type: ZCurve or HilbertCurve",3/15/16,1
Prasannads,whats the version of the spark-etl ( for loading tiffs to hdfs ) which you said before ? ( https://github.com/geotrellis/geotrellis/blob/master/docs/spark-etl/spark-etl-intro.md ) .. Because i am getting 404 error when i tried to see build.sbt,3/15/16,1
pomadchin,"typo, thanks!",3/15/16,1
pomadchin,https://github.com/geotrellis/geotrellis/blob/master/spark-etl/build.sbt,3/15/16,1
lokifacio,"@cdelgadob nope, you have to distribute them yourself",3/15/16,1
Prasannads,"@pomadchin  thanks ! that can be read through the class HadoopLayerReader class in latest commit of bintray version , right?",3/15/16,1
pomadchin,exactly! that_Ñés right!,3/15/16,1
Prasannads,ok. Thanks!,3/15/16,1
pomadchin,"documentation is incomplete though ): but write here all your questions, will try to help you with everything",3/15/16,1
cdelgadob,"@lokifacio thanks, I'll try geotrellis, which seems a bit more mantained than the other (last commit was 2 years ago!)",3/15/16,1
pomadchin,@cdelgadob extremely mantained :D ,3/15/16,1
Prasannads,@pomadchin  Thanks for your quick replies :),3/15/16,1
lokifacio,@pomadchin too much :P,3/15/16,1
pomadchin,much cool so maintained :D,3/15/16,1
pomadchin,rc1 going to be published soon; current master api is quite near to rc1 version,3/15/16,1
Prasannads,I expected the release this monday !,3/15/16,1
cdelgadob,"@pomadchin Yeah, I meant really mantained :D",3/15/16,1
lokifacio,"after resolving all other issues with my last upgrade, I still have the following error I'm bit puzzled on how to solve it:```Error:(45, 65) could not find implicit value for evidence parameter of type spray.json.JsonFormat[geotrellis.spark.SpatialKey]    reader.read[SpatialKey, Tile, TileLayerMetadata[SpatialKey]](LayerId(name, 1))                                                                ^```@pomadchin can you check if you get the same errors on my function definition? (I copy here the updated version)```  def readTiles(path: String, name: String): TileLayerRDD[SpatialKey] = {    val reader = HadoopLayerReader(path)(sc)    reader.read[SpatialKey, Tile, TileLayerMetadata[SpatialKey]](LayerId(name, 1))  }```",3/15/16,1
cdelgadob,so let's see how I put there points in a RDD of some kind that distributes the processing. But first things first: what's a Tile? (and sorry for my ignorance),3/15/16,1
pomadchin,"@lokifacio ```scalaimport spark.io._```should solve your problem (:",3/15/16,1
lokifacio,"@cdelgadob if I understood you properly, you want to evaluate every point against all the other points, right? In that case I think it can easily be solved using spark with a RDD[Point] and a broadcast variable containing the rtree with all your points so it is distributed to every node where each point could be checked against it.",3/15/16,1
pomadchin,@lokifacio the reason that `spark.io.json._` moved to `spark.io.json.Implicits._`,3/15/16,1
lokifacio,"@pomadchin thx man. Btw, is there any way to get that solution from the error or you need to know the API by heart? I like the way implicits improve APIs but I find them really difficult to discover out.",3/15/16,1
pomadchin,"That_Ñés a common solution for Scala, yes implicits _may be_ not obvious. Solution as I think to document these things, and to provide similar structure to provide implicits for any operations you want. like `spark.io.json.Implicits._` `spark.io.accumulo.codecs.Implicits._` and so on; and to add implicits into root package objects (like all implicits for `io` package add into `io` package object) ",3/15/16,1
Prasannads,"@pomadchin  i am now trying the spark-etl , i am getting an error on the stage tile . ""val (zoom, tiled) = etl.tile(sourceTiles)""",3/15/16,1
pomadchin,what error?,3/15/16,1
Prasannads,"could not find implicit value for evidence parameter of type monocle.PLens[geotrellis.vector.ProjectedExtent,geotrellis.vector.ProjectedExtent,geotrellis.vector.ProjectedExtent,geotrellis.vector.ProjectedExtent] Error occurred in an application involving default arguments.not enough arguments for method tile: (implicit evidence$15: monocle.PLens[geotrellis.vector.ProjectedExtent,geotrellis.vector.ProjectedExtent,geotrellis.vector.ProjectedExtent,geotrellis.vector.ProjectedExtent], implicit evidence$16: geotrellis.vector.ProjectedExtent __ê geotrellis.spark.tiling.TilerKeyMethods[geotrellis.vector.ProjectedExtent,K], implicit evidence$17: geotrellis.raster.stitch.Stitcher[geotrellis.raster.Tile], implicit evidence$18: scala.reflect.ClassTag[geotrellis.raster.Tile], implicit evidence$19: geotrellis.raster.Tile __ê geotrellis.raster.merge.TileMergeMethods[geotrellis.raster.Tile], implicit evidence$20: geotrellis.raster.Tile __ê geotrellis.raster.prototype.TilePrototypeMethods[geotrellis.raster.Tile], implicit evidence$21: geotrellis.raster.Tile __ê geotrellis.raster.reproject.TileReprojectMethods[geotrellis.raster.Tile], implicit evidence$22: geotrellis.raster.Tile __ê geotrellis.raster.crop.CropMethods[geotrellis.raster.Tile], implicit evidence$23: geotrellis.spark.SpatialComponent[K], implicit evidence$24: geotrellis.spark.Boundable[K], implicit evidence$25: scala.reflect.ClassTag[K], implicit sc: org.apache.spark.SparkContext)(Int, org.apache.spark.rdd.RDD[(K, geotrellis.raster.Tile)] with geotrellis.spark.Metadata[geotrellis.spark.TileLayerMetadata[K]]). Unspecified value parameters evidence$15, evidence$16, evidence$17, ... Error occurred in an application involving default arguments.",3/15/16,1
Prasannads,version problem ?,3/15/16,1
Prasannads,i am using maven,3/15/16,1
Prasannads,"    <dependency>      <groupId>com.azavea.geotrellis</groupId>      <artifactId>geotrellis-spark_2.10</artifactId>      <version>0.10.0-6c567b2</version>      <type>jar</type>    </dependency>    <dependency>     <groupId>com.azavea.geotrellis</groupId>     <artifactId>geotrellis-spark-etl_2.10</artifactId>     <version>0.10.0-6c567b2</version>     <type>jar</type>    </dependency>    <dependency>     <groupId>org.rogach</groupId>     <artifactId>scallop_2.10</artifactId>     <version>0.9.5</version>    </dependency>",3/15/16,1
pomadchin,"```scalaimport spark.io.ingest._ ```should solve your problem",3/15/16,1
lokifacio,xD,3/15/16,1
pomadchin,(: not enough implicits in the function call scope,3/15/16,1
lokifacio,`import  *.*.*._`,3/15/16,1
lokifacio,"would be nice hehehe",3/15/16,1
Prasannads,@lokifacio  hehe,3/15/16,1
pomadchin,"not so bad, but these implicits errors really could be weird if you are not familiar with scala enough ):",3/15/16,1
Prasannads,sorry.. am new to scala :),3/15/16,1
lokifacio,"well, I'm not done yet with the upgrade :(I get the following error:```Failed to write Layer(name = ""mask"", zoom = 1)geotrellis.spark.io.package$LayerWriteError: Failed to write Layer(name = ""mask"", zoom = 1)```with my current ingest method:```  def writeTiles(tiles: TileLayerRDD[SpatialKey], region: Region, name: String): Unit = {    val writer = HadoopLayerWriter(regionPath(region))(sc)    writer.write(layer(name), tiles, HilbertKeyIndexMethod)}```am I missing another import?",3/15/16,1
pomadchin,":D) that_Ñés not a problem at all, feedback is always appreciated",3/15/16,1
pomadchin,@lokifacio more complete stacktrace?,3/15/16,1
lokifacio,"```geotrellis.spark.io.package$LayerWriteError: Failed to write Layer(name = ""mask"", zoom = 1)	at geotrellis.spark.io.hadoop.HadoopLayerWriter._write(HadoopLayerWriter.scala:39)	at geotrellis.spark.io.hadoop.HadoopLayerWriter._write(HadoopLayerWriter.scala:19)	at geotrellis.spark.io.LayerWriter$class.write(LayerWriter.scala:42)	at geotrellis.spark.io.hadoop.HadoopLayerWriter.write(HadoopLayerWriter.scala:19)	at com.lda.geos.variables.Catalog.writeTiles(Catalog.scala:52)```",3/15/16,1
lokifacio,sorry I should have realized I omitted the line pointing to the failing method ,3/15/16,1
pomadchin,and nothing more? O: that_Ñés complete stacktrace?,3/15/16,1
Prasannads,@pomadchin  sorry i could not find spark.io.ingest._    ,3/15/16,1
pomadchin,"@Prasannads excuse me, my fault: `import geotrellis.spark.ingest._`",3/15/16,1
Prasannads,i have done it already,3/15/16,1
pomadchin,and? O:,3/15/16,1
pomadchin,not working? ,3/15/16,1
Prasannads,yes not working :(,3/15/16,1
lokifacio,"here it goes:```Failed to write Layer(name = ""mask"", zoom = 1)geotrellis.spark.io.package$LayerWriteError: Failed to write Layer(name = ""mask"", zoom = 1)	at geotrellis.spark.io.hadoop.HadoopLayerWriter._write(HadoopLayerWriter.scala:39)	at geotrellis.spark.io.hadoop.HadoopLayerWriter._write(HadoopLayerWriter.scala:19)	at geotrellis.spark.io.LayerWriter$class.write(LayerWriter.scala:42)	at geotrellis.spark.io.hadoop.HadoopLayerWriter.write(HadoopLayerWriter.scala:19)	at com.lda.geos.variables.Catalog.writeTiles(Catalog.scala:52)        .... // I removed testing framework methodsCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0.0 in stage 5.0 (TID 4) had a not serializable result: org.apache.hadoop.io.LongWritableSerialization stack:	- object not serializable (class: org.apache.hadoop.io.LongWritable, value: 19)	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1283)	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1271)	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1270)	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1270)	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:697)	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:697)	at scala.Option.foreach(Option.scala:236)	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:697)	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1496)	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1458)	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1447)	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:567)	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1824)	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1837)	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1914)	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopDataset$1.apply$mcV$sp(PairRDDFunctions.scala:1055)	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopDataset$1.apply(PairRDDFunctions.scala:998)	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopDataset$1.apply(PairRDDFunctions.scala:998)	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:147)	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:108)	at org.apache.spark.rdd.RDD.withScope(RDD.scala:310)	at org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopDataset(PairRDDFunctions.scala:998)	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopFile$2.apply$mcV$sp(PairRDDFunctions.scala:938)	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopFile$2.apply(PairRDDFunctions.scala:930)	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopFile$2.apply(PairRDDFunctions.scala:930)	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:147)	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:108)	at org.apache.spark.rdd.RDD.withScope(RDD.scala:310)	at org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:930)	at geotrellis.spark.io.hadoop.HadoopRDDWriter$.write(HadoopRDDWriter.scala:64)	at geotrellis.spark.io.hadoop.HadoopLayerWriter._write(HadoopLayerWriter.scala:41)	... 64 more```",3/15/16,1
pomadchin,"@Prasannads oh sry, really```scalaimport geotrellis.spark._import geotrellis.spark.io._import geotrellis.spark.io.yourbackend._```",3/15/16,1
pomadchin,"@lokifacio that caused by the reason, that your object, which have to be be spreaded among spark nodes is not serialisable O:",3/15/16,1
Prasannads,@pomadchin Thanks that worked !,3/15/16,1
pomadchin,@Prasannads you are welcome!,3/15/16,1
pomadchin,@lokifacio have you got a more complete example of how you are trying to use reader?,3/15/16,1
lokifacio,but my object is just a `TileLayerRDD[SpatialKey]` I only changed the metadata to change `cellType` to `BitCellType`. (I also tried not changing the metadata),3/15/16,1
pomadchin,"if that_Ñés an rdd operation, then it requires sending these jobs to all executors",3/15/16,1
lokifacio,"```def readTiles(sc: SparkContext, source: String, scheme: LayoutScheme = FloatingLayoutScheme(250)): TileLayerRDD[SpatialKey] = {    val tiff = readGeoTiff(sc, source)    val (_, sourceMetadata) = TileLayerMetadata.fromRdd(tiff, scheme)    val metadata = snappedMetaData(sourceMetadata)    ContextRDD(tiff.tileToLayout(sourceMetadata), metadata)  }```",3/15/16,1
lokifacio,"@pomadchin sure, but don't know why my tiles are not serializable...",3/15/16,1
pomadchin,"be sure you object is serialisable possible issue: if you have smth like:```scalaclass CoolClass {  val writer = _Ñ_ // your backend writer  val reader = _Ñ_ // your backend reader```make it smth like: ```scalaclass CoolClass extends Serializable {  @transient val writer = _Ñ_ // your backend writer  @transient val reader = _Ñ_ // your backend reader```",3/15/16,1
lokifacio,I did this before updating to latest bintray release without problems,3/15/16,1
lokifacio,"I don't think my class should be serializable, it is just part of the driver",3/15/16,1
lokifacio,I'm trying to write some raster after tiling it,3/15/16,1
pomadchin,is it possible to see more complete example?,3/15/16,1
pomadchin,ideally would be if you will try exaclty the same old code but refactored to use new api without additional functions usage ,3/15/16,1
Prasannads,"i am doing  local map algebra operationval TransformRasterRDD = raster1 localAdd raster2",3/15/16,1
lokifacio,"I think I gave you all the code, my driver code is the following:```    val mask = readTiles(sc,  ""mask.tif"")    //  I also tried writing mask directly instead of changing to binaryMask)    val binaryMask = ContextRDD(mask, mask.metadata.copy(cellType = BitCellType))    // test is only to be appended to a default catalog path    // mask is the layer name    // writeTiles is the method I posted before    writeTiles(binaryMask,  ""test"", ""mask"")```",3/15/16,1
lokifacio,"I missed this snippet```  def snappedMetaData(metadata: TileLayerMetadata[SpatialKey]): TileLayerMetadata[SpatialKey] = {    val gridBounds = metadata.mapTransform(metadata.extent)    val snapExtent = metadata.mapTransform(gridBounds)    metadata.copy(extent = snapExtent)  }```",3/15/16,1
Prasannads,@lokifacio  can you please explain what the snappedMetaData does ?,3/15/16,1
lokifacio,"it just corrects the extent of the resulting `TileLayerRDD`. When you load a raster whose dimensions don't fit in the schema provided, the `tileToLayout` method adds new cells so it matches it. Thus the extent covered by all cells may differ from the extent of your input. Just note that this was my understanding of how geotrellis works after several tests, so maybe @pomadchin  or @lossyrob can point us to a proper way to do it",3/15/16,1
Prasannads,"ok. What i have planned is .. ingest the tiffs into Hdfs using the spark-etl library load stage as @pomadchin  said , read the following way  and do local map algebra operation and store the raster back into hdfs.   def readHdfsTiles(path: String): TileLayerRDD[SpatialKey] = {    implicit val sc = SparkUtils.createSparkContext(""Ingest"")    //layer name for reading the RasterRDD    val nlcd1 = new String(""nlcd1"")    val nlcd2 = new String(""nlcd2"")    val reader1 = HadoopLayerReader(path)(sc)    val raster1 = reader1.read[SpatialKey, Tile, TileLayerMetadata[SpatialKey]](LayerId(nlcd1, 1))    val raster2 = reader1.read[SpatialKey, Tile, TileLayerMetadata[SpatialKey]](LayerId(nlcd2, 1))   // Add 5 to each cell  val TransformRasterRDD = raster1 localAdd raster2    val layerid = new LayerId(""newnlcd"",1)  val spatialkey = new SpatialKey(256, 256)  val keybounds = new keyboundimpl().KeyBounds(spatialkey)    val keyindex = new keyindexmethodimpl().createIndex(keybounds)    //Writing the resultant RDD to HDFS  val writer = HadoopLayerWriter(path)(sc)  writer.write(layerid, TransformRasterRDD, keyindex)",3/15/16,1
Prasannads,i am not sure if am on the right way,3/15/16,1
Prasannads,"I am new to scala , sorry about the bad programming ",3/15/16,1
pomadchin,"@Prasannads ~ a right way, you tried tom compile? sry for a long response",3/15/16,1
echeipesh,"@lokifacio: re your ""object not serializable_Ñù error. You would get that if you didn_Ñét enable `.set(""spark.serializer"", ""org.apache.spark.serializer.KryoSerializer_Ñù)` for your `SparkConf`. Can you double check that it is happening ?",3/15/16,1
Prasannads,"@pomadchin  - no problem :) i am stuck with writing the raster rddhadoopLayerWriter expects [layerid,rdd(spatialkey,tile)],KeyIndex] . when i implement and override the abstract KeyIndex , the method toIndex returns LONG type .",3/15/16,1
lokifacio,"@echeipesh probably not as I'm using SparkContext given by SharedSparkContext scala FunSuite, I'll try that",3/15/16,1
pomadchin,@echeipesh :+1:,3/15/16,1
Prasannads,"Error for writer.write(layerid, rddWithContext1, keyindex) -- overloaded method value write with alternatives: [K, V, M](id: geotrellis.spark.LayerId, layer: org.apache.spark.rdd.RDD[(K, V)] with geotrellis.spark.Metadata[M], keyIndexMethod: geotrellis.spark.io.index.KeyIndexMethod[K])(implicit evidence$15: geotrellis.spark.io.avro.AvroRecordCodec[K], implicit evidence$16: spray.json.JsonFormat[K], implicit evidence$17: scala.reflect.ClassTag[K], implicit evidence$18: geotrellis.spark.io.avro.AvroRecordCodec[V], implicit evidence$19: scala.reflect.ClassTag[V], implicit evidence$20: spray.json.JsonFormat[M], implicit evidence$21: monocle.PLens[M,M,geotrellis.spark.Bounds[K],geotrellis.spark.Bounds[K]])Unit <and> [K, V, M](id: geotrellis.spark.LayerId, layer: org.apache.spark.rdd.RDD[(K, V)] with geotrellis.spark.Metadata[M], keyIndex: geotrellis.spark.io.index.KeyIndex[K])(implicit evidence$8: geotrellis.spark.io.avro.AvroRecordCodec[K], implicit evidence$9: spray.json.JsonFormat[K], implicit evidence$10: scala.reflect.ClassTag[K], implicit evidence$11: geotrellis.spark.io.avro.AvroRecordCodec[V], implicit evidence$12: scala.reflect.ClassTag[V], implicit evidence$13: spray.json.JsonFormat[M], implicit evidence$14: monocle.PLens[M,M,geotrellis.spark.Bounds[K],geotrellis.spark.Bounds[K]])Unit cannot be applied to (geotrellis.spark.LayerId, org.apache.spark.rdd.RDD[(geotrellis.spark.SpatialKey, geotrellis.raster.Tile)] with geotrellis.spark.Metadata[geotrellis.spark.TileLayerMetadata[geotrellis.spark.SpatialKey]], Long)",3/15/16,1
echeipesh,"@Prasannads You are on the right track, but for your writer, you need not try to get an instance of KeyIndex yourself, you should be able to use the overload that asks for `KeyIndexMethod`.",3/15/16,1
Prasannads,@echeipesh  ok! let me try that,3/15/16,1
Prasannads,@echeipesh  can you please tell me how the keybounds works ?  It takes spatial key and gives the boundary?,3/15/16,1
echeipesh,"It_Ñés more descriptive. `KeyBounds[K]` defines a region in some-diemensional space by specifying minimumum and maximum key. So if K is `SpatialKey` it defines a rectangle region of tiles. If it is `SpaceTimeKey` it defines a cube/stack of spatial regions, with time being the third dimension.",3/15/16,1
echeipesh,"We use it because we need to know the space keys will occupy so we can index them using a space filing curve, which requires a bounded space. For the most part the actual value of `KeyBounds[K]` should be more or less transperent to you in normal usage. We will gather its value when ingesting tiles, save it when saving it using a writer and load it when using a layer reader.",3/15/16,1
lokifacio,@echeipesh I get a NullPointerException when I set the KryoSerializer,3/15/16,1
lokifacio,"```java.lang.NullPointerException	at geotrellis.spark.io.hadoop.HadoopAttributeStore$.apply(HadoopAttributeStore.scala:149)```",3/15/16,1
echeipesh,"Where that will break down is when you do operations that modify the keys of the RDD, adding/changing/dividing them, without going through API that tries to maintain that validity of KeyBounds _Ñ_ but that should be _ÑÒburn the bridge when you get to it_Ñù type of deal.",3/15/16,1
echeipesh,@lokifacio very strange _Ñ_ pretty much the only way that could happen is if your spark context value is actually null.,3/15/16,1
Prasannads,@echeipesh  ..  ok.Are the min and max keys depend on something else or our own values ?,3/15/16,1
echeipesh,You mentioned you_Ñére trying to do this in tests. Check out how we setup the context in our tests: https://github.com/geotrellis/geotrellis/blob/master/spark/src/test/scala/geotrellis/spark/TestEnvironment.scala,3/15/16,1
echeipesh,Although for `spark.kryo.registrator` you should use `geotrellis.spark.io.kryo.KryoRegistrator`. We make a spetial registrator for tests just so we can set mandetory registration in tests and make sure we_Ñére catching any new types we_Ñére introducing and registering them with Kryo.,3/15/16,1
echeipesh,"@Prasannads They_Ñére generated from your own values. They may not be one of the values in your dataset. For instance if you have two record dataset with keys `SpatialKey(1,2)` `SpatialKey(3,1)` the keyBound would actually be `KeyBounds(SpatialKey(1,1), SpatialKey(3,2))`.",3/15/16,1
echeipesh,We use an instance of type class `Boundable` to essentially reduce over the keys in your dataset and generate those bounding key values.,3/15/16,1
echeipesh,for instance: https://github.com/geotrellis/geotrellis/blob/master/spark/src/main/scala/geotrellis/spark/SpatialKey.scala#L22-L29,3/15/16,1
lokifacio,@echeipesh the NullPointerException was caused by a simplifcation in my tests. It seems to work now. Thanks.,3/15/16,1
echeipesh,"@lokifacio no probem, glad to help  :)",3/15/16,1
lossyrob,@pomadchin https://github.com/geotrellis/geotrellis/blob/master/accumulo/src/main/scala/geotrellis/spark/io/accumulo/AccumuloRDDWriter.scala#L32,3/15/16,1
lossyrob,https://github.com/geotrellis/geotrellis/blob/master/accumulo/src/main/scala/geotrellis/spark/io/accumulo/AccumuloWriteStrategy.scala#L49,3/15/16,1
pomadchin,@lossyrob https://github.com/geotrellis/geotrellis/pull/1390/files,3/15/16,1
echeipesh,"@pomadchin @lossyrob regarding the version conflicts with avro and the registration. I wonder if a good solution would be to move the Avro Schema to test kryo register. In reality it is only ever sent out once per writing a layer, so the extra overhead of including a full class name is nothing and since we effectivly don_Ñét have control over the version of Avro being brought in by spark we may find that any workaround we make is going to be fragile.",3/15/16,1
pomadchin,@echeipesh any workaroung in fact is fragile. moreover any client custom hadoop build depends on its own avro ,3/15/16,1
echeipesh,"yep, exactly what I am thinking.",3/15/16,1
echeipesh,"In fact since we_Ñére writing docs, I really need to make a section on dependency management issues since it comes up so often and is so frustrating.",3/15/16,1
lossyrob,I would say we shouldn_Ñét register it if it_Ñés a type coming in from the client,3/15/16,1
lokifacio,would very appreciated ,3/15/16,1
lossyrob,@pomadchin are you making that change? ,3/15/16,1
pomadchin,@lossyrob not to register avro field? ,3/15/16,1
pomadchin,currently I am debugging ingest process (all backends) to be sure that everything works + fixing definetly not working accumulo,3/15/16,1
lossyrob,ok I can make it,3/15/16,1
pomadchin,@lossyrob you want to remove it form registration? now we have to add these lines manually on the client side? hm,3/15/16,1
lossyrob,you don_Ñét have to add them,3/15/16,1
lossyrob,it just means that the schema will be encoded with a string to identify the class,3/15/16,1
lossyrob,which causes a little bit of bloat,3/15/16,1
lossyrob,for serialization,3/15/16,1
lossyrob,"but if a client wants to solve that, then yeah the would have to register the schema themselves with Kryo",3/15/16,1
lossyrob,"which makes sense, because it_Ñés a client side dependency",3/15/16,1
lossyrob,"if we don_Ñét have control over the Avro they are using, then we shouldn_Ñét register it",3/15/16,1
pomadchin,:+1:,3/15/16,1
lossyrob,"unless we explicitly state, _ÑÒthis is the avro version you have to use""",3/15/16,1
lossyrob,but then that_Ñés a bit out of their control becasue taht is deteremined by the hadoop version,3/15/16,1
lossyrob,so it_Ñés a bit of dependency hell,3/15/16,1
jasonbeverage,"You might be talking about what I'm running into now.  I upgraded our test project to use 0.10.0-80046e3 from bintray and now all spark-etl ingests have stopped working for me.  I get this stack trace```Exception in thread ""main"" java.lang.NoSuchMethodError: org.apache.avro.Schema$Field.<init>(Ljava/lang/String;Lorg/apache/avro/Schema;Ljava/lang/String;Ljava/lang/Object;)V	at geotrellis.spark.io.kryo.KryoRegistrator.registerClasses(KryoRegistrator.scala:213)	at org.apache.spark.serializer.KryoSerializer$$anonfun$newKryo$4.apply(KryoSerializer.scala:120)	at org.apache.spark.serializer.KryoSerializer$$anonfun$newKryo$4.apply(KryoSerializer.scala:120)	at scala.Option.foreach(Option.scala:236)	at org.apache.spark.serializer.KryoSerializer.newKryo(KryoSerializer.scala:120)	at org.apache.spark.serializer.KryoSerializerInstance.borrowKryo(KryoSerializer.scala:237)	at org.apache.spark.serializer.KryoSerializerInstance.<init>(KryoSerializer.scala:222)	at org.apache.spark.serializer.KryoSerializer.newInstance(KryoSerializer.scala:138)	at org.apache.spark.broadcast.TorrentBroadcast$.blockifyObject(TorrentBroadcast.scala:201)	at org.apache.spark.broadcast.TorrentBroadcast.writeBlocks(TorrentBroadcast.scala:102)	at org.apache.spark.broadcast.TorrentBroadcast.<init>(TorrentBroadcast.scala:85)	at org.apache.spark.broadcast.TorrentBroadcastFactory.newBroadcast(TorrentBroadcastFactory.scala:34)	at org.apache.spark.broadcast.BroadcastManager.newBroadcast(BroadcastManager.scala:63)	at org.apache.spark.SparkContext.broadcast(SparkContext.scala:1327)	at org.apache.spark.rdd.NewHadoopRDD.<init>(NewHadoopRDD.scala:76)	at org.apache.spark.SparkContext$$anonfun$newAPIHadoopRDD$1.apply(SparkContext.scala:1135)	at org.apache.spark.SparkContext$$anonfun$newAPIHadoopRDD$1.apply(SparkContext.scala:1130)	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:147)	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:108)	at org.apache.spark.SparkContext.withScope(SparkContext.scala:709)	at org.apache.spark.SparkContext.newAPIHadoopRDD(SparkContext.scala:1130)	at geotrellis.spark.etl.s3.GeoTiffS3Input.apply(GeoTiffS3Input.scala:15)	at geotrellis.spark.etl.Etl.load(Etl.scala:86)	at geotrellis.spark.etl.Etl$.ingest(Etl.scala:44)	at geotrellis.spark.etl.SinglebandIngest$delayedInit$body.apply(SinglebandIngest.scala:13)	at scala.Function0$class.apply$mcV$sp(Function0.scala:40)	at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:12)	at scala.App$$anonfun$main$1.apply(App.scala:71)	at scala.App$$anonfun$main$1.apply(App.scala:71)	at scala.collection.immutable.List.foreach(List.scala:318)	at scala.collection.generic.TraversableForwarder$class.foreach(TraversableForwarder.scala:32)	at scala.App$class.main(App.scala:71)	at geotrellis.spark.etl.SinglebandIngest$.main(SinglebandIngest.scala:11)	at geotrellis.spark.etl.SinglebandIngest.main(SinglebandIngest.scala)	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)	at java.lang.reflect.Method.invoke(Method.java:606)	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:674)	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180)	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205)	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:120)	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)```",3/15/16,1
jasonbeverage,"Running on EMR 4.2.0 with spark 1.5.2.  I'm passing this in for the spark conf as well```spark.serializer org.apache.spark.serializer.KryoSerializerspark.kryo.registrator geotrellis.spark.io.kryo.KryoRegistrator```",3/15/16,1
echeipesh,That_Ñés exactly what_Ñés happening there,3/15/16,1
pomadchin,@jasonbeverage update up to master branch,3/15/16,1
pomadchin,solution is only in the latest commit (workaround);,3/15/16,1
jasonbeverage,"Ok, should I be passing in that spark.serializer and spark.kyro.registrator setting as well?",3/15/16,1
lossyrob,yeah you still need those settings,3/15/16,1
jasonbeverage,Ok.  master branch or the new 0.10 branch?,3/15/16,1
lossyrob,master,3/15/16,1
lossyrob,I_Ñém running some tests before I update 0.10,3/15/16,1
lossyrob,"this will make it into 0.10.0-RC1 since I_Ñéve been having trouble publishing, so after this fix gets in I_Ñéll be wrestling again with sonatype",3/15/16,1
jasonbeverage,"Ok awesome, thanks for the help.",3/15/16,1
jasonbeverage,"I'm getting this with the master when doing a scripts/publish-local.sh```[error] /home/jason/geotrellis/vector/src/main/scala/geotrellis/vector/Extent.scala:31: value reproject is not a member of geotrellis.vector.Extent[error]     extent.reproject(crs, dest)[error]            ^[error] one error found```",3/15/16,1
pomadchin,@jasonbeverage `rm -r vector/target`,3/15/16,1
pomadchin,some weird sbt issues ):,3/15/16,1
lokifacio,"btw, @dwins, I checked your commit fixed the extent reprojection issue",3/15/16,1
jasonbeverage,Great thanks.,3/15/16,1
lossyrob,there_Ñés a `scripts/cleanall.sh` to give it a good scrubbing,3/15/16,1
lossyrob,@lokifacio nice!,3/15/16,1
jasonbeverage,"Awesome, I'll run that and give it another go @lossyrob ",3/15/16,1
dwins,glad to hear it @lokifacio ,3/15/16,1
jasonbeverage,"Getting further I guess, now my ingest is dying with this:```16/03/15 17:40:05 INFO DAGScheduler: ResultStage 94 (foreachPartition at S3RDDWriter.scala:42) finished in 0.271 s16/03/15 17:40:05 INFO YarnScheduler: Removed TaskSet 94.0, whose tasks have all completed, from pool 16/03/15 17:40:05 INFO DAGScheduler: Job 10 finished: foreachPartition at S3RDDWriter.scala:42, took 0.540979 s16/03/15 17:40:05 INFO Etl: geotrellis.spark.tiling.ZoomedLayoutScheme@ad02020Exception in thread ""main"" geotrellis.vector.ExtentRangeError: Invalid Extent: xmin must be less than xmax (xmin=28.0, xmax=27.0)	at geotrellis.vector.Extent.<init>(Extent.scala:46)	at geotrellis.spark.ingest.Pyramid$.up(Pyramid.scala:51)	at geotrellis.spark.ingest.Pyramid$.up(Pyramid.scala:109)	at geotrellis.spark.ingest.Pyramid$.up(Pyramid.scala:99)	at geotrellis.spark.etl.Etl.savePyramid$1(Etl.scala:177)	at geotrellis.spark.etl.Etl.save(Etl.scala:186)	at geotrellis.spark.etl.Etl$.ingest(Etl.scala:48)	at geotrellis.spark.etl.SinglebandIngest$delayedInit$body.apply(SinglebandIngest.scala:13)	at scala.Function0$class.apply$mcV$sp(Function0.scala:40)	at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:12)	at scala.App$$anonfun$main$1.apply(App.scala:71)	at scala.App$$anonfun$main$1.apply(App.scala:71)	at scala.collection.immutable.List.foreach(List.scala:318)	at scala.collection.generic.TraversableForwarder$class.foreach(TraversableForwarder.scala:32)	at scala.App$class.main(App.scala:71)	at geotrellis.spark.etl.SinglebandIngest$.main(SinglebandIngest.scala:11)	at geotrellis.spark.etl.SinglebandIngest.main(SinglebandIngest.scala)	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)	at java.lang.reflect.Method.invoke(Method.java:606)	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:674)	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180)	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205)	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:120)	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)```",3/15/16,1
jasonbeverage,Is that fixed here?  https://github.com/geotrellis/geotrellis/commit/50cbbd3c1fd0fdbf4879c2b7a511f72ac09baa2a,3/15/16,1
echeipesh,"Not quite that, @pomadchin and I are tracking down that issue right now.",3/15/16,1
lossyrob,"We tracked it down (I_Ñém pretty sure), going to wait for this fix to make it into RC1 as well. Thanks for finding it @jasonbeverage ",3/15/16,1
jasonbeverage,Thanks for tracking it down :),3/15/16,1
jasonbeverage,"@lossyrob did you push some fix to the master for that issue?  I can try it out if you'd like, I didn't see anything new in the master.",3/15/16,1
lossyrob,"currently working on the fix, I_Ñéll ping you when it_Ñés up.",3/15/16,1
jasonbeverage,"Great, thanks.",3/15/16,1
lossyrob,"@jasonbeverage  this is the fix, if you want to test it in branch before it makes it_Ñés way to master https://github.com/geotrellis/geotrellis/pull/1392",3/15/16,1
jasonbeverage,Cool thanks Rob.  Will give it a try.,3/15/16,1
pomadchin,@lossyrob https://github.com/pomadchin/geotrellis/blob/fix/accumulo-ordering/accumulo/src/main/scala/geotrellis/spark/io/accumulo/AccumuloRDDWriter.scala#L36 should there be used `groupByKey(numPartitions = raster.partitions.length)` ?,3/15/16,1
pomadchin,actually this pr solves accumulo issue,3/15/16,1
jasonbeverage,"Ok @lossyrob, it's running the ingest now, will let you know if it works.",3/15/16,1
jasonbeverage,"Ingest appeared to work @lossyrob , thanks!",3/15/16,1
lossyrob,"there was actually a problem with the bounds, so your pyramid that succeeded might look very weird if you viewed it on a map",3/15/16,1
lossyrob,I just pushed up a fix for that,3/15/16,1
jasonbeverage,Ok I'll try to grab it too.,3/15/16,1
jasonbeverage,Is there an example of how to use the new ColorClassifer stuff?,3/15/16,1
jasonbeverage,"I'm having a heck of time getting it to work, I must be doing something silly.",3/15/16,1
jasonbeverage,Like it's ALWAYS just using the first value in the list and never picking up anything else.,3/15/16,1
jasonbeverage,The dataset is just a floating point dataset with values from -1.0 to 1.0.,3/15/16,1
jasonbeverage,"Sorry, -2.0 to 1.0",3/15/16,1
jasonbeverage,"I'm trying this:  ```val breaks = Array[Double](-1.0, 0.0, 1.0)val colors = Array[RGBA](RGBA(255,255,0,255), RGBA(255,255,255,255), RGBA(255,0,0,255))val colorClass = BlendingColorClassifier(breaks, colors)```And only the areas that are less than -1.0 are showing up, and they are all hot pink.",3/15/16,1
lossyrob,interesting. I had a similar experience recently,3/15/16,1
jasonbeverage,"I can move that -1.0 up closer to 0 and I get more pixels showing through, still hot pink.  Same thing happens with a StrictDoubleColorClassifier",3/15/16,1
lossyrob,@moradology do you know what_Ñés going on with that,3/15/16,1
moradology,taking a look,3/15/16,1
jasonbeverage,I think there is an overflow issues going on.  If you do toColorMap on that color classifier you get this:,3/16/16,1
jasonbeverage,"```res2: geotrellis.raster.render.ColorMap = DoubleColorMap(Map(-1.0 -> -65281, 0.0 -> -1, 1.0 -> -16776961),ColorMapOptions(LessThan,0,0,false))```",3/16/16,1
jwalgran,"If I ingest a raster into S3 and make a TMS pyramid, can a GeoTrellis 0.10.0 service reading tiles from that raster determine the maximum zoom level? Is it recorded in metadata somewhere?",3/16/16,1
moradology,@jasonbeverage I'm not sure. those integers seem to be meaningful (if not visually ideal because it is an RGB interpolation),3/16/16,1
moradology,"still investigating, but those integers seems reasonable, interpolating from 255,255,255 to 255,0,0 the middle result is 255,255,0",3/16/16,1
moradology,checking with `java.awt.Color`just to avoid bad assumptions,3/16/16,1
lossyrob,@jwalgran https://github.com/geotrellis/geotrellis/blob/master/s3/src/main/scala/geotrellis/spark/io/s3/S3AttributeStore.scala,3/16/16,1
lossyrob,"```scalaval (bucket: String, prefix: String) = ???val attributeStore = S3AttributeStore(bucket, prefix)val maxZoomLevel = attributeStore.layerIds.map(_.zoom).max```",3/16/16,1
jwalgran,Thanks.,3/16/16,1
lossyrob,"hasn_Ñét made it_Ñés way through the maven central indexing system yet, but `0.10.0-RC1` is released :tada: ",3/16/16,1
jwalgran,:clap: ,3/16/16,1
pomadchin,:tada: ,3/16/16,1
lokifacio,congrats guys,3/16/16,1
timothymschier,"Congrats Rob and team, good job :clap:",3/16/16,1
owcm,Congrats to you guys,3/16/16,1
lossyrob,"thanks everyone! and thanks for helping flush out bugs, it_Ñés extremely helpful to get extra eyes on this before it goes into the wild",3/16/16,1
pomadchin,"@moradology @lossyrob @jamesmcclain @echeipesh  https://github.com/pomadchin/geotrellis-chatta-demo/tree/spark-version https://github.com/pomadchin/geodocker-cluster/tree/feature/gt-chatta/install/geotrellisProbably there should be more important information in README_Ñés but I refocused on integr-tests to rebase it to use RC1; any comments are appreciated ",3/16/16,1
jasonbeverage,Congrats on getting the 0.10 out!,3/16/16,1
jasonbeverage,"@moradology I'm wrong about an overflow error, I mistook the negative numbers as ranges and not colors.",3/16/16,1
jasonbeverage,but either I'm not getting how this new color classifier stuff works (probable) or something isn't right.,3/16/16,1
jasonbeverage,Just taking something simple I would expect to work (basically from https://github.com/geotrellis/geotrellis/blob/master/docs/raster/rendering.md),3/16/16,1
moradology,Something is definitely not right,3/16/16,1
moradology,"and truly, thank you for bringing it to light",3/16/16,1
jasonbeverage,"```val ndviColorBreaks = new StrictDoubleColorClassifier  // Every value below 100 should be red  ndviColorBreaks.classify(100.0, RGBA(255, 0, 0, 255))// render the tile tile.renderPng(ndviColorBreaks).bytes```",3/16/16,1
jasonbeverage,And the no data areas are blue and I can't see any data.  ,3/16/16,1
jasonbeverage,"Ok, thanks again for looking into it, I'm just making sure I'm not misunderstanding something or abusing code :)",3/16/16,1
pomadchin,"@jasonbeverage before your comment there also thought that probbaly my problems, and spend some time debugging, and then you throwed this bug here ",3/16/16,1
lossyrob,"yeah it_Ñés clearly a bug on our side, we_Ñére working on the render stuff to find it, fix it, and sure the whole package up",3/16/16,1
jasonbeverage,"What it looks like to me is that it's checking the first break and color and in the map, and only accepting pixels that pass that test, skipping the other breaks.",3/16/16,1
jasonbeverage,"Ok great, I'll quiet down and let you all look at it :)  If I find a solution I'll let you know.",3/16/16,1
lossyrob,hey the more eyes and ideas the better,3/16/16,1
lossyrob,esp. if you can find a unit-test-izable instance of the issue,3/16/16,1
jasonbeverage,"Well, just a guess but perhaps this block is the issue?  https://github.com/geotrellis/geotrellis/blob/master/raster/src/main/scala/geotrellis/raster/render/PngRenderMethods.scala#L55",3/16/16,1
jasonbeverage,"My data is floating point data, and it looks like that might fall back to the else statement that converts it to ByteCellType.",3/16/16,1
lossyrob,"we did add floating point tile rendering, and it looks like this wasn_Ñét updated",3/16/16,1
lossyrob,def looks wrong,3/16/16,1
jasonbeverage,jpeg render methods are the same.,3/16/16,1
owcm,"Quick question I think,  if I want to find the max cell value in a RDD of rasters is there a existing op method I can use or do I interate over all cells.",3/16/16,1
pomadchin,"@owcm mb smth like```scala(rdd: TileLayerRDD[K]).map { case (k, v) => v.findMinMax._2 }.max()```",3/16/16,1
owcm,@pomadchin thankyou,3/16/16,1
dwins,looks like spark-core should be in provided scope here? https://github.com/geotrellis/geotrellis-sbt-template/blob/master/build.sbt#L25,3/16/16,1
dwins,@lossyrob does that make sense?,3/16/16,1
lossyrob,yeah,3/16/16,1
lossyrob,so is the bloat just coming from the template codebase?,3/16/16,1
dwins,cuts the assembly size pretty much in half,3/16/16,1
lossyrob,nice,3/16/16,1
echeipesh,"Yes, if you intend to use the program with _ÑÉspark-submit'",3/16/16,1
dwins,` 68M    target/scala-2.10/geotrellis-ingest-test-assembly-0.1.0.jar`,3/16/16,1
Prasannads,"I am trying to preserve the metadata through raster operations like below,val rddWithContext: RDD[(SpatialKey, Tile)] with Metadata[TileLayerMetadata[SpatialKey]] = raster1.withContext { _ localAdd raster2 }and am writing the resultant raster back  to HDFS  like this   val writer = HadoopLayerWriter(path)(sc)  writer.write(layerid, rddWithContext, keyindex)but am getting below error in the writer.write()type mismatch; found : Unit required: geotrellis.spark.TileLayerRDD[geotrellis.spark.SpatialKey] (which expands to) org.apache.spark.rdd.RDD[(geotrellis.spark.SpatialKey, geotrellis.raster.Tile)] with geotrellis.spark.Metadata[geotrellis.spark.TileLayerMetadata[geotrellis.spark.SpatialKey]]",3/17/16,1
lossyrob,somehow rddWithContext is taking on Unit,3/17/16,1
lossyrob,if that_Ñés all the code though that can_Ñét be right,3/17/16,1
lossyrob,can you paste in the code with more context?,3/17/16,1
Prasannads,"```scalaclass keyindeximpl extends KeyIndexMethod[SpatialKey] {  override def createIndex(paramKeyBounds: KeyBounds[SpatialKey]) : KeyIndex[SpatialKey]}object RasterOperations {   def readHdfsTiles(path: String): TileLayerRDD[SpatialKey] = {    implicit val sc = SparkUtils.createSparkContext(""Ingest"")    //layer name for reading the RasterRDD    val nlcd1 = new String(""nlcd1"")    val nlcd2 = new String(""nlcd2"")    val reader1 = HadoopLayerReader(path)(sc)    val raster1 = reader1.read[SpatialKey, Tile, TileLayerMetadata[SpatialKey]](LayerId(nlcd1, 1))    val raster2 = reader1.read[SpatialKey, Tile, TileLayerMetadata[SpatialKey]](LayerId(nlcd2, 1))   // Add RasterRdd with another RasterRDD  val rddWithContext: RDD[(SpatialKey, Tile)] with Metadata[TileLayerMetadata[SpatialKey]] = raster1.withContext { _ localAdd raster2 }      val layerid = new LayerId(""newnlcd"",1)  val spatialkey = new SpatialKey(256, 256)   val keybounds = new KeyBounds(SpatialKey(1,1), SpatialKey(3,2))  val keyindex = new keyindeximpl().createIndex(keybounds)    //Writing the resultant RDD to HDFS  val writer = HadoopLayerWriter(path)(sc)  writer.write(layerid, rddWithContext, keyindex)}}```",3/17/16,1
Prasannads,i have pasted my full code,3/17/16,1
lossyrob,"if you surround things in three back ticks, it formats the code",3/17/16,1
pomadchin,@Prasannads you can use '\```scala' to fromat code,3/17/16,1
Prasannads,ok. let me ,3/17/16,1
lossyrob,the backticks need to be on their own lines,3/17/16,1
lossyrob,"and in the first set, it should look like ```scala",3/17/16,1
lossyrob,nice,3/17/16,1
lossyrob,"`keyindeximpl` has an implimentation of `createIndex`, it_Ñés not abstract right?",3/17/16,1
Prasannads,its an abstract .,3/17/16,1
Prasannads,"```scalapublic abstract interface KeyIndexMethod<K>  extends Serializable{  public abstract int resolution(double paramDouble1, double paramDouble2);    public abstract KeyIndex<K> createIndex(KeyBounds<K> paramKeyBounds);}```",3/17/16,1
pomadchin,"@Prasannads 1. `writer.write` is `(Id, RDD[(K, V)] with Metadata[M], KeyIndex[K]) => Unit` function",3/17/16,1
lossyrob,"what about if you put in `ZCurveKeyIndexMethod` in the write call, like",3/17/16,1
lossyrob,"`writer.write(layerid, rddWithContext, ZCurveKeyIndexMethod`",3/17/16,1
lossyrob,oh oh right,3/17/16,1
lossyrob,this is a Scala/Java thing,3/17/16,1
lossyrob,so in scala,3/17/16,1
pomadchin,readHdfsTiles should be `String => Unit` ,3/17/16,1
lossyrob,the last line in a statement body is the return value,3/17/16,1
lossyrob,"so a block of code evaluates, and whatever the last line in the block evaluates to, that_Ñés what you_Ñére returning",3/17/16,1
lossyrob,"in this case, `writer.write` is returning Unit",3/17/16,1
lossyrob,but you_Ñéve marked the return type of the method as `TileLayerRDD[SpatialKey]`,3/17/16,1
pomadchin,"yes, that_Ñés what compiler error about ",3/17/16,1
lossyrob,"```scalaclass keyindeximpl extends KeyIndexMethod[SpatialKey] {  override def createIndex(paramKeyBounds: KeyBounds[SpatialKey]) : KeyIndex[SpatialKey]}object RasterOperations {   def readHdfsTiles(path: String): TileLayerRDD[SpatialKey] = {    implicit val sc = SparkUtils.createSparkContext(""Ingest"")    //layer name for reading the RasterRDD    val nlcd1 = new String(""nlcd1"")    val nlcd2 = new String(""nlcd2"")    val reader1 = HadoopLayerReader(path)(sc)    val raster1 = reader1.read[SpatialKey, Tile, TileLayerMetadata[SpatialKey]](LayerId(nlcd1, 1))    val raster2 = reader1.read[SpatialKey, Tile, TileLayerMetadata[SpatialKey]](LayerId(nlcd2, 1))   // Add RasterRdd with another RasterRDD  val rddWithContext: RDD[(SpatialKey, Tile)] with Metadata[TileLayerMetadata[SpatialKey]] = raster1.withContext { _ localAdd raster2 }      val layerid = new LayerId(""newnlcd"",1)  val spatialkey = new SpatialKey(256, 256)   val keybounds = new KeyBounds(SpatialKey(1,1), SpatialKey(3,2))  val keyindex = new keyindeximpl().createIndex(keybounds)    //Writing the resultant RDD to HDFS  val writer = HadoopLayerWriter(path)(sc)  writer.write(layerid, rddWithContext, keyindex)  rddWithContext}}```",3/17/16,1
lossyrob,should compile,3/17/16,1
lossyrob,good call @pomadchin ,3/17/16,1
pomadchin,:D ,3/17/16,1
Prasannads,@lossyrob  yes .. i got it .. let me try the one you have sent,3/17/16,1
Prasannads,@pomadchin  thanks !,3/17/16,1
Prasannads,"@lossyrob  , @pomadchin  any documentation on how to get a tiff out of this from hdfs?",3/17/16,1
lossyrob,"do you want a single geotiff? you would have to read the layer out, call `.stitch` to convert it into a Raster, and then use that to create a `SinglebandGeoTiff`, and then write it out",3/17/16,1
lossyrob,I can point you to all the relevant code,3/17/16,1
lossyrob,https://github.com/geotrellis/geotrellis/blob/master/spark/src/main/scala/geotrellis/spark/io/hadoop/HadoopLayerReader.scala#L64,3/17/16,1
lossyrob,which you could read a full layer out with this method https://github.com/geotrellis/geotrellis/blob/master/spark/src/main/scala/geotrellis/spark/io/LayerReader.scala#L19,3/17/16,1
lossyrob,or query usgin this method https://github.com/geotrellis/geotrellis/blob/master/spark/src/main/scala/geotrellis/spark/io/FilteringLayerReader.scala#L56,3/17/16,1
Prasannads,@lossyrob  thanks !. i want to get a tiff out of this rddWithContext,3/17/16,1
Prasannads,but rddWithContext is already is a raster right?,3/17/16,1
lossyrob,oh ok,3/17/16,1
lossyrob,yeah,3/17/16,1
lossyrob,"so you_Ñév written it to HDFS, but now you just want to get a GeoTiff out of the same RDD",3/17/16,1
lossyrob,"```scalaval raster = rddWithContext.stitchGeoTiff(raster, rddWithContext.metadata.crs).write(_ÑÒ/path/to/write.tiff_Ñù)```",3/17/16,1
Prasannads,yes that is exactly  what i want to do,3/17/16,1
Prasannads,@lossyrob  thanks for the snippet :),3/17/16,1
lossyrob,"stitch pulls together all the raster data onto the driver, so if it_Ñés very large you wouldn_Ñét want to do that, but if it will all fit into memory that_Ñés fine",3/17/16,1
Prasannads,"if am doing that way , i dont have to do write the raster using HadoopLayerWriter, right",3/17/16,1
Prasannads,?,3/17/16,1
lossyrob,yeah you don_Ñét need to save it off into a backend,3/17/16,1
lossyrob,"we do that if we want to not have to tile it again, and so we can run spatial or spatiotemporal queries against teh data",3/17/16,1
lossyrob,useful for large datasets,3/17/16,1
Prasannads,ok. that is great. ,3/17/16,1
Prasannads,the write path can be a local path ?,3/17/16,1
lossyrob,yeah actually has to be,3/17/16,1
lossyrob,the geotiff writer won_Ñét write to HDFS,3/17/16,1
Prasannads,ok! the geotiff writer class is in the  geotrellis-spark?,3/17/16,1
lossyrob,"no, it_Ñés in geotrellis-raster (which is a transitive dependency of geotrellis-spark",3/17/16,1
pomadchin,`geotrellis.raster.io.geotiff.writer`,3/17/16,1
lossyrob,`import geotrellis.raster.io.geotiff._`,3/17/16,1
lossyrob,don_Ñét need the writer package,3/17/16,1
Prasannads,ok . thanks !,3/17/16,1
lossyrob,just `GeoTiff`,3/17/16,1
lossyrob,which has a write method off of it,3/17/16,1
pomadchin,@lossyrob still no classifier fixes in #1404 ?,3/17/16,1
Prasannads,"I am trying to load tiffs in to HDFS using the following wayhttps://github.com/geotrellis/geotrellis/blob/master/docs/spark-etl/spark-etl-intro.mdRun Command :spark-submit \--class TiffIngest.TiffIngestHadoop.Ingest \--master local \--driver-memory 2G \/home/brillio/jars/TiffIngestHadoop-0.0.1-SNAPSHOT-jar-with-dependencies.jar \--input file:/home/brillio/geotiff/e_tile.tif --format geotiff \--output hadoop -O path=""wasb://sparkbrillio@geospark.blob.core.windows.net/user/8020/user/geotiff1"" \--layer nlcd --crs EPSG:3857 --pyramid --layoutScheme tmsThis is the error am getting```scalaException in thread ""main"" java.lang.RuntimeException: Unable to find input module of type 'file:/home/brillio/geotiff/e_tile.tif' for format `tiff        at scala.sys.package$.error(package.scala:27)        at geotrellis.spark.etl.Etl$$anonfun$3.apply(Etl.scala:84)        at geotrellis.spark.etl.Etl$$anonfun$3.apply(Etl.scala:84)        at scala.Option.getOrElse(Option.scala:120)        at geotrellis.spark.etl.Etl.load(Etl.scala:84)        at TiffIngest.TiffIngestHadoop.Ingest$delayedInit$body.apply(Ingest.scala:20)        at scala.Function0$class.apply$mcV$sp(Function0.scala:40)        at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:12)        at scala.App$$anonfun$main$1.apply(App.scala:71)        at scala.App$$anonfun$main$1.apply(App.scala:71)        at scala.collection.immutable.List.foreach(List.scala:318)        at scala.collection.generic.TraversableForwarder$class.foreach(TraversableForwarder.scala:32)        at scala.App$class.main(App.scala:71)        at TiffIngest.TiffIngestHadoop.Ingest$.main(Ingest.scala:15)        at TiffIngest.TiffIngestHadoop.Ingest.main(Ingest.scala)        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)        at java.lang.reflect.Method.invoke(Method.java:606)        at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:685)        at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180)        at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205)        at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:120)        at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)```",3/17/16,1
pomadchin,"```spark-submit \--class TiffIngest.TiffIngestHadoop.Ingest \--master local \--driver-memory 2G \/home/brillio/jars/TiffIngestHadoop-0.0.1-SNAPSHOT-jar-with-dependencies.jar \--input hadoop --format geotiff --cache NONE -I path=file:/home/brillio/geotiff/e_tile.tif \--output hadoop -O path=""wasb://sparkbrillio@geospark.blob.core.windows.net/user/8020/user/geotiff1"" \--layer nlcd --crs EPSG:3857 --pyramid --layoutScheme tms```",3/17/16,1
Prasannads,@pomadchin  -  i am bit confused. my input tiff is in local path . but why hadoop?,3/17/16,1
pomadchin,,3/17/16,1
pomadchin,@Prasannads same overload :),3/17/16,1
pomadchin,it is a smart adapter to read from hdfs/local ,3/17/16,1
pomadchin,"aditional usage example for out chatta demo project: https://github.com/pomadchin/geotrellis-chatta-demo/blob/spark-version/geotrellis/ingest.sh",3/17/16,1
Prasannads,ah .. ok :) i thought i could remove because my file is in the local path :),3/17/16,1
Prasannads,"```scalajava.lang.AssertionError: assertion failed: Polygon Empty: POLYGON EMPTY        at scala.Predef$.assert(Predef.scala:179)        at geotrellis.vector.Polygon.<init>(Polygon.scala:83)        at geotrellis.vector.Polygon$.jtsToPolygon(Polygon.scala:28)        at geotrellis.vector.Polygon$.apply(Polygon.scala:71)        at geotrellis.vector.reproject.Reproject$.apply(Reproject.scala:42)        at geotrellis.vector.reproject.Implicits$ReprojectPolygon.reproject(Implicits.scala:37)        at geotrellis.raster.reproject.ReprojectRasterExtent$.reprojectExtent(ReprojectRasterExtent.scala:36)        at geotrellis.raster.reproject.ReprojectRasterExtent$.apply(ReprojectRasterExtent.scala:41)        at geotrellis.raster.reproject.ReprojectRasterExtent$.apply(ReprojectRasterExtent.scala:101)        at geotrellis.spark.reproject.TileRDDReproject$$anonfun$5$$anonfun$apply$2.apply(TileRDDReproject.scala:152)        at geotrellis.spark.reproject.TileRDDReproject$$anonfun$5$$anonfun$apply$2.apply(TileRDDReproject.scala:149)        at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)        at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)        at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)        at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.insertAll(BypassMergeSortShuffleWriter.java:99)        at org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:73)        at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:73)        at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)        at org.apache.spark.scheduler.Task.run(Task.scala:88)        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)        at java.lang.Thread.run(Thread.java:745)```",3/17/16,1
Prasannads,now i get this error. problem with the tiff file?,3/17/16,1
pomadchin,mm not sure,3/17/16,1
pomadchin,"what scala version in compiled app, what spark scala version, etc ?",3/17/16,1
pomadchin,"or probably bad geotiff tags, you are correct",3/17/16,1
Prasannads,scala version is 2.10.6 ,3/17/16,1
pomadchin,checkout geotiff tags ,3/17/16,1
Prasannads,ok .let me try with some other geotiffs,3/17/16,1
Prasannads,"@pomadchin  i am using the following maven dependencies for this spark-etl job . that should not be a problem , right?    <dependency>	  <groupId>com.azavea.geotrellis</groupId>	  <artifactId>geotrellis-spark_2.10</artifactId>	  <version>0.10.0-RC1</version>    </dependency>    <dependency>     <groupId>com.azavea.geotrellis</groupId>     <artifactId>geotrellis-spark-etl_2.10</artifactId>     <version>0.10.0-RC1</version>    </dependency>",3/17/16,1
pomadchin,yes,3/17/16,1
pomadchin,other tiffs fail too?,3/17/16,1
pomadchin,can you throw me an example tiff?,3/17/16,1
Prasannads,"now i get different error ```scalajava.lang.ArrayIndexOutOfBoundsException        at java.lang.System.arraycopy(Native Method)        at geotrellis.raster.io.geotiff.UByteGeoTiffTile.mutable(UByteGeoTiffTile.scala:30)        at geotrellis.raster.io.geotiff.GeoTiffTile.toArrayTile(GeoTiffTile.scala:348)        at geotrellis.raster.io.geotiff.reader.GeoTiffReader$.readSingleband(GeoTiffReader.scala:90)        at geotrellis.raster.io.geotiff.reader.GeoTiffReader$.readSingleband(GeoTiffReader.scala:61)        at geotrellis.raster.io.geotiff.SinglebandGeoTiff$.apply(SinglebandGeoTiff.scala:40)        at geotrellis.spark.io.hadoop.formats.GeotiffInputFormat.read(GeotiffInputFormat.scala:28)        at geotrellis.spark.io.hadoop.formats.BinaryFileInputFormat$$anonfun$createRecordReader$1.apply(BinaryFileInputFormat.scala:34)        at geotrellis.spark.io.hadoop.formats.BinaryFileInputFormat$$anonfun$createRecordReader$1.apply(BinaryFileInputFormat.scala:34)        at geotrellis.spark.io.hadoop.formats.BinaryFileRecordReader.initialize(BinaryFileInputFormat.scala:18)        at org.apache.spark.rdd.NewHadoopRDD$$anon$1.<init>(NewHadoopRDD.scala:153)        at org.apache.spark.rdd.NewHadoopRDD.compute(NewHadoopRDD.scala:124)        at org.apache.spark.rdd.NewHadoopRDD.compute(NewHadoopRDD.scala:65)        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:300)        at org.apache.spark.rdd.RDD.iterator(RDD.scala:264)        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:300)        at org.apache.spark.rdd.RDD.iterator(RDD.scala:264)        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)        at org.apache.spark.scheduler.Task.run(Task.scala:88)        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)```",3/17/16,1
Prasannads,[![cea.tif](https://files.gitter.im/geotrellis/geotrellis/LEhy/thumb/cea.tiff)](https://files.gitter.im/geotrellis/geotrellis/LEhy/cea.tif),3/17/16,1
pomadchin,hm,3/17/16,1
pomadchin,ok will check it ,3/17/16,1
pomadchin,thanks!,3/17/16,1
Prasannads,that was the tiff i got ArrayIndexOutOfBoundsException error,3/17/16,1
Prasannads,ok!,3/17/16,1
rshirochenko,"Working to ingest some landsat data. Stacked with error, how to fix it? ```val ingestElement = (TemporalProjectedExtent(geoTiff.extent, geoTiff.crs, mtl.dateTime), geoTiff.tile)val sourceTile = sc.parallelize(Seq(ingestElement))val (_, tileLayerMetadata) = TileLayerMetadata.fromRdd(sourceTile, FloatingLayoutScheme(512))<console>:70: error: ambiguous implicit values: both method withProjectedExtentTemporalTilerKeyMethods in package spark of type [K](self: K)(implicit evidence$10: monocle.PLens[K,K,geotrellis.vector.ProjectedExtent,geotrellis.vector.ProjectedExtent], implicit evidence$11: monocle.PLens[K,K,geotrellis.spark.TemporalKey,geotrellis.spark.TemporalKey])geotrellis.spark.withProjectedExtentTemporalTilerKeyMethods[K] and method withProjectedExtentTilerKeyMethods in package spark of type [K](self: K)(implicit evidence$12: monocle.PLens[K,K,geotrellis.vector.ProjectedExtent,geotrellis.vector.ProjectedExtent])geotrellis.spark.withProjectedExtentTilerKeyMethods[K] match expected type geotrellis.spark.TemporalProjectedExtent => geotrellis.spark.tiling.TilerKeyMethods[geotrellis.spark.TemporalProjectedExtent,K2]       val (_, metadata) = TileLayerMetadata.fromRdd(sourceTile, FloatingLayoutScheme(512))```",3/17/16,1
pomadchin,"@Prasannads if it is not difficult, can you throw first tiff that caused empty polygon err?",3/17/16,1
Prasannads,ok ,3/17/16,1
Prasannads,[![e_tile.tif](https://files.gitter.im/geotrellis/geotrellis/ibxB/thumb/e_tile.tiff)](https://files.gitter.im/geotrellis/geotrellis/ibxB/e_tile.tif),3/17/16,1
pomadchin,cool,3/17/16,1
pomadchin,"@rshirochenko  what_Ñés the type of sourceTile? `RDD[(TemporalProjectedExtent, Tile)]` ?",3/17/16,1
rshirochenko,"@pomadchin  RDD[TemporalProjectedExtent, MultibandTile)]",3/17/16,1
Prasannads,"@pomadchin  i was able to load the tiff ""e_tile.tiff"" with the HadoopIngestCommand.scala",3/17/16,1
pomadchin,@rshirochenko you tried to rewrite with expected types explicitly?,3/17/16,1
pomadchin,"```scaladef ingestElement: (TemporalProjectedExtent, MultibandTile) = (TemporalProjectedExtent(geoTiff.extent, geoTiff.crs, dateTime), geoTiff.tile)def sourceTile: RDD[(TemporalProjectedExtent, MultibandTile)] = sc.parallelize(Seq(ingestElement))val (_, tileLayerMetadata: TileLayerMetadata[SpaceTimeKey]) = TileLayerMetadata.fromRdd[TemporalProjectedExtent, MultibandTile, SpaceTimeKey](sourceTile, FloatingLayoutScheme(512))```",3/17/16,1
pomadchin,"think there is some type mismatch, and compiler throws you incorrect implicit derivation",3/17/16,1
rshirochenko,"```<console>:70: error: ambiguous implicit values: both method withProjectedExtentTemporalTilerKeyMethods in package spark of type [K](self: K)(implicit evidence$10: monocle.PLens[K,K,geotrellis.vector.ProjectedExtent,geotrellis.vector.ProjectedExtent], implicit evidence$11: monocle.PLens[K,K,geotrellis.spark.TemporalKey,geotrellis.spark.TemporalKey])geotrellis.spark.withProjectedExtentTemporalTilerKeyMethods[K] and method withProjectedExtentTilerKeyMethods in package spark of type [K](self: K)(implicit evidence$12: monocle.PLens[K,K,geotrellis.vector.ProjectedExtent,geotrellis.vector.ProjectedExtent])geotrellis.spark.withProjectedExtentTilerKeyMethods[K] match expected type geotrellis.spark.TemporalProjectedExtent => geotrellis.spark.tiling.TilerKeyMethods[geotrellis.spark.TemporalProjectedExtent,K2]       lazy val (_, tileLayerMetadata: TileLayerMetadata[SpaceTimeKey]) = TileLayerMetadata.fromRdd(sourceTile, FloatingLayoutScheme(512))                                                                                                   ^<console>:70: warning: non-variable type argument geotrellis.spark.SpaceTimeKey in type pattern geotrellis.spark.TileLayerMetadata[geotrellis.spark.SpaceTimeKey] is unchecked since it is eliminated by erasure       lazy val (_, tileLayerMetadata: TileLayerMetadata[SpaceTimeKey]) = TileLayerMetadata.fromRdd(sourceTile, FloatingLayoutScheme(512))```",3/17/16,1
pomadchin,"and yes, note that you probably need to specify type params in a `fromRdd` function call",3/17/16,1
pomadchin,"```scalaval (_, tileLayerMetadata: TileLayerMetadata[SpaceTimeKey]) = TileLayerMetadata.fromRdd[TemporalProjectedExtent, MultibandTile, SpaceTimeKey](sourceTile, FloatingLayoutScheme(512))```",3/17/16,1
rshirochenko,"it works, thx",3/17/16,1
rshirochenko,What is new name for MultiBandRasterRDD?,3/17/16,1
pomadchin,`MultibandTileLayerRDD`,3/17/16,1
pomadchin,"`MultiBand => Multiband``RasterRDD => TileLayerRDD``RasterMetaData => TileLayerMetadata`",3/17/16,1
rshirochenko,,3/17/16,1
rshirochenko,"What I am doing wrong? ```val attributeStore = FileAttributeStore(catalogPath)val writer = FileLayerWriter(attributeStore)val keyIndex = ZCurveKeyIndexMethod.byMilliseconds(1000L * 60 * 60 * 24) writer.write(LayerId(layerName, zoom), rdd2, keyIndex)<console>:160: error: could not find implicit value for evidence parameter of type spray.json.JsonFormat[geotrellis.spark.SpaceTimeKey]                  writer.write(LayerId(layerName, zoom), rdd2, keyIndex)```",3/17/16,1
pomadchin,import geotrellis.spark.io._,3/17/16,1
Prasannads,"@pomadchin  i am trying spatial join ```scalaval rddWithContext3: RDD[(SpatialKey, (Tile, Tile))] with Metadata[Bounds[SpatialKey]] = raster1.spatialJoin(raster2).withContext { _.combineValues(Add(_, _)) }.mapContext{ bounds => updateLayout(raster1.metadata, bounds) }```",3/17/16,1
rshirochenko,thx,3/17/16,1
Prasannads,"I am getting error ""not found: value Add""",3/17/16,1
pomadchin,to be sure that all readers / writers have necessary implicits just import spark._ and spark.io._,3/17/16,1
pomadchin,@Prasannads http://geotrellis.github.io/scaladocs/latest/#geotrellis.package,3/17/16,1
pomadchin,sry I'm from mobile ,3/17/16,1
Prasannads,@pomadchin  no problem :),3/17/16,1
owcm,"I have my sbt project pointing to the bintrayrepo, to get the 0.10.-RC1 should I be using 0.10.0-6220609",3/17/16,1
owcm,in azavea / geotrellis,3/17/16,1
dwins,0.10.0-RC1 will get it,3/17/16,1
lokifacio,what's the best way to apply a focal only to NoData cells?,3/17/16,1
owcm,"@dwins  Thanks. That worked for geotrellis-engine, geotrellis-raster and geotrellis-spark packages.   Unresolved path for services and geotools.  Are those packages deprecated or just need to use other versions?",3/17/16,1
pomadchin,@owcm these packages are not in rc1; deprecated and removed,3/17/16,1
pomadchin,also geotrellis engine is deprecated,3/17/16,1
owcm,@dwins @pomadchin thank you,3/17/16,1
pomadchin,"@owcm oh, \+ geotools renamed to shapefile",3/17/16,1
dwins,"well, geotrellis-geotools was effectively renamed to geotrellis-shapefile right?",3/17/16,1
dwins,too slow :'(,3/17/16,1
pomadchin,@dwins :d ,3/17/16,1
owcm,That worked,3/17/16,1
rshirochenko,"Trying to ingest landsat pyramid images to local folder (using FileLayerWriter).  Code works fine, but the final size of 2 full-band Landsat ingested images is 8mb only. My  code part for ingesting is here https://github.com/rshirochenko/geotrellis-learn/blob/master/src/main/scala/geotrellis/testim/Main.scala. Any suggestions what it could be?",3/17/16,1
owcm,"Trying to just create a new Tif from a ProjectRaster.  Old code:```      val ProjectedRaster( lcProjTile, lcProjExtent, lcProjCRS) = lcProjectedRaster.reproject(WebMercator)       // Create Geotiff with new projection        lcReprojGeoTiff = SinglebandGeoTiff( lcProjTile, lcProjExtent, lcProjCRS )```Importing geotrellis.raster.io.geotiff  but with RC1 it complains about invalid signature.  Looks like ``` object SinglebandGeoTiff {  def apply(    tile: Tile,    extent: Extent,    crs: CRS  ): SinglebandGeoTiff =    new SinglebandGeoTiff(tile, extent, crs, Tags.empty, GeoTiffOptions.DEFAULT)```Should support what I want to do.",3/17/16,1
dwins,@owcm what is the compile error exactly?,3/17/16,1
pomadchin,"@owcm ```scalaval ProjectedRaster(Raster(tile, extent), crs) = lcProjectedRaster.reproject(WebMercator)```",3/17/16,1
owcm,"@pomadchin @dwins Okay if I rewrite as ```                val ProjectedRaster(Raster(lcProjTile, lcProjExtent), crs) = lcProjectedRaster.reproject(WebMercator)                // Create Geotiff with new projection                lcReprojGeoTiff = SinglebandGeoTiff( lcProjTile, lcProjExtent, crs )```It compiles.  Thanks again",3/17/16,1
owcm,"Just trying to write out a png of a tile:``` val intBasedClassifier = StrictColorClassifier(Array[(Double, RGBA)]((1.0,                            RGBA(0x00000000)), (2.0, RGBA(0x00FF00FF))))                  var png = lclTile.renderPng( intBasedClassifier )                  png.write( ""hlzout.png"")```As I understand  StrictColorClassifier everything below 1.0 should get the first color and anything below 2.0 should get the second color.   I know my tile has valid values but nothing shows up in the PNG file.Also should ColorRamps not be used at all in the new release.",3/17/16,1
lossyrob,"@owcm geotools has moved to `geotrellis-shapefile` (since that_Ñés all it really was, a shapefile reader)",3/17/16,1
lossyrob,oh I see that_Ñés an answered question_Ñ_sorry trying to catch up,3/17/16,1
lossyrob,we have an Azavea annual day-long meeting so that_Ñés why all the azavea peeps are silent today :),3/17/16,1
owcm,Understand that,3/17/16,1
lossyrob,@owcm rendering is currently broken. I have a PR https://github.com/geotrellis/geotrellis/pull/1404 that should fix it. Hopefully we can get that into a next RC tomorrow.,3/17/16,1
owcm,"@lossyrob okay, thought I was going crazy.  ",3/17/16,1
lossyrob,sorry about that. you are definitely not crazy.,3/17/16,1
lossyrob,it_Ñés unfortunately a bug that slipped through,3/17/16,1
owcm,"Doing the upgrade to  0.10.0-RC1,  my existing code relied on the spark packages RasterMetaData and RasterRDD objects/classes in the new environment what have they been replaced with?",3/17/16,1
owcm,Maybe those constructs where all tied to the old version of the Tiler I was using.,3/17/16,1
pomadchin,"Depends on #1375 - [x] MultiBand -> Multiband- [x] SingleBand -> Singleband- [x] MetaData -> Metadata- [x] multiBand -> multiband- [x] singleBand -> singleband- [x] metaData -> metadata- [x] RasterRDD[K] -> TileLayerRDD[K]- [x] MultiBandRDD[K] -> MultibandTileLayerRDD[K]- [x] RasterMetadata -> TileLayerMetadata",3/17/16,1
owcm,@pomadchin thanks for pointing in the right direction.,3/17/16,1
pomadchin,We have no Tiler anymore; ,3/17/16,1
pomadchin,Now it's a TilerMethods ,3/17/16,1
pomadchin,typeclass*,3/17/16,1
Prasannads,"i am trying to do an spatial join as mentioned in the https://github.com/geotrellis/geotrellis/blob/master/docs/spark/map-algebra.md . can i have a code snippet of ""updateLayout""  that how it is has to be implemented?",3/18/16,1
Prasannads,can i merge multiple tiff image into one tiff image ?,3/18/16,1
pomadchin,you mean merge multiband tiff bands or different tiffs ? Yes definitely you can merge them ,3/18/16,1
Prasannads,@pomadchin  different tiffs,3/18/16,1
pomadchin,there is a combine operation for Tile type ,3/18/16,1
Prasannads,TileRDDMerge ?,3/18/16,1
pomadchin,sec not near laptop; 5 min,3/18/16,1
Prasannads,sure :),3/18/16,1
pomadchin,"@Prasannads ```scalaval t0: GeoTiff[Tile] = _val t1: GeoTiff[Tile] = _t0.tile.combine(t1.tile){ case (i1, i2) => i1 + i2 }```",3/18/16,1
Prasannads,@pomadchin  thanks !. i have only Raster[Tile] after some operations . ,3/18/16,1
pomadchin,@Prasannads then `combine` operation also defined (but via implicit conversion to `Tile`) for `Raster[Tile]` type,3/18/16,1
Prasannads,@pomadchin  ok thanks!,3/18/16,1
Prasannads,"@pomadchin  i have loaded the tiffs using the following run command ```scalaspark-submit \--class TiffIngest.TiffIngestHadoop.Ingest \--master local[*] \--driver-memory 2G \/home/brillio/jars/TiffIngestHadoop-0.0.1-SNAPSHOT-jar-with-dependencies.jar \--input hadoop --format geotiff --cache NONE -I path=""file:///home/brillio/tiffs/SJER2013_DTMHill.tif"" \--output hadoop -O path=""wasb://sparkbrillio@geospark.blob.core.windows.net/user/8020/user/geotrellis-spark"" \--layer nlcd1 --crs EPSG:3857 --pyramid --layoutScheme tms```When i do localAdd between two raster , the resultant tiffs are always plain black tiffs . am i doing something wrong ?",3/18/16,1
pomadchin,can you show code example?,3/18/16,1
pomadchin,sounds like overload / smth else,3/18/16,1
pomadchin,also have you tried to write saved tiles to fs to look how tiffs were ingested?,3/18/16,1
Prasannads,"```scala val raster1 = reader.read[SpatialKey, Tile, TileLayerMetadata[SpatialKey]](LayerId(nlcd1, 1))    val raster2 = reader.read[SpatialKey, Tile, TileLayerMetadata[SpatialKey]](LayerId(nlcd2, 1))   // Add 1 to each cell on RasterRdd   val rddWithContext: RDD[(SpatialKey, Tile)] with Metadata[TileLayerMetadata[SpatialKey]] = raster1.withContext { _ localAdd 1 }```",3/18/16,1
Prasannads,here i didnt try to write the rddWithContext  ..,3/18/16,1
lokifacio,"I'm getting corrupted tiff after chaining two focal operations as follows:```scalavar tiles: TileLayerRDD[SpatialKey] = ???val focal = tiles.focalMean(Circle(5))write(focal, path1)val focal2 = focal.focalMean(Circle(5))write(focal2, path2)def write(tiles: TileLayerRDD[SpatialKey], path: String): Unit = {    GeoTiff(tiles.stitch, tiles.metadata.extent, tiles.metadata.crs).write(path)}```I open those tiff with QGis and the second one values are all interpreted as na. When I try to open its properties I get a ""bad allocation"" error.",3/18/16,1
Prasannads,i am writing the tiff out of the rddWithContext directly.,3/18/16,1
lokifacio,could be related to @Prasannads  issue?,3/18/16,1
lokifacio,the second one is also black when rendered because of the lack of range information,3/18/16,1
Prasannads,@lokifacio  are you getting just plain black images ?,3/18/16,1
Prasannads,"i tried local operations such as union , intersection  .. resulting tiffs were same :(",3/18/16,1
lokifacio,"QGis fails to determine raster values, and thus it renders data cells black",3/18/16,1
pomadchin,@lokifacio @Prasannads you tried just to save tiffs before focal operations?,3/18/16,1
pomadchin,it is possible to reproduce this behaviour on any tiff?,3/18/16,1
Prasannads,@pomadchin  try to save the tiffs before operations to see if they are same as before ? am i right ?,3/18/16,1
pomadchin,yes,3/18/16,1
Prasannads,ok. let me try and let you know,3/18/16,1
pomadchin,"to see where everythong goes wrong, you know",3/18/16,1
Prasannads,ok!,3/18/16,1
lokifacio,"I did it in other examples, I'm re-running now writing before as well.",3/18/16,1
pomadchin,"flow:* ingestet tiles (correct or not)* and to dump tiles after each operations",3/18/16,1
pomadchin,sry for that :D) will try a bit debugging,3/18/16,1
pomadchin,also remember about issues with certain tiles ingesting ,3/18/16,1
lokifacio,I've done plenty of tests with ingest/write,3/18/16,1
lokifacio,and it seems fine for me,3/18/16,1
pomadchin,"hm, so you are sure about focal / local operations?",3/18/16,1
lokifacio,"moreover, I can do one focal and its result seems correct",3/18/16,1
lokifacio,the problem is when I try two chain two focals...,3/18/16,1
lokifacio,I've done following test:,3/18/16,1
Prasannads,@lokifacio  were you able to see the tiffs in the windows image viewer?,3/18/16,1
Prasannads,i means the tiffs which are created after focal operations,3/18/16,1
lokifacio,read ingested tiles -> write (ok) -> focal  -> write(ok) -> focal -> write(ko - cells seem right but fails to get value ranges => thus black) ->  focal -> write(ko - raster visualization is clearly corrupted),3/18/16,1
lokifacio,@Prasannads I use QGis,3/18/16,1
Prasannads,ok.,3/18/16,1
lokifacio,"every focal is applied to the previous one, of course",3/18/16,1
pomadchin,hm you tried too view in qgis your tiff after each focal operation?,3/18/16,1
pomadchin,i am cuirious how focal operations effect data,3/18/16,1
Prasannads,@pomadchin  i tried saving the tiles before i do any operation on it . i can see only black image. should i use any tiff viewer like QGis? i use windows photo viewer . i dont think that is a problem. because i am able to see other tiffs downloaded from the internet,3/18/16,1
pomadchin,"yes, try QGis",3/18/16,1
pomadchin,but yes,3/18/16,1
pomadchin,that_Ñés a bit weird,3/18/16,1
pomadchin,can you give me srcs?,3/18/16,1
pomadchin,or I already have them,3/18/16,1
pomadchin,that previous images you posted there withh ingest errs,3/18/16,1
lokifacio,"@pomadchin I've tried with several rasters, it seems it doesn't depend on the source",3/18/16,1
pomadchin,ok,3/18/16,1
lokifacio,"the focal seems to work fine, btw",3/18/16,1
lokifacio,"at least, the first time ;)",3/18/16,1
pomadchin,"so what we have now: * write / read seem to be correct* one focal operation seems ok* second focal apply etc seems not ok",3/18/16,1
lokifacio,if you want I'll try other ops (add or double) to check if I get the same result as @Prasannads ,3/18/16,1
lokifacio,@pomadchin this is even more intriguing for me... I've read raster resulting from first focal which seems right in QGIS and applying a focal to it gives the same results as chaining the two focals from the source raster... Do you know if there is some kind of tiff validator out there?,3/18/16,1
lokifacio,"I've just checked raster information using QGIS and I found a possible clue: original raster has   `NoDataValue=-1.6999999999999999e+308` whereas focal output has `NoData Value=nan`, could it be the cause of the problem?",3/18/16,1
lokifacio,the rest of metadata are exactly the same,3/18/16,1
lokifacio,"@Prasannads localAdd works fine for me, even chaining several operations",3/18/16,1
Prasannads,[![UTM2GTIF.TIF](https://files.gitter.im/geotrellis/geotrellis/Ync4/thumb/UTM2GTIF.tiff)](https://files.gitter.im/geotrellis/geotrellis/Ync4/UTM2GTIF.TIF),3/18/16,1
Prasannads,[![SP27GTIF.TIF](https://files.gitter.im/geotrellis/geotrellis/Ync4/thumb/SP27GTIF.tiff)](https://files.gitter.im/geotrellis/geotrellis/Ync4/SP27GTIF.TIF),3/18/16,1
Prasannads,@pomadchin  these are the files am using now .,3/18/16,1
Prasannads,@lokifacio  ok. let me try the qgis ,3/18/16,1
pomadchin,kk guyz,3/18/16,1
pomadchin,will start with debugging in a couple of minutes! thanks for your help!,3/18/16,1
pomadchin,@Prasannads can you throw here what exact operations on tiles you have? (i mean code sample with localAdd usage),3/18/16,1
Prasannads,"```scala  val rddWithContext: RDD[(SpatialKey, Tile)] with Metadata[TileLayerMetadata[SpatialKey]] = raster1.withContext { _ localAdd 1 }  val rddWithContext3: RDD[(SpatialKey, Tile)] with Metadata[TileLayerMetadata[SpatialKey]] = raster2.withContext { _ localDivide 2 }  //val rddWithContext4: RDD[(SpatialKey, Tile)] with Metadata[TileLayerMetadata[SpatialKey]] = TileRDDMerge.apply(raster1, raster2)  // Union  val rddWithContext1: RDD[(SpatialKey, Tile)] with Metadata[TileLayerMetadata[SpatialKey]] = raster1.withContext { _ union(raster2) }    //Intersection  val rddWithContext2: RDD[(SpatialKey, Tile)] with Metadata[TileLayerMetadata[SpatialKey]] = raster1.withContext { _ intersection(raster2) }```",3/18/16,1
Prasannads,"reading the tiles```scala val raster1 = reader.read[SpatialKey, Tile, TileLayerMetadata[SpatialKey]](LayerId(nlcd1, 1)) val raster2 = reader.read[SpatialKey, Tile, TileLayerMetadata[SpatialKey]](LayerId(nlcd2, 1))```",3/18/16,1
Prasannads,@pomadchin  thanks :),3/18/16,1
Prasannads,@lokifacio  if possible can you please share the tiffs you tried ?,3/18/16,1
lokifacio,"not right now :( I'm at office and uploading service is blocked, but it was just a generic raster. I will try yours if you want",3/18/16,1
Prasannads,@lokifacio  ok.. thanks !,3/18/16,1
Prasannads,@pomadchin  no luck with QGIS .. i see the same black images,3/18/16,1
lokifacio,"@Prasannads I also get black tiff with your input, even if I try to just write it once it is read",3/18/16,1
lokifacio,I see your input has byte data,3/18/16,1
lokifacio,maybe the issue can be there?,3/18/16,1
pomadchin,@lokifacio good to know that you also tried,3/18/16,1
lokifacio,"@pomadchin I just run few test, first I run the localAdd sequence I used before with my rasters, then I tried writing after reading and finally I tried reprojecting before writing same result for all of them, QGIS range: nan-0",3/18/16,1
lokifacio,"sorry, the range for the focal result was incremented in the added amount: 100, 200, 300 after three chained operations...so  I guess the problem is just raster reading",3/18/16,1
Prasannads,@lokifacio  thanks ! any link can you point me out to get some proper tiffs.,3/18/16,1
lokifacio,@Prasannads I'm testing now with this [raster](http://sedac.ciesin.columbia.edu/downloads/data/aglands/aglands-croplands-2000/eu-croplands-geotif.zip). ,3/18/16,1
lokifacio,@pomadchin I get the same wrong results for the focal ops and the expected results for the localAdd ops,3/18/16,1
Prasannads,@lokifacio  thanks . let me try it .,3/18/16,1
Prasannads,"@lokifacio  @pomadchin  i am getting this error when loading the tiff into hdfs  with the data @lokifacio  pointed me .",3/18/16,1
Prasannads,"```scalajava.lang.RuntimeException: Cannot create a polygon with unclosed exterior: LINESTRING (10833213.874946382 «è_, 10833213.874946382 «è_, 10833213.874946382 «è_, 10833213.874946382 «è_, 10833213.874946382 «è_, 10833213.874946382 «è_, 10833213.874946382 «è_, 10833213.874946382 «è_, 10833213.874946382 «è_, 10833213.874946382 «è_, 10833213.874946382 «è_, 10833213.874946382 «è_, 10833213.874946382 «è_, 10833213.874946382 «è_, 10833213.874946382 «è_, 10833213.874946382 «è_, 10833213.874946382 «è_, 10833213.874946382 «è_, 10833213.874946382 «è_, 10833213.874946382 «è_, 10833213.874946382 «è_, 10833213.874946382 «è_, 10833213.874946382 «è_, 10833213.874946382 «è_, 10833213.874946382 «è_, 10833213.874946382 «è_, 10833213.874946382 «è_, 10833213.874946382 «è_, 10833213.874946382 «è_, 10833213.874946382 -43219151.55746001, 10833213.874946382 -34065939.8179188, 10833213.874946382 -30453289.197224993, 10833213.874946382 -28160993.24109334, 10833213.874946382 -26478131.628126472, 10833213.874946382 -25147731.948600315, 10833213.874946382 -24047370.013657365, 10833213.874946382 -23109057.108369168, 10833213.874946382 -22291095.395947885, 10833213.874946382 -21566057.53880232, 10833213.874946382 -20914936.632186808, 10833213.874946382 -20324017.1147586, 10833213.874946382 -19783076.40009428, 10833213.874946382 -19284291.044194072, 10833213.874946382 -18821540.168177523, 10833213.874946382 -18389944.670305897, 10833213.874946382 -17985552.493802346, 10833213.874946382 -17605117.692724742, 10833213.874946382 -17245941.629605487, 10833213.874946382 -16905756.44925561, 10833213.874946382 -16582638.0031136, 10833213.874946382 -16274939.720361717, 10833213.874946382 -15981241.655663801, 10879776.050237084 -15981241.655663801, 10926338.225527786 -15981241.655663801, 10972900.40081849 -15981241.655663801, 11019462.57610919 -15981241.655663801, 11066024.751399893 -15981241.655663801, 11112586.926690595 -15981241.655663801, 11159149.101981297 -15981241.655663801, 11205711.277272 -15981241.655663801, 11252273.452562703 -15981241.655663801, 11298835.627853405 -15981241.655663801, 11345397.803144107 -15981241.655663801, 11391959.978434809 -15981241.655663801, 11438522.153725512 -15981241.655663801, 11485084.329016214 -15981241.655663801, 11531646.504306918 -15981241.655663801, 11578208.679597618 -15981241.655663801, 11624770.854888322 -15981241.655663801, 11671333.030179024 -15981241.655663801, 11717895.205469726 -15981241.655663801, 11764457.38076043 -15981241.655663801, 11811019.556051131 -15981241.655663801, 11857581.731341833 -15981241.655663801, 11904143.906632535 -15981241.655663801, 11950706.081923239 -15981241.655663801, 11997268.257213939 -15981241.655663801, 12043830.432504643 -15981241.655663801, 12090392.607795345 -15981241.655663801, 12136954.783086047 -15981241.655663801, 12183516.95837675 -15981241.655663801, 12230079.133667452 -15981241.655663801, 12276641.308958156 -15981241.655663801, 12323203.484248856 -15981241.655663801, 12369765.659539558 -15981241.655663801, 12416327.83483026 -15981241.655663801, 12462890.010120964 -15981241.655663801, 12509452.185411667 -15981241.655663801, 12556014.360702367 -15981241.655663801, 12602576.535993071 -15981241.655663801, 12649138.711283773 -15981241.655663801, 12695700.886574475 -15981241.655663801, 12742263.061865177 -15981241.655663801, 12788825.23715588 -15981241.655663801, 12835387.412446583 -15981241.655663801, 12881949.587737286 -15981241.655663801, 12928511.763027987 -15981241.655663801, 12975073.938318688 -15981241.655663801, 13021636.11360939 -15981241.655663801, 13068198.288900096 -15981241.655663801, 13114760.464190796 -15981241.655663801, 13161322.639481498 -15981241.655663801, 13207884.814772204 -15981241.655663801, 13207884.814772204 -16274939.72036171, 13207884.814772204 -16582638.0031136, 13207884.814772204 -16905756.44925561, 13207884.814772204 -17245941.629605487, 13207884.814772204 -17605117.69272473, 13207884.814772204 -17985552.493802346, 13207884.814772204 -18389944.670305897, 13207884.814772204 -18821540.168177523, 13207884.814772204 -19284291.04419406, 13207884.814772204 -19783076.40009428, 13207884.814772204 -203```",3/18/16,1
lokifacio,which one are you using? I'm using europe raster (just changed the url to point to the raster url),3/18/16,1
Prasannads,ok!. i took the global one . will try the updated one,3/18/16,1
Prasannads,@lokifacio  the europe raster works! finally am able to see the actual image without any operation .. will try the local and focal operations on it!,3/18/16,1
pomadchin,Interesting,3/18/16,1
Prasannads,"@pomadchin  i want to write the rasterrdd of spatial join into tiff .below is my code ```scalaval rasterres3 = EuropeSpatialJoinAsia.stitch    GeoTiff(rasterres3, EuropeSpatialJoinAsia.metadata.crs).write(""/home/brillio/tiffs/EuropeSpatialJoinAsia.tiff"")```but am getting this error```scalavalue stitch is not a member of org.apache.spark.rdd.RDD[(geotrellis.spark.SpatialKey, (geotrellis.raster.Tile, geotrellis.raster.Tile))] with geotrellis.spark.Metadata[geotrellis.spark.Bounds[geotrellis.spark.SpatialKey]]```",3/18/16,1
lokifacio,"@Prasannads I think you're missing an import, not sure which one",3/18/16,1
lokifacio,import geotrellis.spark.tiling._,3/18/16,1
lokifacio,import geotrellis.raster.io.geotiff._,3/18/16,1
lokifacio,import geotrellis.spark._,3/18/16,1
Prasannads,@lokifacio  i have those imports,3/18/16,1
Prasannads,"guys , any idea on this ?```scalaException in thread ""main"" java.lang.IllegalArgumentException: requirement failed: nonEmpty input        at scala.Predef$.require(Predef.scala:233)        at geotrellis.spark.stitch.TileLayoutStitcher$.stitch(StitchRDDMethods.scala:21)        at geotrellis.spark.stitch.SpatialTileLayoutRDDMethods.stitch(StitchRDDMethods.scala:42)        at RasterData.RasterOperations.MapAlgebraOperations$.readHdfsTiles(MapAlgebraOperations.scala:97)        at RasterData.RasterOperations.MapAlgebraOperations$.main(MapAlgebraOperations.scala:27)        at RasterData.RasterOperations.MapAlgebraOperations.main(MapAlgebraOperations.scala)        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)        at java.lang.reflect.Method.invoke(Method.java:606)        at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:685)        at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180)        at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205)        at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:120)        at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)```",3/18/16,1
pomadchin,"@Prasannads @lokifacio yes, we have problems, working on that; thanks for your feedback! (focal chaining)",3/18/16,1
echeipesh,"@Prasannads yes, whatever `readHdfsTiles` is producing is actually an empty RDD, good thing to do is verify why it is. Stitch needs at least one tile to stitch something together",3/18/16,1
echeipesh,"@pomadchin am I off base here, is this related to the problems we_Ñére having ?",3/18/16,1
pomadchin,"@echeipesh yeah, have example of following tiles:1. incorrect ingest (polygon & out of bound exceptions: probably just wrong tags there)2. chaining focal operations cause tiles damage: also have tiles with that problem",3/18/16,1
pomadchin,"I_Ñéll throw you links, probably we can somhow share that stuff in order not to do same things at once ._.",3/18/16,1
echeipesh,"Yes, thanks for the bootstrap, I_Ñém just parsing the chat.",3/18/16,1
pomadchin,@lokifacio reproduced (thanks to him) a bad focal operation behave using that tile: https://urs.earthdata.nasa.gov/oauth/authorize?client_id=9l9yCHEF4zcZStCzop00yw&response_type=code&redirect_uri=http%3A%2F%2Fsedac.ciesin.columbia.edu%2Furs&state=aHR0cDovL3NlZGFjLmNpZXNpbi5jb2x1bWJpYS5lZHUvZG93bmxvYWRzL2RhdGEvYWdsYW5kcy9hZ2xhbmRzLWNyb3BsYW5kcy0yMDAwL2V1LWNyb3BsYW5kcy1nZW90aWYuemlw,3/18/16,1
echeipesh,"Cool, thank you for a head start @pomadchin @lokifacio ",3/18/16,1
owcm,"Sure it is something I am not understanding with the new TileLayerRDD approach but I was hoping to get some advice.I am taking a TileLayerRDD[SpatialKey] and running a few focal ops:```// Do a focalMeanval meanRDD:TileLayerRDD[SpatialKey] = rddItem.focalMean(Square(cellsize.toInt))// Do a slope calculationval slopeRDD:TileLayerRDD[SpatialKey] = meanRDD.slope(1)// Create a RDD that is maps cells 1 to 0 based on slope rangeval slopeClassificationRDD:RDD[(SpatialKey,Tile)] = slopeRDD.localMapDouble {x => if ( x > 0 && x < slope.toDouble) 1.0 else 0.0 }// Then do a focal sum on the slope classificationval focalsumRDD = slopeClassificationRDD.focalSum( Circle(neighborSize) )```Problem as I see it is when I do the slopeRDD.localMapDouble I am returned a RDD and not a TileLayerRDD.  Focalsum does not need to be a method available to RDD_Ñés.   So I would think I would need to convert back to a TileLayerRDD, before doing the FocalSum. What is the easiest way to take a RDD[(SpatialKey,Tile)] and translate to TileLayerRDD[SpatialKey].",3/18/16,1
lossyrob,you have to thread the metadata context through using `withContext`,3/18/16,1
lossyrob,`slopeRDD.withContext(_.localMapDouble {x => if ( x > 0 && x < slope.toDouble) 1.0 else 0.0 })`,3/18/16,1
owcm,ahh,3/18/16,1
lossyrob,"because the local operations aren_Ñét on TileLayerRDD, just regular RDD of tiles",3/18/16,1
lossyrob,"so if you call something (like spark RDD operations), it_Ñéll return RDDs, so it_Ñés on the client to thread through the metadata if they want to keep it around",3/18/16,1
owcm,"Okay, that is making sense, and now my compiler error is resolved.  Thanks",3/18/16,1
pomadchin,"@Prasannads e_tile.tiff has no projection information in tiff header; in old versions we were not reading tags from tiff file, but now it is significant; to ingest this file just use:```bash$ gdal_edit.py -a_srs EPSG:3857 e_tile.tif ```",3/18/16,1
rshirochenko,"@lossyrob Trying to ingest  landsat images in 0.10.0-RC1 based on your landsat-example. Changed refactored parts in code, it works, but resulted folder is 8mb size only. Can you please have a look (https://github.com/rshirochenko/geotrellis-learn/blob/master/src/main/scala/geotrellis/testim/Main.scala) and write what can be a problem ",3/18/16,1
pomadchin,"@lossyrob @dwins do you know projection ?```+proj=cea +lon_0=-117.333333333333 +lat_ts=33.75 +x_0=0 +y_0=0 +datum=NAD27 +units=m +no_defs ```",3/18/16,1
dwins,@pomadchin  you can try searching for that exact text in `proj4/src/main/resources/geotrellis/proj4/nad/epsg`,3/18/16,1
lossyrob,@rshirochenko I_Ñém not sure. Is there anything else yo ucan tell me besides the file sizes? can you read values out and are they bad values? is it not the extent you expect? does the metadata seem correct?,3/18/16,1
pomadchin,"we have ```# NSIDC EASE-Grid Global<3410> +proj=cea +lon_0=0 +lat_ts=30 +x_0=0 +y_0=0 +a=6371228 +b=6371228 +units=m +no_defs  <>```",3/18/16,1
lossyrob,@rshirochenko I should just be able to clone this code and run it yeah?,3/18/16,1
lossyrob,and see the issue?,3/18/16,1
rshirochenko,"@lossyrob yeah, I think maybe i used refactored methods in a wrong way",3/18/16,1
dwins,@pomadchin yep not quite the same. what are you trying to accomplish exactly? you might not need an epsg code,3/18/16,1
pomadchin,"yes, this proj has no espg code; we have troubles with ingesting a tile with this projection",3/18/16,1
pomadchin,searching google found only one tile with this projection though,3/18/16,1
pomadchin,:D,3/18/16,1
dwins,hm,3/18/16,1
dwins,any idea how the projection is affecting the ingest?,3/18/16,1
pomadchin,"yes, we read tags, so that_Ñés important",3/18/16,1
pomadchin,this projection causes index out of bounds exception,3/18/16,1
dwins,Ok. Well comparing `proj4/src/main/resources/geotrellis/proj4/nad/epsg` with `proj4/src/test/resources/proj4-epsg.csv` the first few codes that use `+proj=cea` are failing so there may be a bug in this projection formula,3/18/16,1
dwins,"something we need to prioritize for the release, you think?",3/18/16,1
pomadchin,I am not sure about that (think all in all that would be fixed :D) but good to know that probably that caused by our bug ohh ,3/18/16,1
dwins,"yes, ideally we'd be as good as proj.4 :)",3/18/16,1
dwins,actually our epsg database only uses proj=cea in three coordinate reference systems so it's probably not the best place to spend energy,3/18/16,1
pomadchin,ouufff,3/18/16,1
pomadchin,"checked ESPG:3410 found really crazy bug",3/18/16,1
pomadchin,so yes probably we are not good with cea,3/18/16,1
pomadchin,:D,3/18/16,1
pomadchin,*thorwing here not to forget* http://pastebin.com/PP1LN4Qy,3/18/16,1
dwins,kind of weird the way that formatter prints NaN coordinates,3/18/16,1
dwins,think it's actually something inherited from some Java NumberFormat instance,3/18/16,1
pomadchin,"yes, checked on a random tile, converted even to EPSG:3410 ingest failed ._. ",3/18/16,1
pomadchin,oh ok,3/18/16,1
pomadchin,:D,3/18/16,1
lossyrob,"does it fail if it_Ñés a single tile, outside of ingest?",3/18/16,1
lossyrob,might be a serialization issue,3/18/16,1
pomadchin,"@Prasannads so, the second tile, has a very specific projection; tested on different tiles, so we probably have bugs with this projection; if you want to ingest it you have to reproject it to any other projection an to ingest; that_Ñés the only workaround currently; though - really weird projection, found only one mention of it",3/18/16,1
dwins,"that coordinate system is marked failing in the metacrs test, so if serialization is a problem it is not the only one",3/18/16,1
lossyrob,oh ok,3/18/16,1
pomadchin,"it fails during ingest, on collecting metadata",3/18/16,1
lossyrob,not so weird if one of the first users of 0.10 is hitting against it :/,3/18/16,1
pomadchin,"so, yes, reproduced this bug on other tiles, just reprojected to `cea` projection (even not this custom provided by @Prasannads but even to EPSG:3410)",3/18/16,1
pomadchin,):,3/18/16,1
pomadchin,but good to know that we know where to dig,3/18/16,1
dwins,"FWIW here are the top 10 parameters that occur in failing CRSs from the epsg database: ```(+datum=WGS84,118)(+ellps=intl,127)(+proj=lcc,159)(+ellps=WGS72,224)(+towgs84=0,0,0,0,0,0,0,240)(+ellps=GRS80,347)(+south,388)(+proj=utm,969)(+units=m,1107)(+no_defs,1171)```",3/18/16,1
echeipesh,"tracked down the focal operation problem, turns out a floating point error for this particular raster that resulted in -Infinity values instead of ND. -Infinity cells really confuse QGIS apparently.",3/18/16,1
lossyrob,so not a general issue? should we be mapping all those Infinity values to NaN maybe?,3/18/16,1
echeipesh,"It_Ñés pretty general, it could happen with any double cursor actually",3/18/16,1
echeipesh,"We_Ñére keeping track of sum,count and due to floating point rounding you can end up with count =0 and sum = some small value.",3/18/16,1
lossyrob,oof,3/18/16,1
lossyrob,well if count is 0 we should just do 0,3/18/16,1
lossyrob,is it just on ND tiles?,3/18/16,1
echeipesh,It was happening around ND values,3/18/16,1
echeipesh,the end result was that ND values were turned to -Infinity.,3/18/16,1
lossyrob,gotcha,3/18/16,1
echeipesh,So essentially when it reached a region where there was nothing to feed the neighborhood it was in this invalid state,3/18/16,1
lossyrob,so really we should just be more careful around ND values in our focal ops,3/18/16,1
echeipesh,"Yeah, not sure if it_Ñés more correct to hard code ND when count == 0 or to reset sum when count == 0",3/18/16,1
echeipesh,I would gest that resetting would be more correct,3/18/16,1
lossyrob,hm I mean if it says if(count == 0) Double.NaN that reads right to me,3/18/16,1
echeipesh,"right, but then we_Ñére propagating rounding error into the future sums",3/18/16,1
lossyrob,oh I see,3/18/16,1
lossyrob,yeah that makes sense,3/18/16,1
lossyrob,@jamesmcclain fyi here_Ñés valid metadata as an example for 0.10.0-RC1 layers https://gist.github.com/lossyrob/d081757252db8ed783ea,3/18/16,1
jamesmcclain,@lossyrob Thank you,3/18/16,1
jamesmcclain,Is `setPartitionCount` the new name for `setMaxKeys` in `S3InputFormat`?,3/18/16,1
echeipesh,"No, it doest a different thing. `setPartitionCount` number of partitions while `setMaxKeys` would set max number of keys per partition.",3/18/16,1
echeipesh,"Unless there is a reason to set it, it should be omitted as `S3InputFormat` will try to limit the size of partitions based on key size",3/18/16,1
jamesmcclain,I see,3/18/16,1
jamesmcclain,Thanks,3/18/16,1
pomadchin,@lossyrob have you got an old format (just asked but who knows what you have) :D ? ,3/18/16,1
pomadchin,difference not only in `keyIndex` ?,3/18/16,1
lossyrob,https://gist.github.com/lossyrob/2f13d6c34bceb98a2696,3/18/16,1
lossyrob,and the keybounds moved around,3/18/16,1
pomadchin,gotcha,3/18/16,1
pomadchin,thx Rob!,3/18/16,1
jamesmcclain,"The `TileLayerMetadata` constructor takes a `bounds` argument that its antecedent did not.  I am computing that new parameter like this: `val bounds = tileSets.map({ case (_,rdd) => Bounds.fromRdd(rdd) }).reduce(_ combine _)`.  Is this correct/performant?",3/18/16,1
lossyrob,depends on what tilesets is,3/18/16,1
jamesmcclain,"tileSets is a set of zoom level, RDD pairs",3/18/16,1
jamesmcclain,"It seems to produce an answer at least as large as it should be, but possibly not tight",3/18/16,1
jamesmcclain,Also wondering about spark performance considerations,3/18/16,1
echeipesh,"Wait, that may not make sense. Bounds are only valid per zoom level as they_Ñére bounds on a TileLayout.",3/18/16,1
lossyrob,"for those following along - this required reducing along a sequence of TileLayerMetadata, since it already had the respective TileLayerRDD bounds",3/18/16,1
lossyrob,@rshirochenko one thing to do is to replace some code here like this:,3/18/16,1
lossyrob,"```scala    // val zoom = tileSets.head._1    // val headRdd = tileSets.head._2    // val rdd = ContextRDD(    //   sc.union(tileSets.map(_._2)),    //   TileLayerMetadata[SpaceTimeKey](    //     headRdd.metadata.cellType,    //     headRdd.metadata.layout,    //     tileSets.map { case (_, rdd) => (rdd.metadata.extent) }.reduce(_.combine(_)),    //     headRdd.metadata.crs,    //     headRdd.metadata.bounds    //   )    // )    val zoom = tileSets.head._1    val rdd = ContextRDD(      sc.union(tileSets.map(_._2)),      tileSets.map(_._2.metadata).reduce(_ merge _)    )```",3/18/16,1
lossyrob,"actually, that does seem pretty small though. When I run I get around 40 megs. I_Ñém going to keep investigating.",3/18/16,1
lossyrob,"I_Ñém counting 3871866880 bytes in the RDD that are not NoData, so that seems right",3/18/16,1
lossyrob,"This is me when I do `sbt run` on a long running task and walk away to do other stuff, only to come back and realize it was asking me which main method I wanted to run. http://i.imgur.com/N5BmNPg.jpg",3/18/16,1
timothymschier,:laughing: ,3/19/16,1
lossyrob,Awful Bug Avoidance Reminder: `n == Double.NaN` does *not* ever produce a true value. You have to use `isData(n)` which translates to `java.lang.Double.isNaN(n)` or else it always fails.,3/19/16,1
echeipesh,"Damn, I fell for that one?",3/19/16,1
lossyrob,"No, it_Ñés in the user defined nodata stuff",3/19/16,1
moradology,that's my bad. i actually wasn't aware that was a thing until today,3/19/16,1
moradology,http://data.phl.opendata.arcgis.com/datasets/b54ec5210cee41c3a884c9086f7af1be_0.geojson,3/19/16,1
lossyrob,?,3/19/16,1
moradology,"ah, wrong window sorry about that",3/19/16,1
lossyrob,"@rshirochenko thanks for bringing the ingest size issue to our attention, that flushed out some major bugs that are addressed in https://github.com/geotrellis/geotrellis/pull/1411; I_Ñém releasing RC2 tonight with those fixes (as well as the rendering fixes)",3/19/16,1
moradology,this looks very useful: https://github.com/tpolecat/tut,3/19/16,1
moradology,it compiles markdown examples,3/19/16,1
moradology,as part of your build,3/19/16,1
lossyrob,"interesting. not sure how that would play along with our `docs` folder, since it builds it into the target directory",3/19/16,1
owcm,"Hey, I was trying to extend LayerWriter, using FileLayerWriter as a template.  When I compile under SBT I get an error : ```geotrellis.spark.Component[<error>,geotrellis.spark.Bounds[K]] does not take type parameters[error]   M: JsonFormat: Component[?, Bounds[K]]``` Code block it seems to be complaining about:```class PNGFileWriter(                       val attributeStore: AttributeStore,                       catalogPath: String                       ) extends LayerWriter[LayerId] with LazyLogging {  protected def _write[  K: AvroRecordCodec: JsonFormat: ClassTag,  V: AvroRecordCodec: ClassTag,  M: JsonFormat: Component[?, Bounds[K]]  ](layerId: LayerId, rdd: RDD[(K, V)] with Metadata[M], keyIndex: KeyIndex[K]): Unit = {```Wondered if there is an import that I am missing.  Thanks in advance.",3/20/16,1
pomadchin,"@owcm two possible things: spark._ import and kind projector plugin, do you use it?",3/20/16,1
lossyrob,@owcm yeah you need kind projector for that `?` syntax in the context bounds. Just add this to your build.sbt https://github.com/geotrellis/geotrellis/blob/master/build.sbt#L32,3/20/16,1
lossyrob,"For those of you that didn't see the mailing list post, `0.10.0-RC2` is out",3/20/16,1
pomadchin,@lossyrob :tada: :tada: :tada: :tada: ,3/20/16,1
lokifacio,"I'll try it out tomorrow. Thx",3/20/16,1
rshirochenko,"@lossyrob  thanks, this release has solved size ingest problem).With which method I can query  SpaceTimeKey  layer by time and save it to tiff?  The full layer object type is provided bellow``` val layer = reader.read[SpaceTimeKey, MultibandTile, TileLayerMetadata[SpaceTimeKey]](l)layer: org.apache.spark.rdd.RDD[(geotrellis.spark.SpaceTimeKey, geotrellis.raster.MultibandTile)] with geotrellis.spark.Metadata[geotrellis.spark.TileLayerMetadata[geotrellis.spark.SpaceTimeKey]] = ContextRDD[2] at RDD at ContextRDD.scala:19```",3/20/16,1
owcm,"Did  StrictColorClassifier get moved out in RC2?  docs/raster/rendering.md still references the approach.  Getting compile errors ""object StrictColorClassifier is not a member of package geotrellis.raster.render"".   Are there change notes that the community should be following to keep up with changes?  I am probably not looking in the right locations. ",3/20/16,1
lossyrob,I did make a note of it in the mailing list note,3/20/16,1
lossyrob,but the docs still need to be changed,3/20/16,1
lossyrob,"StrictColorClassifier is gone, we just have `ColorRamp` and `ColorMap`",3/20/16,1
lossyrob,https://groups.google.com/forum/#!topic/geotrellis-user/AVtF5aCagEc,3/20/16,1
owcm,"Okay, I must be registered on the mailing list, need to add myself",3/20/16,1
lossyrob,@rshirochenko it would be `query` instead of `read`,3/20/16,1
lossyrob,so something like,3/20/16,1
owcm,"@lossyrob  Thanks, looks like what I need.",3/20/16,1
lossyrob,"```scalaval dateTime1: DateTime = ???val dateTime2: DateTime = ???val polygon: Polygon = ???val layer =   reader.query[SpaceTimeKey, MultibandTile, TileLayerMetadata[SpaceTimeKey]](l)    .where(Between(dateTime1, dateTime2))    .where(Intersects(polygon))    .result```",3/20/16,1
lossyrob,"@owcm cool, let me know if you have questions on how to use it",3/20/16,1
lokifacio,"@pomadchin do you need to do something after importing geotrellis into IntelliJ? I tried to run some RasterSpec tests but it fails to make the raster module (it doesn't finde some MultiBand types defined on macros module...) ",3/21/16,1
lokifacio,I get the same error if I open sbt terminal inside IntelliJ and just run compile,3/21/16,1
lokifacio,"```geotrellis\raster\src\main\scala\geotrellis\raster\MultibandTile.scala:33: not found: type MacroCombinableMultibandTile```",3/21/16,1
pomadchin,and sbt from terminal works?,3/21/16,1
lokifacio,I have problems with sbt-launch.jar on windows... I can try on my server running linux,3/21/16,1
pomadchin,ok; but what i can suggest you; to recompile manually 1. macro project 2. raster project,3/21/16,1
lokifacio,but I'd like to develop on this machine,3/21/16,1
lokifacio,from Intellij sbt console?,3/21/16,1
pomadchin,yes,3/21/16,1
lokifacio,same error,3/21/16,1
lokifacio,project macros -> compile -> ok,3/21/16,1
pomadchin,project raster -> compile,3/21/16,1
lokifacio,project raster -> compile -> fails with same error,3/21/16,1
pomadchin,weird,3/21/16,1
pomadchin,can you try to remove raster/target dir?,3/21/16,1
pomadchin,"btw, never used intelij idea sbt console + it_Ñés deprecated due to bugs (?)",3/21/16,1
pomadchin,btw can_Ñét find console in 15/2016.1 intelij o:,3/21/16,1
pomadchin,just common console,3/21/16,1
lokifacio,it's a plugin,3/21/16,1
pomadchin,plugin name,3/21/16,1
pomadchin,will try it,3/21/16,1
lokifacio,so when you compile you do it from terminal?,3/21/16,1
pomadchin,yes,3/21/16,1
pomadchin,never tried to compile any projects with idea (just have no trust to it),3/21/16,1
pomadchin,however; raster should compile,3/21/16,1
lokifacio,xD,3/21/16,1
lokifacio,I thought you were using idea to develop geotrellis,3/21/16,1
pomadchin,I use it as a code editor with smart syntax highlight and sources,3/21/16,1
lokifacio,ah ok,3/21/16,1
lokifacio,this is the plugin I use,3/21/16,1
lokifacio,https://plugins.jetbrains.com/plugin/5007?pr=idea,3/21/16,1
pomadchin,): I have intelij idea 2016,3/21/16,1
pomadchin,installed it couple of days ago,3/21/16,1
pomadchin,hm,3/21/16,1
pomadchin,i have a feeling i know how to solve that,3/21/16,1
lokifacio,I'm using 15.0.3,3/21/16,1
lokifacio,but maybe I'll update to newest realese,3/21/16,1
pomadchin,mark `raster/target/scala-2.10/src_managed/` as a source folder in project settings,3/21/16,1
pomadchin,and `macros/target/scala-2.10/src_managed/`,3/21/16,1
lokifacio,"XXXX/target/scala-2.10/src_managed/**main** is already marked as source folder, but I'm trying marking src_managed as well",3/21/16,1
pomadchin,ok; then yep have no ideas,3/21/16,1
pomadchin,what _Ñés list of files here? `raster/target/scala-2.10/src_managed/main/geotrellis/raster/`,3/21/16,1
lokifacio,"MacroGeotiffMultibandCombiners, MacroMultibandCimbiners and SegmentCombiner",3/21/16,1
pomadchin,and here `macros/target/scala-2.10/src_managed/main/geotrellis/macros/`,3/21/16,1
lokifacio,"now it's taking more time, may that solved it...",3/21/16,1
pomadchin,"(: try to run it with windows sbt console, if the problem would be the same than that_Ñés weird; if there would be no problem, than that_Ñés plugin bug",3/21/16,1
lokifacio,"ok, I'll give it a try",3/21/16,1
pomadchin,"btw, have you tried to compile it using IntelijIdea Build menu?",3/21/16,1
pomadchin,nwo just tried to build with it and build was successful ,3/21/16,1
pomadchin,Build -> Compile module raster,3/21/16,1
lokifacio,@pomadchin marking src_managed as source folders worked out,3/21/16,1
pomadchin,cool,3/21/16,1
lokifacio,"I don't have any Compile module raster option in build menu, how are you importing the project in itellijIdea? I just chose import project->select geotrellis base folder and then select sbt project. It prompts a list of all modules and select all of them",3/21/16,1
pomadchin,"just select raster in the projects tree, after that use Build menu",3/21/16,1
lokifacio,"no, it doesn't change anything",3/21/16,1
pomadchin,[![Screen Shot 2016-03-21 at 11.23.00.png](https://files.gitter.im/geotrellis/geotrellis/tqxQ/thumb/Screen-Shot-2016-03-21-at-11.23.00.png)](https://files.gitter.im/geotrellis/geotrellis/tqxQ/Screen-Shot-2016-03-21-at-11.23.00.png),3/21/16,1
pomadchin,however cool that it works now (:,3/21/16,1
lokifacio,Before further testing I'm going to update IntelliJIdea so we're on the same version,3/21/16,1
lokifacio,"I know, but for me it's disabled xP",3/21/16,1
pomadchin,"want to say, that intelij idea has lots of porblems with syntax analysing",3/21/16,1
pomadchin,"and it can_Ñét understand kind projector plugin, so that_Ñés a bug, i just have no time to report about it; but probably similar issues are present on their tracker",3/21/16,1
lokifacio,"probably sbt integration is not very mature yet, anyway I prefer it to eclipse xD",3/21/16,1
pomadchin,"yes, eclipse not for me too",3/21/16,1
lokifacio,I'm thinking on overloading focal API to add a parameter so we can specify which cells it should be applied to: NoData/ValidCells/All,3/21/16,1
lokifacio,"I could do it as part of my own project but I think it makes more sense to add it to geotrellis, right?",3/21/16,1
pomadchin,"yes, feel free to add all functionality you think would be helpfull for everybody! it is very cool; and we appreciate that ;)",3/21/16,1
pomadchin,:p go go,3/21/16,1
rshirochenko,"@lossyrob Tried it,```val dateTime1:DateTime = new DateTime(2014,5,18,15,39,22)val dateTime2:DateTime = new DateTime(2014,5,20,15,39,30)val polygon = Polygon(Line((-8344438.33493974339216948, 4897927.35531818494200706), (-8339316.00229408871382475, 4882666.40911443624645472), (-8319766.85982469096779823, 4886498.55795095954090357), (-8326465.20683903247117996, 4901716.7104023601859808), (-8344438.33493974339216948, 4897927.35531818494200706)))val layer =   reader.query[SpaceTimeKey, MultibandTile, TileLayerMetadata[SpaceTimeKey]](l)    .where(Between(dateTime1, dateTime2))    .where(Intersects(polygon)).result<console>:82: error: Unable to filter geotrellis.spark.SpaceTimeKey by geotrellis.spark.io.Intersects.type given geotrellis.spark.TileLayerMetadata[geotrellis.spark.SpaceTimeKey], Please provide LayerFilter[geotrellis.spark.SpaceTimeKey, geotrellis.spark.io.Intersects.type, geotrellis.vector.Polygon, geotrellis.spark.TileLayerMetadata[geotrellis.spark.SpaceTimeKey]]result```The metadata, so time and polygon should be in range.```geotrellis.vector.Extent = Extent(-8489029.278978366, 4745838.023395102, -8161850.822193717, 5070636.37355866)geotrellis.spark.Bounds[geotrellis.spark.SpaceTimeKey] = KeyBounds(SpaceTimeKey(2360,3059,1400538071000),SpaceTimeKey(2427,3125,1400538071000))```It works without with time query only, but it gives the SpaceTimeKey type, that I cannot use for GeoTiff save method",3/21/16,1
lokifacio,"@pomadchin getting error on project vector -> compile from current master while compiling on linux terminal```[error] geotrellis/vector/src/main/scala/geotrellis/vector/Extent.scala:31: value reproject is not a member of geotrellis.vector.Extent[error]     extent.reproject(crs, dest)[error]            ^```do you have the same issue?",3/21/16,1
lokifacio,I don't get this error on idea but instead I have errors on spark module xP,3/21/16,1
lokifacio,@pomadchin removing target dir solved the issue,3/21/16,1
lokifacio,:),3/21/16,1
lokifacio,same for spark on intellijIdea,3/21/16,1
pomadchin,"yes, sbt cache error",3/21/16,1
pomadchin,:\,3/21/16,1
lokifacio,"hey, I've found that focal methods API's param naming convention is not very stable. Sometimes Tile is referred as r and others as tile. As I'm reviewing most of the methods do you want me to change them so they're always referred accordingly? which naming style should I use to update the api? short param names (ala python) or more expressive ones (I'd usually prefer this way)? in case of short names I guess it makes more sense for tiles to be t instead of r (maybe before it was referring to rasters?)",3/21/16,1
lokifacio,"@lossyrob I've found this on  Median.scala:```scaladef setValue(x: Int, y: Int) = { resultTile.setDouble(x, y, median) } ```shouldn't be set instead of setDouble?",3/21/16,1
lokifacio,since median and the resulting tile are Int,3/21/16,1
lossyrob,"@lokifacio method param naming consistency is definitely something we want; there are some relics from when we were using Raster instead of tile, or just bad naming, which is where the `r` comes from. The preferred is, like you said, the more expressive ones, so `tile`. If you wanted to make changes in line with that that would be awesome",3/21/16,1
lokifacio,"ok, I'll replace those in my codebase in case I can make a PR ",3/21/16,1
lossyrob,"also, for Median, your totally correct",3/21/16,1
lokifacio,I changed both median and mode,3/21/16,1
lossyrob,in fact I think that would set the wrong value in case the median was NODATA,3/21/16,1
lossyrob,would end up being NODATA.toDouble,3/21/16,1
lossyrob,nice,3/21/16,1
owcm,"Testing my code on spark after upgrading to RC2. Trying to parallelize a number of tiffs I have loaded:``` val tiles = sc.parallelize( tileList, 4 )```Get stack error:```Task serialization failed: org.apache.spark.SparkException: Failed to register classes with Kryoorg.apache.spark.serializer.KryoSerializer.newKryo(KryoSerializer.scala:124)org.apache.spark.serializer.KryoSerializerInstance.borrowKryo(KryoSerializer.scala:237)org.apache.spark.serializer.KryoSerializerInstance.<init>(KryoSerializer.scala:222)org.apache.spark.serializer.KryoSerializer.newInstance(KryoSerializer.scala:138)org.apache.spark.broadcast.TorrentBroadcast$.blockifyObject(TorrentBroadcast.scala:201)org.apache.spark.broadcast.TorrentBroadcast.writeBlocks(TorrentBroadcast.scala:102)org.apache.spark.broadcast.TorrentBroadcast.<init>(TorrentBroadcast.scala:85)org.apache.spark.broadcast.TorrentBroadcastFactory.newBroadcast(TorrentBroadcastFactory.scala:34)org.apache.spark.broadcast.BroadcastManager.newBroadcast(BroadcastManager.scala:63)org.apache.spark.SparkContext.broadcast(SparkContext.scala:1327)org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$submitMissingTasks(DAGScheduler.scala:861)org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$submitStage(DAGScheduler.scala:772)org.apache.spark.scheduler.DAGScheduler.handleJobSubmitted(DAGScheduler.scala:757)org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1466)org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1458)org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1447)org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)```Wondered if anyone else had see similar issue.  My spark job settings have not changed.  Thanks in advance.",3/21/16,1
owcm,"Using ```.setConf(""spark.serializer"", ""org.apache.spark.serializer.KryoSerializer"")        .setConf(""spark.kryo.registrator"", ""geotrellis.spark.io.kyro.KryoRegistrator"")```",3/21/16,1
pomadchin,@owcm can you try this lines https://github.com/pomadchin/geotrellis-chatta-demo/blob/spark-version/geotrellis/src/main/scala/geotrellis/kryo/AvroRegistrator.scala ?,3/21/16,1
pomadchin,* these,3/21/16,1
pomadchin,but not sure about these lines :/ Mb also dirty dependencies (sbt clean && sbt assembly); will have a look at this bug in ~ hour ,3/21/16,1
lokifacio,"@lossyrob which is the preferred style for one line def, with or without braces?",3/21/16,1
owcm,"@pomadchin  Got the class named correctly .setConf(""spark.kryo.registrator"", ""geotrellis.spark.io.kryo.KryoRegistrator"") instead of kyro so I am past that point.  Classes are registering now",3/21/16,1
echeipesh,"@lokifacio one line defs are prefered without braces, with the body on a new indented line",3/21/16,1
lokifacio,thx,3/21/16,1
lossyrob,"@echeipesh @lokifacio not necessarily true. I don_Ñét prefer without braces over with braces, and often choose with braces if I think it_Ñés clearer",3/21/16,1
lossyrob,wait I_Ñém thinking of an if statement,3/21/16,1
lossyrob,`def`s that right. no braces.,3/21/16,1
lokifacio,"@lossyrob my 2cents, I personally preferred also braces as they to prevent some bugs during refactoring...",3/21/16,1
lossyrob,can you give an example?,3/21/16,1
lokifacio,"CellwiseSumCalc:``` def setValue(x: Int, y: Int) = resultTile.set(x, y, total)```",3/21/16,1
lossyrob,"yeah, I_Ñéd say no braces there",3/21/16,1
lossyrob,when would that become an issue with refactoring,3/21/16,1
lokifacio,"sometimes you use braces sometimes not, just trying to guess style ",3/21/16,1
lossyrob,oops :) when do I use braces in that situation?,3/21/16,1
lokifacio,on the same line?,3/21/16,1
lokifacio,not there,3/21/16,1
lokifacio,in general (and mostly in other languages where are optionals),3/21/16,1
lossyrob,in fact for `def_Ñés` if it_Ñés just one statement I prefer it to be without braces,3/21/16,1
lossyrob,like,3/21/16,1
lokifacio,I'm not sure if you or someone else :P,3/21/16,1
lokifacio,but  I found some def with braces,3/21/16,1
lokifacio,"Conway.scala```      def reset() = { count = 0 }```",3/21/16,1
lossyrob,"```scaladef foo(rdd: RDD[_]): RDD =  rdd    .transform    .doStuff    .lookMaNoVals```",3/21/16,1
lossyrob,"that way I know it_Ñés one statement, and if I had to add a val, I_Ñém forced to use braces",3/21/16,1
lossyrob,yeah that_Ñés someone else_Ñ_.Erik Oscheim actually,3/21/16,1
lossyrob,dunno if he_Ñés modified his style by now,3/21/16,1
lokifacio,"ok, I'll stick to what @echeipesh  said, no braces and new line",3/21/16,1
echeipesh,"I think we have to fess up that we don_Ñét have a super strict style guide that we enforce and prefered methods for dealing with style is shifting a little as our coding approach changes (more generics, more type classes. Plus we generally don_Ñét go back to re-style code, just change it as we refactor. But yeah, simple def on line line without braces has been pretty stable, and it makes chaining like Rob is showing more clear.",3/21/16,1
lokifacio,"cool, I know, it is normal in such active developments",3/21/16,1
lossyrob,"yeah, and we all have slightly different styles. I think if there_Ñés nothing too aggregious and it isn_Ñét unclear, that_Ñés ok. I_Ñéd mark it on the PR",3/21/16,1
jasonbeverage,"If I do an Intersect query on a MultiPolygon like this:```    val layer = reader                .query[SpatialKey, Tile, TileLayerMetadata[SpatialKey]](layerId)                .where(Intersects(polygon))                .result```will the pixels in the tiles that are returned be masked out by the polygon or is it going to just return all the raw tiles that intersect the multipolygon?",3/21/16,1
pomadchin,raw tiles,3/21/16,1
pomadchin,"if you want to make, you have to mask manually O: and to stitch tiles, if you want a single stitched and masked tiff",3/21/16,1
pomadchin,"you can crop by polygon in addition, to remove blank spaces caused by masking",3/21/16,1
jasonbeverage,"Ok, basically I'm trying to do a per pixel operation on a layer, but only apply the operation to the pixels that actually intersect the polygon.",3/21/16,1
jamesmcclain,You might find this example helpful: https://github.com/WikiWatershed/mmw-geoprocessing/blob/develop/summary/src/main/scala/SummaryJob.scala,3/21/16,1
jamesmcclain,"That is pre-RC[12] code, though",3/21/16,1
jamesmcclain,There is a pull request up which translates it to RC1-compatible code,3/21/16,1
jasonbeverage,"Cool thanks, I'll check it out.",3/21/16,1
owcm,With the new ColorMap and ColorRamp schemes should the old ColorRamps i.e. ColorRamps.HeatmapYellowToRed even be used at all?,3/21/16,1
moradology,The old `ColorRamp` arrays will still be included for use,3/21/16,1
moradology,https://github.com/geotrellis/geotrellis/blob/master/raster/src/main/scala/geotrellis/raster/render/ColorRamps.scala,3/21/16,1
lossyrob,@owcm yes those color ramps can all be used,3/21/16,1
lossyrob,something like `ColorRamps.HeadmapYellowToRed.toColorMap(tile.histogram)`,3/21/16,1
jasonbeverage,"Is there a generic way I can get to the attributeStore of a FilteringLayerReader?  Most specifically the list of LayerIds.  I've written a little utility class to instantiate the LayerReader type from the command line parameters (kind of like how ETL works) so that I don't have to code directly against the specific reader types (accumulo, s3, hadoop).  ",3/21/16,1
jasonbeverage,When I was directly instantiating an AccumuloLayerReader I would write reader.attributeStore.layerIds and loop over them.  Now I've just got a FilteringLayerReader and I get:,3/21/16,1
jasonbeverage,"```value layerIds is not a member of geotrellis.spark.io.FilteringLayerReader[geotrellis.spark.LayerId]```",3/21/16,1
jasonbeverage,"Never mind, not important :)  I just wrote a big if then and it looks like it works good enough for now.",3/21/16,1
lossyrob,"I would say that the utility should capture both the attribute store and the reader, and use the attribute store to  create the reader",3/21/16,1
jamesmcclain,I am trying to migrate some files form the old metadata format to the new,3/21/16,1
jamesmcclain,"We moved the keyBounds key into a keyIndex.properties.keyBounds and added keyIndex.type, removed keyIndex.obj, and added metadata.bounds (which was the same as keyBounds except with a different name)",3/21/16,1
jamesmcclain,(And I should add that the metadata file name does not match the directory name in the catalog),3/21/16,1
jamesmcclain,Is there anything more that needs to be done?  Should this have worked?,3/21/16,1
pomadchin,"yes, that should be ok",3/21/16,1
jamesmcclain,"Okay, I am getting the following exception:```spray.json.DeserializationException: TileLayerMetadata expected        at geotrellis.spark.io.json.Implicits$$anon$1.read(Implicits.scala:88)        at geotrellis.spark.io.json.Implicits$$anon$1.read(Implicits.scala:67)        at spray.json.JsValue.convertTo(JsValue.scala:31)        at geotrellis.spark.io.BlobLayerAttributeStore$class.readLayerAttributes(AttributeStore.scala:75)        at geotrellis.spark.io.s3.S3AttributeStore.readLayerAttributes(S3AttributeStore.scala:20)        at geotrellis.spark.io.s3.S3LayerReader.liftedTree1$1(S3LayerReader.scala:41)        at geotrellis.spark.io.s3.S3LayerReader.read(S3LayerReader.scala:40)        at geotrellis.spark.io.s3.S3LayerReader.read(S3LayerReader.scala:26)        at geotrellis.spark.io.FilteringLayerReader.read(FilteringLayerReader.scala:40)        at geotrellis.spark.io.FilteringLayerReader.read(FilteringLayerReader.scala:47)        at geotrellis.spark.io.FilteringLayerReader$$anonfun$query$1.apply(FilteringLayerReader.scala:61)        at geotrellis.spark.io.FilteringLayerReader$$anonfun$query$1.apply(FilteringLayerReader.scala:61)        at geotrellis.spark.io.BoundLayerQuery.result(LayerQuery.scala:52)        at org.wikiwatershed.mmw.geoprocessing.SummaryJob$.queryAndCropLayer(SummaryJob.scala:120)        at org.wikiwatershed.mmw.geoprocessing.SummaryJob$.runJob(SummaryJob.scala:38)        at org.wikiwatershed.mmw.geoprocessing.SummaryJob$.runJob(SummaryJob.scala:25)        at spark.jobserver.JobManagerActor$$anonfun$spark$jobserver$JobManagerActor$$getJobFuture$4.apply(JobManagerActor.scala:254)        at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)        at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)        at java.lang.Thread.run(Thread.java:745)16/03/21 16:11:03 INFO JobStatusActor: Job 84adefb1-0067-4644-aaa0-b692aca88081 finished with an error```",3/21/16,1
pomadchin,kk,3/21/16,1
pomadchin,will force S3,3/21/16,1
pomadchin,/_/,3/21/16,1
pomadchin,ingesting tiles ,3/21/16,1
jamesmcclain,The code is here: https://github.com/jamesmcclain/mmw-geoprocessing/tree/feature/update-for-RC1,3/21/16,1
pomadchin,"yes, downloading sample tiles to ingest into real S3",3/21/16,1
jamesmcclain,Okay,3/21/16,1
owcm,"ColorMap Question. I set up a  simple colormap that I wanted to use against a slope result.```  val slopeMap = ColorMap(    Map(      3.5 -> RGB(0,255,0).int,      7.5 -> RGB(63, 255 ,51).int,      11.5 -> RGB(102,255,102).int,      15.5 -> RGB(178, 255,102).int,      19.5 -> RGB(255,255,0).int,      23.5 -> RGB(255,255,51).int,      26.5 -> RGB(255,153, 51).int,      31.5 -> RGB(255,128,0).int,      35.0 -> RGB(255,51,51).int,      40.0 ->RGB(255,0,0).int    )  )```Run a slope op on a TileLayerRDD:  val slopeRdd = tilelayerRDD.slope(1.0)Then tile it using renderPng using the ColorMap to color different slope ranges:pngBytes = raster.tile.renderPng(slopeMap).bytes.   Write the bytes out to file and everything is coming up green, my first grouping.   I know I have slope values  in the other categories.   Anything look obviously wrong?",3/21/16,1
moradology,"Interesting. nothing looks obviously wrong, and when i try this locally, it seems to render appropriately",3/21/16,1
owcm,"@moradology thanks for checking, yea I am scratching my head",3/21/16,1
moradology,"would it be possible to post a gist of the surrounding code?",3/21/16,1
owcm,"@moradology  Sure:``` case ( sp, raster) =>                    val re = raster.rasterExtent                    val tileList =  mutable.ListBuffer[(SpatialKey, Array[Byte])]()                    var pngBytes:Array[Byte] =raster._1.renderPng(colorBreaks).bytes                    try {                      if (operation == ""artslope"") {                        pngBytes = raster._1.renderPng(artColorBreaks).bytes                      } else if (operation == ""tpi"") {                        pngBytes = raster._1.renderPng(tpiColorBreaks).bytes                      } else if (operation == ""aspect"") {                        pngBytes = raster._1.renderPng(aspectColorBreaks).bytes                      } else if (operation == ""hlz"") {                        pngBytes = raster._1.renderPng(hlzColorbreaks).bytes                      } else if (operation == ""hover"") {                        pngBytes = raster._1.renderPng(hoverColorBreaks).bytes                      }                      else {                        val colorRampToUse = cw.getColorRampByOp(operation )                        val result = colorRampToUse.toColorMap(raster._1.histogram)                        pngBytes = raster._1.renderPng(result).bytes                      }                      val newEntry = (sp, pngBytes)                      tileList += newEntry                    } catch {                      case e: Exception => println( ""Error on reading tile "" + e.getMessage)                    }                    tileList```",3/21/16,1
owcm,"``` pngBytes = raster._1.renderPng(artColorBreaks).bytes```artColorBreaks in this case = slopeMap",3/21/16,1
owcm,"@moradology  The code I posted runs up on Spark in Amazon EMR.  If I do something like this in local mode things work:```val png = lclTile.renderPng(artClassifierMap)png.write( fileName + "".png"")```",3/21/16,1
moradology,very much appreciated i'm poking around right now,3/21/16,1
lossyrob,oh strange that it works in the local case but not the spark case,3/21/16,1
lossyrob,that_Ñés good information,3/21/16,1
moradology,"the peculiar thing to me, is that the spark code for rendering really adds very little beyond the (working) local case",3/21/16,1
owcm,[![18STJ4726.tif.png](https://files.gitter.im/geotrellis/geotrellis/Pywr/thumb/18STJ4726.tif.png)](https://files.gitter.im/geotrellis/geotrellis/Pywr/18STJ4726.tif.png),3/21/16,1
owcm,"I am seeing different behaviour between the 2 environments. I have another Colormap for Aspect.   Works very nicely locally when working on individual Singleband Geotiffs.  ",3/21/16,1
owcm,Trying to get you an image of what shows up on EMR,3/21/16,1
moradology,is it not just green?,3/21/16,1
moradology,sorry if i missed something - just trying to get all the details in order,3/21/16,1
owcm,"I understand, and I am trying not to confuse the issue.  Aspect was another example of a Colormap that I was seeing different results in the 2 environments.  Thought it might help.",3/21/16,1
owcm,[![Screen Shot 2016-03-21 at 7.20.01 PM.png](https://files.gitter.im/geotrellis/geotrellis/ELZN/thumb/Screen-Shot-2016-03-21-at-7.20.01-PM.png)](https://files.gitter.im/geotrellis/geotrellis/ELZN/Screen-Shot-2016-03-21-at-7.20.01-PM.png),3/21/16,1
owcm,[![Screen Shot 2016-03-21 at 7.20.01 PM.png](https://files.gitter.im/geotrellis/geotrellis/ELZN/thumb/Screen-Shot-2016-03-21-at-7.20.01-PM.png)](https://files.gitter.im/geotrellis/geotrellis/ELZN/Screen-Shot-2016-03-21-at-7.20.01-PM.png),3/21/16,1
owcm,Same color map but looks generalized when using aspect with TileLayerRDD operations.,3/21/16,1
owcm,Same color map but looks generalized when using aspect with TileLayerRDD operations.,3/21/16,1
owcm,Up on spark all my operations are being done with the TileLayerRDD operations and then I am using the Pyramiding functionality to get the Z/X/Y mapping if that makes any sense.,3/21/16,1
owcm,Up on spark all my operations are being done with the TileLayerRDD operations and then I am using the Pyramiding functionality to get the Z/X/Y mapping if that makes any sense.,3/21/16,1
owcm,"  val aspectClassifierMap = ColorMap(    Map(      22.5 -> RGBA(255,0,0,255).int,      67.5 -> RGBA(231,113,36,255).int,      112.5 -> RGBA(246,235,20,255).int,      157.5 -> RGBA(171,208,80,255).int,      202.5 -> RGBA(57,198,240,255).int,      247.5 -> RGBA(70,152,211,255).int,      292.5 -> RGBA(42,46,127,255).int,      337.5 -> RGBA(165,34,120,255).int,      360.0 -> RGBA(255,0,0,255).int    )  )",3/21/16,1
owcm,"  val aspectClassifierMap = ColorMap(    Map(      22.5 -> RGBA(255,0,0,255).int,      67.5 -> RGBA(231,113,36,255).int,      112.5 -> RGBA(246,235,20,255).int,      157.5 -> RGBA(171,208,80,255).int,      202.5 -> RGBA(57,198,240,255).int,      247.5 -> RGBA(70,152,211,255).int,      292.5 -> RGBA(42,46,127,255).int,      337.5 -> RGBA(165,34,120,255).int,      360.0 -> RGBA(255,0,0,255).int    )  )",3/21/16,1
owcm,In this case I am using RGBA values was not sure if that was allowed.,3/21/16,1
owcm,In this case I am using RGBA values was not sure if that was allowed.,3/21/16,1
moradology,it is definitely helpful.,3/21/16,1
moradology,it is definitely helpful.,3/21/16,1
moradology,"and yeah, RGB is just some sugar for RGBA with an 100% opacity",3/21/16,1
moradology,"and yeah, RGB is just some sugar for RGBA with an 100% opacity",3/21/16,1
owcm,Thats what I thought,3/21/16,1
owcm,Thats what I thought,3/21/16,1
owcm,@moradology @lossyrob  I think there is something questionable going on with ColorMaps in spark.   Not sure how to prove it to you.  I can post my code if you want to review. But I have 3 Analytics that have problems with visualization after the upgrade.  Doing the tests locally seem to work but working with TileLayer and Pyramiding I have issues.  I can post my full code  some where if that would help.,3/22/16,1
owcm,@moradology @lossyrob  I think there is something questionable going on with ColorMaps in spark.   Not sure how to prove it to you.  I can post my code if you want to review. But I have 3 Analytics that have problems with visualization after the upgrade.  Doing the tests locally seem to work but working with TileLayer and Pyramiding I have issues.  I can post my full code  some where if that would help.,3/22/16,1
moradology,"@owcm I had to shift gears, but I'll be spending more time trying to figure this out tomorrow. Thanks again",3/22/16,1
moradology,"@owcm I had to shift gears, but I'll be spending more time trying to figure this out tomorrow. Thanks again",3/22/16,1
owcm,"@moradology ok, let me know if you need anything more from me.",3/22/16,1
owcm,"@moradology ok, let me know if you need anything more from me.",3/22/16,1
lossyrob,Sounds like a serialization issue. Gonna look into it when I get a chance,3/22/16,1
lossyrob,Sounds like a serialization issue. Gonna look into it when I get a chance,3/22/16,1
lokifacio,"there is no method to check if a tile cell is Data/NoData directly given its coords, right?",3/22/16,1
lokifacio,"there is no method to check if a tile cell is Data/NoData directly given its coords, right?",3/22/16,1
lokifacio, we have to get the value and check using isData method,3/22/16,1
lokifacio, we have to get the value and check using isData method,3/22/16,1
pomadchin,"@lokifacio yep, you are right",3/22/16,1
pomadchin,"@lokifacio yep, you are right",3/22/16,1
pomadchin,"so you think to have an overload methods `isNoData/isData(col, row)` is a good idea?",3/22/16,1
pomadchin,"so you think to have an overload methods `isNoData/isData(col, row)` is a good idea?",3/22/16,1
lokifacio,"I would say so, it avoids client side celltype checking in order to use proper get method",3/22/16,1
lokifacio,"I would say so, it avoids client side celltype checking in order to use proper get method",3/22/16,1
lokifacio,don't you think so?,3/22/16,1
lokifacio,don't you think so?,3/22/16,1
lokifacio,just each tile knows which is the proper way to check that,3/22/16,1
lokifacio,just each tile knows which is the proper way to check that,3/22/16,1
pomadchin,can you show an example of code you want to achieve?,3/22/16,1
pomadchin,can you show an example of code you want to achieve?,3/22/16,1
lokifacio,"```val isValidData = if (tile.cellType.isFloatingPoint) {        isData(tile.getDouble(x, y))      } else {        isData(tile.get(x, y))       }```",3/22/16,1
lokifacio,"```val isValidData = if (tile.cellType.isFloatingPoint) {        isData(tile.getDouble(x, y))      } else {        isData(tile.get(x, y))       }```",3/22/16,1
lokifacio,btw I did a first approach on cellwisecalculation to select on which cells the op should be applied,3/22/16,1
lokifacio,btw I did a first approach on cellwisecalculation to select on which cells the op should be applied,3/22/16,1
lokifacio,"I have doubts about default param values and ordering, I chose the one it seemed more reasonable to me",3/22/16,1
lokifacio,"I have doubts about default param values and ordering, I chose the one it seemed more reasonable to me",3/22/16,1
lokifacio,and some naming,3/22/16,1
lokifacio,and some naming,3/22/16,1
pomadchin,and your suggested api is? (just a quick example with first naming came in your head),3/22/16,1
pomadchin,and your suggested api is? (just a quick example with first naming came in your head),3/22/16,1
lokifacio,"```def focalMean(n: Neighborhood, target: FocalTarget = FocalTarget.All)```",3/22/16,1
lokifacio,"```def focalMean(n: Neighborhood, target: FocalTarget = FocalTarget.All)```",3/22/16,1
lokifacio,"`FocalTarget` is an enum which can be All, NoData, Data",3/22/16,1
lokifacio,"`FocalTarget` is an enum which can be All, NoData, Data",3/22/16,1
lokifacio,"here the api for me is clear, is just when propagating calls to FocalCalculation subclasses where you have bounds (sometimes optional, sometimes not) involved",3/22/16,1
lokifacio,"here the api for me is clear, is just when propagating calls to FocalCalculation subclasses where you have bounds (sometimes optional, sometimes not) involved",3/22/16,1
lokifacio,@pomadchin I'm finishing some stuff and later I will try to publish what I've done so you can comment on it before modifying the rest of the focal operations,3/22/16,1
lokifacio,@pomadchin I'm finishing some stuff and later I will try to publish what I've done so you can comment on it before modifying the rest of the focal operations,3/22/16,1
lokifacio,@pomadchin any naming suggestion for `target: FocalTarget`?,3/22/16,1
lokifacio,@pomadchin any naming suggestion for `target: FocalTarget`?,3/22/16,1
pomadchin,may be `cell` or `cellType`?,3/22/16,1
pomadchin,may be `cell` or `cellType`?,3/22/16,1
pomadchin,nope `cellType` would be a bad name because we have a `CellType`,3/22/16,1
pomadchin,nope `cellType` would be a bad name because we have a `CellType`,3/22/16,1
lokifacio,yep,3/22/16,1
lokifacio,yep,3/22/16,1
lokifacio,and has to give some clue about the intent like target/apply or something like that...,3/22/16,1
lokifacio,and has to give some clue about the intent like target/apply or something like that...,3/22/16,1
pomadchin,i like word `domain`,3/22/16,1
pomadchin,i like word `domain`,3/22/16,1
pomadchin,cellDomain,3/22/16,1
pomadchin,cellDomain,3/22/16,1
lokifacio,"R's counter part uses NoDataOnly as boolean, but in that case you can't specify DataOnly",3/22/16,1
lokifacio,"R's counter part uses NoDataOnly as boolean, but in that case you can't specify DataOnly",3/22/16,1
lokifacio,"@lossyrob ,  @echeipesh, @pomadchin could you please take a look at https://github.com/lokifacio/geotrellis/commit/8792adab726e14e55a80a77b90772dbdb0b61654. I'll be away for the rest of the week but next week I would like to finish updating the rest of the focal operations taking into account your feerdback. Atm I've updated and tested focal Mean/Sum with CellWiseCalculation",3/22/16,1
lokifacio,"@lossyrob ,  @echeipesh, @pomadchin could you please take a look at https://github.com/lokifacio/geotrellis/commit/8792adab726e14e55a80a77b90772dbdb0b61654. I'll be away for the rest of the week but next week I would like to finish updating the rest of the focal operations taking into account your feerdback. Atm I've updated and tested focal Mean/Sum with CellWiseCalculation",3/22/16,1
lokifacio,Have a nice week :),3/22/16,1
lokifacio,Have a nice week :),3/22/16,1
lossyrob,"thanks, we_Ñéll take a look! have a good one.",3/22/16,1
lossyrob,"thanks, we_Ñéll take a look! have a good one.",3/22/16,1
jasonbeverage,"Hi all, I've got a general spark question.",3/22/16,1
jasonbeverage,"Hi all, I've got a general spark question.",3/22/16,1
jasonbeverage,Can you use local variables in a spark operation and have it serialized correctly or do you need to do something fancy?,3/22/16,1
jasonbeverage,Can you use local variables in a spark operation and have it serialized correctly or do you need to do something fancy?,3/22/16,1
jasonbeverage,"Basically within my operations I need to use a MultiPolygon I've loaded and the Metadata of a layer, but if I run using a master that is not local[*] (like mesos or yarn) it gives me NullPointer exceptions.  ",3/22/16,1
jasonbeverage,"Basically within my operations I need to use a MultiPolygon I've loaded and the Metadata of a layer, but if I run using a master that is not local[*] (like mesos or yarn) it gives me NullPointer exceptions.  ",3/22/16,1
jasonbeverage,So with local[*] everything works.,3/22/16,1
jasonbeverage,So with local[*] everything works.,3/22/16,1
pomadchin,"vars have to be serializable, that_Ñés why we have lots of `@transient` notations inside Etl object;however multipolygon operation should workif it is heavy you can broadcast it using `sc.broadcast`",3/22/16,1
pomadchin,"vars have to be serializable, that_Ñés why we have lots of `@transient` notations inside Etl object;however multipolygon operation should workif it is heavy you can broadcast it using `sc.broadcast`",3/22/16,1
jasonbeverage,"Something like this ``` val multiPolygon : MultiPolygon = ??? val metadata = layer.metadata layer.flatMap{ case(key,tile) => doSomething(multiPolygon, metadata) }```",3/22/16,1
jasonbeverage,"Something like this ``` val multiPolygon : MultiPolygon = ??? val metadata = layer.metadata layer.flatMap{ case(key,tile) => doSomething(multiPolygon, metadata) }```",3/22/16,1
jasonbeverage,"Ok, i tried that with broadcast as well and same issue.",3/22/16,1
jasonbeverage,"Ok, i tried that with broadcast as well and same issue.",3/22/16,1
pomadchin,and what_Ñés the doSomething function?,3/22/16,1
pomadchin,and what_Ñés the doSomething function?,3/22/16,1
lossyrob,what_Ñés the stack trace on the null pointer exception?,3/22/16,1
lossyrob,what_Ñés the stack trace on the null pointer exception?,3/22/16,1
echeipesh,"That should work actually, spark should be picking it up and serializing the multiPolygon",3/22/16,1
echeipesh,"That should work actually, spark should be picking it up and serializing the multiPolygon",3/22/16,1
lossyrob,"@owcm I made a test that tries your color ramp against an elevation tile, and makes sure a spark application of coloring it matches a non-spark coloring, and it worked. Going to try to test more, but if there_Ñés some more code I can look at on github that would let me see what_Ñés going on, that would be helpful",3/22/16,1
lossyrob,"@owcm I made a test that tries your color ramp against an elevation tile, and makes sure a spark application of coloring it matches a non-spark coloring, and it worked. Going to try to test more, but if there_Ñés some more code I can look at on github that would let me see what_Ñés going on, that would be helpful",3/22/16,1
jasonbeverage,"The full stack trace is this```16/03/22 20:41:56 WARN scheduler.TaskSetManager: Lost task 0.0 in stage 0.0 (TID 0, ip-10-0-1-197.ec2.internal): java.lang.NullPointerException	at bda.geotrellis.ComputeVolume$.tileVolume(ComputeVolume.scala:48)	at bda.geotrellis.ComputeVolume$$anonfun$6.apply(ComputeVolume.scala:99)	at bda.geotrellis.ComputeVolume$$anonfun$6.apply(ComputeVolume.scala:99)	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)	at org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:209)	at org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:73)	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:73)	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)	at org.apache.spark.scheduler.Task.run(Task.scala:88)	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)	at java.lang.Thread.run(Thread.java:745)```",3/22/16,1
jasonbeverage,"The full stack trace is this```16/03/22 20:41:56 WARN scheduler.TaskSetManager: Lost task 0.0 in stage 0.0 (TID 0, ip-10-0-1-197.ec2.internal): java.lang.NullPointerException	at bda.geotrellis.ComputeVolume$.tileVolume(ComputeVolume.scala:48)	at bda.geotrellis.ComputeVolume$$anonfun$6.apply(ComputeVolume.scala:99)	at bda.geotrellis.ComputeVolume$$anonfun$6.apply(ComputeVolume.scala:99)	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)	at org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:209)	at org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:73)	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:73)	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)	at org.apache.spark.scheduler.Task.run(Task.scala:88)	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)	at java.lang.Thread.run(Thread.java:745)```",3/22/16,1
lossyrob,what_Ñés at `ComputeVolume.scala:48`?,3/22/16,1
lossyrob,what_Ñés at `ComputeVolume.scala:48`?,3/22/16,1
jasonbeverage,"```    volumeRDD.flatMap{ case(key,tile) => computeVolume(bcMetadata.value, key, tile, bcMultiPolygon.value) }             .reduceByKey(_ + _)             .collect()```",3/22/16,1
jasonbeverage,"```    volumeRDD.flatMap{ case(key,tile) => computeVolume(bcMetadata.value, key, tile, bcMultiPolygon.value) }             .reduceByKey(_ + _)             .collect()```",3/22/16,1
lossyrob,well the only thing I see there that seems like it would throw a null exception is `bcMetadata` being null or `bcMulitPolygon` being null,3/22/16,1
lossyrob,well the only thing I see there that seems like it would throw a null exception is `bcMetadata` being null or `bcMulitPolygon` being null,3/22/16,1
jasonbeverage,It's the line that tries to use the broadcasted variables.  I tried to use it without the broadcast variables and smae thing.,3/22/16,1
jasonbeverage,It's the line that tries to use the broadcasted variables.  I tried to use it without the broadcast variables and smae thing.,3/22/16,1
lossyrob,same thing on the same line?,3/22/16,1
lossyrob,same thing on the same line?,3/22/16,1
jasonbeverage,"Yeah.  And like I said, if I run it in local[*] it all works fine.",3/22/16,1
jasonbeverage,"Yeah.  And like I said, if I run it in local[*] it all works fine.",3/22/16,1
lossyrob,"because if you_Ñére not trying to access something out of the `null`, it really shouldn_Ñét throw that exception until you do",3/22/16,1
lossyrob,"because if you_Ñére not trying to access something out of the `null`, it really shouldn_Ñét throw that exception until you do",3/22/16,1
echeipesh,Is `computeVolume` a method on the `ComputeVolume` ?,3/22/16,1
echeipesh,Is `computeVolume` a method on the `ComputeVolume` ?,3/22/16,1
jasonbeverage,Yeah it is.,3/22/16,1
jasonbeverage,Yeah it is.,3/22/16,1
lossyrob,are you sure that its the multipolygon that is null? I would break out that statement into `val`_Ñés and try to pinpoint what the null is,3/22/16,1
lossyrob,are you sure that its the multipolygon that is null? I would break out that statement into `val`_Ñés and try to pinpoint what the null is,3/22/16,1
jasonbeverage,so in theory I shouldn't need to use a broadcast variable there or is that the correct way to go about it?,3/22/16,1
jasonbeverage,so in theory I shouldn't need to use a broadcast variable there or is that the correct way to go about it?,3/22/16,1
lossyrob,you shouldn_Ñét need to broadcast,3/22/16,1
lossyrob,you shouldn_Ñét need to broadcast,3/22/16,1
jasonbeverage,It's either the multipolygon or the metadata.,3/22/16,1
jasonbeverage,It's either the multipolygon or the metadata.,3/22/16,1
echeipesh,and there is actually an assignment to them in the same block like you_Ñéve shown `val metadata = `? That should be perfect for spark actually.,3/22/16,1
echeipesh,and there is actually an assignment to them in the same block like you_Ñéve shown `val metadata = `? That should be perfect for spark actually.,3/22/16,1
jasonbeverage,"Yeah right, there is.",3/22/16,1
jasonbeverage,"Yeah right, there is.",3/22/16,1
jasonbeverage,"Ok, that helps knowing I don't need to broadcast.  It's gotta be something else then.",3/22/16,1
jasonbeverage,"Ok, that helps knowing I don't need to broadcast.  It's gotta be something else then.",3/22/16,1
lossyrob,"yeah. the thing to consider with broadcasting is, if you_Ñére using Kryo, then broadcast vars are serialized using Kryo, but closures are serialized via java serializaiton",3/22/16,1
lossyrob,"yeah. the thing to consider with broadcasting is, if you_Ñére using Kryo, then broadcast vars are serialized using Kryo, but closures are serialized via java serializaiton",3/22/16,1
lossyrob,"and since both are giving you null, that_Ñés pretty strange",3/22/16,1
lossyrob,"and since both are giving you null, that_Ñés pretty strange",3/22/16,1
lossyrob,"and that it_Ñés the whole thing that is null, not some property within it",3/22/16,1
lossyrob,"and that it_Ñés the whole thing that is null, not some property within it",3/22/16,1
lossyrob,"```scalaval multiPolygon : MultiPolygon = ??? val metadata = layer.metadata layer.flatMap{ case(key,tile) =>   val crs =  metadata.crs  val x = multiPolygon.polygons  doSomething(multiPolygon, metadata) }```",3/22/16,1
lossyrob,"```scalaval multiPolygon : MultiPolygon = ??? val metadata = layer.metadata layer.flatMap{ case(key,tile) =>   val crs =  metadata.crs  val x = multiPolygon.polygons  doSomething(multiPolygon, metadata) }```",3/22/16,1
lossyrob,would that fail on the two vals?,3/22/16,1
lossyrob,would that fail on the two vals?,3/22/16,1
echeipesh,"Also as a note, functions will take `null` really happily without throwing until it_Ñés referenced",3/22/16,1
echeipesh,"Also as a note, functions will take `null` really happily without throwing until it_Ñés referenced",3/22/16,1
jasonbeverage,Let me try that and I'll let you know.  That'll tell me what's null then for sure.,3/22/16,1
jasonbeverage,Let me try that and I'll let you know.  That'll tell me what's null then for sure.,3/22/16,1
jasonbeverage,"```val mp = multiPolygon.polygonsval srs = metadata.crs```I did that and it dies on the multiPolygon line.  I know it's not null b/c I print out the MultiPolygon at the beginning of the program and it's fine.",3/22/16,1
jasonbeverage,"```val mp = multiPolygon.polygonsval srs = metadata.crs```I did that and it dies on the multiPolygon line.  I know it's not null b/c I print out the MultiPolygon at the beginning of the program and it's fine.",3/22/16,1
echeipesh,"polygons is a lazy val, going to test real quick that doesn_Ñét have some quirky behavior. Could you do the same test but with `multiPolygon.area` ?",3/22/16,1
echeipesh,"polygons is a lazy val, going to test real quick that doesn_Ñét have some quirky behavior. Could you do the same test but with `multiPolygon.area` ?",3/22/16,1
echeipesh,"oh, nvm, that_Ñés also a lazy val.",3/22/16,1
echeipesh,"oh, nvm, that_Ñés also a lazy val.",3/22/16,1
jasonbeverage,I just switched the order so the metadata is first and it dies there too.,3/22/16,1
jasonbeverage,I just switched the order so the metadata is first and it dies there too.,3/22/16,1
lossyrob,initialization might be an issue,3/22/16,1
lossyrob,initialization might be an issue,3/22/16,1
echeipesh,:scratching my head:,3/22/16,1
echeipesh,:scratching my head:,3/22/16,1
lossyrob,stop cratching you head that_Ñés bad for you,3/22/16,1
lossyrob,stop cratching you head that_Ñés bad for you,3/22/16,1
echeipesh,really weirdly appropriate translation there,3/22/16,1
echeipesh,really weirdly appropriate translation there,3/22/16,1
lossyrob,I would say it might be an initialization error if it didnt work in local[*],3/22/16,1
lossyrob,I would say it might be an initialization error if it didnt work in local[*],3/22/16,1
lossyrob,is it potentially a binary incompatibility problem? Like the deserialization is failing and that_Ñés somehow producing a null?,3/22/16,1
lossyrob,is it potentially a binary incompatibility problem? Like the deserialization is failing and that_Ñés somehow producing a null?,3/22/16,1
echeipesh,I wonder if you could even assign `val x = 33` outside of the flatMap and reference it inside.,3/22/16,1
echeipesh,I wonder if you could even assign `val x = 33` outside of the flatMap and reference it inside.,3/22/16,1
jasonbeverage,I'll try it.,3/22/16,1
jasonbeverage,I'll try it.,3/22/16,1
jasonbeverage,"That could be an issue, I'm using geotrellis-ec2-cluster but I upgraded spark to 1.5.2 manually on the master and followers.",3/22/16,1
jasonbeverage,"That could be an issue, I'm using geotrellis-ec2-cluster but I upgraded spark to 1.5.2 manually on the master and followers.",3/22/16,1
jasonbeverage,and I probably screwed something up.,3/22/16,1
jasonbeverage,and I probably screwed something up.,3/22/16,1
jasonbeverage,Although doing an ETL ingest worked fine.,3/22/16,1
jasonbeverage,Although doing an ETL ingest worked fine.,3/22/16,1
jasonbeverage,Yeah something is busted.  I did x = 33 and the value in the flatMap of x is 0,3/22/16,1
jasonbeverage,Yeah something is busted.  I did x = 33 and the value in the flatMap of x is 0,3/22/16,1
lossyrob,that is bizarre,3/22/16,1
lossyrob,that is bizarre,3/22/16,1
jasonbeverage,I'm going to test this on EMR later and see if it's some configuration issue.,3/22/16,1
jasonbeverage,I'm going to test this on EMR later and see if it's some configuration issue.,3/22/16,1
jasonbeverage,Although I've got no idea what it would be.,3/22/16,1
jasonbeverage,Although I've got no idea what it would be.,3/22/16,1
lossyrob,fun with Spark :/,3/22/16,1
lossyrob,fun with Spark :/,3/22/16,1
jasonbeverage,I know just enough spark and scala to be right on the line of useful and dangerous :),3/22/16,1
jasonbeverage,I know just enough spark and scala to be right on the line of useful and dangerous :),3/22/16,1
lossyrob,I think that describes everyone using spark lol,3/22/16,1
lossyrob,I think that describes everyone using spark lol,3/22/16,1
jasonbeverage,I'm going to try this on EMR later and see if it's a configuration issue.,3/22/16,1
jasonbeverage,I'm going to try this on EMR later and see if it's a configuration issue.,3/22/16,1
owcm,@lossyrob  I will try to get it up on gitub will you can see it tonight,3/22/16,1
owcm,@lossyrob  I will try to get it up on gitub will you can see it tonight,3/22/16,1
lossyrob,"ok cool, thanks @owcm ",3/23/16,1
lossyrob,"@owcm also it might be somethign with the datatype, do you have a geotiff that I can use to test?",3/23/16,1
owcm,"Yes, I can grab one one of the geotiffs",3/23/16,1
owcm,@lossyrob  https://drive.google.com/folderview?id=0B1YZbN2gCsmSUkJiSmRVbVctWEU&usp=sharing,3/23/16,1
lossyrob,which tif should I dl?,3/23/16,1
owcm,18STJ4921.tif,3/23/16,1
lossyrob,"cool, thanks.",3/23/16,1
lossyrob,Is this something like your going for? (on another elevation tile),3/23/16,1
lossyrob,http://i.imgur.com/y7RvSrs.png,3/23/16,1
owcm,Yes,3/23/16,1
lossyrob,"And so you_Ñére actually doing that color ramp to the slope calc, not the actual elevation values?",3/23/16,1
owcm,Which colorMap did you use?,3/23/16,1
lossyrob,"```scala      val colorMap =        ColorMap(          Map(            3.5 -> RGB(0,255,0).int,            7.5 -> RGB(63, 255 ,51).int,            11.5 -> RGB(102,255,102).int,            15.5 -> RGB(178, 255,102).int,            19.5 -> RGB(255,255,0).int,            23.5 -> RGB(255,255,51).int,            26.5 -> RGB(255,153, 51).int,            31.5 -> RGB(255,128,0).int,            35.0 -> RGB(255,51,51).int,            40.0 -> RGB(255,0,0).int          )        )```",3/23/16,1
owcm,"Yes, I was trying to apply that to the result of a slope calc, correct.  Not the raw elevation values.",3/23/16,1
lossyrob,ok. was it always with the result of operations that you were seeing this behavior?,3/23/16,1
lossyrob,"more specifically, was it always with focal operations?",3/23/16,1
lossyrob,@lossyrob is taking shots in the dark,3/23/16,1
lossyrob,"This is what I get with slope, computed with just a single tile, but when I run it through spark I get the same thing in tile form:",3/23/16,1
lossyrob,http://i.imgur.com/sV3JsKO.png,3/23/16,1
lossyrob,wait_Ñ_slope returns radians,3/23/16,1
owcm,Result of ops yes. ,3/23/16,1
lossyrob,oh actually it_Ñés in degrees,3/23/16,1
owcm,"  val slopeRdd = rddItem.slope(1.0), I thought it was degrees.  Now I am doing this using TileLayer RDDs",3/23/16,1
lossyrob,"ok, I see the tile you gave me when I compute the slope, it_Ñés all green, in both the spark and the local case",3/23/16,1
lossyrob,"```console  Metadata:    STATISTICS_MAXIMUM=89.99965948335    STATISTICS_MEAN=89.986861077265    STATISTICS_MINIMUM=69.416556890551    STATISTICS_STDDEV=0.030446666789966```",3/23/16,1
lossyrob,that doesn_Ñét seem right,3/23/16,1
owcm,"With qgis I get these stats:STATISTICS_MAXIMUM=208.60534667969STATISTICS_MEAN=171.55848580571STATISTICS_MINIMUM=143.64501953125STATISTICS_STDDEV=14.130230050985",3/23/16,1
lossyrob,thats for the raw data,3/23/16,1
lossyrob,I did that on the slope,3/23/16,1
lossyrob,via gdalinfo,3/23/16,1
lossyrob,but if that_Ñés degrees from horizontal that would mean most of the slope is almost 90 degrees,3/23/16,1
owcm,"Right, does not sound right",3/23/16,1
lossyrob,"ok, it looks like the green is coming from there being no fallback color",3/23/16,1
lossyrob,"which we can_Ñét actually do right now because until we release with @moradology_Ñés fix around fallback colors and indexed png_Ñés, it won_Ñét actually write to the png",3/23/16,1
lossyrob,but you should be able to change the highest color with something like,3/23/16,1
lossyrob,"```scala      val colorMap =        ColorMap(          Map(            3.5 -> RGB(0,255,0).int,            7.5 -> RGB(63, 255 ,51).int,            11.5 -> RGB(102,255,102).int,            15.5 -> RGB(178, 255,102).int,            19.5 -> RGB(255,255,0).int,            23.5 -> RGB(255,255,51).int,            26.5 -> RGB(255,153, 51).int,            31.5 -> RGB(255,128,0).int,            35.0 -> RGB(255,51,51).int,            40.0 -> RGB(255,0,0).int,            90.0 -> RGB(0, 0, 255).int // color all very high numbers as blue          )        )```",3/23/16,1
owcm,Okay,3/23/16,1
lossyrob,"`The use of a z-factor is essential for correct slope calculations when the surface z units are expressed in units different from the ground x,y units.`",3/23/16,1
lossyrob,ground units are in EPSG:4326,3/23/16,1
owcm,"Well I have been reprojecting to WebMercator, before doing the Slope calculation",3/23/16,1
lossyrob,"ok, that_Ñés what I_Ñém trying now",3/23/16,1
lossyrob,this is what I get,3/23/16,1
lossyrob,http://i.imgur.com/AWLZL06.png,3/23/16,1
lossyrob,and the same with running it through spark. does that look like what your getting?,3/23/16,1
lossyrob,gdalinfo -stats gives,3/23/16,1
lossyrob,"```console    STATISTICS_MAXIMUM=53.560650400723    STATISTICS_MEAN=3.7320345438362    STATISTICS_MINIMUM=0.0012267263399767    STATISTICS_STDDEV=3.1241776538249```",3/23/16,1
lossyrob,so that seems pretty in line,3/23/16,1
owcm,"Yea, I am currently looking at the file in qgis, and what you are getting makes sense.    I need to go back and retest.   I know in the Spark environment where I am using the Pyramiding to create the Z/X/Y tiling I was getting pure green.  So it must be something I am doing wrong.",3/23/16,1
owcm,I will test on the individual file again and make sure I am getting the same results as you.,3/23/16,1
jasonbeverage,"I figured out my issue earlier, sorry for the noise.  I missed this little tidbit from the spark documentation""Note that applications should define a main() method instead of extending scala.App. Subclasses of scala.App may not work correctly.""",3/23/16,1
jasonbeverage,that part should be in bold :),3/23/16,1
jasonbeverage,Not subclassing App and just doing a regular main worked like a charm.,3/23/16,1
dwins,"not too surprising, `App` does weird stuff to make `main` call the constructor",3/23/16,1
echeipesh,"I guess its the result of the `DelayedInit` machinery in `App`, still crazy consequence.",3/23/16,1
pomadchin,yes it always causes unexpected bugs ._.,3/23/16,1
lossyrob,"Initialization bugs. They produce weird nulls. Delayed init will do it, I usually see it with trait initialization order.",3/23/16,1
jasonbeverage,"Yeah it was an annoying issue b/c I came into this not knowing Scala, and I saw lots of applications using App (even the ETL Ingest apps) so I figured that was the way to go :)",3/23/16,1
pomadchin,also as a workaround with App extended objects it_Ñés possible just to use call-by-name everywhere :D so lazy vals and functions are your friends,3/23/16,1
jasonbeverage,for now I'm just going to go with mains :)  I still need to level up my scala-fu,3/23/16,1
pomadchin,:+1:,3/23/16,1
lossyrob,"yeah I switched to never using the App stuff ever, it_Ñés less magical and less room for bugz",3/23/16,1
dwins,now i'm wondering if macro annotations could let you have a bare def converted to a runnable object,3/23/16,1
dwins,`@entryPoint def MyApp(args: Array[String]): Unit` expanding to an object with a main method seems convenient,3/23/16,1
pomadchin,"probably we can implement it, and that_Ñés really not hard to implement smth like that",3/23/16,1
pomadchin,but it seems to me that to use it we need to pull macro paradise plugin,3/23/16,1
dwins,guess it can go on the back burner for now,3/23/16,1
moradology,Do we have a style-consensus on companion objects after classes?,3/23/16,1
moradology,or is that not something we care about,3/23/16,1
echeipesh,I_Ñém not sure we have a strict style. I prefer to put it after myself.,3/23/16,1
lossyrob,I like to put it before,3/23/16,1
lossyrob,if it_Ñés smaller then the class or trait def,3/23/16,1
pomadchin,:D,3/23/16,1
pomadchin,think the answer is depends,3/23/16,1
pomadchin,in some cases we put it under the trait in some above ._.,3/23/16,1
moradology,cool. i'll just stay hands off then,3/23/16,1
moradology,"as i write docs, that is",3/23/16,1
moradology,"(my preference is definitely for companions after, fwiw)",3/23/16,1
moradology,"adding documentation, there are some methods that maybe could be removed: https://github.com/geotrellis/geotrellis/blob/master/vector/src/main/scala/geotrellis/vector/affine/PointTransformationMethods.scala",3/23/16,1
moradology,"you can't rescale a point or rotate a point, forinstance",3/23/16,1
moradology,but maybe we want that for parity with methods on other geoms? I'm curious to hear thoughts on that,3/23/16,1
lossyrob,yeah I_Ñém sure it was done because those things are available generally on geometries,3/23/16,1
dwins,would you say that you can't multiply a number by 1 or add 0 to it?,3/23/16,1
moradology,that's a fair point,3/23/16,1
lossyrob,:fire:,3/23/16,1
lossyrob,"the thing is, those methods aren_Ñét written against geometry genericly",3/23/16,1
lossyrob,"so there_Ñés probably a better way to do it where it makes more sense to treat it generically, and call rotate on a point",3/23/16,1
moradology,the 1 and 0 point makes a lot of sense for scale - an infinitely small scale multiplied by whatever remains infinitely small,3/23/16,1
moradology,rotation over a point is less obviously analogous,3/23/16,1
lossyrob,"but the way it_Ñés set up, not sure",3/23/16,1
dwins,I guess a generic implementation would suffer a bit performancewise because of having to test the class to find which wrapper to use,3/23/16,1
lossyrob,0 * x seems to be a rotation analogy,3/23/16,1
moradology,perhaps. the intuitions here are messy,3/23/16,1
dwins,I see that we have https://github.com/geotrellis/geotrellis/blob/master/vector/src/main/scala/geotrellis/vector/affine/AffineTransformation.scala#L18-L25,3/23/16,1
dwins,"having both the MethodExtensions version of these methods and the ""an affine transform is a function"" version seems unnecessary",3/23/16,1
lossyrob,yeah,3/23/16,1
dwins,where are the implicit conversions to PointTransformationMethods anyway?,3/23/16,1
dwins,oh i see. they're in the geotrellis.vector package object,3/23/16,1
dwins,"```scala> Point(1,2).scale(2,4)res1: geotrellis.vector.Point = POINT (2 8)```",3/23/16,1
dwins,"It's not a no-op anyway because you're not scaling the point, you're scaling the plane that the point is on",3/23/16,1
dwins,@moradology @lossyrob ,3/23/16,1
moradology,aaaah. that's a big difference,3/23/16,1
lossyrob,"oh, weird",3/23/16,1
dwins,makes sense if you think about it because applying a transform works vertex-by-vertex on a line or polygon. if scaling a point was a no-op then scaling anything else would be as well,3/23/16,1
lossyrob,yeah that makes sense,3/23/16,1
lossyrob,"you need to scale from some center point, which is always 0,0",3/23/16,1
dwins,exactly,3/23/16,1
moradology,cool. thanks @dwins ,3/23/16,1
moradology,that also helps to make sense of how shear is supposed to work given only two numbers as arguments,3/23/16,1
jasonbeverage,"Hey all, has the old ""clobber"" option from ETL been replaced with anything else?",3/24/16,1
jasonbeverage,I'm doing an ingest to HDFS over an existing layer and I get a failure b/c of a directory already existing in hdfs,3/24/16,1
jasonbeverage,Like Caused by: java.lang.Exception: Directory already exists: /layers/mylayer/18,3/24/16,1
pomadchin,@jasonbeverage was just removed ): https://github.com/geotrellis/geotrellis/commit/4dc26f43a9a4b394d3f86b4d69e2efba46348ca0,3/24/16,1
pomadchin,guess it would be not bad to have such option though,3/24/16,1
jasonbeverage,"No worries, just making sure I knew what was going on :)",3/24/16,1
jasonbeverage,"I've got a couple of scripts I'm running through that ingest data, do something on the ingested data and then just report timings.  And some of the tests use the same data so the second run through things were dying and I didn't notice that directory exists error deep in the stack trace until now :)",3/24/16,1
lossyrob,"the clobber option was kind of a lie_Ñ_it wasn_Ñét actually doing the thing it was proporting to do, so we took it out",3/24/16,1
jasonbeverage,Makes sense.,3/24/16,1
jasonbeverage,"I've got a conceptual question about deploying and scaling out a spray based server using geotrellis.  I've got a couple of little server programs that do various things based on your landsatdemo @lossyrob.  I'm able to run them using spark-submit, that works fine.  Normally if I were want to scale out a web service, I'd just run multiple instances of the same behind an ELB, but I'm trying to understand the relationship between how spray and spark relate.",3/24/16,1
jasonbeverage,"For example, if you spark-submit a spray server on a 1000 node spark cluster, is there any benefit to trying to scale out the spray server or is spark somehow doing that?",3/24/16,1
lossyrob,well there_Ñés the bottleneck of handling requests,3/24/16,1
lossyrob,and then the bottleneck of running thing through the spark master,3/24/16,1
lossyrob,they_Ñére both two points of contention that should be considered,3/24/16,1
lossyrob,"if you have 1000 spray service endpoints all making job requests to the same spark master, that_Ñés going to cause a lot of contention",3/24/16,1
lossyrob,"you can actually run multiple spark masters against the same cluster; however this is a more advnaced setup that we haven_Ñét actually put into practice yet, so I can_Ñét speak to it too deeply",3/24/16,1
lossyrob,you could also run smaller clusters that load out subsets of data into ephemeral storage from a large distributed store like S3 and parallelize tasking that way,3/24/16,1
jasonbeverage,"Yeah, as just a raw tile server I'm curious as to what the benefits of using spark really are.",3/24/16,1
lossyrob,"also if your spark jobs are running on small subsets, you can just have them load from S3 and run the spark _ÑÒcluster_Ñù on a single server",3/24/16,1
lossyrob,"ah, so for serving out tiles",3/24/16,1
jasonbeverage,Like just on serving out tiles.,3/24/16,1
lossyrob,unless you_Ñére doing some grouping operations,3/24/16,1
lossyrob,it doesn_Ñét need spark,3/24/16,1
jasonbeverage,"Yeah sorry, just using the TileReader",3/24/16,1
lossyrob,yeah. no spark required,3/24/16,1
lossyrob,spark does all the heavy lifting in the preprocesses step,3/24/16,1
lossyrob,so that the tile readers can just read what tiles they need and use geotrellis-raster_Ñés core types to do the calculations and paint tiles,3/24/16,1
lossyrob,which is much faster than going throuhg spark-anything,3/24/16,1
jasonbeverage,Why am I thinking I needed spark to even instantiate a tile reader :)  ,3/24/16,1
lossyrob,"we_Ñére talking about making a way to run the LayerQuery_Ñés against somethign that just does parallel fetches of tiles, so even in small grouping cases you can avoid using spark",3/24/16,1
jasonbeverage,Was that for Accumulo maybe?,3/24/16,1
lossyrob,hopefully not ,3/24/16,1
lossyrob,I thougt we got rid of that,3/24/16,1
jasonbeverage,"Ok, that might be true.  I got that from looking at the spark version of the chatta demo I think.  https://github.com/pomadchin/geotrellis-chatta-demo/blob/spark-version/geotrellis/src/main/scala/geotrellis/chatta/ChattaServiceActor.scala",3/24/16,1
lossyrob,"so, that chatta demo calculates breaks",3/24/16,1
lossyrob,"which is an operation that happens on a layer, so an RDD of tiles, so it needs the spark stuff",3/24/16,1
lossyrob,"but for the actual tms service endpoint, no SparkContext should be required",3/24/16,1
jasonbeverage,Ok very good to know.,3/24/16,1
jasonbeverage,So a spray service should just be able to run using java -cp then.,3/24/16,1
jasonbeverage,This is the reason I thought I had to have a spark context to use a tile reader:  https://github.com/geotrellis/geotrellis/blob/master/spark/src/main/scala/geotrellis/spark/io/hadoop/HadoopTileReader.scala#L21,3/24/16,1
jasonbeverage,"The tile readers have implicit spark contexts, so I get build errors unless I define one.",3/24/16,1
lossyrob,dang. that_Ñés my fault,3/24/16,1
lossyrob,is it all the tile readers or just hadoop?,3/24/16,1
jasonbeverage,Maybe just hadoop,3/24/16,1
jasonbeverage,Accumulo doesn't and neither does s3,3/24/16,1
jasonbeverage,"So just hadoop, there ya go.",3/24/16,1
lossyrob,oh actually hadoop does need one,3/24/16,1
lossyrob,because it doesn_Ñét actually have the ability to read a single tile not via spark,3/24/16,1
lossyrob,"which is something we would just have to work out, but no pre-0.10",3/24/16,1
jasonbeverage,Ok awesome.,3/24/16,1
jasonbeverage,That helps me wrap my head around how to scale up tile serving then and makes more sense.,3/24/16,1
jasonbeverage,I could definitely see the benefit of using spark for distributed raster calculations but couldn't get how it would help for just serving tiles :),3/24/16,1
lossyrob,yeah all of the light lifting stuff should just use the core geotrellis stuff,3/24/16,1
owcm,Question: I am loading a set of  elevation rasters into a TileLayerRDD layout and doing my map algebra ops.  I wanted to load a landcover raster into another TileLayerRDD and perform some additional map algebra ops with my first  TileLayerRDD.  The landcover  raster extent overlays the rasters in my first TileLayerRDD.  I am using the same scheme Layout schemes  and Tiling approaches.  I was assuming the SpatialKeys would be in alignment so I could do operations between the two TileLayerRDD sets.  Does this sound logical?,3/25/16,1
lossyrob,"If they are both FloatingLayoutSchemes that were deteremined by the layers, no",3/25/16,1
lossyrob,but if they are both manually created LayoutDefinitions (e.g. you set the extent and resolution explicitly),3/25/16,1
lossyrob,or if they are ZoomedLayoutScheme for the same CRS,3/25/16,1
lossyrob,then yes,3/25/16,1
owcm,"Okay, I am using  ZoomedLayoutScheme",3/25/16,1
owcm,Should conceivably work then.,3/25/16,1
lossyrob,"yup, your good to go. If you were to join by keys, those tiles would align",3/25/16,1
owcm,"Well I will show my ignorance,  how do you do that?```val speedTPILCRDD = speedTPIRDD.localMultiply( lcModifiedRDD ).join()```",3/25/16,1
owcm,"Does this sound right:``` val speedTPILCRDD = speedTPIRDD.join(lcModifiedRDD).mapValues { case (tile1, tile2) => Multiply( tile1, tile2)}```",3/25/16,1
echeipesh,"Both are good, localMultiply will do the join itself",3/25/16,1
owcm,@echeipesh ,3/25/16,1
owcm,@echeipesh  Thanks,3/25/16,1
owcm,@echeipesh One thing I forgot to consider is that one set of data is 1 m and the other is 30m.  When dealing with individual tiles.  I have done a resample operation that has worked.   When things are in RDD format can you resample?,3/25/16,1
dwins,"Sorry guys, on a pretty bad internet connection right now. Today I was just planning on getting #1421 through review and seeing if there are any existing ignored tests in `proj4/src/test/` that could be enabled",3/25/16,1
lossyrob,https://github.com/geotrellis/geotrellis/blob/master/raster/src/main/scala/geotrellis/raster/merge/SinglebandTileMergeMethods.scala#L107,3/25/16,1
lossyrob,should look like https://github.com/geotrellis/geotrellis/blob/master/raster/src/main/scala/geotrellis/raster/merge/SinglebandTileMergeMethods.scala#L118,3/25/16,1
lossyrob,@dwins that sounds good. I_Ñéll be merging everything in and publishing tonight/tomorrow morning.,3/25/16,1
lossyrob,"We decided to go with an 0.10.0-RC3, since there was a bug discoverd in RC2 and Eugene suggested that we have a no-change RC before we do the final, which is a good idea.",3/25/16,1
dwins,+1 to the final release having no changes from the last RC,3/25/16,1
lossyrob,"@owcm if they_Ñére zoom leveled, they were resampled during ingest. So it_Ñés a matter of, are those layers at the same zoom level, then yes you can join, otherwise you_Ñéll have to resample the smaller zoom to the bigger zoom",3/25/16,1
owcm,@lossyrob Ran a test and no they do not have the same zoom level.   The raster elevation 1m data is resolving to a Zoom level of 17.  The Landcover data that is at 30 meter when ingested  has a a zoom level of 13,3/25/16,1
owcm,@lossyrob so I guess the #L108 and #118 links to SinglebandTileMerge for my benefit?,3/25/16,1
owcm,@lossyrob guess in the RDD realm I was trying to understand how to resample the smaller to the bigger zoom.,3/25/16,1
lossyrob,@owcm https://github.com/geotrellis/geotrellis/blob/master/spark/src/main/scala/geotrellis/spark/resample/ZoomResample.scala,3/25/16,1
lossyrob,"`rdd.resampleToZoom(13, 17, geotrellis.raster.resample.Bilinear)`",3/25/16,1
owcm,"@lossyrob, Nice thank you forwarding that.",3/25/16,1
pomadchin,@lossyrob would it be usefull to have such function but for `Tile` / `Raster` types (tile server with extrazooming as a use case)?,3/25/16,1
lossyrob,"yeah, I_Ñéve implmented that other places",3/25/16,1
lossyrob,not sure where the best place to do that for tiles is,3/25/16,1
lossyrob,"but it_Ñés not hard, since we have Tile resampling and can get the extent from the metadata, and get the desired tile size from the ZoomedLayoutScheme",3/25/16,1
lossyrob,"spent some time chasing down a bug today, looks like RC3 is going to be released over the weekend instead of tonight, just as a heads up",3/25/16,1
owcm,"I wanted to try bit packing the Int values in the tiles I am creating.  Packing a value in the 'R' 8 bit value, 'G' bit value etc.  Then write out the file as a PNG.  Is there any way I can call renderPng and have the file written without any color translation.  Just have the raw bits in my Tile cells written to the png file. Thanks in advance",3/27/16,1
lossyrob,just call renderPng without a color map,3/27/16,1
owcm,"@lossyrob  Ended up trying that out and worked out fine, thanks.",3/28/16,1
lokifacio,,3/28/16,1
lossyrob,"```scala        val pngs = rdd.renderPng(colorMap).mapValues(_.bytes)        pngs.saveToS3(keyToPath, { putObject =>          putObject.withCannedAcl(CannedAccessControlList.PublicRead)        })```",3/28/16,1
lossyrob,"^- example of modifying the ACL in a saveToS3 call, proposed ",3/28/16,1
bneff,Is there a way to rasterize a multi polygon as a BitArrayTile instead of Array[Int]?,3/28/16,1
lossyrob,I think you_Ñéd have to do the rasterization manually,3/28/16,1
echeipesh,By manually he means using this with a mutable tile: https://github.com/geotrellis/geotrellis/blob/master/raster/src/main/scala/geotrellis/raster/rasterize/Rasterizer.scala#L110,3/28/16,1
echeipesh,Mutable `ArrayTile` : ),3/28/16,1
lossyrob,"```scalaval tile = BitArrayTile.empty(rasterExtent.cols, rasterExtent.rows)PolygonRasterizer.foreachCellByPolygon(polygon, rasterExtent) { (col, row) =>  tile.set(col, row, 1)}```",3/28/16,1
lossyrob,something like that,3/28/16,1
lossyrob,"or there_Ñés newer API around that, let me do a take 2",3/28/16,1
lossyrob,"```scalaval tile = BitArrayTile.empty(rasterExtent.cols, rasterExtent.rows)rasterExtent.foreach(polygon) { (col, row) =>  tile.set(col, row, 1)}```",3/28/16,1
bneff,@lossyrob @echeipesh thank you both!,3/28/16,1
lokifacio,I'm getting this error on Travis CI https://travis-ci.org/geotrellis/geotrellis/jobs/119189646#L9890. Any clues?,3/29/16,1
moradology,Looks like it can't find JAI core: ` javax.media#jai_core;1.1.3!jai_core.jar`,3/29/16,1
moradology,i'm seeing a 404 right now from maven: http://central.maven.org/maven2/javax/media/jai_core/1.1.3/jai_core-1.1.3.jar,3/29/16,1
lokifacio,"ok, so it's not related to my PR, right?",3/29/16,1
moradology,I believe not,3/29/16,1
moradology,the java gods are angry,3/29/16,1
moradology,Looking to the POM: http://repo1.maven.org/maven2/javax/media/jai_core/1.1.3/jai_core-1.1.3.pom,3/29/16,1
moradology,it appears as though there's some kind of DNS issue at the moment...,3/29/16,1
moradology,https://jai.dev.java.net/binary-builds.html,3/29/16,1
moradology,:disappointed: ,3/29/16,1
lossyrob,jai is not on maven central,3/29/16,1
lossyrob,it_Ñés on the boundless repo,3/29/16,1
lossyrob,"and this happens quite often. I hate that dependency, but it_Ñés transitive, with the shapefile project",3/29/16,1
lokifacio,so I need to do something once it is available to check my PR in travis? ,3/29/16,1
dwins,that seems unnecessary,3/29/16,1
mchendricks1,"Hello. Getting an error with the latest landsat tutorial (https://github.com/geotrellis/geotrellis-landsat-tutorial)[error] (*:update) sbt.ResolveException: unresolved dependency: com.azavea.geotrellis#geotrellis-spark_2.10;0.10.0-SNAPSHOT: not found[error] Total time: 32 s, completed Mar 29, 2016 10:24:23 AM",3/29/16,1
lossyrob,@mchendricks1  https://github.com/geotrellis/geotrellis-landsat-tutorial/blob/master/build.sbt#L33 should say RC2 not SNAPSHOT,3/29/16,1
lossyrob,fixing that now,3/29/16,1
lossyrob,thanks for pointing that out,3/29/16,1
pomadchin,"@lokifacio @moradology https://github.com/geotrellis/geotrellis/blob/master/shapefile/build.sbt#L9http://download.osgeo.org/webdav/geotools/javax/media/jai_core/1.1.3/",3/29/16,1
mchendricks1,You're welcome. Thanks for getting it fixed quickly.,3/29/16,1
pomadchin,anyway weird exception with jai_core ,3/29/16,1
lokifacio,@pomadchin finally I switched to `TargetCell` instead of `FocalTarget`/`CellDomain`,3/29/16,1
SDeathLeaDer,,3/29/16,1
SDeathLeaDer,,3/29/16,1
SDeathLeaDer,"Hi, i tested geotrellis-landsat-tutorial of geotrellis and all its ok, but the problem is that when i want to deploy in cluster mode. I modified only .setMaster(""spark://192.168.1.102:7077""). The error is: https://gist.github.com/SDeathLeaDer/151116ede3e2d5a7b151",3/29/16,1
SDeathLeaDer,,3/29/16,1
SDeathLeaDer,"Thanks also, i resolved it. I only should to have  the version of client and server of spark on geotrellis  the same. ",3/29/16,1
lossyrob,@SDeathLeaDer which client/server version do you mean?,3/29/16,1
SDeathLeaDer,"@lossyrob means that, I had to change the version in the file build.sbt in the part of spark-core of project  by the same version that i used for deploying my cluster (start-all.sh).",3/29/16,1
lossyrob,oh gotcha,3/29/16,1
lossyrob,"you could also make the spark dependency as `% provided` so that it doesn_Ñét get bundled into the assembly, so it would just use the `spark-submit` spark dependencies",3/29/16,1
stliang,What happened to http://geotrellis.io/tutorials/webservice/spray/ ? this link is broken,3/29/16,1
SDeathLeaDer,"@lossyrob  i have another problem in the same example of geotrellis-landsat-tutoria, when i want to ingest in cluster mode. Its throws me: ""java.lang.ClassNotFoundException: geotrellis.spark.io.hadoop.formats.MultibandGeoTiffInputFormat"". I look that i had the library in my project.",3/29/16,1
lossyrob,@moradology any insight on the broken spray link?,3/29/16,1
lossyrob,@SDeathLeaDer how are you running it? Did you do `assembly`?,3/29/16,1
SDeathLeaDer,"@lossyrob  i run this by console ""./sbt compile run"", i only changed the version 1.6.0 of spark-core from build.sbt. I think so that class MultibandGeoTiffInputFormat.java doesn't exists in version of 1.6.0 :( . Which is the equivalent class ?.",3/29/16,1
lossyrob,MultibandGeoTiffInputFormat is a GeoTrellis class,3/29/16,1
lossyrob,so it does not exist in spark-core at all,3/29/16,1
lossyrob,how are you setting it to run in cluster mode?,3/29/16,1
lossyrob,"the way to do this would be to set up your spark cluster, then use the `./sbt assembly` command to build a fat jar (a jar that has all of the dependencies and everything baked in)",3/29/16,1
lossyrob,and then use `spark-submit`to submit the job to spark,3/29/16,1
lossyrob,"the landsat tutorial is not currently set up to run in a clustered environment, it was more exemplary of running spark in a local setting_Ñ_so the level of effort to run it in a cluster is higher than setting config and running sbt run (although it is very possible, and I believe someone has already done so)",3/29/16,1
lossyrob,the JAR you get from `./sbt assembly` woudl be the `<application-jar>` mentioned in this documentation: http://spark.apache.org/docs/latest/submitting-applications.html,3/29/16,1
moradology,@stliang that link seems to have gone missing with some recent changes to the documentation structure of the site. This `.rst` should have the content you're after until a new AMI gets cut https://github.com/geotrellis/geotrellis-site/blob/d4d9d447b29ea8f4937053708fc2a8bf5f3157c0/docs/tutorials/webservice/spray.rst,3/30/16,1
lossyrob,0.10.0-RC3 is out https://groups.google.com/forum/#!topic/geotrellis-user/FVKT_pFE_Xk,3/30/16,1
pomadchin,:tada:,3/30/16,1
lokifacio,@echeipesh what do you think of my PR? any suggestion?,3/30/16,1
pomadchin,"@lokifacio we are concentrated on release, sry for slow replies :) guyz will have a look, thanks for your pr!",3/30/16,1
lokifacio,"@pomadchin sure, just asking him because he posted about the use case",3/30/16,1
lokifacio,"is there any chance it can be included in this release? if not, you should at least fix the median/mode CellWiseCalculation setValue method which uses setDouble instead of Int set",3/30/16,1
echeipesh,@lokifacio I_Ñém chewing on it. I think it makes a lot of sense that we have ability to do something like this in a single pass. Both keeping the region borders through focal calc and using focal calculations to fill in NoData regions make a lot of sense.,3/30/16,1
echeipesh,Alternatives require doing some masking pre/post operation and seem like they would be unweildy.,3/30/16,1
echeipesh,Have you ever seen cases where the decision to populate the cell is more comlicated? For instance dependant on the cell value  (other than data/nodata) or the value of cells in the focal neighborhood?,3/30/16,1
lokifacio,"No, although it could make sense for someone...I was considering passing a filter function instead but thought that would make the api less clear  for most common cases",3/30/16,1
echeipesh,"It_Ñés probably not going to make it into `0.10.0`, feature freeze is supposed to be activate right now. But the `0.10.1` is only 3 months away from release to get us into a more regular cycle.",3/30/16,1
lokifacio,"how do you get a RDD[(ProjectedExtent, Tile)] (with a single tile) from a Raster[Tile]?",3/30/16,1
lokifacio,"```val pe = ProjectedExtent(tile.extent, crs)val rdd = sc.parallelize(Array((pe, tile)))```is this the right way?",3/30/16,1
lokifacio,"well, actually it should be: `sc.parallelize(Array[(ProjectedExtent, Tile)]((pe, tile)))`",3/30/16,1
lossyrob,@lokifacio where were you saying about cellwise calculation should use setDouble?,3/30/16,1
lokifacio,Median and Mode if my memory doesn't fail me,3/30/16,1
lokifacio,Median.scala at line 61,3/30/16,1
lokifacio,"IntArrayTileResult should use the int version, right?",3/30/16,1
lossyrob,eayh,3/30/16,1
lossyrob,I think Eugene might have already made that change,3/30/16,1
lossyrob,https://github.com/geotrellis/geotrellis/blob/master/raster/src/main/scala/geotrellis/raster/mapalgebra/focal/Median.scala#L61,3/30/16,1
lossyrob,https://github.com/geotrellis/geotrellis/blob/master/raster/src/main/scala/geotrellis/raster/mapalgebra/focal/Mode.scala#L70,3/30/16,1
lokifacio,"well, it seems he didn't",3/30/16,1
lokifacio,"is should be result.set(x, y, mode) instead of setDouble",3/30/16,1
owcm,"Seems like Geotrellis has support to read GeoJson files.   Is there an example anyone can point me to, to see how it is done.  Looking at Master code I did not see any clear examples.",3/30/16,1
dwins,there's an example of doing it with a String here: https://github.com/geotrellis/geotrellis/blob/master/docs/vector/geojson-support.md,3/30/16,1
dwins,@owcm and you can see some places in the tests where a file is loaded to a String before being parsed with that approach. https://github.com/geotrellis/geotrellis/blob/master/vector-test/src/test/scala/spec/geotrellis/vector/dissolve/DissoveMethodsSpec.scala#L24,3/30/16,1
lossyrob,@lokifacio oh right. I thought that was the double version,3/30/16,1
lossyrob,actually,3/30/16,1
lossyrob,we don_Ñét have an Int/Double version,3/30/16,1
lossyrob,so it should be setDouble to handle both cases,3/30/16,1
lossyrob,thoguht it returns an IntArrayTileResult,3/30/16,1
lossyrob,which is not good if we have an incoming Double tile,3/30/16,1
lossyrob,seems like we should change that to have a Double tile case,3/30/16,1
lossyrob,"@owcm https://github.com/geotrellis/geotrellis/blob/master/docs/vector/geojson-support.md, if these docs don_Ñét handle your case or give you the right info, please let us know so we can update",3/30/16,1
lossyrob,,3/30/16,1
owcm,"@dwins,@lossyrob Thanks for the docs, I am following them. Thanks Let you know if I have issues.",3/30/16,1
owcm,"@lossyrob Followed the GeoJSON Feature example you supplied and was able to have that work.  I have shape files I converted using QGIS or OGR2OGR to geojson. I can open them up and start parsing them but I get error:```Message: Feature expected to have ""ID"" field Trace: geotrellis.vector.io.json.JsonFeatureCollectionMap.geotrellis$vector$io$json$JsonFeatureCollectionMap$$getFeatureID(JsonFeatureCollectionMap.scala:48)</br>geotrellis.vector.io.json.JsonFeatureCollectionMap$$anonfun$getAll$1.apply(JsonFeatureCollectionMap.scala:65)```Clearly a check in the code to create the Feature map.   These converters don't add the ID field. I don't see the ID field as a required field in the GeoJSON spec, but it is there in your example.   Have you heard of this issue before?  In my case I am working with a file of LineStrings.",3/31/16,1
dwins,@owcm if you use `JsonFeatureCollectionMap`  then the ID is required because that is used for the map key,3/31/16,1
dwins,Parsing to `JsonFeatureCollection` as done in the test I linked should not have this requirement.,3/31/16,1
owcm,@dwins  Right you are.  I was to focused on the example. Thanks,3/31/16,1
dwins,This would probably be a good addition to the docs that we linked though.,3/31/16,1
moradology,@owcm @dwins I'll make an issue to track this,3/31/16,1
lokifacio,I'm trying geotrellis on our cluster and it seems it is only using one executor even I have an RDD with 70 tiles,3/31/16,1
lokifacio,do I need to repartition it manually?,3/31/16,1
echeipesh,can you check how many partitions it actually has?,3/31/16,1
echeipesh,To answer the question we try to partition reasonably whenever it_Ñés possible but sometimes you have to do it manually. But also YARN in particular defaults to using only 2 executors and in cluster deploy mode that ends up with an executor for driver and one for the worker.,3/31/16,1
lokifacio,I'm launching with yarn --num-executors 40  and I get TileLayerRDD's  **count= 77** and **partitions.length=1**,3/31/16,1
echeipesh,"Yep, that not great partitioning, where are you reading it from?",3/31/16,1
lokifacio,hadoop ingested tiles ,3/31/16,1
echeipesh,fresh from ingest or from HadoopLayerReader ?,3/31/16,1
lokifacio,HadoopLayerReader,3/31/16,1
lokifacio,I have my own ingest code using HadoopLayerWriter,3/31/16,1
echeipesh,What is supposed to happen there is that the hadoop layer writer writes sequence file of hadoop block size (I think that_Ñés 64MB by default) and on read each block becomes it_Ñés own partition. If those are 256x256 tiles then they_Ñére only 40MB uncompressed together (assuming they_Ñére 8 bytes per cell). So in this case you_Ñéd need to repartition them manually. There is no logic to enforce minimum partition count on read.,3/31/16,1
lokifacio,"why repartitioning a `TileLayerRDD[SpatialKey]` returns a `RDD[(Tile, Tile)]`?",3/31/16,1
pomadchin,Have you a code sample?,3/31/16,1
pomadchin,or just this type repatriation?,3/31/16,1
lokifacio,"```scalaval reader: HadoopLayerReader = ?reader.read(layer).repartition(20)```",3/31/16,1
echeipesh,"because it extends `RDD[(SpatialKey, Tile)]` so `repartition` is on base RDD class. To save the metadata try `rdd.withContext( _.repartition(20))`",3/31/16,1
lokifacio,"already tried that, but I got a `ContextRDD[Tile, Tile, Bounds[Tile]]` instead of a TileLayerRDD which I get when I use withContext with other ops",3/31/16,1
lokifacio,"@echeipesh what I meant was where the first Tile comes from,  `TileLayerRDD[SpatialKey]` is supposed to be `RDD[(SpatialKey, Tile)]`, isn't it?",3/31/16,1
echeipesh,"Sorry that doesn_Ñét sound right, so `reader.read(layer).withContext(_.repartition(20))` has result of that signature ?",3/31/16,1
lokifacio,it's what I get from IDE,3/31/16,1
echeipesh,"lemme double check, if thats true something is horribly wrong.",3/31/16,1
echeipesh,Hijacking this spec: https://github.com/geotrellis/geotrellis/blob/master/spark/src/test/scala/geotrellis/spark/io/hadoop/HadoopSpatialSpec.scala,3/31/16,1
echeipesh,"```scala    it(""should handle layer names with spaces"") {      val layer = AllOnesTestFile      val layerId = LayerId(""Somelayer"", 10)      writer.write[SpatialKey, Tile, TileLayerMetadata[SpatialKey]](layerId, layer, ZCurveKeyIndexMethod)      val backin = reader.read[SpatialKey, Tile, TileLayerMetadata[SpatialKey]](layerId)      backin.count should be (42)      val repartitioned: TileLayerRDD[SpatialKey] = backin.withContext(_.repartition(10))      repartitioned.count should be (42)      info(s""read partitions: ${backin.partitions.length}"")      info(s""repartitioned partitions: ${repartitioned.partitions.length}"")      info(s""repartitioned  ${repartitioned}"")    }```produces```[info] HDFS layer names[info] - should handle layer names with spaces[info]   + read partitions: 1 [info]   + repartitioned partitions: 10 [info]   + repartitioned  ContextRDD[262] at RDD at ContextRDD.scala:19 [info] - should fail gracefully with colon in name```",3/31/16,1
echeipesh,"@lossyrob Also that spec, if one actually materialized it will fail with:",3/31/16,1
echeipesh,"```[info] - should handle layer names with spaces *** FAILED ***[info]   org.apache.hadoop.mapreduce.lib.input.InvalidInputException: Input Pattern file:/var/folders/js/6kszfm816577hpvg8fqvqgv80000gq/T/testFiles/geotrellis.spark.io.hadoop.HadoopSpatialSpec/Some layer/10/*[0-9]*/data matches 0 files[info]   at org.apache.hadoop.mapreduce.lib.input.FileInputFormat.listStatus(FileInputFormat.java:285)[info]   at org.apache.hadoop.mapreduce.lib.input.SequenceFileInputFormat.listStatus(SequenceFileInputFormat.java:59)[info]   at org.apache.hadoop.mapreduce.lib.input.FileInputFormat.getSplits(FileInputFormat.java:340)[info]   at org.apache.spark.rdd.NewHadoopRDD.getPartitions(NewHadoopRDD.scala:115)[info]   at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:239)[info]   at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:237)[info]   at scala.Option.getOrElse(Option.scala:120)[info]   at org.apache.spark.rdd.RDD.partitions(RDD.scala:237)[info]   at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35)[info]   at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:239)```",3/31/16,1
lossyrob,:(,3/31/16,1
lossyrob,"bad tests, man.",3/31/16,1
echeipesh,https://gist.github.com/echeipesh/766fcf60d0bbb389adb4,3/31/16,1
aaron-santos,"I updated to RC3 and I got a tricky one: `Cannot deserialize key index for key type geotrellis.spark.KeyBounds. You need to register this key type using the config item geotrellis.spark.io.index.registrator`",4/1/16,1
lossyrob,that is a strange one,4/1/16,1
lossyrob,can you post the offending metadata?,4/1/16,1
aaron-santos,"I've `.set(""spark.serializer"", ""org.apache.spark.serializer.KryoSerializer"")` and `.set(""spark.kryo.registrator"", ""geotrellis.spark.io.kryo.KryoRegistrator"")`",4/1/16,1
lossyrob,update from what to RC3?,4/1/16,1
lossyrob,because there was a metadata change,4/1/16,1
lossyrob,which requires migration of metadata,4/1/16,1
lossyrob,starting in RC1,4/1/16,1
aaron-santos,Fresh catalog,4/1/16,1
lossyrob,hmm,4/1/16,1
lossyrob,sounds like a dirty cache,4/1/16,1
lossyrob,have you tried sbt cleaning everything,4/1/16,1
aaron-santos,I'll try that,4/1/16,1
aaron-santos,"Ah, I believe I got it. I had extended TileLayerMetadata and forgot to include the new fields in the (de)serializer object",4/1/16,1
aaron-santos,d'oh,4/1/16,1
aaron-santos,thanks for the reminder :),4/1/16,1
lossyrob,"oh, nice",4/1/16,1
aaron-santos,"btw, the new Ingest object is slick. It cuts down on a lot of extra code",4/1/16,1
lossyrob,"nice, yeah I like how it ended up looking",4/1/16,1
echeipesh,We almost deleted the `Ingest` objects at least _Ñ_ 3 times :),4/1/16,1
echeipesh,glad it found its useful encarnation,4/1/16,1
aaron-santos,"ya, we're able to add some additional metadata fields in the sink which is awesome",4/1/16,1
enhan,"Hello Geotrellis team & enthousiasts,I am quite new to GIS so my question might be badly phrased. I have huge csv files containing population data by km2 that I need to transform as image tiles for 1ëçlatitude x 1ëçlongitude and a fixed 500px width and varying height (we use this format for other stuffs, unfortunately, we can't change this easily). Does Geotrellis fit this task or am I in the wrong place ? Thanks for the insights !",4/4/16,1
lossyrob,@echeipesh here_Ñés those emr scripts I was talking about https://github.com/lossyrob/geotrellis-ned-example/tree/master/emr,4/4/16,1
lossyrob,@enhan yeah that_Ñés something GeoTrellis could help out with,4/4/16,1
lossyrob,"there_Ñés a lot of moving parts there, like what sort of image tiles, how big are the tiles, how big is the input, what sort of output you need, but yeah you_Ñére definitely in the right place for those types of questions",4/4/16,1
lossyrob,"```scala  def levelStream[    K: SpatialComponent: ClassTag,    V <: CellGrid: ClassTag: ? => TileMergeMethods[V]: ? => TilePrototypeMethods[V],    M: Component[?, LayoutDefinition]: Component[?, Bounds[K]]  ](rdd: RDD[(K, V)] with Metadata[M],    layoutScheme: LayoutScheme,    startZoom: Int,    endZoom: Int,    options: Options  ): Stream[(Int, RDD[(K, V)] with Metadata[M])] =    (startZoom, rdd) #:: {      if (startZoom > endZoom) {        val (nextZoom, nextRdd) = Pyramid.up(rdd, layoutScheme, startZoom, options)        levelStream(nextRdd, layoutScheme, nextZoom, endZoom, options)      } else {        Stream.empty      }    }```",4/4/16,1
lossyrob,"```scala      // NLCD pyramid stream      val nlcdLevels =        Pyramid.levelStream(nlcd, targetLayoutScheme, nZoom, NearestNeighbor)      // Elevation pyramid stream      val elevationLevels =        Pyramid.levelStream(elevation, targetLayoutScheme, eZoom, Bilinear)      nlcdLevels        .zip(elevationLevels)        .foreach { case ((z, nlcdLayer), (_, elevationLayer)) =>          val layerId = LayerId(""nlcd-shade"", z)          val keyToPath = SaveToS3.spatialKeyToPath(layerId, ""s3://geotrellis-test/{name}/{z}/{x}/{y}.png"")          nlcdLayer            .color(NLCD.colorMap)            .join(elevationLayer)            .join(elevationLayer.hillshade(altitude = 60))            .mapValues { case ((coloredNlcdTile, elevationTile), hillshadeTile) =>              color(coloredNlcdTile, elevationTile, hillshadeTile, elevationSaturationMap).renderPng().bytes            }            .saveToS3(keyToPath, { putObject =>              putObject.withCannedAcl(CannedAccessControlList.PublicRead)            })        }```",4/4/16,1
echeipesh,"+1, that_Ñés a nice API completion.",4/4/16,1
jfklingler,Noob warning. So I_Ñém trying take a set of points with a value and generate a krig raster. The raster I end up with has a bunch of circles for different values. All of the examples I_Ñéve been given for the expected result are smooth(-ish) topo-map looking rasters. What_Ñés up with the circles?,4/4/16,1
jfklingler,I_Ñém sure I_Ñém leaving out every possible detail you need to answer the question. Sorry - I don_Ñét know what details are important.,4/4/16,1
jfklingler,"The variorgram and semivariogram are pretty close, or so I_Ñéve been told.",4/4/16,1
jfklingler,I_Ñém trying to do ordinary kriging with a spherical model.,4/4/16,1
jfklingler,"And the circles/values are all in the right places, it_Ñés the circles that are giving people grief. If I set the krig bandwidth to 1, I don_Ñét get circles anymore. It looks like the 3-point-min rule kicks in and I get wierd shapes.",4/4/16,1
jfklingler,"On a mostly different subject, is it possible to make the krig algorithm use more than one core?",4/4/16,1
jfklingler,"And I doubt it_Ñés relevant, but I_Ñém viewing the rasters in QGIS.",4/4/16,1
lossyrob,"@vishalanand would be the best person to answer these questions, since it_Ñés his feature.",4/4/16,1
aaron-santos,"If I'm reading metadata using an AttributeStore from inside a spark task, will I need to register any deserializers?",4/4/16,1
lossyrob,"AttributeStores are not serializable, but they shouldn_Ñét really be shipped around",4/4/16,1
lossyrob,"I think at least. I don_Ñét think we marked them serailizable_Ñ_which attribute store, s3?",4/4/16,1
aaron-santos,Say I wrote my own AttributeStore,4/4/16,1
aaron-santos,"I mean, that's what I did/am doing",4/4/16,1
aaron-santos,So I have an elasticsearch attribute store that is serializable,4/4/16,1
aaron-santos,the elasticsearch client it uses internally is marked as transient and lazy,4/4/16,1
lossyrob,are you getting a serialization error? The stack for that is usually pretty specific about what it chokes on,4/4/16,1
lossyrob,"ok, following so far",4/4/16,1
aaron-santos,"```[error] spray.json.DeserializationException: Cannot deserialize key index for key type geotrellis.spark.KeyBounds. You need to register this key type using the config item geotrellis.spark.io.index.registrator[error]         at geotrellis.spark.io.json.KeyIndexFormats$KeyIndexJsonFormatFactory$.getKeyIndexJsonFormat(KeyIndexFormats.scala:102)[error]         at geotrellis.spark.io.json.KeyIndexFormats$class.keyIndexJsonFormat(KeyIndexFormats.scala:107)[error]         at geotrellis.spark.io.json.Implicits$.keyIndexJsonFormat(Implicits.scala:19)```",4/4/16,1
lossyrob,what sort of key are you using,4/4/16,1
aaron-santos,"the caller method in my code```  def readLayerAttributes[H: JsonFormat, M: JsonFormat, K: ClassTag](id: LayerId): LayerAttributes[H, M, K] = {                                                                                                                                                                                                                                                                                                                          val blob = read[JsValue](id, AttributeStore.Fields.metadataBlob).asJsObject                                                                                                                                                                                                                                                                                                                                                          println(s""Converting keyIndex ${AttributeStore.Fields.keyIndex} ${blob.fields(AttributeStore.Fields.keyIndex)} to KeyIndex[K]"")                                                                                                                                                                                                                                                                                                      LayerAttributes(                                                                                                                                                                                                                                                                                                                                                                                                                       blob.fields(AttributeStore.Fields.header).convertTo[H],                                                                                                                                                                                                                                                                                                                                                                              blob.fields(AttributeStore.Fields.metadata).convertTo[M],                                                                                                                                                                                                                                                                                                                                                                            blob.fields(AttributeStore.Fields.keyIndex).convertTo[KeyIndex[K]],                                                                                                                                                                                                                                                                                                                                                                  blob.fields(AttributeStore.Fields.schema).convertTo[Schema]                                                                                                                                                                                                                                                                                                                                                                        )                                                                                                                                                                                                                                                                                                                                                                                                                                  }         ```",4/4/16,1
aaron-santos,SpatialKey,4/4/16,1
lossyrob,oh,4/4/16,1
aaron-santos,ahh that's is isn't it!,4/4/16,1
lossyrob,this looks like the old metadata problem,4/4/16,1
lossyrob,thinking keybounds is a key index,4/4/16,1
aaron-santos,"The line being printed looks like this ```Converting keyIndex keyIndex {""type"":""zorder"",""properties"":{""keyBounds"":{""minKey"":{""col"":1387,""row"":2858},""maxKey"":{""col"":1387,""row"":2859}}}} to KeyIndex[K]```",4/4/16,1
lossyrob,that doesn_Ñét seem wrong,4/4/16,1
lossyrob,and it_Ñés failing on `blob.fields(AttributeStore.Fields.keyIndex).convertTo[KeyIndex[K]]` ?,4/4/16,1
aaron-santos,that's right,4/4/16,1
aaron-santos,The exception is spray.json.DeserializationException: Cannot deserialize key index for key type geotrellis.spark.KeyBounds. You need to register this key type using the config item geotrellis.spark.io.index.registrator,4/4/16,1
lossyrob,what_Ñés this look like `blob.fields(AttributeStore.Fields.header)`,4/4/16,1
aaron-santos,"That's ```{""type"":""zorder"",""properties"":{""keyBounds"":{""minKey"":{""col"":1387,""row"":2858},""maxKey"":{""col"":1387,""row"":2859}}}}```",4/4/16,1
aaron-santos,sorry,4/4/16,1
aaron-santos,misred,4/4/16,1
aaron-santos,misread*,4/4/16,1
aaron-santos,one moment,4/4/16,1
lossyrob,"because waht it looks like is that it_Ñés reading in the key type (the actual type name) from the header as _ÑÒgeotrellis.spark.KeyBounds""",4/4/16,1
aaron-santos,"header ```{""format"":""s3"",""key"":""xxxxxxxxxx/dc185a9f-d534-41f6-935b-beda526f636a/13"",""keyClass"":""geotrellis.spark.SpatialKey"",""valueClass"":""geotrellis.raster.Tile"",""bucket"":""xxxxxxxxxxxx""}```",4/4/16,1
lossyrob,is there any way K is somehow a KeyBounds,4/4/16,1
aaron-santos,Is there any easy way to print out the type of K? Otherwise I'll do some digging,4/4/16,1
lossyrob,"if you have a class tag, `classTag[K]` might give the type name",4/4/16,1
aaron-santos,"You're right ""K geotrellis.spark.KeyBounds""",4/4/16,1
aaron-santos,so I need to track down how I'm constructing my attribute store,4/4/16,1
lossyrob,yeah you_Ñére plugging in a Keybounds somewhere,4/4/16,1
aaron-santos,Gotcha,4/4/16,1
aaron-santos,Thanks for your help,4/4/16,1
lossyrob,no problem,4/4/16,1
SDeathLeaDer,"@lossyrob  thanks for response, i used ""./sbt assembly"" for generate the jar and run using ""./spark-submit"" and all is ok. But as you said i need to learn more things when is cluster mode. I need to set the path where the tiles are ingested, in the documentation says that http://spark.apache.org/docs/latest/submitting-applications.html, i need to used ""local://"", it is correct? because i am using the cluster mode in standalone. In the part of ServeNDVI of the tiles i need to specify too the path.",4/5/16,1
lossyrob,"`local://`for what? If you_Ñére talking about the master URL, it would just be `local[*]`",4/5/16,1
lossyrob,(the star means you use all CPUs available. `local[2]` for instance would use only 2 cores,4/5/16,1
SDeathLeaDer,"i am talking to variable ""outputPath"" in the IngestImage class of the tutorial geotrellis-landsat-tutorial , ""outputPath""  is used  as input  to create writer for the tiles.",4/5/16,1
SDeathLeaDer,"When i used the value of ""outputPath""  by default of the tutorial, it throws me: ""Exception in thread ""main"" geotrellis.spark.io.package$LayerWriteError: Failed to write Layer(name = ""landsat"", zoom = 15)"" and it is by:  ""java.io.FileNotFoundException: /opt/spark-1.6.0-bin-hadoop2.6/work/app-20160405114808-0001/1/data/catalog/landsat/15/607577348 (No such file or directory)""",4/5/16,1
lossyrob,`file://`,4/5/16,1
lossyrob,is for the local file system,4/5/16,1
lossyrob,but yeah. are you running it on a cluster? because you don_Ñét want to save to the filesystem of nodes,4/5/16,1
lossyrob,you_Ñéd have to save it to something like HDFS,4/5/16,1
lossyrob,or S3,4/5/16,1
SDeathLeaDer,"i am running it on a cluster  in standalone cluster mode, yes i want to save to nodes.",4/5/16,1
lossyrob,"ok, you_Ñéll have to use HDFS then",4/5/16,1
lossyrob,`hdfs://` ,4/5/16,1
rajadain,"Hi, quick question about counting cell values from a raster. If I have a raster with a finite set of discrete values, does GT have an inbuilt operation to group and count them? For example, if I have a raster in which each cell corresponds to some land use, is there a way I can know how many of each type there are in an extent? Or will I have to loop through each cell and populate an array myself?",4/5/16,1
moradology,on spark or locally?,4/5/16,1
rajadain,Spark,4/5/16,1
moradology,check out geotrellis.spark.summary.StatsTileRDDMethods,4/5/16,1
moradology,RDD.histogram should provide you with a histogram,4/5/16,1
moradology,there may be memory issues if you're working with massive amounts of values and you collect to the Driver,4/5/16,1
rajadain,Cool! Yeah I think histogram fits my needs exactly! https://github.com/geotrellis/geotrellis/blob/master/spark/src/main/scala/geotrellis/spark/summary/StatsTileRDDMethods.scala#L33-L37 Thanks @moradology ,4/5/16,1
rajadain,Ah good to know. Will try out with different extents,4/5/16,1
moradology,you're probably safe if you're dealing with sane numbers - i think @lossyrob experienced this while pushing things to logical extremes,4/5/16,1
lossyrob,"That API will shift a bit. calling `histogram` will do a streaming histogram, which is an approximation. There will be a `histogramExactInt` which will only deal with integer counts and could blow up in size, overflow the counts, etc, but will work for smaller RDDs",4/5/16,1
lossyrob,but the thing you_Ñére looking for is not hard to write in RDD form,4/5/16,1
lossyrob,,4/5/16,1
lossyrob,"```scalalayer.map { case (_, tile) =>  var sum = 0L  tile.foreach { z => if(z == targetLandCover) z += 1 }}.reduce(_ + _)```",4/5/16,1
dwins,the last line in the body of the map shouldn't be a foreach. i think you need another line that just says `sum` so that is the result of the lambda,4/5/16,1
rajadain,"Alright. I think we're doing something similar in MMW, except with `scala.collection.mutable`for performance reasons. I was just checking there isn't a newer, in-built way to do it",4/5/16,1
dwins,"`z += 1` won't work either, probably should be incrementing `sum` there",4/5/16,1
lossyrob,"```scalalayer.map { case (_, tile) =>  var sum = 0L  tile.foreach { z => if(z == targetLandCover) sum+= 1 }  sum}.reduce(_ + _)```",4/5/16,1
lossyrob,yup yup,4/5/16,1
rajadain,thanks all!,4/5/16,1
echeipesh,"```.mapContext{ md => md.copy(bounds = for (b <- md.bounds) yield KeyBounds(b.minKey.spatialKey, b.maxKey.spatialKey)) }```",4/5/16,1
echeipesh,"Ok, so the reason we_Ñére seeing the problem with float32 geotiff written by geotrellis in QGIS is because of the differences of float parsing between JVM and whatever QGIS is using:",4/6/16,1
echeipesh,"```scala> val nd = ""1.0000000200408773E20""nd: String = 1.0000000200408773E20scala> nd.toFloat.toStringres16: String = 1.0E20scala> nd.toFloat.toDouble.toStringres17: String = 1.0000000200408773E20scala> ""1.0E20"".toFloat.toDouble.toStringres18: String = 1.0000000200408773E20```",4/6/16,1
echeipesh,"When we_Ñére writing the file we_Ñére writing the `NoData=1e20`, which is perfectly correct for us, which is why we can read it back in and not have to worry about it.",4/6/16,1
echeipesh,"But I_Ñém guessing QGIS parses it as double, in which case `1E20 != 1.0000000200408773E20`",4/6/16,1
echeipesh,The result is that QGIS messes up the histogram and and renders the cells as black.,4/6/16,1
echeipesh,"digging into java standard library to find the reason why it_Ñés ok to represent that value as 1E20, I_Ñém guessing that_Ñés just the nearest value to actual 1E20 that can be represented with 32 floating point bits of precision and the conversion to/from is consistent. But I_Ñéd like to have some code to point to.",4/6/16,1
pomadchin,"yep, and i am curious where are that lines that changed the behavior of habdling float tiles?",4/6/16,1
echeipesh,"yeah, me too :)",4/6/16,1
echeipesh,"For the record this is not a problem with GDAL as it is able to count the summary statistics correctly:``` Min=261.850 Max=284.981  Minimum=261.850, Maximum=284.981, Mean=278.533, StdDev=3.298  NoData Value=1e+20```",4/6/16,1
pomadchin,found it,4/6/16,1
pomadchin,https://github.com/geotrellis/geotrellis/commit/8c728e63e92043ef418909532a184e08edd2cb42#diff-d2a033a1c0dcf7d3b07d7f194233b6ddR27,4/6/16,1
pomadchin,if we_Ñéll rollback `isNoData(n)` to `n == Float.NaN` works fine,4/6/16,1
pomadchin,"yep, double checked that",4/6/16,1
echeipesh,"Yeah, but nothing `== Float.NaN`, so it was never written",4/6/16,1
pomadchin,"._.this `isNoData` macro is `java.lang.Float.isNaN($f)`",4/6/16,1
lossyrob,"@echeipesh ok so, its really not a problem then with the RC",4/6/16,1
lossyrob,because it_Ñés handling the float data according to sound logic,4/6/16,1
lossyrob,"it was just passing through NaN_Ñés before, so QGis would render that correctly",4/6/16,1
lossyrob,even though it wasn_Ñét the offical NoData value,4/6/16,1
echeipesh,"Yeah, we can be friendlier if we did `noDataValue.toDouble.toString` for float, that will print string with ""higher precision",4/6/16,1
echeipesh,"It seems like something funky is going on, because QGIS property window seems to print the nodata value correctly, with full prevision. And it doesn_Ñét explain why max value is `9.999e19`. ",4/6/16,1
lossyrob,"that seems like an easy fix, to just convert it to Double before writing the nodata string",4/6/16,1
lossyrob,sorta makes sense anyway,4/6/16,1
echeipesh,"Not sure it makes sense, but it is easy enough  :)",4/6/16,1
lossyrob,https://github.com/geotrellis/geotrellis/blob/master/raster/src/main/scala/geotrellis/raster/io/geotiff/writer/TiffTagFieldValue.scala#L55,4/6/16,1
lossyrob,well it makes sense that it_Ñés a floating point value,4/6/16,1
lossyrob,that is the no data value,4/6/16,1
lossyrob,so really we represent that as a double value,4/6/16,1
lossyrob,I don_Ñét think GDAL or anyone writes them out specifically as float32 values,4/6/16,1
lossyrob,they_Ñére all just double values,4/6/16,1
echeipesh,"ah, so it makes sense as a convention for geotiff tag",4/6/16,1
rajadain,"Quick raster cell-size question: Given an ingested raster, is the cell size (in real world values, like ft or m) stored somewhere in it, allowing me to get it somehow from the raster itself? Or should I specify a ""known"" value, which I know because I ingested the raster?",4/6/16,1
lossyrob,yeah that _ÑÒconvention_Ñù is not so defined,4/6/16,1
lossyrob,it_Ñés a GDAL derived tag,4/6/16,1
lossyrob,I_Ñém gonna check the goal source,4/6/16,1
lossyrob,@rajadain the cellsize is in map coordinates,4/6/16,1
lossyrob,"so if you_Ñére map coordinates represent _ÑÒreal world_Ñù values, then you can get that info",4/6/16,1
lossyrob,"if it_Ñés in something like latlng, you have to do the math",4/6/16,1
lossyrob,@echeipesh  https://github.com/OSGeo/gdal/blob/trunk/gdal/frmts/gtiff/geotiff.cpp#L503 it just takes a double,4/6/16,1
rajadain,@lossyrob got it https://github.com/geotrellis/geotrellis/blob/2402607b0c6f5a4e087bc46fe32c5f56e206947e/raster/src/main/scala/geotrellis/raster/RasterExtent.scala#L276-L278 thanks!,4/6/16,1
lossyrob,no problem,4/6/16,1
lossyrob,"if you need real world distance, this is an example of doing some UTM zone trickery https://github.com/geotrellis/geotrellis/blob/master/spark/src/main/scala/geotrellis/spark/tiling/ZoomedLayoutScheme.scala#L55",4/6/16,1
rajadain,woah cool!,4/6/16,1
lossyrob,"general FYI, I have to fix some masking code (really flesh it out a bit more than it is), but I was able to render the NLCD/Hillshade combo successfuly which was my last push for testing, so I want to release RC4 tomorrow (or Friday if any other bugs come up) and then wait a couple beats before pushing that forward as the final release",4/6/16,1
lokifacio,is it possible to rasterize a RDD[PointFeature] directly into a TileLayerRDD?,4/7/16,1
lokifacio,@lossyrob any clue?,4/7/16,1
mchendricks1,"I'm running geotrellis-ec2-cluster with three nodes on virtualbox. I built the landsat demo fat jar to include geotrellis-spark-etl (and geotrellis-s3). I run the following command:root@leader:/vagrant# spark-submit --class geotrellis.spark.etl.SinglebandIngest --master local[*] --driver-memory 2G /vagrant/demo-assembly-0.1.0.jar --input hadoop --format geotiff  --cache NONE -I path=""file:///vagrant/LC80140322014139LGN00/"" --output accumulo -O instance=""b751b275-51e4-4936-a90d-dbab617100b4"" table=""nlcd-table"" user=""root"" password=""password"" zookeeper=""zookeeper.service.geotrellis-spark.internal:2181"" --layer nlcd-tms --crs EPSG:3857 --pyramid --layoutScheme tmsThis is using RC2 geotrellis libraries/build (since RC3 didn't build correctly under landsat demo as of couple of days ago).I get the following error:16/04/07 16:49:23 INFO executor.Executor: Running task 4.0 in stage 0.0 (TID 4)16/04/07 16:49:23 INFO rdd.NewHadoopRDD: Input split: file:/vagrant/LC80140322014139LGN00/LC80140322014139LGN00_B3.TIF:0+12207703816/04/07 16:49:24 INFO scheduler.TaskSetManager: Finished task 3.0 in stage 0.0 (TID 3) in 9203 ms on localhost (4/12)KilledIt appears to get about four files in and then chokes. Same failure happens with 122MB files and 525KB files. (They are Landsat geotiff files).Any suggestions, plz?",4/7/16,1
lossyrob,sorry in meetings all day over here at azavea...,4/7/16,1
lossyrob,"@lokifacio nothing straightforward as in library functionality, but you would figure out what tile layout you would want, then capture the points in the extents of SpatialKey tiles, then rasterize each tile",4/7/16,1
lossyrob,`0.10.0-RC4` is out,4/10/16,1
pomadchin,: tada: :tada: ,4/10/16,1
aaron-santos,_ª¥ç,4/10/16,1
mchendricks1,Hello. I'm upgrading the cloudera versions in geotrellis-ec2-cluster to cdh5.6 to use apache spark (apparently 1.5.0). I noticed that in the landsat demo and the geodocker repos that spark 1.5.2 is used. Is that a floor or might I skirt by with 1.5.0?,4/11/16,1
echeipesh,"@mchendricks1 There should not be any API breakage between 1.5.2 and 1.5.0 so you should be gtg, but as I recall there were fairly major bugs that were fixed from 1.5.0 to 1.5.1 so hopefully that will not hit you.",4/11/16,1
mchendricks1,@echeipesh Ah. Excellent! Thx!,4/11/16,1
pomadchin,"Was tested a bit different version of geodocker cluster for a real distributed cluster https://github.com/geotrellis/geodocker-cluster/tree/master/prod Hope to see some feedback ;)",4/12/16,1
mchendricks1,"@pomadchin  I plan on trying this now. However, I wanted to ask why the move away from Mesos between geotrellis-ec2-cluster deployment to the geodocker-cluster model?",4/12/16,1
echeipesh,"@mchendricks1 We are looking for ways to make deployment more universal. Docker should be an answer to that. We should be able to deploy it locally, on AWS ECS, google cloud. Even Mesos can deploy docker container apps. I_Ñém going to see if I can use geodocker cluster to get accumulo on EMR with ease.",4/12/16,1
mchendricks1,"@pomadchin This is listed twice but generates an error:# Node2, Node3./3-hadoop-data.sh -t=0.1.0 -hma=GeoServer1./3-hadoop-data.sh -t=0.1.0 -hma=GeoServer1",4/12/16,1
mchendricks1,"@pomadchin And this line is in there once, but should they be different for the two worker nodes?# Node2, Node3./2-spark-worker.sh -t=0.1.0 -hma=GeoServer1 -sm=GeoServer2",4/12/16,1
pomadchin,@mchendricks1 ty for notice,4/12/16,1
mchendricks1,"@pomadchin I'm trying to run this command:spark-submit \--class geotrellis.spark.etl.SinglebandIngest \--master local[*] \--driver-memory 2G \/vagrant/demo-assembly-0.1.0.jar \--input hadoop --format geotiff  --cache NONE -I path=""file:///vagrant/ONEFILE/"" \--output accumulo -O instance=""geotrellis-accumulo-cluster"" table=""nlcd-table"" user=_ÑÒaccumulo_Ñù password=_ÑÒsecret_Ñù zookeeper=""zookeeper.service.geotrellis-spark.internal:2181"" \--layer nlcd-tms --crs EPSG:3857 --pyramid --layoutScheme tms",4/12/16,1
mchendricks1,"However, it isn't working.",4/12/16,1
mchendricks1,What's the proper way to run geotrellis-spark-etl in this docker configuration?,4/12/16,1
pomadchin,@mchendricks1 are you inside any spark container?,4/12/16,1
mchendricks1,Nope.,4/12/16,1
mchendricks1,Which is my question of how to do that?,4/12/16,1
pomadchin,"so, docker container it is an abstract sort of VM;if you started cluster using `runners` script you have forwarded volume by default on spark workers: https://github.com/geotrellis/geodocker-cluster/blob/master/prod/runners/2-spark-worker.sh#L38copy your jar into: `/data/gt/spark`after go into you worker container:```bashdocker exec -it spark-worker bash```go there:```bash/data/spark```and run:```bashspark-submit \--class geotrellis.spark.etl.SinglebandIngest \--master local[*] \--driver-memory 2G \$JAR \--input hadoop --format geotiff --cache NONE -I path=""file:///vagrant/ONEFILE/"" \--output accumulo -O instance=""geotrellis-accumulo-cluster"" table=""nlcd-table"" user=_ÑÒaccumulo_Ñù password=_ÑÒsecret_Ñù zookeeper=""zookeeper.service.geotrellis-spark.internal:2181"" \--layer nlcd-tms --crs EPSG:3857 --pyramid --layoutScheme tms```",4/12/16,1
pomadchin,"if you have no forwarded volume, then copy you jar inside the runing container:```bashdocker cp $JAR spark-worker:/data/```after that go into container:```bashdocker exec -it spark-worker bash``````bashcd /data/spark```and run:```bashspark-submit \--class geotrellis.spark.etl.SinglebandIngest \--master local[*] \--driver-memory 2G \$JAR \--input hadoop --format geotiff --cache NONE -I path=""file:///vagrant/ONEFILE/"" \--output accumulo -O instance=""geotrellis-accumulo-cluster"" table=""nlcd-table"" user=_ÑÒaccumulo_Ñù password=_ÑÒsecret_Ñù zookeeper=""zookeeper.service.geotrellis-spark.internal:2181"" \--layer nlcd-tms --crs EPSG:3857 --pyramid --layoutScheme tms```",4/12/16,1
pomadchin,"sry for that, for distributed env has no demo guide yet ):",4/12/16,1
pomadchin,"but it is in progress, thanks for your feedback!",4/12/16,1
mchendricks1,I understand. Thanks for the support!,4/12/16,1
mchendricks1,"When I try to run ""docker exec -it spark-worker bash"" it gives an error. ",4/12/16,1
mchendricks1,"docker ps says:472a0857bed6        daunnc/geodocker-spark-worker:0.1.0       ""/sbin/entrypoint.sh ""   33 minutes ago      Restarting (0) 6 minutes ago                       spark-worker",4/12/16,1
mchendricks1,"So the spark-worker nodes seem tock in a ""restarting"" state.",4/12/16,1
mchendricks1,"And yes, I did use the ""runners"" scripts.",4/12/16,1
mchendricks1,Let me google this issue for a bit...,4/12/16,1
pomadchin,hm,4/12/16,1
pomadchin,are you sure that all ports are free for master?,4/12/16,1
mchendricks1,Well it's doing this on the workers as well - as it turns out.,4/12/16,1
mchendricks1,I changed the scripts to use less memory to see if that would help - it didn't.,4/12/16,1
mchendricks1,I'm using VMs that are created from the geotrellis-ec2-cluster vagrant setup.,4/12/16,1
mchendricks1,(commenting out the ansible configuration).,4/12/16,1
pomadchin,"you can try these steps to debug (that would be awesome):1. kill all spark containers2. ```scaladocker run --rm -it \  --net=host \  --env=""SPARK_DAEMON_MEMORY=2g"" \  --env=""SPARK_SUBMIT_DRIVER_MEMORY=1g"" \  --env=""HADOOP_MASTER_ADDRESS=${HADOOP_MASTER_ADDRESS}"" \  daunnc/geodocker-spark-master:0.1.0 bash```",4/12/16,1
mchendricks1,I was just whittling down the script file to get to that command. Let me try yours.,4/12/16,1
pomadchin,HADOOP_MASTER_ADDRESS its ip / name of your master,4/12/16,1
pomadchin,"3. `/sbin/entrypoint.sh`4. `spark-class org.apache.spark.deploy.master.Master --host $(hostname)`",4/12/16,1
pomadchin,` _ÑÓrm` flag means that container would be removed after you_Ñéll break main process,4/12/16,1
pomadchin,`-it` means terminal mode in order let you to use bash ,4/12/16,1
mchendricks1,I believe it's running now.,4/12/16,1
pomadchin,hm,4/12/16,1
pomadchin,in another terminal,4/12/16,1
pomadchin,try same with worker,4/12/16,1
pomadchin,"but replace master run cmd with: ```spark-class org.apache.spark.deploy.worker.Worker --host $(hostname) spark://sparkmaster:7077```",4/12/16,1
pomadchin,i am curious what went wrong using scripts,4/12/16,1
mchendricks1,Should I use the master docker image as well or the work docker image?,4/12/16,1
pomadchin,"```docker run --rm -it \  --net=host \  --env=""SPARK_DAEMON_MEMORY=2g"" \  --env=""SPARK_SUBMIT_DRIVER_MEMORY=1g"" \  --env=""SPARK_WORKER_CORES=8"" \  --env=""SPARK_WORKER_MEMORY=16g"" \  --env=""HADOOP_MASTER_ADDRESS=${HADOOP_MASTER_ADDRESS}"" \  --env=""SPARK_MASTER=${SPARK_MASTER}"" \  --volume=${VOLUME:-""/data/gt/spark""}:/data/spark \  daunnc/geodocker-spark-worker:0.1.0```",4/12/16,1
pomadchin,volume replace with desired dir on a local fs,4/12/16,1
mchendricks1,That seems to be running correctly as well.,4/12/16,1
pomadchin,check now master_ip:8080 in browser,4/12/16,1
pomadchin,ah ,4/12/16,1
pomadchin,probably i know the reason,4/12/16,1
mchendricks1,I can see the spark UI,4/12/16,1
pomadchin,cool,4/12/16,1
mchendricks1,"Yes, thank you!",4/12/16,1
pomadchin,the reason is that your images has no entry point ,4/12/16,1
pomadchin,sec,4/12/16,1
pomadchin,will republish images,4/12/16,1
pomadchin,i thought i did it ):,4/12/16,1
mchendricks1,It happens.,4/12/16,1
mchendricks1,"BTW, the spark.deploy.work.Worker command fails. Can't connect to leader:7077.",4/12/16,1
mchendricks1,I'm looking to see if that port is exposed through the container.,4/12/16,1
pomadchin,`--net=host` means that container uses host machine network,4/12/16,1
mchendricks1,"I did run all three containers (each running in their on virtualbox VM instance) with ""--net=host"" option. If I'm reading the documentation and your comment correctly this does seem like the right setting.",4/12/16,1
lossyrob,@dwins that tile server looks great,4/12/16,1
lossyrob,have you gotten the elevation ingest to run to feed it?,4/12/16,1
dwins,,4/12/16,1
dwins,i'll try the elevation ingest again to remind myself what was giving me trouble,4/12/16,1
lossyrob,"ok, yeah any dataset would do I guess. so I see this serves stats_Ñ_is that if they are available? where would they be stored, attribute store for zoom 0 or something?",4/12/16,1
dwins,no they are recomputed on request. there are endpoints under each tile to get stats for just one tile so I don't think they'd need to be stored,4/12/16,1
lossyrob,oh gotcha,4/12/16,1
dwins,"`data/zoom/x/y` is a png tile, `data/zoom/x/y/histo` is a histogram for that tile",4/12/16,1
dwins,or you can do /grid to see the exact cell values fwiw,4/12/16,1
lossyrob,how is that returned?,4/12/16,1
lossyrob,like what format,4/12/16,1
dwins,it just returns the `asciiGrid` method's output,4/12/16,1
lossyrob,ah gotcha,4/12/16,1
lossyrob,"we had some work about returning a UTFGrid from @notthatbreezy, we could get that in at some point, would be a useful endpoint",4/12/16,1
dwins,yes that would be nice,4/12/16,1
dwins,was thinking it would be good to be able to get cell values on mouse over instead of just doing the ascii grid,4/12/16,1
dwins,"i have a little html page with leaflet i've been playing with, just puts the layer coming from geotrellis in a global var so i can call methods on it in developer tools. didn't put it in the repo yet",4/12/16,1
lossyrob,ok nice. I always just use geojson.io and do the metadata -> add layer. I really want to make a version of geojson.io that_Ñés geared towards working with and sharing tile server layers; that would be useful here,4/12/16,1
lossyrob,but one-pagers are a good way to go in the meantime,4/12/16,1
pomadchin,@mchendricks1 ty for finding bug in spark runners,4/12/16,1
pomadchin,fixed; publishing new images right now,4/12/16,1
mchendricks1,Excellent!,4/12/16,1
mchendricks1,@pomadchin Is there a script that is going to restart and pull down the latest containers or do I just need to start from scratch?,4/12/16,1
pomadchin,"stop old container, and pull down new (but warn, not published yet; p.s. decided not to update version)```bashdocker pull daunnc/geodocker-spark-master:0.1.0docker pull daunnc/geodocker-spark-worker:0.1.0```",4/12/16,1
pomadchin,so long due to bitbucket speed %),4/12/16,1
pomadchin,```33.3%```,4/12/16,1
mchendricks1,Ah. Duly noted... ;-),4/12/16,1
mchendricks1,"@pomadchin Is GeoServer1 supposed to be the spark master in this command from the readme?# Node2, Node3./2-spark-worker.sh -t=0.1.0 -hma=GeoServer1 -sm=GeoServer2",4/12/16,1
pomadchin,"```bash## Spark# Node1./1-spark-master.sh -t=0.1.0 -hma=GeoServer1# Node2, Node3./2-spark-worker.sh -t=0.1.0 -hma=GeoServer1 -sm=GeoServer1```",4/12/16,1
pomadchin,fixed that,4/12/16,1
mchendricks1,Cool.,4/12/16,1
pomadchin,"hma ~ hadoop master addresssm ~ spark master",4/12/16,1
pomadchin,detailed info about params can be found there https://github.com/geotrellis/geodocker-cluster/tree/master/prod/runners,4/12/16,1
pomadchin,so the example looks like: Node 1 is a manager node (all master processes there); Nodes 2 and 3 are workers,4/12/16,1
pomadchin,"though zoo is on Node 1 2 and 3, because for zookeeper we need min 3 nodes to work, for a distriuted mode",4/12/16,1
mchendricks1,Makes sense.,4/12/16,1
pomadchin,@mchendricks1 pushed and tested; thanks for trying,4/12/16,1
mchendricks1,"Alright, let me give it a go...",4/12/16,1
pomadchin,"```bashdocker pull daunnc/geodocker-spark-master:0.1.0docker pull daunnc/geodocker-spark-worker:0.1.0```",4/12/16,1
mchendricks1,"The docker pull seems to have gone through correctly. Unfortunately, I'm still getting the same restarting issue.",4/12/16,1
mchendricks1,I might have updated the git repo after the pull. Let me retry.,4/12/16,1
mchendricks1,Deleting all the old containers named spark-master or spark-worker,4/12/16,1
pomadchin,"1. stop all containers2. delete them3. pull repo4. update old images: ```bashdocker rmi -f daunnc/geodocker-spark-master:0.1.0 docker && \docker rmi -f daunnc/geodocker-spark-worker:0.1.0 && \pull daunnc/geodocker-spark-master:0.1.0 && \docker pull daunnc/geodocker-spark-worker:0.1.0```\5. start via scripts",4/12/16,1
shiraeeshi,"I've been struggling with this error for some time, it doesn't let me start playing with chatta-demo",4/13/16,1
shiraeeshi,I've created this issue https://github.com/geotrellis/geotrellis/issues/1445,4/13/16,1
shiraeeshi,"I know that you're all familiar with the need to update kryo dependency, so this issue is no news for you",4/13/16,1
shiraeeshi,The main question is: how to make chatta demo work?,4/13/16,1
pomadchin,@shiraeeshi let me have a look,4/13/16,1
pomadchin,i started response but probably we can talk there first,4/13/16,1
pomadchin,is that a problem during the ingest process?,4/13/16,1
shiraeeshi,no,4/13/16,1
pomadchin,map?,4/13/16,1
shiraeeshi,it get raised after getting to the main page from the browser,4/13/16,1
pomadchin,checking that right now,4/13/16,1
pomadchin,by the way what geodocker cluster version do you use? ,4/13/16,1
pomadchin,it_Ñés a dev version?,4/13/16,1
shiraeeshi,yes,4/13/16,1
shiraeeshi,and also I start just a single node,4/13/16,1
shiraeeshi,(it's probably not related but anyway),4/13/16,1
pomadchin,@shiraeeshi what method breaks?,4/13/16,1
shiraeeshi,what do you mean?,4/13/16,1
pomadchin,error log; what actually cases error you reported,4/13/16,1
shiraeeshi,it's com.esotericsoftware.kryo.serializers.MapSerializer.setGenerics(MapSerializer.java:53),4/13/16,1
pomadchin,higher,4/13/16,1
pomadchin,i mean smth more complete,4/13/16,1
pomadchin,if you can provide full stack trace that would be awesome; because this err is useless (in fact),4/13/16,1
shiraeeshi,"I provided in https://github.com/geotrellis/geotrellis/issues/1445, but I can paste it here too (but it's a lot of lines)",4/13/16,1
pomadchin,a bit higher if possible;,4/13/16,1
shiraeeshi,"what do you mean by ""higher""?",4/13/16,1
pomadchin,"i mean more complete stack trace, that would be much easier to find what happened, if i could know a bad function call",4/13/16,1
pomadchin,was that just tile reader or that was smth special; because i could not reproduce that err,4/13/16,1
shiraeeshi,"ok, I'll attach a file",4/13/16,1
pomadchin,cool thanks,4/13/16,1
shiraeeshi,[log.run-server.log](https://files.gitter.im/geotrellis/geotrellis/NftW/log.run-server.log),4/13/16,1
pomadchin,thanks that what i needed,4/13/16,1
pomadchin,"so to clearify:   * geodocker cluster (dev, scala 2.10) (images latest tag)is that correct information?  ",4/13/16,1
shiraeeshi,yes,4/13/16,1
pomadchin,"cool, today in ~hour-two would be fix / response about that fact ._._Ñé i thinks that in theory that can be cause by these lines: https://github.com/pomadchin/geotrellis-chatta-demo/blob/spark-version/geotrellis/src/main/scala/geotrellis/kryo/AvroRegistrator.scala#L7-L12",4/13/16,1
shiraeeshi,great,4/13/16,1
shiraeeshi,thank you,4/13/16,1
pomadchin,probably a quick workaround would be to replace `def apply(conf: SparkConf) = conf` or to remove it in a service code,4/13/16,1
pomadchin,but for some reason i cant get that error ._.,4/13/16,1
pomadchin,have you ingested data?,4/13/16,1
shiraeeshi,yes,4/13/16,1
pomadchin,"really weird, ok cool thanks for a bug report!",4/13/16,1
shiraeeshi,I'll report about your suggested workaround,4/13/16,1
lossyrob,Thats using RC4? Sounds like the double histogram treemap serialization bug i fixed in rc4 (see commit history for geotrellis kryo registrator),4/13/16,1
pomadchin,"Chatta demo uses rc 4 ._. Guess have to check everything again, though Locally have no such problems",4/13/16,1
shiraeeshi,that temporary workaround didn't work,4/13/16,1
shiraeeshi,"about rc4: I noticed that docker didn't build geotrellis because of some error, I don't remember it, it said something like failing to find v-rc4-something in git, if I'm not mistaken",4/13/16,1
shiraeeshi,I think it picked geotrellis jars from maven central repo,4/13/16,1
shiraeeshi,here is that line:,4/13/16,1
pomadchin,local build dosnt matter,4/13/16,1
shiraeeshi,error: pathspec 'v0.10.0-RC4' did not match any file(s) known to git.,4/13/16,1
pomadchin,"@shiraeeshi an you try this? ```bashdocker exec -it master1 bash -c "". ~/.bashrc; cd ./geotrellis-chatta-demo/geotrellis && spark-submit --class geotrellis.chatta.Main target/scala-2.10/GeoTrellis-Tutorial-Project-assembly-0.1-SNAPSHOT.jar""```",4/13/16,1
shiraeeshi,"ok, I'll try it now",4/13/16,1
pomadchin,"thats another way of submitting server, probably that can help to avoid this dep bug",4/13/16,1
shiraeeshi,it works,4/13/16,1
pomadchin,ok thanks @shiraeeshi ! ,4/13/16,1
pomadchin,that explains why i couldn_Ñét cath that bug; probably avro mismatch error or smth like that; would fix chatta demo and chatta scripts,4/13/16,1
pomadchin,"the best solution may be just to change the deployment method, and just to use spark-submit to start server",4/13/16,1
shiraeeshi,will you paste this command to git repo?,4/13/16,1
shiraeeshi,"I mean it's not a temporary workaround but a complete fix, right?",4/13/16,1
pomadchin,"yep, and also would fix an install script to checkout the correct tag;\+ want to investigate what library mismatch cased that issue (im pretty sure that avro mismatch, but anyway)",4/13/16,1
shiraeeshi,"I don't know if its related, but I've commented out some lines in some build script",4/13/16,1
shiraeeshi,I'll paste it here,4/13/16,1
shiraeeshi,it's from geodocker-cluster/dev/nodes/build.sh:,4/13/16,1
shiraeeshi,,4/13/16,1
pomadchin,":D) \``` you can use markdown to wirte there",4/13/16,1
shiraeeshi,":) sorry, I'm editing it",4/13/16,1
shiraeeshi,,4/13/16,1
shiraeeshi,"```rm -f DockerfileMaster && git checkout DockerfileMastersed ""s/daunnc\/geodocker-base:latest/daunnc\/geodocker-base:${BASE_TAG}/g"" DockerfileMaster > DockerfileMaster.newrm -f DockerfileMaster && mv DockerfileMaster.new DockerfileMasterrm -f DockerfileSlave && git checkout DockerfileSlavesed ""s/daunnc\/geodocker-base:latest/daunnc\/geodocker-base:${BASE_TAG}/g"" DockerfileSlave > DockerfileSlave.newrm -f DockerfileSlave && mv DockerfileSlave.new DockerfileSlave```",4/13/16,1
shiraeeshi,just to clarify: it works only when geotrellis.kryo.AvroRegistrator do nothing.,4/13/16,1
pomadchin,so the solution: 1. to uncomment avro registrator; 2. to run via spark-submit ,4/13/16,1
pomadchin,will double check what_Ñés exactly wrong and why it happend,4/13/16,1
pomadchin,@shiraeeshi https://github.com/geotrellis/geotrellis/issues/1445,4/13/16,1
lossyrob,"so currently the status quo, in geodockercluster and elsewhere, is to always run spark jobs through `spark-submit`, yeah?",4/13/16,1
pomadchin,"so that depends on situation o: but to avoid deps hell yes;anyway i believe that there could be cases, that spark-submit would crash too, because it forces app to use cluster deps",4/13/16,1
pomadchin,think that we should just manage cluster dependencies carefully ._.,4/13/16,1
lossyrob,"Could be; but this isn_Ñét the first time I_Ñéve seen dep issues this way around, so I think we should be recommending using spark-submit by default, unless there is a good & specific reason not to",4/13/16,1
pomadchin,yep,4/13/16,1
lossyrob,"actually, to be fair, I have seen the other way too",4/13/16,1
lossyrob,"like the hadoop version we were installing with geotrellis-ec2-cluster, that had old joda time and aws dependencies",4/13/16,1
pomadchin,"yes, so probably we can point somewhere in docs, that we should be carefull with cluster provided libs",4/13/16,1
lossyrob,"so maybe dep hell is always unavoidable, though if we have to be consistent in one way vs the other, spark-submit and using the clusters dependencies seems to be the thing people do",4/13/16,1
lossyrob,yeah,4/13/16,1
lossyrob,has anyone seen this from geodocker-cluster?,4/14/16,1
lossyrob,"```console[error] /data/geotrellis/proj4/src/main/java/org/osgeo/proj4j/util/PolarCoordinate.java:18:  error: unmappable character for encoding ASCII[error]         return String.format(""<??%f, ??%f>"", lam, phi);```",4/14/16,1
lossyrob,seems like some encoding is getting mucked up,4/14/16,1
lossyrob,,4/14/16,1
pomadchin,@lossyrob https://github.com/geotrellis/geodocker-cluster/issues/22,4/14/16,1
pomadchin,"@lossyrob can you ty to install inside master container following?```bashapt-get install language-pack-en && \locale-gen en_US && \update-locale LANG=en_US.UTF-8 LC_CTYPE=en_US.UTF-8```",4/14/16,1
pomadchin,"and to run```bashexport LANG=en_US.UTF-8export LC_CTYPE=en_US.UTF-8```before installing geotrellis",4/14/16,1
pomadchin,"however this shouldn_Ñét be reflected on chatta demo, it pulls deps from maven repo ",4/14/16,1
pomadchin,going to update images,4/14/16,1
pomadchin,I am pretty sure that this settings depend on default host settings,4/14/16,1
pomadchin,sorry for that ,4/14/16,1
lossyrob,ok. Yeah it actually built chatta demo fine,4/14/16,1
lossyrob,but just choked on that point,4/14/16,1
lossyrob,"sorry have to step away from it for a little, right now things are ingesting, I can try stuff out if you need a bit later",4/14/16,1
pomadchin,"thanks Rob! I am really sure that that caused just by the default locale dumb thing yep, anyway I_Ñéll publish new versions and you just would have to pull down updated images; in `prod` images i fixed that issue",4/14/16,1
pomadchin,https://github.com/geotrellis/geodocker-cluster/blob/master/prod/base/Dockerfile#L15-L20,4/14/16,1
lossyrob,"gotcha, i figured it might have been a dev vs prod thing",4/14/16,1
pomadchin,building and publishing o: will pong you there after update,4/14/16,1
mchendricks1,"@pomadchin I wasn't able to get the spark-master to run correctly through the scripts (on my virtualbox instances - from the entrypoint problem). However, everything else ran correctly through the scripts. Spark kept crashing in the laptop VMs - out-of-memory issues so I moved it up to AWS EC2 instances. It has been a problem ever since. I still get the docker restarting issues - this time around accumulo-master:```8a45d7242cc7        daunnc/geodocker-accumulo-master:0.1.0    ""/sbin/entrypoint.sh ""   59 seconds ago       Up 58 seconds                                        accumulo-master7878dd1d26cf        daunnc/geodocker-accumulo-master:0.1.0    ""/sbin/entrypoint.sh ""   About a minute ago   Restarting (255) 7 seconds ago                       accumulo-init```",4/14/16,1
mchendricks1,"Also, is the accumulo-init docker container supposed to run off the accumulo-master image - as is implied by the output above? (Second line).",4/14/16,1
pomadchin,oh damn; probably that could be a problem ._.,4/14/16,1
pomadchin,kill both of them ,4/14/16,1
pomadchin,and try to start master,4/14/16,1
pomadchin,you are right i found a typo in init script,4/14/16,1
pomadchin,probably there also caould be smth wrong during accumulo init,4/14/16,1
pomadchin,"so you can try to kill all accumulo stuff (to repeat everything again);and after that: ```docker run -it \  --name=accumulo-init \  --net=host \  --env=""HADOOP_MASTER_ADDRESS=${HADOOP_MASTER_ADDRESS}"" \  --env=""ACCUMULO_ZOOKEEPERS=${ACCUMULO_ZOOKEEPERS}"" \  --env=""ACCUMULO_SECRET=${ACCUMULO_SECRET}"" \  --env=""ACCUMULO_PASSWORD=${ACCUMULO_PASSWORD}"" \  daunnc/geodocker-accumulo-master:${TAG:-""latest""} bash```",4/14/16,1
pomadchin,inside it try to run `accumulo init --instance-name ${INSTANCE_NAME} --password ${ACCUMULO_PASSWORD}`,4/14/16,1
pomadchin,I_Ñém interested in a log msg,4/14/16,1
mchendricks1,"I've been getting this other problem as well:```",4/14/16,1
mchendricks1,"```root@ip-172-31-46-128:/home/ubuntu/GEOINT/geodocker-cluster/prod/runners# docker rm 7878dd1d26cfError response from daemon: Driver aufs failed to remove root filesystem 7878dd1d26cfdc532b3ce427c545f00c5bb531e7ed6c70620b6f85ef6b5a605e: rename /var/lib/docker/aufs/diff/f0f97d94a22193dd878a641d818b0f477920b698b65237c58a0629ec9cc10998 /var/lib/docker/aufs/diff/f0f97d94a22193dd878a641d818b0f477920b698b65237c58a0629ec9cc10998-removing: device or resource busy```",4/14/16,1
pomadchin,try to add force flag,4/14/16,1
pomadchin,`-f`,4/14/16,1
mchendricks1,It might be related to the typo you found. At first I thought it was related to EBS problems.,4/14/16,1
mchendricks1,"Yeah, eventually it will go away.",4/14/16,1
pomadchin,sounds like aufs bug ._.,4/14/16,1
mchendricks1,After a reboot.,4/14/16,1
mchendricks1,Ah.,4/14/16,1
pomadchin,updated init script ,4/14/16,1
mchendricks1,"Have you tried this in AWS? Or better yet, what OS version are you using?",4/14/16,1
pomadchin,i tried on centos and on ubuntu ,4/14/16,1
pomadchin,digital ocean,4/14/16,1
pomadchin,16 gb machines,4/14/16,1
mchendricks1,I'm using ubuntu 14.04 m3.xlarge machines.,4/14/16,1
mchendricks1,Docker 1.11.xx,4/14/16,1
pomadchin,should work though,4/14/16,1
pomadchin,so let_Ñés start with init; probably there could be smth with zoo_Ñés avaibility ,4/14/16,1
mchendricks1,"Alright, I'll stop, clean, and restart everything on all the nodes.",4/14/16,1
pomadchin,i setuped logs into stdout,4/14/16,1
pomadchin,"so you can use the command below to see logs```bashdocker logs -f --tail 10 <container-name or hash>```",4/14/16,1
pomadchin,10 is number of lines,4/14/16,1
mchendricks1,"@pomadchin Got a script parse error in accumulo-init:```docker: Error parsing reference: "" "" is not a valid repository/tag.See 'docker run --help'../1-accumulo-init.sh: line 39: --env=HADOOP_MASTER_ADDRESS=leader: command not found```",4/14/16,1
mchendricks1,I don't think it likes the command line parameter.,4/14/16,1
pomadchin,how do you start it?,4/14/16,1
pomadchin,try to pull it ._.,4/14/16,1
mchendricks1,"```./1-accumulo-init.sh -t=0.1.0 -hma=leader -az=""leader,follower01,follower02"" -as=secret -ap=GisPwd -in=gis```",4/14/16,1
pomadchin,"yep, pls pull again;were wrong spaces after slash %)",4/14/16,1
pomadchin,"and after it will stop try to look through logs: ```bashdocker logs -f --tail 50 accumulo-init```",4/14/16,1
pomadchin,if there would be smth like Y / N lag; that_Ñés because you already have inited accumulo ,4/14/16,1
pomadchin,"you can force init: ```bashdocker run -it \  --name=accumulo-init \  --net=host \  --env=""HADOOP_MASTER_ADDRESS=leader"" \  --env=""ACCUMULO_ZOOKEEPERS=leader,follower01,follower02"" \  --env=""ACCUMULO_SECRET=secret"" \  --env=""ACCUMULO_PASSWORD=GisPwd"" \  daunnc/geodocker-accumulo-master:0.1.0 bash# inside docker$ hadoop fs -rm -r /accumulo$ accumulo init --instance-name ${INSTANCE_NAME} --password ${ACCUMULO_PASSWORD}```",4/14/16,1
shiraeeshi,> just to clarify: it works only when geotrellis.kryo.AvroRegistrator do nothing.,4/14/16,1
shiraeeshi,"I rechecked it: I was wrong, it works regardless of AvroRegistrator being in enabled or disabled state.",4/14/16,1
pomadchin,"@shiraeeshi yep, yesterday i checked that fact, so registration lines are still into the chatta demo project",4/14/16,1
mchendricks1,"accumulo-init is failing for a new reason now:```",4/14/16,1
mchendricks1,"```./1-accumulo-init.sh -t=0.1.0 -hma=leader -az=""leader,follower01,follower02"" -as=secret -ap=GisPwd -in=gis./1-accumulo-init.sh -t=0.1.0 -hma=leader -az=""leader,follower01,follower02"" -as=secret -ap=GisPwd -in=gis```",4/14/16,1
mchendricks1,"```Status: Downloaded newer image for daunnc/geodocker-accumulo-master:0.1.02016-04-14 19:27:55,240 [vfs.AccumuloVFSClassLoader] WARN : ignoring classpath entry hdfs://leader/accumulo/system-classpath/[^.].*.jar2016-04-14 19:27:55,303 [vfs.AccumuloVFSClassLoader] WARN : ignoring classpath entry hdfs://leader/accumulo/system-classpath/[^.].*.jar2016-04-14 19:27:55,567 [fs.VolumeManagerImpl] WARN : dfs.datanode.synconclose set to false in hdfs-site.xml: data loss is possible on hard system reset or power loss2016-04-14 19:27:55,569 [init.Initialize] INFO : Hadoop Filesystem is hdfs://leader2016-04-14 19:27:55,569 [init.Initialize] INFO : Accumulo data dirs are [hdfs://leader/accumulo]2016-04-14 19:27:55,569 [init.Initialize] INFO : Zookeeper server is leader,follower01,follower022016-04-14 19:27:55,569 [init.Initialize] INFO : Checking if Zookeeper is available. If this hangs, then you need to make sure zookeeper is running2016-04-14 19:27:55,728 [init.Initialize] ERROR: FATAL It appears the directories [hdfs://leader/accumulo] were previously initialized.2016-04-14 19:27:55,728 [init.Initialize] ERROR: FATAL: Change the property instance.volumes to use different filesystems,2016-04-14 19:27:55,728 [init.Initialize] ERROR: FATAL: or change the property instance.dfs.dir to use a different directory.2016-04-14 19:27:55,728 [init.Initialize] ERROR: FATAL: The current value of instance.dfs.uri is ||2016-04-14 19:27:55,729 [init.Initialize] ERROR: FATAL: The current value of instance.dfs.dir is |/accumulo|2016-04-14 19:27:55,729 [init.Initialize] ERROR: FATAL: The current value of instance.volumes is |hdfs://leader/accumulo|```",4/14/16,1
pomadchin,"ye, try to reinit accumulo",4/14/16,1
pomadchin,"```bashdocker run -it \  --name=accumulo-init \  --net=host \  --env=""HADOOP_MASTER_ADDRESS=leader"" \  --env=""ACCUMULO_ZOOKEEPERS=leader,follower01,follower02"" \  --env=""ACCUMULO_SECRET=secret"" \  --env=""ACCUMULO_PASSWORD=GisPwd"" \  daunnc/geodocker-accumulo-master:0.1.0 bash# inside docker$ hadoop fs -rm -r /accumulo$ accumulo init --instance-name ${INSTANCE_NAME} --password ${ACCUMULO_PASSWORD}```",4/14/16,1
mchendricks1,Well I stopped all of the containers and deleted them and the images...?,4/14/16,1
pomadchin,"you see, that by default it forwards hdfs into /data directory",4/14/16,1
pomadchin,to reduce impact of running it inside the container we have to forward heavy volumes outside the container,4/14/16,1
pomadchin,so you need to cleanup hdfs,4/14/16,1
mchendricks1,Ah.,4/14/16,1
mchendricks1,Can I just delete the data/gt directory?,4/14/16,1
mchendricks1,On all three nodes.,4/14/16,1
pomadchin,"dont recommend you (: faster solution is ```bashdocker exec -it hadoop-data hadoop fs -rm -r /accumulo```",4/14/16,1
mchendricks1,Ok.,4/14/16,1
mchendricks1,"```cb6c92495ba3        daunnc/geodocker-accumulo-master:0.1.0    ""/sbin/entrypoint.sh ""   2 minutes ago        Exited (0) 2 minutes ago                            accumulo-init```",4/14/16,1
pomadchin,cool,4/14/16,1
pomadchin,what_Ñés in logs?,4/14/16,1
pomadchin,"```bashdocker logs -f --tail 50 accumulo-init```",4/14/16,1
mchendricks1,Still getting the accumulo-init container in the accumulo-master.,4/14/16,1
pomadchin,"yes, that_Ñés correct",4/14/16,1
mchendricks1,"```2016-04-14 19:34:18,478 [vfs.AccumuloVFSClassLoader] WARN : ignoring classpath entry hdfs://leader/accumulo/system-classpath/[^.].*.jar2016-04-14 19:34:18,533 [vfs.AccumuloVFSClassLoader] WARN : ignoring classpath entry hdfs://leader/accumulo/system-classpath/[^.].*.jar2016-04-14 19:34:18,769 [fs.VolumeManagerImpl] WARN : dfs.datanode.synconclose set to false in hdfs-site.xml: data loss is possible on hard system reset or power loss2016-04-14 19:34:18,771 [init.Initialize] INFO : Hadoop Filesystem is hdfs://leader2016-04-14 19:34:18,772 [init.Initialize] INFO : Accumulo data dirs are [hdfs://leader/accumulo]2016-04-14 19:34:18,772 [init.Initialize] INFO : Zookeeper server is leader,follower01,follower022016-04-14 19:34:18,772 [init.Initialize] INFO : Checking if Zookeeper is available. If this hangs, then you need to make sure zookeeper is runningInstance name ""gis"" exists. Delete existing entry from zookeeper? [Y/N] : ```",4/14/16,1
mchendricks1,Unfortunately most the of accumulo containers are stuck in a restart sequence.,4/14/16,1
pomadchin,"you see, because accumulo is not inited",4/14/16,1
pomadchin,"so there are two options; to restart with cleaning /data folder or to kill accumulo containersto init it manually and to start them one by one with viewing logs",4/14/16,1
pomadchin,"to init manually: ```bashdocker run -it \  --name=accumulo-init \  --net=host \  --env=""HADOOP_MASTER_ADDRESS=leader"" \  --env=""ACCUMULO_ZOOKEEPERS=leader,follower01,follower02"" \  --env=""ACCUMULO_SECRET=secret"" \  --env=""ACCUMULO_PASSWORD=GisPwd"" \  daunnc/geodocker-accumulo-master:0.1.0 bash# inside docker$ hadoop fs -rm -r /accumulo$ accumulo init --instance-name ${INSTANCE_NAME} --password ${ACCUMULO_PASSWORD}```",4/14/16,1
mchendricks1,Let me try that...,4/14/16,1
pomadchin,"also in order to avoid restarting (restart happens due to some error) you can remove restart option: https://github.com/geotrellis/geodocker-cluster/blob/master/prod/runners/2-accumulo-master.sh#L35",4/14/16,1
pomadchin,Sorry for that ): for some reason on my machines that just worked,4/14/16,1
mchendricks1,Do I need to run this init manually on each node?,4/14/16,1
pomadchin,thanks a lot for trying it,4/14/16,1
pomadchin,"nope, only once",4/14/16,1
mchendricks1,"Well if this works, it will save me a lot of time as well. :-)",4/14/16,1
pomadchin,it makes neccesary records in zookeeper; and formats accumulo folder on hdfs ,4/14/16,1
pomadchin,so just once on one machine ,4/14/16,1
mchendricks1,"Alright, I ran the manual container init you copied.",4/14/16,1
mchendricks1,Do I just run the accumulo-init script?,4/14/16,1
pomadchin,what_Ñés the output after this manual run?,4/14/16,1
mchendricks1,"```root@ip-172-31-46-128:/opt/accumulo# accumulo init --instance-name gis --password secret2016-04-14 19:51:46,676 [vfs.AccumuloVFSClassLoader] WARN : ignoring classpath entry hdfs://leader/accumulo/system-classpath/[^.].*.jar2016-04-14 19:51:46,747 [vfs.AccumuloVFSClassLoader] WARN : ignoring classpath entry hdfs://leader/accumulo/system-classpath/[^.].*.jar2016-04-14 19:51:47,048 [fs.VolumeManagerImpl] WARN : dfs.datanode.synconclose set to false in hdfs-site.xml: data loss is possible on hard system reset or power loss2016-04-14 19:51:47,049 [init.Initialize] INFO : Hadoop Filesystem is hdfs://leader2016-04-14 19:51:47,050 [init.Initialize] INFO : Accumulo data dirs are [hdfs://leader/accumulo]2016-04-14 19:51:47,050 [init.Initialize] INFO : Zookeeper server is leader,follower01,follower022016-04-14 19:51:47,050 [init.Initialize] INFO : Checking if Zookeeper is available. If this hangs, then you need to make sure zookeeper is runningInstance name ""gis"" exists. Delete existing entry from zookeeper? [Y/N] : Y2016-04-14 19:51:52,632 [Configuration.deprecation] INFO : dfs.replication.min is deprecated. Instead, use dfs.namenode.replication.min2016-04-14 19:51:52,951 [Configuration.deprecation] INFO : dfs.block.size is deprecated. Instead, use dfs.blocksize2016-04-14 19:51:53,741 [conf.AccumuloConfiguration] INFO : Loaded class : org.apache.accumulo.server.security.handler.ZKAuthorizor2016-04-14 19:51:53,743 [conf.AccumuloConfiguration] INFO : Loaded class : org.apache.accumulo.server.security.handler.ZKAuthenticator2016-04-14 19:51:53,746 [conf.AccumuloConfiguration] INFO : Loaded class : org.apache.accumulo.server.security.handler.ZKPermHandler```",4/14/16,1
pomadchin,cool,4/14/16,1
pomadchin,just start master via script,4/14/16,1
pomadchin,and throw me logs there,4/14/16,1
pomadchin,"```bashdocker logs -f --tail 50 accumulo-master```",4/14/16,1
pomadchin,"before start you can remove `restart=always` flag, to avoid restarting accidents",4/14/16,1
mchendricks1,"```2016-04-14 19:53:56,614 [impl.MetricsSystemImpl] INFO : Scheduled snapshot period at 10 second(s).2016-04-14 19:53:56,614 [impl.MetricsSystemImpl] INFO : Accumulo metrics system started2016-04-14 19:53:56,633 [master.Master] INFO : Setting master lock data to leader:99992016-04-14 19:53:56,646 [replication.WorkDriver] INFO : Initializing work assigner implementation of org.apache.accumulo.master.replication.UnorderedWorkAssigner2016-04-14 19:53:56,662 [replication.WorkDriver] INFO : Starting replication work assignment thread using org.apache.accumulo.master.replication.UnorderedWorkAssigner2016-04-14 19:53:56,670 [master.Master] INFO : Started replication coordinator service at leader:100012016-04-14 19:53:56,905 [server.Accumulo] WARN : System swappiness setting is greater than ten (60) which can cause time-sensitive operations to be delayed.  Accumulo is time sensitive because it needs to maintain distributed lock agreement.```",4/14/16,1
pomadchin,ok,4/14/16,1
pomadchin,now tracer,4/14/16,1
pomadchin,after that gc ,4/14/16,1
pomadchin,and monitor,4/14/16,1
pomadchin,"all in all you should have just one master process, one gc, one monitor and one tracer on a leader node",4/14/16,1
mchendricks1,those appear to be up and stable currently.,4/14/16,1
pomadchin,ok,4/14/16,1
pomadchin,start tserver on leader,4/14/16,1
pomadchin,and throw log there,4/14/16,1
mchendricks1,"In the readme, tserver is only run on the worker nodes.",4/14/16,1
mchendricks1,You still want me to run tserver on the leader?,4/14/16,1
pomadchin,"hm, then follow readme; lets start it on a worker node",4/14/16,1
pomadchin,and throw log there,4/14/16,1
pomadchin,lets start only one tserver (: to see how it goes,4/14/16,1
mchendricks1,"```2016-04-14 19:59:23,202 [client.ClientServiceHandler] ERROR: ThriftSecurityExceptionThriftSecurityException(user:root, code:BAD_CREDENTIALS)	at org.apache.accumulo.server.security.SecurityOperation.authenticate(SecurityOperation.java:207)	at org.apache.accumulo.server.security.AuditedSecurityOperation.authenticate(AuditedSecurityOperation.java:663)```",4/14/16,1
pomadchin,hm,4/14/16,1
pomadchin,"can you run this cmd? ```bashdocker exec -it accumulo-tserver cat ${ACCUMULO_CONF_DIR}/accumulo-site.xml```",4/14/16,1
mchendricks1,"as-is: ```root@ip-172-31-40-52:/home/ubuntu/GEOINT/geodocker-cluster/prod/runners# docker exec -it accumulo-tserver cat ${ACCUMULO_CONF_DIR}/accumulo-site.xmlcat: /accumulo-site.xml: No such file or directory```",4/14/16,1
mchendricks1,missed the path,4/14/16,1
mchendricks1,Hold on.,4/14/16,1
pomadchin,/opt/accumulo/conf,4/14/16,1
mchendricks1,"```  <property>    <name>trace.user</name>    <value>root</value>  </property>  <property>    <name>trace.token.property.password</name>        <value>GisPwd</value>  </property>  <property>    <name>instance.secret</name>    <value>secret</value>    <description>A secret unique to a given instance that all servers must know in order to communicate with one another.      Change it before initialization. To      change it later use ./bin/accumulo org.apache.accumulo.server.util.ChangeSecret --old [oldpasswd] --new [newpasswd],      and then update this file.    </description>  </property>```",4/14/16,1
pomadchin,"```bashdocker exec -it accumulo-master cat /opt/accumulo/conf/accumulo-site.xml```",4/14/16,1
pomadchin,guess the same o:,4/14/16,1
pomadchin,and what_Ñés in the web ui: leader:50095 ?,4/14/16,1
mchendricks1,The file contents look the same on those three fields at least.,4/14/16,1
pomadchin,i have some ideas that that_Ñés a not critical err,4/14/16,1
pomadchin,related to sending logs to master,4/14/16,1
pomadchin,can you check web ui and nodes amount there?,4/14/16,1
mchendricks1,The web ui says there are 2 tablet servers,4/14/16,1
pomadchin,(that_Ñés a bug that need to be fixed but probably you can start to work with that),4/14/16,1
pomadchin,cool,4/14/16,1
pomadchin,that means that works O:,4/14/16,1
mchendricks1,Thank goodness!,4/14/16,1
pomadchin,"yep, probably you started your tablets too quickly? or they were launched before master or smth like that? any thoughts why containers were restarting?",4/14/16,1
mchendricks1,both spark-workers are stuck restarting,4/14/16,1
pomadchin,oh spark workers,4/14/16,1
mchendricks1,and accumulo-tracer is stuck restarting as well.,4/14/16,1
pomadchin,ok; so that_Ñés a credensials problem,4/14/16,1
pomadchin,accumulo tracer collects logs,4/14/16,1
pomadchin,"will debug that, that_Ñés not critical though and you can start work with accumulo",4/14/16,1
pomadchin,you can just kill it,4/14/16,1
pomadchin,:D,4/14/16,1
mchendricks1,"Ok, cool. And the spark-workers restarting?",4/14/16,1
pomadchin,hm you said you pulled them today from repo?,4/14/16,1
mchendricks1,Yes.,4/14/16,1
pomadchin,so the first step kill them all (rm -rf),4/14/16,1
mchendricks1,I just went to leader:8080 UI and the workers aren't showing up.,4/14/16,1
pomadchin,wow,4/14/16,1
pomadchin,so master works,4/14/16,1
pomadchin,that_Ñés cool,4/14/16,1
pomadchin,dont kill it,4/14/16,1
pomadchin,"kill just workers and lets start them but remove that line in a start script: https://github.com/geotrellis/geodocker-cluster/blob/master/prod/runners/2-spark-worker.sh#L31",4/14/16,1
mchendricks1,Ok.,4/14/16,1
pomadchin,"after that logs our best friend ): ```bashdocker logs -f --tail 50 spark-worker```",4/14/16,1
mchendricks1,"The spark-workers are failing because of this spark-master log:```",4/14/16,1
mchendricks1,"```16/04/14 20:15:56 WARN remote.ReliableDeliverySupervisor: Association with remote system [akka.tcp://sparkWorker@ip-172-31-34-10:35672] has failed, address is now gated for [5000] ms. Reason: [Disassociated] 1```",4/14/16,1
pomadchin,and what_Ñés in spark workers?,4/14/16,1
mchendricks1,"```16/04/14 20:18:42 INFO worker.Worker: Connecting to master leader:7077...16/04/14 20:18:48 INFO worker.Worker: Retrying connection to master (attempt # 1)16/04/14 20:18:48 ERROR util.SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[sparkWorker-akka.actor.default-dispatcher-3,5,main]java.util.concurrent.RejectedExecutionException: Task java.util.concurrent.FutureTask@23711d0c rejected from java.util.concurrent.ThreadPoolExecutor@1b851d20[Running, pool size = 1, active threads = 0, queued tasks = 0, completed tasks = 1]	at java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2047)```",4/14/16,1
pomadchin,"lets kill workersand restart master",4/14/16,1
pomadchin,by restart i mean kill it (rm -rf),4/14/16,1
pomadchin,and start it,4/14/16,1
pomadchin,i_Ñém interested in reason why master is so dumb,4/14/16,1
mchendricks1,"Same error on worker...```6/04/14 20:31:05 INFO server.AbstractConnector: Started SelectChannelConnector@0.0.0.0:808116/04/14 20:31:05 INFO util.Utils: Successfully started service 'WorkerUI' on port 8081.16/04/14 20:31:05 INFO ui.WorkerWebUI: Started WorkerWebUI at http://172.31.40.52:808116/04/14 20:31:05 INFO worker.Worker: Connecting to master leader:7077...16/04/14 20:31:18 INFO worker.Worker: Retrying connection to master (attempt # 1)16/04/14 20:31:18 ERROR util.SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[sparkWorker-akka.actor.default-dispatcher-3,5,main]java.util.concurrent.RejectedExecutionException: Task java.util.concurrent.FutureTask@42faa072 rejected from java.util.concurrent.ThreadPoolExecutor@67ccd831[Running, pool size = 1, active threads = 0, queued tasks = 0, completed tasks = 0]	at java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2047)	at java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:823)```",4/14/16,1
pomadchin,this ip address,4/14/16,1
pomadchin,172.31.40.52,4/14/16,1
pomadchin,can you throw there me first lines of your `/etc/hosts`?,4/14/16,1
mchendricks1,"```root@ip-172-31-40-52:/home/ubuntu/GEOINT/geodocker-cluster/prod/runners# more /etc/hosts127.0.0.1 localhost172.31.46.128 leader172.31.40.52 follower01172.31.34.10 follower02```",4/14/16,1
pomadchin,172.31.40.52 that_Ñés follower,4/14/16,1
pomadchin,O:,4/14/16,1
pomadchin,you started your master on follower?,4/14/16,1
mchendricks1,Nope,4/14/16,1
mchendricks1,Not using the scripts at least...,4/14/16,1
pomadchin,:D ,4/14/16,1
pomadchin,"and what_Ñés master logs?  i think that there could be some error due to bad hosts file ._. it tries to bind master on a local 0.0.0.0 ",4/14/16,1
pomadchin,or to localhost,4/14/16,1
pomadchin,probably smth could go wrong ._.,4/14/16,1
pomadchin,what_Ñés the master log after restart?,4/14/16,1
mchendricks1,"```16/04/14 20:30:30 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkMaster@ip-172-31-46-128:7077]16/04/14 20:30:30 INFO util.Utils: Successfully started service 'sparkMaster' on port 7077.16/04/14 20:30:30 INFO master.Master: Starting Spark master at spark://ip-172-31-46-128:707716/04/14 20:30:30 INFO master.Master: Running Spark version 1.5.216/04/14 20:30:30 INFO server.Server: jetty-8.y.z-SNAPSHOT16/04/14 20:30:30 INFO server.AbstractConnector: Started SelectChannelConnector@0.0.0.0:808016/04/14 20:30:30 INFO util.Utils: Successfully started service 'MasterUI' on port 8080.16/04/14 20:30:30 INFO ui.MasterWebUI: Started MasterWebUI at http://172.31.46.128:808016/04/14 20:30:30 INFO server.Server: jetty-8.y.z-SNAPSHOT16/04/14 20:30:30 INFO server.AbstractConnector: Started SelectChannelConnector@ip-172-31-46-128:606616/04/14 20:30:30 INFO util.Utils: Successfully started service on port 6066.16/04/14 20:30:30 INFO rest.StandaloneRestServer: Started REST server for submitting applications on port 606616/04/14 20:30:30 INFO master.Master: I have been elected leader! New state: ALIVE16/04/14 20:31:06 ERROR akka.ErrorMonitor: dropping message [class akka.actor.ActorSelectionMessage] for non-local recipient [Actor[akka.tcp://sparkMaster@leader:7077/]] arriving at [akka.tcp://sparkMaster@leader:7077] inbound addresses are [akka.tcp://sparkMaster@ip-172-31-46-128:7077]akka.event.Logging$Error$NoCause$16/04/14 20:31:19 INFO master.Master: ip-172-31-40-52:49612 got disassociated, removing it.16/04/14 20:31:19 WARN remote.ReliableDeliverySupervisor: Association with remote system [akka.tcp://sparkWorker@ip-172-31-40-52:49612] has failed, address is now gated for [5000] ms. Reason: [Disassociated] 16/04/14 20:31:19 INFO master.Master: ip-172-31-40-52:49612 got disassociated, removing it.```",4/14/16,1
pomadchin,"can you fix /etc/hosts first line to ```bash127.0.0.1 localhost.localdomain localhost```?",4/14/16,1
pomadchin,that really sucks sometimes; because spark sometimes binds on a wrong ip address :\,4/14/16,1
mchendricks1,"yes, I was running into that yesterday on my laptop with the VMs and multiple NICs.",4/14/16,1
pomadchin,need to fix it inside image some how i think by providing globals ~like SPARK_LOCAL_IP or smth like that (don_Ñét remember actually),4/14/16,1
mchendricks1,Still exiting due to the same error.,4/14/16,1
mchendricks1,"Alright, well I need to go get ready to pick up my kid. I'll jump back into this tomorrow and see if using IP addresses instead makes a difference.",4/14/16,1
mchendricks1,Thx for your help today.,4/14/16,1
pomadchin,yep; thank you!,4/14/16,1
pomadchin,"some ideas: so to the problem probably in wrong ip addreses binding, so you can start master: ```bashdocker run \  --name=spark-master \  --net=host \  --detach \  --restart=always \  --env=""SPARK_DAEMON_MEMORY=2g"" \  --env=""SPARK_SUBMIT_DRIVER_MEMORY=1g"" \  --env=""HADOOP_MASTER_ADDRESS=${HADOOP_MASTER_ADDRESS}"" \  daunnc/geodocker-spark-master:${TAG:-""latest_Ñù}  spark-class org.apache.spark.deploy.master.Master --host leader```and workers```bashdocker run \  --name=spark-worker \  --net=host \  --detach \  --restart=always \  --env=""SPARK_DAEMON_MEMORY=2g"" \  --env=""SPARK_SUBMIT_DRIVER_MEMORY=1g"" \  --env=""SPARK_WORKER_CORES=8"" \  --env=""SPARK_WORKER_MEMORY=16g"" \  --env=""HADOOP_MASTER_ADDRESS=${HADOOP_MASTER_ADDRESS}"" \  --env=""SPARK_MASTER=${SPARK_MASTER}"" \  --volume=${VOLUME:-""/data/gt/spark""}:/data/spark \  daunnc/geodocker-spark-worker:${TAG:-""latest_Ñù} spark-class org.apache.spark.deploy.worker.Worker --host follower01 spark://leader:7077```",4/14/16,1
pomadchin,note that external names passed there ,4/14/16,1
lokifacio,I'm doing some interpolation tests with `VectorToRaster.idwInterpolation`,4/15/16,1
lokifacio,why is there only the Int version?,4/15/16,1
jnh5y,@lossyrob @echeipesh there's an SFCurve PR https://github.com/locationtech/sfcurve/pull/14 from Emilio.  Do either of you have any time/interest in reviewing it today?,4/15/16,1
lossyrob,looking now,4/15/16,1
lossyrob,"@lokifacio because we didn_Ñét make a Double version, apparently. that_Ñés an oversight by the project. I don_Ñét think that_Ñéll get into 0.10.0 but the code should be easy enough to grab if you need to modify it to use Doubles. Also that would be a good 0.10.1 contribution :)",4/15/16,1
mchendricks1,@pomadchin After you point about the DNS names and spark network binding I was able to find the internal network DNS that worked and now have a functioning-ish three node cluster.,4/15/16,1
mchendricks1,"However, there is still a problem with spark-etl-ingest and Accumulo.",4/15/16,1
mchendricks1,"When I run this command:```",4/15/16,1
mchendricks1,"```spark-submit \--class geotrellis.spark.etl.SinglebandIngest \--master spark://${MY_LEADER_NAME}:7077 \--executor-memory 32m \--executor-cores 1 \--driver-memory 32m \/data/spark/demo-assembly-0.1.0.jar \--input hadoop --format geotiff  --cache NONE -I path=""file:///data/spark/LC80140322014139LGN00/"" \--output accumulo -O instance=gis table=nlcd-table user=root password=secret zookeeper=${MY_LEADER_NAME}:2181 \--layer nlcd-tms --crs EPSG:3857 --pyramid --layoutScheme tms```",4/15/16,1
mchendricks1,"I get the following error when the job runs...```16/04/15 16:01:47 WARN client.ClientConfiguration: Found no client.conf in default paths. Using default client configuration values.16/04/15 16:01:47 INFO zookeeper.ZooKeeper: Client environment:zookeeper.version=3.4.6-1569965, built on 02/20/2014 09:09 GMT16/04/15 16:01:47 INFO zookeeper.ZooKeeper: Client environment:host.name=ip-172-31-43-173.us-west-2.compute.internal16/04/15 16:01:47 INFO zookeeper.ZooKeeper: Client environment:java.version=1.8.0_7716/04/15 16:01:47 INFO zookeeper.ZooKeeper: Client environment:java.vendor=Oracle Corporation16/04/15 16:01:47 INFO zookeeper.ZooKeeper: Client environment:java.home=/usr/lib/jvm/java-8-oracle/jre16/04/15 16:01:47 INFO zookeeper.ZooKeeper: Client environment:java.class.path=/etc/spark:/opt/spark/lib/spark-assembly-1.5.2-hadoop2.6.0.jar:/opt/spark/lib/datanucleus-core-3.2.10.jar:/opt/spark/lib/datanucleus-api-jdo-3.2.6.jar:/opt/spark/lib/datanucleus-rdbms-3.2.9.jar:/opt/hadoop/etc/hadoop/16/04/15 16:01:47 INFO zookeeper.ZooKeeper: Client environment:java.library.path=/usr/java/packages/lib/amd64:/usr/lib64:/lib64:/lib:/usr/lib16/04/15 16:01:47 INFO zookeeper.ZooKeeper: Client environment:java.io.tmpdir=/tmp16/04/15 16:01:47 INFO zookeeper.ZooKeeper: Client environment:java.compiler=<NA>16/04/15 16:01:47 INFO zookeeper.ZooKeeper: Client environment:os.name=Linux16/04/15 16:01:47 INFO zookeeper.ZooKeeper: Client environment:os.arch=amd6416/04/15 16:01:47 INFO zookeeper.ZooKeeper: Client environment:os.version=3.13.0-74-generic16/04/15 16:01:47 INFO zookeeper.ZooKeeper: Client environment:user.name=root16/04/15 16:01:47 INFO zookeeper.ZooKeeper: Client environment:user.home=/root16/04/15 16:01:47 INFO zookeeper.ZooKeeper: Client environment:user.dir=/data/spark/LC80170302015307LGN0016/04/15 16:01:47 INFO zookeeper.ZooKeeper: Initiating client connection, connectString=ip-172-31-40-6:2181 sessionTimeout=30000 watcher=org.apache.accumulo.fate.zookeeper.ZooSession$ZooWatcher@184afb7816/04/15 16:01:47 INFO zookeeper.ClientCnxn: Opening socket connection to server ip-172-31-40-6.us-west-2.compute.internal/172.31.40.6:2181. Will not attempt to authenticate using SASL (unknown error)16/04/15 16:01:47 INFO zookeeper.ClientCnxn: Socket connection established to ip-172-31-40-6.us-west-2.compute.internal/172.31.40.6:2181, initiating session16/04/15 16:01:47 INFO zookeeper.ClientCnxn: Session establishment complete on server ip-172-31-40-6.us-west-2.compute.internal/172.31.40.6:2181, sessionid = 0x1541a8040f70002, negotiated timeout = 30000Exception in thread ""main"" org.apache.accumulo.core.client.AccumuloSecurityException: Error BAD_CREDENTIALS for user root - Username or Password is Invalid	at org.apache.accumulo.core.client.impl.ServerClient.execute(ServerClient.java:63)```",4/15/16,1
lossyrob,can you run `accumulo shell -u root -p secret` on the master?,4/15/16,1
mchendricks1,"This is under geodocker, so I'll assume that you mean run that command under the container running accumulo-master.",4/15/16,1
lossyrob,"(assuming that the accumulo password is _ÑÒsecret_Ñù, which we_Ñéve been using as a default mainly)",4/15/16,1
lossyrob,yeah,4/15/16,1
mchendricks1,Same error.,4/15/16,1
lossyrob,"ok, so something with the credentaisl are messed up",4/15/16,1
mchendricks1,"```docker exec -it accumulo-master bashroot@ip-172-31-40-6:/opt/accumulo# accumulo shell -u root -p secret```",4/15/16,1
mchendricks1,@pomadchin and I went through that yesterday while working through the spark-master problems. I had run into it under the geotrellis-ec2-cluster vagrant deployment as well.,4/15/16,1
mchendricks1,I could only accumulo shell when I sudo -i to root.,4/15/16,1
lossyrob,so accumulo is only allowing you to access it if your in the superuser account? interesting,4/15/16,1
mchendricks1,Under the geotrellis-ec2-cluster vagrant deployment - yes.,4/15/16,1
pomadchin,:0 hm,4/15/16,1
pomadchin,would be near laptop in 40 mins,4/15/16,1
mchendricks1,Ok. I'll noodle around with this for a bit more.,4/15/16,1
pomadchin,"Btw, what's with spark containers?",4/15/16,1
mchendricks1,The spark containers appear to be running fine at this point.,4/15/16,1
mchendricks1,I can see them in the spark-master UI.,4/15/16,1
mchendricks1,The fat jar went through.,4/15/16,1
pomadchin,Cool,4/15/16,1
mchendricks1,Everything that was failing under my low powered laptop VMs seems to be working fine in m4.large/xlarge VMs.,4/15/16,1
pomadchin,Want to believe that ,4/15/16,1
pomadchin,Oh got it,4/15/16,1
mchendricks1,"I'm trying 'GisPwd' instead - from the geodocker README - which is a different ""password or secret"". In any event, that seems to have worked.",4/15/16,1
mchendricks1,"So ""root"" and ""GisPwd"" are the default combination it seems.",4/15/16,1
mchendricks1,"OK. Got further into the spark-etl process but got a new error.```16/04/15 16:31:22 INFO scheduler.DAGScheduler: Job 3 finished: saveAsNewAPIHadoopFile at AccumuloWriteStrategy.scala:59, took 5.819818 sException in thread ""main"" geotrellis.spark.io.package$LayerWriteError: Failed to write Layer(name = ""nlcd-tms"", zoom = 11)Caused by: org.apache.accumulo.core.client.AccumuloException: hdfs://ip-172-31-40-6/geotrellis-ingest/ea76d8de-665d-4697-944e-eb884cb679de-Mxa9LH8gYh: java.io.IOException: hdfs://ip-172-31-40-6/geotrellis-ingest/ea76d8de-665d-4697-944e-eb884cb679de-Mxa9LH8gYh is not in a volume configured for Accumulo	at org.apache.accumulo.core.client.impl.TableOperationsImpl.doFateOperation(TableOperationsImpl.java:300)Caused by: ThriftTableOperationException(tableId:3, tableName:null, op:BULK_IMPORT, type:BULK_BAD_INPUT_DIRECTORY, description:hdfs://ip-172-31-40-6/geotrellis-ingest/ea76d8de-665d-4697-944e-eb884cb679de-Mxa9LH8gYh: java.io.IOException: hdfs://ip-172-31-40-6/geotrellis-ingest/ea76d8de-665d-4697-944e-eb884cb679de-Mxa9LH8gYh is not in a volume configured for Accumulo)	at org.apache.accumulo.core.master.thrift.FateService$waitForFateOperation_result$waitForFateOperation_resultStandardScheme.read(FateService.java:```",4/15/16,1
mchendricks1,"This is the command I ran.```spark-submit \--class geotrellis.spark.etl.SinglebandIngest \--master spark://${MY_LEADER_NAME}:7077 \--executor-memory 32m \--executor-cores 1 \--driver-memory 32m \/data/spark/demo-assembly-0.1.0.jar \--input hadoop --format geotiff  --cache NONE -I path=""file:///data/spark/LC80170302015307LGN00/"" \--output accumulo -O instance=gis table=nlcd_table user=root password=GisPwd zookeeper=${MY_LEADER_NAME}:2181 \--layer nlcd-tms --crs EPSG:3857 --pyramid --layoutScheme tms```",4/15/16,1
mchendricks1,"@lossyrob BTW, this is stuck on geotrellis RC2 because I couldn't get the landsat-demo to build when I changed the version to RC4. Some classes had been renamed/moved, etc.",4/15/16,1
mchendricks1,I will try the other demo that looks like it includes what I need and is kept up to date.,4/15/16,1
lossyrob,I_Ñéll update landsat demo. it_Ñés not many changes,4/15/16,1
Ronald33,"Hi, a question I don't found the library import geotrellis.source.{ValueSource, RasterSource} in the version 0.10 someone can tell me where is that library ?",4/15/16,1
lossyrob,The ValueSource and RasterSource code is deprecated in 0.10. What are you trying to do?,4/15/16,1
Ronald33,"I'm trying show a ARG, something similar to the example ""geotrellis-spray-tutorial"" but with the version 0.10",4/15/16,1
lossyrob,I would recommend looking to https://github.com/geotrellis/geotrellis-landsat-tutorial instead of the spray tutorial,4/15/16,1
Ronald33,"Great, thanks @lossyrob ",4/15/16,1
pomadchin,"@mchendricks1 is it ok right now? will setup this env on amazon too soon; ",4/15/16,1
mchendricks1,"@pomadchin I'm still getting this error:```Caused by: ThriftTableOperationException(tableId:3, tableName:null, op:BULK_IMPORT, type:BULK_BAD_INPUT_DIRECTORY, description:hdfs://ip-172-31-40-6/geotrellis-ingest/bdbe291f-aab4-4093-8ab0-a0c3e12d968b-CkKQwZfWzj: java.io.IOException: hdfs://ip-172-31-40-6/geotrellis-ingest/bdbe291f-aab4-4093-8ab0-a0c3e12d968b-CkKQwZfWzj is not in a volume configured for Accumulo)```",4/15/16,1
pomadchin,is it master? `ip-172-31-40-6`,4/15/16,1
mchendricks1,Yes.,4/15/16,1
pomadchin,"what_Ñés the result of ?```bashdocker exec -it spark-worker hadoop fs -ls /docker exec -it spark-worker hadoop fs -ls hdfs://ip-172-31-40-6/docker exec -it spark-worker hadoop fs -ls hdfs://ip-172-31-40-6/geotrellis-ingest/```",4/15/16,1
mchendricks1,"```root@ip-172-31-40-8:/data/gt/spark# docker exec -it spark-worker hadoop fs -ls /Found 2 itemsdrwxr-xr-x   - root supergroup          0 2016-04-15 15:21 /accumulodrwxr-xr-x   - root supergroup          0 2016-04-15 18:26 /geotrellis-ingest```",4/15/16,1
mchendricks1,It looks like that through the HDFS master UI as well.,4/15/16,1
pomadchin,"and ```bashdocker exec -it spark-worker hadoop fs -ls hdfs://ip-172-31-40-6/```",4/15/16,1
pomadchin,the same?,4/15/16,1
mchendricks1,"```root@ip-172-31-40-8:/home/ubuntu/GEOINT/geodocker-cluster/prod/runners# docker exec -it spark-worker hadoop fs -ls hdfs://ip-172-31-40-6/Found 2 itemsdrwxr-xr-x   - root supergroup          0 2016-04-15 15:21 hdfs://ip-172-31-40-6/accumulodrwxr-xr-x   - root supergroup          0 2016-04-15 18:26 hdfs://ip-172-31-40-6/geotrellis-ingest```",4/15/16,1
mchendricks1,"When I run this command, it hangs (from spark-worker).",4/15/16,1
mchendricks1,"```docker exec -it spark-worker hadoop fs -ls hdfs://ip-172-31-40-6/geotrellis-ingest/```",4/15/16,1
pomadchin,really weird,4/15/16,1
pomadchin,hm,4/15/16,1
pomadchin,think that_Ñés hostnames problem ._.,4/15/16,1
mchendricks1,...geesh...,4/15/16,1
pomadchin,"what_Ñés the ?```bashdocker exec -it spark-worker cat $HADOOP_CONF_DIR/core-site.xml```",4/15/16,1
mchendricks1,Doesn't look like there's a conf dir.,4/15/16,1
mchendricks1,"```# docker exec -it spark-worker ls /opt/hadoopLICENSE.txt  README.txt  etc	  lib	   sbinNOTICE.txt   bin	 include  libexec  share/```",4/15/16,1
mchendricks1,I'm assuming it would be under /opt/hadoop,4/15/16,1
pomadchin,`/opt/hadoop/etc/hadoop/core-site.xml`,4/15/16,1
mchendricks1,"```# docker exec -it spark-worker cat /opt/hadoop/etc/hadoop/core-site.xml<?xml version=""1.0"" encoding=""UTF-8""?><?xml-stylesheet type=""text/xsl"" href=""configuration.xsl""?><configuration>  <property>    <name>fs.defaultFS</name>    <value>hdfs://ip-172-31-40-6</value>  </property>  <property>     <name>hadoop.tmp.dir</name>     <value>file:///data/hdfs/tmp</value>  </property>```",4/15/16,1
pomadchin,cool,4/15/16,1
pomadchin,and on master node?,4/15/16,1
pomadchin,and i am curious about smae thigs on accumulo master and accumulo tablet,4/15/16,1
pomadchin,probably some where it different (smth like localhost / another name / ip address) and it cant resolve that it_Ñés the same name ,4/15/16,1
mchendricks1,"```root@ip-172-31-40-6:/# docker exec -it spark-master cat /opt/hadoop/etc/hadoop/core-site.xml<?xml version=""1.0"" encoding=""UTF-8""?><?xml-stylesheet type=""text/xsl"" href=""configuration.xsl""?><configuration>  <property>    <name>fs.defaultFS</name>    <value>hdfs://ip-172-31-40-6</value>  </property>  <property>     <name>hadoop.tmp.dir</name>     <value>file:///data/hdfs/tmp</value>  </property>```",4/15/16,1
mchendricks1,There is a difference in accumulo-master.,4/15/16,1
mchendricks1,"```# docker exec -it accumulo-master cat /opt/hadoop/etc/hadoop/core-si<?xml version=""1.0"" encoding=""UTF-8""?><?xml-stylesheet type=""text/xsl"" href=""configuration.xsl""?><configuration>  <property>    <name>fs.defaultFS</name>    <value>hdfs://ip-172-31-40-6.us-west-2.compute.internal</value>  </property>  <property>     <name>hadoop.tmp.dir</name>     <value>file:///data/hdfs/tmp</value>  </property>```",4/15/16,1
pomadchin,think that_Ñés the reason ._.,4/15/16,1
pomadchin,you can manually change that value to correct,4/15/16,1
mchendricks1,Ok. I'll clean up everything. I think I did start those before I figured out the shortname was the answer for making spark-master/-workers work.,4/15/16,1
pomadchin,"yep, also check accumulo cofiguration (name shoud correspond to hadoop core-site and should be same on all nodes)",4/15/16,1
mchendricks1,"@pomadchin Alright, I was able to get the spark-etl to run mostly as would be expected.",4/15/16,1
mchendricks1,"I don't know enough to confirm the output, however - I didn't even confirm that it was there.",4/15/16,1
mchendricks1,"I did note however that while it ran fine on small landsat files (~525kb band geotiffs files), it seems to have been experiencing all manner of out of memory issues when dealing with larger (~122mb band geotiffs).",4/15/16,1
mchendricks1,"```spark-submit \--class geotrellis.spark.etl.SinglebandIngest \--master spark://${MY_LEADER_NAME}:7077 \--executor-memory 3G \--executor-cores 1 \--deploy-mode cluster \--driver-memory 3G \/data/spark/GeoTrellis-Tutorial-Project-assembly-0.1-SNAPSHOT.jar \--input hadoop --format geotiff  --cache NONE -I path=""file:///data/spark/LC80140322014139LGN00/"" \--output accumulo -O instance=gis table=nlcd_table user=root password=GisPwd zookeeper=${MY_LEADER_NAME}:2181 \--layer nlcd-tms --crs EPSG:3857 --pyramid --layoutScheme tms```",4/15/16,1
mchendricks1,"Also, it wasn't obvious that it was using both nodes to perform the ingest (data was copied locally to both nodes). However, the may be beyond the example SinglebandIngest capabilities.",4/15/16,1
mchendricks1,"Finally, on the larger LC80140322014139LGN00 landsat file, it looks like it took down the m4.xlarge EC2 instance with the settings specified above and whatever configuration is in the docker instances and those geodocker launch shell scripts.",4/15/16,1
lossyrob,what sort of memory errors are being reported? Are the tasks throwing OutOfMemory exceptions?,4/15/16,1
lossyrob,https://forums.databricks.com/questions/277/how-do-i-avoid-the-no-space-left-on-device-error.html,4/15/16,1
lossyrob,^- for @echeipesh ,4/15/16,1
mchendricks1,"```16/04/15 20:00:13 WARN scheduler.TaskSetManager: Lost task 7.2 in stage 1.0 (TID 41, ip-172-31-40-197): java.lang.OutOfMemoryError: Java heap space	at scala.reflect.ManifestFactory$$anon$6.newArray(Manifest.scala:90)	at scala.reflect.ManifestFactory$$anon$6.newArray(Manifest.scala:88)	at scala.Array$.ofDim(Array.scala:218)	at geotrellis.raster.io.geotiff.util.ByteBufferExtensions$ByteBufferUtilities.getSignedByteArray(ByteBufferExtensions.scala:178)	at geotrellis.raster.io.geotiff.reader.GeoTiffReader$.readSections$1(GeoTiffReader.scala:260)	at geotrellis.raster.io.geotiff.reader.GeoTiffReader$.readGeoTiffInfo(GeoTiffReader.scala:279)	at geotrellis.raster.io.geotiff.reader.GeoTiffReader$.readSingleband(GeoTiffReader.scala:67)	at geotrellis.raster.io.geotiff.reader.GeoTiffReader$.readSingleband(GeoTiffReader.scala:61)	at geotrellis.raster.io.geotiff.SinglebandGeoTiff$.apply(SinglebandGeoTiff.scala:40)	at geotrellis.spark.io.hadoop.formats.GeotiffInputFormat.read(GeotiffInputFormat.scala:28)	at geotrellis.spark.io.hadoop.formats.BinaryFileInputFormat$$anonfun$createRecordReader$1.apply(BinaryFileInputFormat.scala:34)	at geotrellis.spark.io.hadoop.formats.BinaryFileInputFormat$$anonfun$createRecordReader$1.apply(BinaryFileInputFormat.scala:34)	at geotrellis.spark.io.hadoop.formats.BinaryFileRecordReader.initialize(BinaryFileInputFormat.scala:18)	at org.apache.spark.rdd.NewHadoopRDD$$anon$1.<init>(NewHadoopRDD.scala:153)	at org.apache.spark.rdd.NewHadoopRDD.compute(NewHadoopRDD.scala:124)	at org.apache.spark.rdd.NewHadoopRDD.compute(NewHadoopRDD.scala:65)	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:300)	at org.apache.spark.rdd.RDD.iterator(RDD.scala:264)	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:300)	at org.apache.spark.rdd.RDD.iterator(RDD.scala:264)	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:73)	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)	at org.apache.spark.scheduler.Task.run(Task.scala:88)	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)	at java.lang.Thread.run(Thread.java:745)```",4/15/16,1
lossyrob,that_Ñés with one file?,4/15/16,1
pomadchin,Probably more ram required for a job :0,4/15/16,1
echeipesh,A helpful thing would be to make partitions smaller if thats an option. Could be running out of memory trying to read a set of geotiffs.,4/15/16,1
mchendricks1,"Yes, you are correct in that it is ingesting and processing all the band files (12-15 files @ 122mb each)",4/16/16,1
lokifacio,@lossyrob I pretend to create a branch to add the double version. I'll create a PR once I have it working. ,4/18/16,1
lokifacio,"Once you finish with this release cicle I would like to discuss about the Tile API. I think having set/get and setDouble/getDouble makes easy to make mistakes and generates a bit of code bloating. I guess you've already discussed about this and would like to hear your thoughts on that. Good work, anyway, I think geotrellis has a lot of future ;)",4/18/16,1
pomadchin,"@lokifacio thanks, good to know",4/18/16,1
pomadchin,btw what are your thoughts about set / get ? :D ,4/18/16,1
lokifacio,"I've only seen a subset of all the API, but I found several places where you have to if/else depending on celltype to use the proper setter/getter.",4/18/16,1
lokifacio,"Also it would be useful to have a special setter to NoData so you don't  have pass the NoData value yourself (it should be something the underlying type should now, also this would make easier to maintain code in case the type def changes in a future)",4/18/16,1
lokifacio,Which is the rationale behind having both variants? performance? ,4/18/16,1
lokifacio,"may it is not possible to use just one setter/getter and let the subclasses deal with that, but in that case maybe it would be better to pass a parameterized function to at least write once some implementations",4/18/16,1
mchendricks1,"Are there performance guidelines for configuring Geotrellis? Imagery size to machine hardware capability, partition and RDDs, etc?",4/18/16,1
pomadchin,"@mchendricks1 think not yet, but that_Ñés a good question and it_Ñés in process; also we have benchmarks for our releases, but i am not sure it is published somewhere in a readble form ",4/18/16,1
lossyrob,"@lokifacio yes, it is ugly for performance reasons",4/18/16,1
pomadchin,@lokifacio Rob has a cool experience in profiling such things ,4/18/16,1
lossyrob,there_Ñés a long history of trying to refactor that to something not ugly,4/18/16,1
lossyrob,"one of the biggest wins there is using macros to create mutliple overrides of `isNoData` and `isData`, without the performance cost",4/18/16,1
lossyrob,https://github.com/geotrellis/geotrellis/blob/master/macros/src/main/scala/geotrellis/macros/NoDataMacros.scala,4/18/16,1
lossyrob,"Unfortunatly, as far as setting NoData, there really isn_Ñét a trick we can pull like that here",4/18/16,1
lossyrob,"the performance cost of *not* using `set(col, row, NODATA)` or `setDouble(col, row, Double.NaN)` is heavy.",4/18/16,1
lossyrob,"and it_Ñés something we decided way back, and keep deciding, that the performance cost isn_Ñét worth the code aesthetic",4/18/16,1
lossyrob,"we_Ñéve tried really hard to figure out how to use generics, or specialization, to be able to do something like `get[Double]` or have a `Tile[Double]`. It_Ñés almost impossible to avoid boxing in that situation, which slows down the most performance critical code more than 100%",4/18/16,1
lossyrob,"Having a setter on NoData means we have to throw an instance of a type, or an object around instead of dealing with primatives",4/18/16,1
lossyrob,"this is going to be slower. but having a `tile.cellType.setNoData(tile)` or something, which is not pretty, would be the closest to what your talking about I think, with only slight performance cost. I_Ñém not sure it_Ñés better than just using `set` or `setDouble` correctly (which, if you were to `set(col, row, NODATA)` on a double-backed tile, it would do the appropriate conversion to Double.NaN, based on these macros:https://github.com/geotrellis/geotrellis/blob/master/macros/src/main/scala/geotrellis/macros/TypeConversionMacros.scala)",4/18/16,1
lossyrob,"or maybe your suggestion is to have a `setNoData(col, row)` method",4/18/16,1
lossyrob,for mutable tiles,4/18/16,1
lossyrob,"which doesn_Ñét seem like a bad idea. Although it would translate exactly to `set(col, row, NODATA)`",4/18/16,1
lossyrob,https://github.com/geotrellis/geotrellis/pull/1449/files,4/18/16,1
lossyrob,last change before release,4/18/16,1
lossyrob,I_Ñém making a feature list that will serve as _ÑÒrelease notes_Ñù as part of the release_Ñ_there_Ñés been so much change between 0.9 and 0.10 that I figure it_Ñés best to just catalog what GeoTrellis can do now instead of trying to track what changed.,4/18/16,1
lossyrob,https://gist.github.com/lossyrob/bc1cf89624f514628e192ab7ba066f2c,4/18/16,1
lossyrob,"That_Ñés a first cut at it. Can people take a look and suggest changes/additions/modifications? That would be a big help, thanks",4/18/16,1
dwins,The proj4 and vector sections seem good to me.  It might be more accurate to say we look up CRSs by EPSG code rather than that we create them.,4/18/16,1
lossyrob,"cool, made that change, thanks",4/18/16,1
dwins,"Putting on my copy editor hat I would suggest using the same verb tense for each bullet point (Represent CRSs and Translate them) or (Representing CRSs and Transforming them), makes for a little less context switching for the reader",4/18/16,1
lossyrob,good point,4/18/16,1
lossyrob,"should I say `- Provides a scala idiomatic wrapper around JTS types: Point, Line (LineString in JTS), Polygon, MultiPoint, MultiLine (MultiLineString in JTS), MultiPolygon, GeometryCollection` instead of `- Scala idiomatic wrapper around JTS types: Point, Line (LineString in JTS), Polygon, MultiPoint, MultiLine (MultiLineString in JTS), MultiPolygon, GeometryCollection`",4/18/16,1
dwins,"I wouldn't try to force every bullet point to start with a verb if it doesn't seem natural, but that change sounds good to me.",4/18/16,1
lossyrob,ok cool. this is good stuff,4/18/16,1
echeipesh,https://github.com/geotrellis/geotrellis/blob/master/raster/src/main/resources/myapp-catalog.json ?,4/18/16,1
lossyrob,http://search.maven.org/#search%7Cga%7C1%7Cg%3A%22com.azavea.geotrellis%22%20AND%20v%3A%220.10.0%22,4/19/16,1
lossyrob,https://www.youtube.com/watch?v=XyCn8IC5RpE,4/19/16,1
moradology,+1 on that nina simone,4/19/16,1
pomadchin,:tada: ,4/19/16,1
lokifacio,"@lossyrob thanks for your explanation. I was pretty sure there should be some reasons/constraints on that subject. I could experience some of them myself yesterday while adding the double version of the IDW code. I tried to do some refactors and couldn't find any solution that would solve the issue of code duplication...unfortunately I'm quite new to scala so I lack the understanding to figure out some solutions. I come from a C++ background and some things I tried for java generics failed while it would have worked on c++. I agree that performance is more important than code aesthetics, but code duplication is also something that should be addressed to ease upkeep. ",4/19/16,1
mchendricks1,Congrats on the release!,4/19/16,1
lossyrob,Thanks @mchendricks1!,4/19/16,1
lossyrob,@pomadchin are you using http://datastax.github.io/java-driver/manual/ for the Cassandra stuff?,4/19/16,1
pomadchin,@lossyrob yep,4/19/16,1
lossyrob,"cool, how_Ñés that working?",4/19/16,1
pomadchin,but there is not too much information about cassandra mr jobs :\,4/19/16,1
lossyrob,why would we go through MR?,4/19/16,1
pomadchin,thought that_Ñés better idea to deal with rdd's,4/19/16,1
lossyrob,"I was figuring that it would be a mapPartitions, which creates a connection and executes the queries",4/19/16,1
pomadchin,you wanted like s3?,4/19/16,1
lossyrob,Like treating the cassandra client like the s3 client,4/19/16,1
lossyrob,"yeah I think that_Ñés the best approach, based on what I_Ñéve seen from the Cassandra stuff",4/19/16,1
lossyrob,"avoids using the InputFormat, simplifies it a bunch most likely, and perhaps it_Ñés not the most ideal way, but I think it would work pretty solidly",4/19/16,1
lossyrob,"especially if we_Ñére breaking up buckets into partitions, and do CQL range queries over those ranges in multiple threads",4/19/16,1
lossyrob,I dunno how smart it is to try to do multithreading inside of map tasks. but it_Ñés something to explore,4/19/16,1
pomadchin,gotcha,4/19/16,1
lossyrob,well what do you think?,4/19/16,1
pomadchin,"will try that approach, that makes sense ",4/19/16,1
pomadchin,"however want to finish mr style (i think that_Ñés also a correct approach and not really hard, but has tricky settings) ",4/19/16,1
pomadchin,"probably a nice idea would be to compare (just interesting); :D but that_Ñés true only for the case when both variants would be ready; and anyway your words really make sense ",4/19/16,1
rshirochenko,"How to get masked by polygon result for MultibandTileLayer (this code is works ok for TileLayer)?```val polygon = Polygon((6056108.218278322, 4403833.731299025), (6041491.325803506, 4457388.1932941545), (6054684.234535812, 4480423.320252161), (6061977.459015621, 4444740.258737619), (6056108.218278322, 4403833.731299025))val raster = reader.read[SpatialKey, MultibandTile, TileLayerMetadata[SpatialKey]](layerId)val masked = raster.mask(polygon)```",4/19/16,1
pomadchin,@lossyrob writer works pretty fast (i believe) :D,4/19/16,1
lossyrob,"@rshirochenko you should be able to do that in a later release, what version are you on? I think I added that to RC4. 0.10.0 final is out so you should bump up to that",4/19/16,1
lossyrob,@pomadchin the one with the input format?,4/19/16,1
pomadchin,@lossyrob just similar to s3,4/19/16,1
lossyrob,oh,4/19/16,1
lossyrob,nice,4/19/16,1
pomadchin,"ye; no problems to implement s3 styled readers / writers ;; guess we have to test them on a serious data amount and compare to s3/ accumulo",4/19/16,1
lossyrob,"yeah. would be cool to get a cassandra subproject checked in to master, now that we aren_Ñét code-freezin",4/19/16,1
pomadchin,@lossyrob https://lostechies.com/ryansvihla/2014/08/28/cassandra-batch-loading-without-the-batch-keyword/,4/19/16,1
lossyrob,@pomadchin https://twitter.com/SparkNotebook/status/722341646889787392,4/19/16,1
pomadchin,@lossyrob exciting ,4/19/16,1
pomadchin,I see that like: we'll launch kubernetes on mesos on dcos and it is going dynamically rise nodes wherever we want ,4/19/16,1
pomadchin,:dancer: ,4/19/16,1
lokifacio,congrats for the relase :),4/20/16,1
pomadchin,:tada: :tada: :tada: :tada: :tada:,4/20/16,1
lokifacio,"one question about `VectorToRaster.idwInterpolate`: current version has a bug when some point has distance 0 to interpolation point, which cause a div by 0 error, invalidating the results for points where we actually have data. According to wikipedia, when there is a point which has the position than the interpolation point, the interpolation value should be the same as the sample. However, as we are dealing with rasters, do you think we should take into account all the points and just give points at dist 0 a weight of 1 or we should use the value at that position and ignore the rest?",4/20/16,1
lokifacio,"well, actually it should be a bigger weight",4/20/16,1
akarmas,"Hi all,I am completely new to geotrellis and i am facing some issues when trying to build the application.I have set up Hadoop, Spark, Zookeeper and Accumulo and everything runs fine.I cloned geotrellis repo and run 'scripts/buildall.sh' and the process completed successfully.Then i try  ' ./sbt assembly' to create the fat jar in order to use it with Spark but the process is failing with the following errors:[error] (spark/*:assembly) deduplicate: different file contents found in the following:[error] /home/akarmas/.ivy2/cache/com.esotericsoftware.kryo/kryo/bundles/kryo-2.21.jar:com/esotericsoftware/minlog/Log$Logger.class[error] /home/akarmas/.ivy2/cache/com.esotericsoftware.minlog/minlog/jars/minlog-1.2.jar:com/esotericsoftware/minlog/Log$Logger.class[error] deduplicate: different file contents found in the following:[error] /home/akarmas/.ivy2/cache/com.esotericsoftware.kryo/kryo/bundles/kryo-2.21.jar:com/esotericsoftware/minlog/Log.class[error] /home/akarmas/.ivy2/cache/com.esotericsoftware.minlog/minlog/jars/minlog-1.2.jar:com/esotericsoftware/minlog/Log.class[error] (slick/test:test) sbt.TestsFailedException: Tests unsuccessful[error] (accumulo/*:assembly) deduplicate: different file contents found in the following:[error] /home/akarmas/.ivy2/cache/commons-beanutils/commons-beanutils/jars/commons-beanutils-1.7.0.jar:org/apache/commons/beanutils/BasicDynaBean.class[error] /home/akarmas/.ivy2/cache/commons-beanutils/commons-beanutils-core/jars/commons-beanutils-core-1.8.0.jar:org/apache/commons/beanutils/BasicDynaBean.classand continues with more deduplicate errors from accumulo.I have tried setting the mergeStrategy to first in build.sbt but have not succeded.Are there any ideas?Thank you in advance and sorry for the rather long post! ",4/20/16,1
pomadchin,"hi @akarmas !It is not possible to build a geotrellis _ÑÒfat jat_Ñù with all projects included though it is possible to build etl fat jat to use for basic etl ingest jobs: `./sbt spark-etl/assembly`",4/20/16,1
pomadchin,"all projects can be used as deps in your app (app exmples): * https://github.com/echeipesh/landsat-demo/tree/emr* https://github.com/geotrellis/geotrellis-chatta-demo* https://github.com/dwins/geotrellis-viewer",4/20/16,1
akarmas,"Ok, thanks!But this raises another question.In this presentation ( https://archive.fosdem.org/2015/schedule/event/geotrellis_spark/ ) in the live coding session, a spark-shell is launched and all the geotrellis imports and commands are issued in it, right?Without a .jar with all the projects how am i supposed to do the same?I mean how can i launch a spark-shell or a scala interpreter and have all the geotrellis projects defined in it in order for a "" import geotrellis.raster._ "" for example to work? or to reproduce the  'Hello raster' section in README.md at geotrellis github page??Are there any tutorials for such beginners questions?I have searched for a tutorial about these things but i was not able to find one..and this introduction ( http://geotrellis.io/introduction/ ) is missing.Best regards! ",4/20/16,1
pomadchin,@akarmas i belive that was etl assembly or probably just artifacts; or may be that was launched spark context from the ./sbt console ,4/20/16,1
pomadchin,@lossyrob would be there and he can give you a definite answer,4/20/16,1
rshirochenko,What happens with FileTileReader in RC4 0.10.0 version?,4/20/16,1
pomadchin,"@rshirochenko that_Ñés `FileValueReader` now, related pr: https://github.com/geotrellis/geotrellis/pull/1437",4/20/16,1
echeipesh,"@akarmas At the time of that FOSDEM presentation we were were building a fat jar from the `geotrellis-spark`, since then we_Ñéve factored out the project to make it a nicer dependency. So for instance it doesn_Ñét even have readers/writers for s3 as that would create a possible conflict with spark dependency. So as of right now the `spark-etl` project fat jar is the closest thing to what was being built there, so you_Ñére on the right track.",4/20/16,1
echeipesh,"@akarmas also if you are looking to use spark-shell to mess around with a local context you can just drop into sbt `console` in the `spark` project, you_Ñéll get all the imports that are availble there. You can start a local spark context with `implicit val sc = geotrellis.spark.util.SparkUtils.createLocalSparkContext(_ÑÒlocal[*]_Ñù, _ÑÒTest Context_Ñù)` ",4/20/16,1
echeipesh,"We_Ñéll have to make a walk-through intro post haste now that we_Ñére released. Since you_Ñére new to the project if you could throw your questions here or make a list of questions you have as you_Ñére looking at the project it would really help me make a good intro, I would really appreciate it : )",4/20/16,1
akarmas,"Thank you guys for the info. I was really at a loss..Sure, i 'll provide you with a list of questions soon enough after i have played around a bit.. :) ",4/20/16,1
lokifacio,"I'm having a problem with the following workflow with a HDFS catalog: ingest - read - write- read raster  using `HadoopLayerWriter` and `HadoopLayerReader`. Initial ingest reports `float64ud-1.7E308` cell type and works fine, but tile layer metadata after second write reports `float64udInfinity` cell type, which fails on the subsequent read. I've checked the cell type parsing code and it doesn't handle Infinity value. I wonder where this should be fixed on metadata writing or on cell type parsing from string? ",4/22/16,1
echeipesh,"Sounds like both are a problem. Infinity is a value, so there should be a way for it to stand for NoData. And changing NoData values on write isn_Ñét correct either. I imagine it would throw off the `IsData` checking for these tiles when they_Ñére read after the second write.",4/22/16,1
echeipesh,What_Ñés probably happening is that it_Ñés reading it incorrectly:,4/22/16,1
echeipesh,"```scalascala> CellType.fromString(""float64ud-1.7E308"")res0: geotrellis.raster.CellType = float64udInfinity```",4/22/16,1
arjpuriCMU,"Hey guys, I don't quite understand why DensifyMethods is a trait, what benefit does this have? https://github.com/geotrellis/geotrellis/blob/master/vector/src/main/scala/geotrellis/vector/densify/DensifyMethods.scala",4/22/16,1
echeipesh,"It_Ñés implemented by an implicit conversion here:https://github.com/geotrellis/geotrellis/blob/master/vector/src/main/scala/geotrellis/vector/package.scala#L69. What it allows us to do is to write generic code later with signatures like `def doUsefulWork[T: ? => DensityMethods[T]](things: Seq[T])`, this is basically the same as a View Bound pattern.",4/22/16,1
echeipesh,"@arjpuriCMU So to answer your question directly, it gives us a type we can use in that signature.",4/22/16,1
lossyrob,@jpolchlo https://github.com/geotrellis/geotrellis/blob/48162b824df222afbd75c6495fa1e4bc00344fd9/accumulo/src/test/scala/geotrellis/spark/io/accumulo/AccumuloAttributeStoreSpec.scala#L16,4/22/16,1
pljplj,I've been trying to get the geodocker dev version to work. I can install it but I can't get the Chattanooga data to run.  Does this work with the latest release?  Has it been tried? ,4/22/16,1
lossyrob,@pljplj it has been tested with the latest release. What issues are you having?,4/22/16,1
pljplj,"it hangs when trying to run the ingest.sh  -- WARN ThriftTransportPool: Thread ""main"" stuck on IO to master1.gt:9999 (0) for at least 120042 ms",4/22/16,1
lossyrob,are you running docker on a VM?,4/22/16,1
pljplj,yes,4/22/16,1
lossyrob,what_Ñés the resources your giving it? My experience is that I have to bump up the memory to about 4G and give it some cores in order for it to run through the chatta demo,4/22/16,1
pljplj,I'm just running the scripts.  How would I give it more memory and/or cores?,4/22/16,1
lossyrob,the VM,4/22/16,1
lossyrob,are you using docker-machine?,4/22/16,1
lossyrob,is it a virtualbox vm?,4/22/16,1
pljplj,"Cores in use: 4 Total, 0 UsedMemory in use: 13.6 GB Total, 0.0 B Used",4/22/16,1
pljplj,no for docker-machine. It isn't virtualbox,4/22/16,1
lossyrob,"ok. if that_Ñés the cases, and the containers have access to all that resource, then what I_Ñém guessing may not be the problem",4/22/16,1
pljplj,AFAIK the containers have access to all that resource.  I can see the Spark Manager at 8080 and I can see the Map of chatta but  it won't load the data,4/22/16,1
pomadchin,@pljplj probably problem is memory; sounds like Accumulo has not enough ram / disk space avail,4/22/16,1
shiraeeshi,@lossyrob new project structure is ready,4/27/16,1
shiraeeshi,https://github.com/geotrellis/geotrellis/pull/1459,4/27/16,1
shiraeeshi,"with python's imports logic it was a little tricky to avoid circular imports, but at least doit function works (it's the only function I've tested so far)",4/27/16,1
shiraeeshi,here's how to invoke it:,4/27/16,1
shiraeeshi,"```from geotrellis.python.util.doit import doitb = doit('/path/to/landsat')```",4/27/16,1
ameetkini,"@lossyrob or someone else, as you know I'm setting up a geotrellis cluster here at MITRE",4/27/16,1
ameetkini,what is the best way to expose this data via a UI ,4/27/16,1
ameetkini,"geoserver plugin, some other web UI packaged with geotrellis?",4/27/16,1
lossyrob,We just set up spray service endpoints for things like TMS and JSON endpoints,4/27/16,1
lossyrob,"we_Ñére currently working on integration with GeoWave, which should be delivered around end of summer, which would allow for our data to be exposed via geoserver",4/27/16,1
ameetkini,oh neat. And are those spray service endpoints already on github?,4/27/16,1
lossyrob,There_Ñés not a packaged set of endpoints we publish. Users create the endpoints themselves,4/27/16,1
lossyrob,but there_Ñés a lot of work out there as examples,4/27/16,1
lossyrob,here_Ñés a more recent one ,4/27/16,1
lossyrob,https://github.com/dwins/geotrellis-viewer,4/27/16,1
lossyrob,that project might end up turning into a generalized published tile server for serving geotrellis tiles out of our spark-backed backends,4/27/16,1
ameetkini,"So there are a couple projects here looking to try out geotrellis and they don't have bandwidth to do any custom dev during the try-out phase, just ingest a few tiles and show them via a UI. One of those projects is also looking at GeoMesa and looks like they have a geoserver plugin (hence, UI) already packaged in. Should I point them towards the viewer as the closest it gets to a packaged UI in geotrellis, until the GeoWave integration is released?",4/27/16,1
lossyrob,"yeah that would be the closest we have at this point, I think. Also there_Ñés some work on a UI viewer that @moradology is doing, he might be able to point you further.",4/27/16,1
lossyrob,"I have to step out, for a while, be back later this afternoon. Good to hear from you again Ameet :)",4/27/16,1
ameetkini,thanks @lossyrob ,4/27/16,1
moradology,"hey @ameetkini - i think the geotrellis viewer + something like leaflet to hit its `/gt/tms/x/y/z` endpoint is the best bet as of this moment. later this week/next week, i anticipate code being pushed which makes things a bit more turnkey, so to speak",4/27/16,1
gischina,hello everybody,4/28/16,1
gischina,geotrellis with ambari ???,4/28/16,1
pomadchin,"@gischina hi, what do you mean? definitely you can use ambari for managing your hadoop cluster, why not?",4/28/16,1
gischina, ambari  how to install geotrellis?,4/28/16,1
gischina,@pomadchin ,4/28/16,1
gischina,Novice learning how geotrellis«_ª,4/28/16,1
pomadchin,"you don_Ñét need ""special settings"" for you hadoop cluster to use geotrellis; probably you can start from [here](https://github.com/geotrellis/geotrellis/blob/master/docs/spark-etl/spark-etl-intro.md) to have better understanding of what_Ñés going on. ",4/28/16,1
gischina,thanks  @pomadchin ,4/28/16,1
pomadchin,"geotrellis is a geospatial llibrary wich provides you different kinds of functions to store / transform / query your raster data; and there is no need in putting geotrellis into your cluster class path, you have to put it into your _ÑÒfat jar_Ñù to run jobs. There are some usage examples: * https://github.com/dwins/geotrellis-viewer* https://github.com/lossyrob/landsat-demo* https://github.com/geotrellis/geotrellis-chatta-demo* https://github.com/moradology/gt-admin/tree/scalajs-client",4/28/16,1
gischina,thanks  @pomadchin ,4/28/16,1
gischina,geotrellis local debugging environment need to install the spark?     how do remote debugging?,4/28/16,1
ameetkini,thanks @moradology ,4/28/16,1
ameetkini,"all, what's the current method to export a HDFS resident layer to geotiff?",4/28/16,1
ameetkini,can't find it here - https://github.com/geotrellis/geotrellis/tree/master/docs/spark-etl,4/28/16,1
lossyrob,"you_Ñéd have to read the layer (via the HadoopLayerReader, either as the full layer or via some bounding box query), and then use `stitch` to stitch the layer into a single Raster[Tile] or Raster[MultibandTile], and then use the GeoTiff writing code to write it out as a geotiff",4/28/16,1
lossyrob,"e.g. `GeoTiff(raster, layer.metadata.crs).write(_ÑÒ/path/to/geotiff.tif_Ñù)`",4/28/16,1
ameetkini,"gotcha, thanks @lossyrob I'll try that out",4/28/16,1
ameetkini,"@lossyrob , not sure why the compiler doesn't like this",4/28/16,1
ameetkini,"```val reader = HadoopLayerReader(new Path(""/user/geotrellis/output""))  val layerId = LayerId(""test-12-tiffs"", 14)  val layer = reader.read[SpatialKey, Tile, TileLayerMetadata[SpatialKey]](layerId)```",4/28/16,1
ameetkini,"```Error:(20, 75) could not find implicit value for evidence parameter of type spray.json.JsonFormat[geotrellis.spark.SpatialKey]  val layer = reader.read[SpatialKey, Tile, TileLayerMetadata[SpatialKey]](layerId)```",4/28/16,1
ameetkini,just following the example [here](https://github.com/geotrellis/geotrellis/blob/2c17500869c0592a8a9b54776313a2794fbeb1e1/spark/src/test/scala/geotrellis/spark/io/hadoop/HadoopSpatialSpec.scala) ,4/28/16,1
ameetkini,I have to run out but I'll check back in later,4/28/16,1
lossyrob,`import geotrellis.spark.io._`,4/28/16,1
ameetkini,"of course the implicits, I feel like an idiot",4/29/16,1
ameetkini,:) thanks,4/29/16,1
gischina,@ameetkini  geotrellis local debugging environment need to install the spark?     how do remote debugging?,4/29/16,1
ameetkini,"@gischina are you wondering how to debug the stuff that runs in executors? I usually debug in local environment (i.e., setting  spark master to ""local"" such that all spark processes run in the same jvm) ",4/29/16,1
ameetkini,"that lets me set break points, etc. ",4/29/16,1
gischina,eclipse or idea?,4/29/16,1
ameetkini,"that covers about 90% of my cases, for the rest of them in cluster mode, I set log statements. I use Idea",4/29/16,1
gischina,Submit spark job idea how it«_ª,4/29/16,1
gischina,export jar   command line?,4/29/16,1
gischina,I want to know how to set break points  with idea submit job?,4/29/16,1
gischina,"I only used eclipse / tomcat remote debug,set break points",4/29/16,1
ameetkini,"instead of using spark-submit, in local mode, just have your scala main submit the job ",4/29/16,1
gischina,whether the idea spark can eclipse as remote debug?,4/29/16,1
ameetkini,here's an example: [BroadcastTest](https://github.com/apache/spark/blob/master/examples/src/main/scala/org/apache/spark/examples/BroadcastTest.scala),4/29/16,1
ameetkini,just invoke it as you would any Scala main,4/29/16,1
ameetkini,"that'll let you set breakpoints, etc.",4/29/16,1
gischina,thanks  @ameetkini ,4/29/16,1
mchendricks1,"geotrellis-gdal project - I was able to correct most of the small # of build errors since the project was last updated in March before the final release. However, there is a netCdfRDD function that used to be in HadoopSparkContextMethods.scala but now is in GdalHadoopSparkContextMethods.scala. Could someone suggest how I could make the necessary code update?",4/29/16,1
lossyrob,"Hi @mchendricks1, I_Ñém not sure, but I_Ñém going to take a look at it to see what happens when I try to update, will let you know",4/29/16,1
mbertrand,"Hello, I'm just getting started with geotrellis and wanted to try testing it out with PostGIS data.  Does anyone know if there are working examples of this I could look at?  I found this link but the example seems to be incomplete: https://github.com/geotrellis/geotrellis/blob/0.10/docs/slick/slick-intro.md",4/29/16,1
moradology,"Hey matt - i agree that this example needs a good deal of fleshing out, but I think i might be able to help you get something off the ground. do you already have a postgres + postgis extension db up and running?",4/29/16,1
mbertrand,"yes, with mostly vector data",4/29/16,1
moradology,cool. here's a (somewhat old but i think still useful) usage of slick + geotrellis: https://github.com/WorldBank-Transport/open-transit-indicators/tree/master/scala/opentransit/src/main/scala/com/azavea/opentransit,4/29/16,1
moradology,a few things to note: you'll need to get your hands on a slick DB instance,4/29/16,1
moradology,that's created here: https://github.com/WorldBank-Transport/open-transit-indicators/blob/master/scala/opentransit/src/main/scala/com/azavea/opentransit/DatabaseInstance.scala,4/29/16,1
mbertrand,Thanks!  I'll check it out.,4/29/16,1
moradology,"with that, you'll be able to immediately start writing up table definitions as seen in that (incomplete) demo you pointed to",4/29/16,1
moradology,here's an example of that: https://github.com/WorldBank-Transport/open-transit-indicators/blob/master/scala/opentransit/src/main/scala/com/azavea/opentransit/database/BoundarysTable.scala,4/29/16,1
moradology,"to quickly get started, you can pull down the sbt template from here: https://github.com/geotrellis/geotrellis-sbt-template and add your slick import",4/29/16,1
moradology,"worth noting: there's also very good support for producing geojson, if that's of use to you",4/29/16,1
moradology,@ me if you get stuck!,4/29/16,1
mbertrand,"ok, thank you very much!",4/29/16,1
moradology,no problem at all,4/29/16,1
owcm,"Question about error I am seeing when reading in a Geotiff, reprojecting and writing it out to disk with following code:```                    val bytesToUse = Files.readAllBytes(Paths.get(name.toString))                      val gtIn = SingleBandGeoTiff( bytesToUse )                      val projectedRaster = gtIn.reproject(Bilinear, gtIn.crs, LatLng)                      val reprojGeoTiff = SingleBandGeoTiff(projectedRaster._1, projectedRaster._2, LatLng, gtIn.tags, gtIn.options)                      reprojGeoTiff.write(""/Users/chrismangold/develop/preparedata/pipeline/testwrite/"" + fileName)```I read the file in using the following code:```                     val bytesToUse = Files.readAllBytes(Paths.get(name.toString))                      val gtIn = SingleBandGeoTiff( bytesToUse )```",5/2/16,1
owcm,I receive the following stack error:,5/2/16,1
owcm,"```Message: null Trace: geotrellis.raster.io.geotiff.compression.DeflateDecompressor.decompress(DeflateCompression.scala:70)</br>geotrellis.raster.io.geotiff.compression.Decompressor$$anon$2.decompress(Decompressor.scala:52)</br>geotrellis.raster.io.geotiff.GeoTiffSegmentCollection$class.getDecompressedBytes(GeoTiffSegmentCollection.scala:16)</br>geotrellis.raster.io.geotiff.Float32GeoTiffTile.getDecompressedBytes(Float32GeoTiffTile.scala:7)</br>geotrellis.raster.io.geotiff.Float32GeoTiffSegmentCollection$$anonfun$2.apply(Float32GeoTiffSegmentCollection.scala:20)</br>geotrellis.raster.io.geotiff.Float32GeoTiffSegmentCollection$$anonfun$2.apply(Float32GeoTiffSegmentCollection.scala:20)</br>geotrellis.raster.io.geotiff.GeoTiffSegmentCollection$class.getSegment(GeoTiffSegmentCollection.scala:26)</br>geotrellis.raster.io.geotiff.Float32GeoTiffTile.getSegment(Float32GeoTiffTile.scala:7)</br>geotrellis.raster.io.geotiff.Float32GeoTiffTile.mutable(Float32GeoTiffTile.scala:21)</br>geotrellis.raster.io.geotiff.GeoTiffTile.toArrayTile(GeoTiffTile.scala:349)</br>geotrellis.raster.io.geotiff.reader.GeoTiffReader$.readSingleBand(GeoTiffReader.scala:91)</br>geotrellis.raster.io.geotiff.reader.GeoTiffReader$.readSingleBand(GeoTiffReader.scala:60)</br>geotrellis.raster.io.geotiff.SingleBandGeoTiff$.apply(SingleBandGeoTiff.scala:53)</br>tutorial.GeoTrellisService$$anonfun$ingestRoute$9$$anonfun$apply$51$$anonfun$apply$52.apply(GeoTrellisService.scala:1982)</br>tutorial.GeoTrellisService$$anonfun$ingestRoute$9$$anonfun$apply$51$$anonfun$apply$52.apply(GeoTrellisService.scala:1973)</br>scala.collection.immutable.List.foreach(List.scala:318)</br>tutorial.GeoTrellisService$$anonfun$ingestRoute$9$$anonfun$apply$51.apply(GeoTrellisService.scala:1973)</br>tutorial.GeoTrellisService$$anonfun$ingestRoute$9$$anonfun$apply$51.apply(GeoTrellisService.scala:1967)</br>spray.routing.directives.RouteDirectives$$anonfun$complete$1$$anon$3.apply(RouteDirectives.scala:49)</br>spray.routing.directives.RouteDirectives$$anonfun$complete$1$$anon$3.apply(RouteDirectives.scala:48)</br>spray.routing.directives.BasicDirectives$$anon$3$$anonfun$happly$1.apply(BasicDirectives.scala:92)</br>spray.routing.directives.BasicDirectives$$anon$3$$anonfun$happly$1.apply(BasicDirectives.scala:92)</br>spray.routing.directives.BasicDirectives$$anonfun$mapRequestContext$1$$anonfun$apply$1.apply(BasicDirectives.scala:30)</br>spray.routing.directives.BasicDirectives$$anonfun$mapRequestContext$1$$anonfun$apply$1.apply(BasicDirectives.scala:30)</br>spray.routing.directives.BasicDirectives$$anon$3$$anonfun$happly$1.apply(BasicDirectives.scala:92)</br>spray.routing.directives.BasicDirectives$$anon$3$$anonfun$happly$1.apply(BasicDirectives.scala:92)</br>spray.routing.RouteConcatenation$RouteConcatenation$$anonfun$$tilde$1$$anonfun$apply$1.apply(RouteConcatenation.scala:32)</br>spray.routing.RouteConcatenation$RouteConcatenation$$anonfun$$tilde$1$$anonfun$apply$1.apply(RouteConcatenation.scala:31)</br>spray.routing.RequestContext$$anonfun$withRejectionHandling$1.applyOrElse(RequestContext.scala:130)```",5/2/16,1
owcm,"I am running version ""0.10.0-d8968b1"", which I know is old.  Is this a know problem that was resolved",5/2/16,1
lossyrob,"not sure, might be a current bug in the way we_Ñére compressing that data. can you put up the geotiff for download? The original",5/2/16,1
owcm,"@lossyrob Two FIles: https://drive.google.com/file/d/0B1YZbN2gCsmSTjQ0VXQ5eFpES3M/view?usp=sharinghttps://drive.google.com/file/d/0B1YZbN2gCsmSR1AyMl9ha3Q3X0E/view?usp=sharingIn our the 2 originals.   Big issue for me.  Would be interested in any feed back.",5/2/16,1
lossyrob,I_Ñém able to go through those steps with the first TIF you posed with master,5/2/16,1
owcm,@lossyrob So the latest code,5/2/16,1
lossyrob,yeah,5/2/16,1
lossyrob,I_Ñém going to try and run the same thing on d8968b1,5/2/16,1
lossyrob,see if I can replicate,5/2/16,1
owcm,@lossyrob Thanks,5/2/16,1
lossyrob,@owcm my scala console test didn_Ñét fail either on the old version,5/2/16,1
lossyrob,"could you maybe scope the problem down to unit test form, reading one of those files, that fails for you and exhibits that behavoir? ",5/2/16,1
owcm,"yikes, pretty straight forward what I was doing .   You mean a standalone project?",5/2/16,1
owcm,"yes, I can probably do that",5/2/16,1
SDeathLeaDer,"Well, i  achieve to ingest using spark-etl to hdfs in cluster mode, now i want to create a service that read tiles from HDFS an then process.  In the example of @lossyrob  about serveNDVI from landsat, this is done using the class FileTileReader where in as parameter a object of type File with path local.  What class should i use for HDFS?  ",5/2/16,1
lossyrob,"@SDeathLeaDer `HadoopValueReader`. The `TileReader` line was renamed to `ValueReader`, since it doesn_Ñét necessarily have to read Tiles https://github.com/geotrellis/geotrellis/blob/master/spark/src/main/scala/geotrellis/spark/io/hadoop/HadoopValueReader.scala",5/3/16,1
lossyrob,@echeipesh it looks like it was a docker thing.,5/3/16,1
lossyrob,"we_Ñére going to have to set up a _ÑÒdidn_Ñét shut down the cluster_Ñù jar, like a swear jar",5/3/16,1
lossyrob,whoops that was for another channel ha,5/3/16,1
echeipesh,:money_with_wings: ,5/3/16,1
pomadchin,:tada: :tada: :tada: :tada: :tada:,5/3/16,1
fosskers,The laptop lives,5/3/16,1
pomadchin,@fosskers yoo \0,5/3/16,1
owcm,General question.  If I inadvertently call the Tiler with a Tile RDD that includes Tiles with overlapping  extents what is the behavior I should expect.  Will Tiles be merged?  ,5/3/16,1
jamesmcclain,We will have to research that a bit and get back to you,5/3/16,1
jamesmcclain,"I apologize for that, but I have not worked extensively on that particular part of the codebase",5/3/16,1
moradology,would you mind pasting some source for us @owcm ?,5/3/16,1
moradology,that'd be very helpful,5/3/16,1
moradology,a few lines is all,5/3/16,1
lossyrob,"@owcm tiles will be merged, not in any particular order",5/3/16,1
owcm,"```    // Read the geotiff tiles in as an RDD from S3 or HDFS    val inputTiles: RDD[ (ProjectedExtent, Tile) ] = tilePrepRoute (sc, rsIn )    // We'll be tiling the images using a zoomed layout scheme    // in the web mercator format (which fits the slippy map tile specification).    // We'll be creating 256 x 256 tiles.    val layoutScheme = ZoomedLayoutScheme(WebMercator, tileSize = 256)    // Use the ""RasterMetaData.fromRdd"" call to find the zoom    // level that the closest match to the resolution of our source images,    // and derive information such as the full bounding box and data type.    val (zoom, rasterMetaData) =      RasterMetaData.fromRdd(inputTiles, WebMercator, layoutScheme)(_.projectedExtent.extent)val tiled: RDD[(SpatialKey, Tile)] = Tiler( inputTiles, rasterMetaData, Bilinear)```Let me know if this makes sense.  I was wondering if the Geotiffs I read in the first line.  If they had overlapping extents what havoc I could cause.",5/3/16,1
lossyrob,"merging replaces NoData, but for two non-NoData values, it_Ñés a matter of which comes first basically",5/3/16,1
owcm,"Okay so first come first serve, makes sense thanks.",5/3/16,1
maptopixel,"Hi all. I'm trying out the geodocker dev containers to test geotrellis and geomesa. I'm getting a warning ""There are no tablet servers: check that zookeeper and accumulo are running"" when running the ingest or run-test scripts for either library. I guess my cluster is not set up correctly. I had previously built a master and slave node (./build.sh --build-base; ./start-cluster.sh n=2) which seemed fine. Any tips on how I solve this?",5/5/16,1
maptopixel,I'm using a fresh Ubuntu 14.04 (Azure VM) with 28 Gb of ram,5/5/16,1
pomadchin,"hi @maptopixel , after containers start you could reach `host:50095`? ",5/5/16,1
pomadchin,that_Ñés accumulo ui,5/5/16,1
maptopixel,Ah. I can not. I guess I should also see an accumulo process with `docker top master1`,5/5/16,1
maptopixel,(I can't),5/5/16,1
pomadchin,hm,5/5/16,1
pomadchin,"delete containers using:```bashdocker rm -f master1 slave1```and start just one single node  `./start-cluster.sh n=1`after that throw there container log:```bashdocker logs -f --tail 10 master1```",5/5/16,1
pomadchin,and that_Ñés not neccesary to build anything: everything is published to docker hub repository,5/5/16,1
maptopixel,Thanks for the tip. I re-ran as root rather than sudo and can now see the accumulo ui,5/5/16,1
pomadchin,cool! write there any questions you have,5/5/16,1
maptopixel,"Thanks, making progess I think. ",5/5/16,1
maptopixel,"I ran the geotrellis install and Accumulo, Hadoop and Spark all seemed fine. But when I tried the ingest and accumulo seemed to stop.",5/5/16,1
pomadchin,there is a log folder in accumulo,5/5/16,1
pomadchin,"sec, will give you an exact path",5/5/16,1
pomadchin,"but i have a strong feeling, that it cause by a possible issue: https://github.com/geotrellis/geodocker-cluster/issues/21",5/5/16,1
pomadchin,"I am not sure in that, but probably 512mb accumulo configuration is not enough for geomesa iterators",5/5/16,1
maptopixel,"* and I get the ""There are no tablet servers: check that zookeeper and accumulo are running."" again",5/5/16,1
maptopixel,I see,5/5/16,1
maptopixel,So I need to reconfigure to give more to accumulo?,5/5/16,1
maptopixel,I am doing the geotrellis install btw,5/5/16,1
pomadchin,hm,5/5/16,1
pomadchin,let me check,5/5/16,1
pomadchin,"```docker exec -it master1 bashcd /usr/local/accumulo/logstail  tserver_master1.gt.log```",5/5/16,1
maptopixel,Also I did notice that during the compile of geotrellis there was an error (although end it stated it was succssfull),5/5/16,1
maptopixel,"	`/data/geotrellis/proj4/src/main/java/org/osgeo/proj4j/util/PolarCoordinate.java:18:  error: unmappable character for encoding ASCII	`",5/5/16,1
pomadchin,hmm,5/5/16,1
pomadchin,do you use latest image tag?,5/5/16,1
maptopixel,"2016-05-05 11:09:17,257 [zookeeper.ClientCnxn] INFO : Opening socket connection to server master1.gt/172.17.0.2:2181. Will not attempt to authenticate using SASL (unknown error)2016-05-05 11:09:17,257 [zookeeper.ClientCnxn] INFO : Socket connection established to master1.gt/172.17.0.2:2181, initiating session2016-05-05 11:09:17,401 [zookeeper.ClientCnxn] INFO : Session establishment complete on server 	```master1.gt/172.17.0.2:2181, sessionid = 0x154808919e8001b, negotiated timeout = 300002016-05-05 11:09:17,681 [watcher.MonitorLog4jWatcher] INFO : Closing log-forwarding appender2016-05-05 11:09:17,683 [watcher.MonitorLog4jWatcher] INFO : Removing log-forwarding appender2016-05-05 11:09:17,687 [zookeeper.DistributedWorkQueue] INFO : Got unexpected zookeeper event: None for /accumulo/35939e68-b31c-4326-aec7-90a525915897/bulk_failed_copyq2016-05-05 11:09:17,687 [zookeeper.DistributedWorkQueue] INFO : Got unexpected zookeeper event: None for /accumulo/35939e68-b31c-4326-aec7-90a525915897/recovery2016-05-05 11:09:17,687 [zookeeper.DistributedWorkQueue] INFO : Got unexpected zookeeper event: None for /accumulo/35939e68-b31c-4326-aec7-90a525915897/replication/workqueue2016-05-05 11:09:17,692 [tserver.TabletServer] ERROR: Lost tablet server lock (reason = SESSION_EXPIRED), exiting.2016-05-05 11:09:17,692 [server.GarbageCollectionLogger] WARN : GC pause checker not called in a timely fashion. Expected every 30.0 seconds but was 140.0 seconds since last check	```",5/5/16,1
maptopixel,I did use the latest (i.e. I didn't specify),5/5/16,1
pomadchin,looks weird,5/5/16,1
pomadchin,and that_Ñés for a single node cluter?,5/5/16,1
maptopixel,haha. Yes it's a single node.,5/5/16,1
maptopixel,Let me try again with a completely clean install,5/5/16,1
pomadchin,"yep, and probably throw here commands you are trying, and I_Ñéll repeat them",5/5/16,1
maptopixel,okey doke,5/5/16,1
pomadchin,thx Julian,5/5/16,1
maptopixel,Success @pomadchin! I re-ran everything from afresh and the geotrellis demo now works. My guess is that  some environment variables were mismatched from me doing a lot of docker container changes. I did still have the compilation error with the proj4 PolarCoordinate class just so you know,5/5/16,1
maptopixel,Thanks again for your time,5/5/16,1
pomadchin,"@maptopixel thx for report about it, think still problems in locale settings ): thought i solved it inside containers, btw we have a pr by @jamesmcclain  https://github.com/geotrellis/geotrellis/pull/1466",5/5/16,1
thesteve0,@pomadchin greetings and thanks for the invite,5/5/16,1
thesteve0,"@pomadchin  when you say ""solve the networking"" for kube do you mean the networking under kube?",5/5/16,1
thesteve0,If so OpenShift solves that with a provided OpenVSwitch SDN already set up and configured,5/5/16,1
thesteve0,@pomadchin I want to get your stuff running on this to start https://www.openshift.org/vm,5/5/16,1
owcm,"Asking on the off chance some else has seen an issue like this.  I have a very large Tiling job writing temp results to a linux /tmp drive that is undersized.  Error comes back:```: com.esotericsoftware.kryo.KryoException: java.io.IOException: No space left on deviceSerialization trace:array (geotrellis.raster.DoubleArrayTile)	at com.esotericsoftware.kryo.io.Output.flush(Output.java:165)	at com.esotericsoftware.kryo.io.Output.require(Output.java:142)	at com.esotericsoftware.kryo.io.Output.writeLong(Output.java:501)	at com.esotericsoftware.kryo.io.Output.writeDouble(Output.java:630)	at com.esotericsoftware.kryo.io.Output.writeDoubles(Output.java:711)	at com.esotericsoftware.kryo.serializers.DefaultArraySerializers$DoubleArraySerializer.write(DefaultArraySerializers.java:198)	at com.esotericsoftware.kryo.serializers.DefaultArraySerializers$DoubleArraySerializer.write(DefaultArraySerializers.java:187)	at com.esotericsoftware.kryo.Kryo.writeObjectOrNull(Kryo.java:577)	at com.esotericsoftware.kryo.serializers.ObjectField.write(ObjectField.java:68)```Curious if anyone new if there was a setting to redirect the output from the com.esotericsoftware.kryo.serializers to another directory.Thanks in advance",5/5/16,1
owcm,"Might have answered my own questions.   spark.local.dir setting defaulted to	/tmp",5/5/16,1
lossyrob,Yeah means you are writing to small EBS instead of ephemeral. Need to write to the yarn temp folder,5/5/16,1
owcm,@lossyrob But is that controlled through spark.local.dir,5/5/16,1
lossyrob,Not when run through emr step,5/5/16,1
lossyrob,Might be one of the things a step works out for you,5/5/16,1
owcm,Okay,5/5/16,1
lossyrob,So you should find where the mount points are and use that as the spark.local.dir when submitting,5/5/16,1
lossyrob,Though i though yarn was the one yhat took care of that,5/5/16,1
owcm,@lossyrob Yea I am submitting through spark-submit,5/5/16,1
lossyrob,It might be that spark.local.dir is ignored when running through yarn,5/5/16,1
lossyrob,I actually remember discovering that,5/5/16,1
owcm,Interesting,5/5/16,1
lossyrob,Not sure how to get the temp directory. Let me look at some old code for a minute,5/5/16,1
lossyrob,https://github.com/hotosm/oam-server-tiler/blob/master/chunk/chunk.py#L50,5/5/16,1
lossyrob,I was using the current working directory there,5/5/16,1
lossyrob,so when yarn launches the job it_Ñés running in a yarn-applicaiton specific folder,5/5/16,1
lossyrob,that gets removed after the application dies off,5/5/16,1
lossyrob,,5/5/16,1
pomadchin,"@thesteve0 sry for a long reply, yep under the kube, and there is no config for kube to rise containers",5/6/16,1
pomadchin,"@thesteve0 I'll take a look, to be honest never wrote cfgs for open sift is that would be interesting",5/6/16,1
pomadchin,"Any pointers are appreciated, or probably you can make it, and probably I can give you necessary info:0 and help, etc",5/6/16,1
Zionett,"I have a question about filtering layer readers. After```    val queryRdd: RDD[(SpatialKey, Tile)] with Metadata[TileLayerMetadata[SpatialKey]] =       reader         .query[SpatialKey, Tile, TileLayerMetadata[SpatialKey]](layerId)         .where(Intersects(queryExtent))         .result```It looks like queryRdd.metadata.extent is still the extent of the original layer. Is there a way to get the actual extent of the queried tiles?",5/6/16,1
lossyrob,You need to get the extent of the keybounds,5/6/16,1
lossyrob,`queryRdd.metadata.mapTransform(queryRdd.metadata.bounds.get)` i believe,5/6/16,1
Zionett,"That gives me ""could not find implicit value for evidence parameter of type geotrellis.spark.SpatialComponent[geotrellis.spark.KeyBounds[geotrellis.spark.SpatialKey]]""",5/6/16,1
Zionett,"I got it to work with```val extent = queryRdd.keys.map(queryRdd.metadata.mapTransform.apply).reduce(_ combine _)```Seems like a very roundabout way of doing it though",5/6/16,1
Zionett,Thanks for your help!,5/6/16,1
lossyrob,"yeah, you have to do a spark `action` in that way, kick off work, better to save it by just reading metadata and not do unneeded computation :)",5/6/16,1
Zionett,"None of the apply methods for `mapTransform` can take a `keybounds` though... I tried to get the minKey, maxKey of the keybounds and get the extent that way but after some printing I now believe even `queryRdd.metadata.bounds` is still the bounds of the original layer. ",5/6/16,1
lossyrob,"Sorry, convert the keybounds to gridbounds",5/6/16,1
lossyrob,I believe there is a `toGridBounds` or `asGridBound` implicity added to keybounds of key types that have a spatial component,5/6/16,1
Zionett,"```queryRdd.metadata.mapTransform(queryRdd.metadata.bounds.get.toGridBounds)```compiles, but the resulting `Extent` is the original layer's extent, same as```queryRdd.metadata.mapTransform(queryRdd.metadata.gridBounds)```",5/6/16,1
lossyrob,then the original layer_Ñés extent takes up the whole grid,5/6/16,1
lossyrob,how did you tile it out?,5/6/16,1
Zionett,"Hmm... I used spark-etl to load several digital elevation map geotiffs, with the options```spark-submit \--class geotrellis.spark.etl.SinglebandIngest \--master yarn-client \$JAR \--input hadoop --format geotiff --cache NONE -I path=<> \--output s3 -O bucket=<> key=<> \--layer dem --crs EPSG:3857 --pyramid --layoutScheme tms```And I am now trying to query the layer, reproject the result and output a geotiff.",5/6/16,1
lossyrob,"ok, that shouldn_Ñét be too complicated",5/6/16,1
lossyrob,you would query basd on your criteria,5/6/16,1
lossyrob,call the reproject,5/6/16,1
lossyrob,call stitch to get the raster,5/6/16,1
lossyrob,then write out the raster via GeoTiff,5/6/16,1
Zionett,"Yeah, I called stitch first, but then reproject wants the correct extent",5/6/16,1
lossyrob,"```scala  val queryRdd: RDD[(SpatialKey, Tile)] with Metadata[TileLayerMetadata[SpatialKey]] =       reader         .query[SpatialKey, Tile, TileLayerMetadata[SpatialKey]](layerId)         .where(Intersects(queryExtent))         .resultval raster =  queryRdd    .reproject(destCrs, FloatingLayoutScheme(512)    .stitchGeoTiff(raster, destCrs).write(_ÑÒ/path/to/output.tif_Ñù)```",5/6/16,1
lossyrob,"stitch produces a raster, which has an extent, which you could use if you wanted to do the reproject not in the RDD",5/6/16,1
Zionett,I see. For some reason I have been doing `val stitched: Tile = queryRdd.stitch`... It all makes sense now. Really appreciate your help!,5/6/16,1
lossyrob,no worries!,5/6/16,1
lossyrob,https://github.com/geotrellis/geotrellis/blame/master/vector-test/src/test/scala/spec/geotrellis/vector/io/json/CrsSpec.scala#L20 _Ñ_ wuuuut,5/6/16,1
lossyrob,https://map.what3words.com/napkin.maps.sloppy ??,5/6/16,1
rshirochenko,"@lossyrob I ingest landsat data with this code (https://github.com/lossyrob/landsat-demo/blob/master/ingest/src/main/scala/demo/LandsatIngest.scala). There is a problem that metadata writes only for the last image. As the result I can access with LayerReader only the layer with the last image. TileReader(ValuerReader) works fine for images visualization with some modification provided in the same repo. I want to query polygon or time range based on the whole layers images ``` readerSet.multiBandLayerReader.query[SpaceTimeKey, MultibandTile, TileLayerMetadata[SpaceTimeKey]](layer).where(Between(time1_joda, time2_joda)).toRDD ``` and get result only for the the last  image.  The images that i write in one layer are have different time and extent range, maybe this could be a problem? ``` val images =      Array[String](        ""data/bel2/LC81800192016108LGN00"",        ""data/bel2/LC81780222014152LGN00"",        ""data/bel2/LC81810242014157LGN00""      )  ```",5/8/16,1
lossyrob,"are you writing over the layer on each image write? If you_Ñére doing a write and not an update, you will be rewriting the layer metadata each time",5/8/16,1
lossyrob,if you_Ñére doing multiple writes,5/8/16,1
lossyrob,"though if your using landsat-demo, I don_Ñét think that_Ñés the problem",5/8/16,1
lossyrob,"not entirely clear what_Ñés going on. I was ingesting multiple images with that code, that fed a UI which saved off the times into an attribute at zoom level 0 for the layer, so that I could feed a UI that let me select which time I wanted to see, and would bring up that image",5/8/16,1
timothymschier,"Hi folks, I'm trying to use the https://github.com/dwins/geotrellis-viewer project to view tiles.",5/9/16,1
timothymschier,I've ingested using v0.10.0 spark-etl and SpatialKey indexing to an S3 bucket,5/9/16,1
timothymschier,have configured the settings.conf with the bucket and port details,5/9/16,1
timothymschier,"and am running using ``` java -cp /opt/spark-1.5.2-bin-hadoop2.6/lib/spark-assembly-1.5.2-hadoop2.6.0.jar:geotrellis-ingest-test-assembly-0.1.0.jar -Dconfig.file=settings.conf geotrellis.ingest.test.Server```",5/9/16,1
timothymschier,"the error I get is:```[ERROR] [05/09/2016 03:29:53.674] [geotrellis-ingest-test-akka.actor.default-dispatcher-2] [akka://geotrellis-ingest-test/user/geotrellis-ingest-test-service] Error during processing of request HttpRequest(GET,http://ec2-54-174-64-253.compute-1.amazonaws.com:8000/gt/tms/Goonyella_Quick_Ortho_20160108/1/1/1,List(Host: ec2-54-174-64-253.compute-1.amazonaws.com:8000, Upgrade-Insecure-Requests: 1, User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/49.0.2623.87 Safari/537.36, Accept-Encoding: gzip, deflate, sdch, Accept-Language: en-US, en;q=0.8, Via: 1.1 squid.aus.iae.net (squid/3.1.10), X-Forwarded-For: 10.245.35.25, Cache-Control: max-age=2419200, Connection: keep-alive),Empty,HTTP/1.1)org.apache.avro.AvroTypeException: Found geotrellis.raster.ArrayMultibandTile, expecting union        at org.apache.avro.io.ResolvingDecoder.doAction(ResolvingDecoder.java:292)        at org.apache.avro.io.parsing.Parser.advance(Parser.java:88)        at org.apache.avro.io.ResolvingDecoder.readIndex(ResolvingDecoder.java:267)        at org.apache.avro.generic.GenericDatumReader.read(GenericDatumReader.java:155)        at org.apache.avro.generic.GenericDatumReader.readField(GenericDatumReader.java:193)        at org.apache.avro.generic.GenericDatumReader.readRecord(GenericDatumReader.java:183)        at org.apache.avro.generic.GenericDatumReader.read(GenericDatumReader.java:151)        at org.apache.avro.generic.GenericDatumReader.readArray(GenericDatumReader.java:219)        at org.apache.avro.generic.GenericDatumReader.read(GenericDatumReader.java:153)        at org.apache.avro.generic.GenericDatumReader.readField(GenericDatumReader.java:193)        at org.apache.avro.generic.GenericDatumReader.readRecord(GenericDatumReader.java:183)        at org.apache.avro.generic.GenericDatumReader.read(GenericDatumReader.java:151)        at org.apache.avro.generic.GenericDatumReader.read(GenericDatumReader.java:142)        at geotrellis.spark.io.avro.AvroEncoder$.fromBinary(AvroEncoder.scala:55)        at geotrellis.spark.io.s3.S3ValueReader$$anon$1.read(S3ValueReader.scala:42)        at geotrellis.ingest.test.GeotrellisIngestTestService$$anonfun$tms$1.apply(GeotrellisIngestTestServiceActor.scala:120)        at geotrellis.ingest.test.GeotrellisIngestTestService$$anonfun$tms$1.apply(GeotrellisIngestTestServiceActor.scala:116)        at spray.routing.ApplyConverterInstances$$anon$19$$anonfun$apply$4.apply(ApplyConverterInstances.scala:46)        at spray.routing.ApplyConverterInstances$$anon$19$$anonfun$apply$4.apply(ApplyConverterInstances.scala:45)        at spray.routing.ConjunctionMagnet$$anon$1$$anon$2$$anonfun$happly$1$$anonfun$apply$1.apply(Directive.scala:38)        at spray.routing.ConjunctionMagnet$$anon$1$$anon$2$$anonfun$happly$1$$anonfun$apply$1.apply(Directive.scala:37)        at spray.routing.directives.BasicDirectives$$anon$1.happly(BasicDirectives.scala:26)        at spray.routing.ConjunctionMagnet$$anon$1$$anon$2$$anonfun$happly$1.apply(Directive.scala:37)        at spray.routing.ConjunctionMagnet$$anon$1$$anon$2$$anonfun$happly$1.apply(Directive.scala:36)        at spray.routing.directives.BasicDirectives$$anon$2.happly(BasicDirectives.scala:79)        at spray.routing.ConjunctionMagnet$$anon$1$$anon$2.happly(Directive.scala:36)        at spray.routing.Directive$$anon$7$$anonfun$happly$4.apply(Directive.scala:86)        at spray.routing.Directive$$anon$7$$anonfun$happly$4.apply(Directive.scala:86)        at spray.routing.directives.BasicDirectives$$anon$3$$anonfun$happly$1.apply(BasicDirectives.scala:92)        at spray.routing.directives.BasicDirectives$$anon$3$$anonfun$happly$1.apply(BasicDirectives.scala:92)        at spray.routing.directives.ExecutionDirectives$$anonfun$handleExceptions$1$$anonfun$apply$4.apply(ExecutionDirectives.scala:35)        at spray.routing.directives.ExecutionDirectives$$anonfun$handleExceptions$1$$anonfun$apply$4.apply(ExecutionDirectives.scala:33)```",5/9/16,1
timothymschier,any help/direction appreciated :),5/9/16,1
timothymschier,"oh, forgot to mention: I updated the geotrellis build.sbt dependency versions from 0.10.0-RC4 to 0.10.0 in the dwins/geotrellis-viewer project",5/9/16,1
moradology,"@timothymschier if you have docker, i'm going to push up a couple containers which will do this for you",5/9/16,1
moradology,"I'm at home right now, where my internet won't really suffice for the upload, but i can ping you once things are in place and help you with getting started",5/9/16,1
pomadchin,@timothymschier @moradology probably it is possible to solve this _ÑÉhell_Ñé starting server via spark-submit,5/9/16,1
rshirochenko,"@lossyrob Yes, I use landsat-demo code (https://github.com/lossyrob/landsat-demo/blob/master/ingest/src/main/scala/demo/LandsatIngest.scala#L155). Which method can i used to update layer for each time to avoid rewriting? For UI TileLayer(ValueLayer) works fine for me either, so I can show images for different times. But seems that LayerReader cannot work without right metadata information.",5/9/16,1
pomadchin,"@rshirochenko I am not sure that I can follow your talk with @lossyrob, but you can_Ñét use writer to update layeryou absolutely correct, that for layer reader you need correct layer metadata (you need at least valid keyBounds and keyIndex)You can try to use `LayerUpdater`, but pay attention to your layer keyIndex: https://github.com/geotrellis/geotrellis/blob/master/accumulo/src/main/scala/geotrellis/spark/io/accumulo/AccumuloLayerUpdater.scala#L43You have to preset KeyBounds for you potential layer updates",5/9/16,1
timothymschier,"thanks @moradology I'm actually using the old geotrellis-ec2-cluster, updated with Spark 1.5.2, running on AWS",5/9/16,1
timothymschier,"@pomadchin ah good suggestion, I'll look at some examples of using spark-submit... in particular how would I pass the config file parameter?",5/9/16,1
lossyrob,e.g. `--conf 'spark.executor.extraJavaOptions=-Dconfig.resource=app_Ñé`,5/9/16,1
lossyrob,"that does seem very strange, that the Avro schema decoding is failing. This is a fresh ingest via the 0.10.0 release, trying to be read with a 0.10.0 release reader?",5/9/16,1
timothymschier,cheers @lossyrob I'll give that config a shot.,5/9/16,1
timothymschier,yeah I used an EMR cluster to do the ingest,5/9/16,1
timothymschier,and grabbed that dwins/geotrellis-viewer latest and updated the build.sbt to use 0.10.0 versions for the reading,5/9/16,1
timothymschier,"I must still be missing something:``` spark-submit --class geotrellis.ingest.test.Server geotrellis-ingest-test-assembly-0.1.0.jar --conf 'spark.executor.extraJavaOptions=-Dconfig.file=/home/ubuntu/settings.conf' --conf 'spark.executor.extraJavaOptions=-Dconfig.resource=app'```gives me```Exception in thread ""main"" com.typesafe.config.ConfigException$Missing: No configuration setting found for key 'geotrellis.hostname'        at com.typesafe.config.impl.SimpleConfig.findKey(SimpleConfig.java:124)        at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:145)        at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:151)        at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:159)        at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:164)        at com.typesafe.config.impl.SimpleConfig.getString(SimpleConfig.java:206)        at geotrellis.ingest.test.Server$.main(Server.scala:13)        at geotrellis.ingest.test.Server.main(Server.scala)        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)        at java.lang.reflect.Method.invoke(Method.java:606)        at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:674)        at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180)        at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205)        at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:120)        at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)```",5/9/16,1
timothymschier,"I know I'm missing some other confs there, but looks like this error is still related to not being able to read the conf file",5/9/16,1
lossyrob,I_Ñém not too familiar with this viewer. but my guess is that there is a application.conf you need to modify with your settings,5/9/16,1
fosskers,it's the `settings.conf`,5/9/16,1
fosskers,default value for that field is `0.0.0.0`,5/9/16,1
lossyrob,and potentially the avro issue was coming from it trying to read a file on s3 that wasn_Ñét a proper catalog file,5/9/16,1
lossyrob,"it should really be in `application.conf`, yeah?",5/9/16,1
lossyrob,which I_Ñém not finding a resources folder for that in the project,5/9/16,1
dwins,"Yes, normally it would be in `application.conf`",5/9/16,1
dwins,I made a separate `settings.conf` file that isn't packaged in the jar because it was easier to modify settings when I was copying the fat jar to a VM,5/9/16,1
dwins,I can fix this to be done in the usual way and also update to GT 0.10.0 real quick,5/9/16,1
lossyrob,ok. I know we have @fosskers looking at this project,5/9/16,1
lossyrob,Is that on the game plan this week for you @fosskers ?,5/9/16,1
timothymschier,"oh hey @dwins a quick cross-check would be super, I'm happy to be a tester",5/9/16,1
timothymschier,or whoever ends up doing it :),5/9/16,1
dwins,cross check of your settings?,5/9/16,1
timothymschier,no of the project updated with GT 0.10.0,5/9/16,1
dwins,oh sure. wait 5 minutes ;),5/9/16,1
timothymschier,settings are there; they're found when I execute using ```java -cp```,5/9/16,1
timothymschier,geotrellis.hostname is inside the settings.conf which is loaded at execution time,5/9/16,1
timothymschier,thanks man!,5/9/16,1
fosskers,"Yeah @lossyrob , I'm familiar with its setup/deployment. @moradology is getting me more on board tomorrow",5/9/16,1
fosskers,"@timothymschier the docker image that @moradology is publishing coordinates how GT and spark run, so that should lighten the burden for you",5/9/16,1
lossyrob,"cool cool. I know there were a couple of other things on the list, I_Ñéve yet to look how the planning shook out",5/9/16,1
timothymschier,thanks @fosskers that will be something to look into for a future sprint... for now though I just need a really basic TMS server in order to demonstrate some early integration work,5/9/16,1
lossyrob,@timothymschier how are you deploying the spark that is running the service currently? Is it a single EC2 instance?,5/9/16,1
timothymschier,"I've got a geotrellis-ec2-cluster 1-leader 2-follower going on EC2, but I'm SSHd into the master and have built and was just going to run locally there",5/9/16,1
timothymschier,"so yeah, pretty clunky, but I had it there and figured that should work for this simple demo",5/9/16,1
lossyrob,yeah it should,5/9/16,1
dwins,@fosskers Updated master branch on that repo. Going to be AFK(laptop) for a moment but I have chat on my phone if there are any questions.,5/9/16,1
timothymschier,"hey @dwins, you missed geotrellis-raster version update in build.sbt",5/9/16,1
timothymschier,I'll do a pull request if you like,5/9/16,1
fosskers,"To clarify for @timothymschier (assuming that you're using `geotrellis-viewer`), what's upcoming is an updated version of `geotrellis-admin`, which is a little more robust than the old viewer and has nice docker-based deployment",5/9/16,1
lossyrob,which is good. but the simple TMS on geotrellis-ec2-cluster should work. It_Ñés very strange that it_Ñés reading a bad avro file,5/9/16,1
timothymschier,ahh that sounds good... thanks for that clarification,5/9/16,1
lossyrob,"best case is that it_Ñés just being fed a bad file, in which case it would be good to throw a more descriptive error message",5/9/16,1
timothymschier,I saw the old geotrellis-admin and that it wasn't updated yet. Managing GT jobs will be great,5/9/16,1
timothymschier,cool @lossyrob  I'm about to give it a go with an updated geotrellis-viewer,5/9/16,1
lossyrob,ok cool,5/9/16,1
fosskers,yeah the current sexy stuff is in a PR,5/9/16,1
lossyrob,try to verify that the bucket/prefix that you_Ñére pulling from is correct.,5/9/16,1
lossyrob,Although it does seem like it_Ñés pulling the metadata correctly...,5/9/16,1
timothymschier,ok will double-check,5/9/16,1
dwins,@fosskers cool right. I can't merge for a few minutes but this PR looks good,5/9/16,1
fosskers,maybe talk to @moradology first,5/9/16,1
timothymschier,hrmm same problem,5/9/16,1
lossyrob,"@timothymschier to debug I would try to kick up a spark shell with that jar on the path, to try and poke around interactively. See if you can read a single tile. Or boot up EMR with Zeppelin and poke around there. Might be a bit tough if your unfamiliar with the library. But trying to scope it down to a single tile read that exhibits the problem would be good",5/9/16,1
timothymschier,so when I put in an invalid layer then I get a failure with unable to read metadata; supporting @lossyrob 's theory that the S3 creds are right,5/9/16,1
lossyrob,what_Ñés the nature of the data?,5/9/16,1
lossyrob,is it multiband?,5/9/16,1
timothymschier,yep multiband,5/9/16,1
lossyrob,ok,5/9/16,1
timothymschier,SpatialKey,5/9/16,1
lossyrob,what_Ñés the read look like,5/9/16,1
timothymschier,stack trace?,5/9/16,1
lossyrob,one sec,5/9/16,1
lossyrob,https://github.com/dwins/geotrellis-viewer/blob/master/src/main/scala/geotrellis/ingest/test/GeotrellisIngestTestServiceActor.scala#L58,5/9/16,1
timothymschier,"```[ERROR] [05/09/2016 22:58:35.578] [geotrellis-ingest-test-akka.actor.default-dispatcher-5] [akka://geotrellis-ingest-test/user/geotrellis-ingest-test-service] Error during processing of request HttpRequest(GET,http://ec2-54-174-64-253.compute-1.amazonaws.com:8000/gt/tms/Goonyella_Quick_Ortho_20160108/1/1/1,List(Host: ec2-54-174-64-253.compute-1.amazonaws.com:8000, Connection: keep-alive, Upgrade-Insecure-Requests: 1, User-Agent: Mozilla/5.0 (Windows NT 6.3; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/50.0.2661.94 Safari/537.36, Accept-Encoding: gzip, deflate, sdch, Accept-Language: en-US, en;q=0.8),Empty,HTTP/1.1)org.apache.avro.AvroTypeException: Found geotrellis.raster.ArrayMultibandTile, expecting union        at org.apache.avro.io.ResolvingDecoder.doAction(ResolvingDecoder.java:292)        at org.apache.avro.io.parsing.Parser.advance(Parser.java:88)        at org.apache.avro.io.ResolvingDecoder.readIndex(ResolvingDecoder.java:267)        at org.apache.avro.generic.GenericDatumReader.read(GenericDatumReader.java:155)        at org.apache.avro.generic.GenericDatumReader.readField(GenericDatumReader.java:193)        at org.apache.avro.generic.GenericDatumReader.readRecord(GenericDatumReader.java:183)        at org.apache.avro.generic.GenericDatumReader.read(GenericDatumReader.java:151)        at org.apache.avro.generic.GenericDatumReader.readArray(GenericDatumReader.java:219)        at org.apache.avro.generic.GenericDatumReader.read(GenericDatumReader.java:153)        at org.apache.avro.generic.GenericDatumReader.readField(GenericDatumReader.java:193)        at org.apache.avro.generic.GenericDatumReader.readRecord(GenericDatumReader.java:183)        at org.apache.avro.generic.GenericDatumReader.read(GenericDatumReader.java:151)        at org.apache.avro.generic.GenericDatumReader.read(GenericDatumReader.java:142)        at geotrellis.spark.io.avro.AvroEncoder$.fromBinary(AvroEncoder.scala:55)        at geotrellis.spark.io.s3.S3ValueReader$$anon$1.read(S3ValueReader.scala:42)        at geotrellis.ingest.test.GeotrellisIngestTestService$$anonfun$tms$1.apply(GeotrellisIngestTestServiceActor.scala:120)        at geotrellis.ingest.test.GeotrellisIngestTestService$$anonfun$tms$1.apply(GeotrellisIngestTestServiceActor.scala:116)        at spray.routing.ApplyConverterInstances$$anon$19$$anonfun$apply$4.apply(ApplyConverterInstances.scala:46)        at spray.routing.ApplyConverterInstances$$anon$19$$anonfun$apply$4.apply(ApplyConverterInstances.scala:45)        at spray.routing.ConjunctionMagnet$$anon$1$$anon$2$$anonfun$happly$1$$anonfun$apply$1.apply(Directive.scala:38)```",5/9/16,1
timothymschier,brb stand up meeting,5/9/16,1
lossyrob,see the issue there?,5/9/16,1
lossyrob,ok big reveal: this tile server doesn_Ñét work with MultibandTile,5/9/16,1
lossyrob,you_Ñére trying to read a `MultibandTile` with a reader of type `Tile`,5/9/16,1
lossyrob,you could easily tweak. but then you get into a rendering problem,5/9/16,1
lossyrob,"changing that reader to a MultibandTile will let you read your data, but the code after it is about rendering single band tiles. you_Ñéll have to modify that code. but that should at least get you going.",5/9/16,1
timothymschier,back,5/9/16,1
timothymschier,ahhh :headslap:,5/9/16,1
timothymschier,"I'm not familiar enough with the library to have spotted the missing ""Multiband"", though I looked at that line a dozen times",5/9/16,1
lossyrob,"yeah. and tbh I_Ñém not sure how to prevent that. Maybe a blind spot in our system, but you are responsible for knowing the type of thing your trying to pull out, and there_Ñés no way to know you have the wrong thing",5/9/16,1
lossyrob,"or maybe there is. Maybe it can read it from the schema and say, hey you_Ñére asking for the wrong type of thing.",5/9/16,1
lossyrob,feel like writing it up in a github issue for us to check out later? :),5/9/16,1
timothymschier,I manually looked at the metadata to check that I was using the right indexing Key,5/9/16,1
timothymschier,"sure I can write up the problem part at least, even without a proposed solution",5/9/16,1
lossyrob,yeah that_Ñéll be enough. we_Ñéll throw it on the backlog to look into if there_Ñés a way to do it,5/9/16,1
lossyrob,thanks,5/9/16,1
timothymschier,hi again guys,5/9/16,1
timothymschier,wonder if you could help guide me in troubleshooting the next thing,5/9/16,1
timothymschier,perhaps an issue with CRS management,5/9/16,1
timothymschier,"getting 404s like ```No tile: Tile with key SpatialKey(1,2) not found for layer Layer(name = ""Goonyella_Quick_Ortho_20160108"", zoom = 2)```",5/9/16,1
timothymschier,when rendering using Cesium,5/9/16,1
lossyrob,should there be a tile there? What_Ñés the KeyBounds of the data at zoom 2? (can tell from metadata),5/9/16,1
timothymschier,"yeah, if the CRSs line up there should be a tile there",5/9/16,1
timothymschier,will dig into the metadata some more,5/9/16,1
lossyrob,also: Cesium. Awesome. Would love to see that!,5/9/16,1
timothymschier,hehe so would I ;),5/10/16,1
lossyrob,lol. well when you get it working it would be cool to show a screen shot if you could :),5/10/16,1
timothymschier,so silly qns,5/10/16,1
timothymschier,what units are the URL / SpatialKey parameters in?,5/10/16,1
timothymschier,in the metadata there is metadata.extent and layoutDefinition.extent,5/10/16,1
timothymschier,with different sets of bounds,5/10/16,1
lossyrob,"hmm, just the standard slippy map z x y coords",5/10/16,1
lossyrob,"zoom, col, row",5/10/16,1
lossyrob,do you see the keyBounds,5/10/16,1
timothymschier,ah yep,5/10/16,1
timothymschier,ok for zoom 3 it seems only col:7 and row:4 are valid,5/10/16,1
timothymschier,when I put that into the URL I get a different failure now,5/10/16,1
timothymschier,```http://ec2-54-174-64-253.compute-1.amazonaws.com:8000/gt/tms/Goonyella_Quick_Ortho_20160108/3/7/4.png```,5/10/16,1
timothymschier,just gives ```The requested resource could not be found.```,5/10/16,1
timothymschier,which is like a standard Spray message?,5/10/16,1
lossyrob,yeah,5/10/16,1
lossyrob,maybe drop the png,5/10/16,1
timothymschier,same same,5/10/16,1
lossyrob,"what was the address that cesium was pining, that seemed to be at least hitting the endpoint",5/10/16,1
timothymschier,same address; fairly confident of that,5/10/16,1
timothymschier,e.g. if I hit ```http://ec2-54-174-64-253.compute-1.amazonaws.com:8000/gt/tms/Goonyella_Quick_Ortho_20160108/3/7/5``` I get the SpatialKey error again,5/10/16,1
timothymschier,(that's just changing the 4 to a 5),5/10/16,1
lossyrob,interesting,5/10/16,1
timothymschier,no errors in the spark-submit output,5/10/16,1
lossyrob,https://github.com/dwins/geotrellis-viewer/blob/master/src/main/scala/geotrellis/ingest/test/GeotrellisIngestTestServiceActor.scala#L162-L167,5/10/16,1
lossyrob,it_Ñés because it_Ñés looking for those parameters,5/10/16,1
timothymschier,I removed all of that,5/10/16,1
lossyrob,"because they are not there, the route just falls through, doesn_Ñét see...",5/10/16,1
lossyrob,oh ok,5/10/16,1
timothymschier,changing to MultibandTile caused a few compile errors,5/10/16,1
timothymschier,so I hacked at it quite a bit,5/10/16,1
lossyrob,https://github.com/dwins/geotrellis-viewer/blob/master/src/main/scala/geotrellis/ingest/test/GeotrellisIngestTestServiceActor.scala#L122-L167,5/10/16,1
lossyrob,can you remove all that,5/10/16,1
lossyrob,or did you,5/10/16,1
timothymschier,"lol ok sorry I have to look at my stuff, think I went a bit hack-and-slash crazy",5/10/16,1
lossyrob,haha ok. I know the feeling,5/10/16,1
lossyrob,the problem is that the spray route is not getting picked up,5/10/16,1
timothymschier,"e.g. left in the ""type"" pathPrefix, but took out the respondWithMediaType()",5/10/16,1
lossyrob,"it basically tries to load the tile. if it can_Ñét, it fails there",5/10/16,1
lossyrob,"if it can, it goes looking for a way to complete the route. it isn_Ñét finding it there",5/10/16,1
timothymschier,"ok, got it going. Funny how returning a png actually helps a tile server function",5/10/16,1
timothymschier,"we have Cesium rendering issues (due to configuration to support another tile server), but I can get the tiles via Leaflet (tailored from the landsat demo) and also by directly hitting the URLs",5/10/16,1
timothymschier,thanks for your help @lossyrob ,5/10/16,1
lossyrob,nice,5/10/16,1
lossyrob,no problem,5/10/16,1
dholstius_twitter,"Hi all! Forgive the intrusion. I wrote up my geoprocessing problem (""task"") at Stack Overflow ... is geotrellis a good match? http://stackoverflow.com/questions/37191030/bilinear-interpolation-on-integer-coordinates-within-a-delaunay-triangulation",5/12/16,1
dwins,"@dholstius_twitter Apart from the Spark integration, GeoTrellis includes a variety of spatial datatypes and algorithms so you definitely *could* use it for this problem. but it doesn't provide an algorithm for your specific problem AFAIK.  As you note in the SO post it seems like a good use for a GPU and GeoTrellis/Spark doesn't give you any special help in using that.",5/12/16,1
dwins,If you needed to break up a larger set of triangles for parallel processing on multiple machines and then composite the results to produce a final interpolation then there might be a stronger case for GeoTrellis.,5/12/16,1
lsycool,"Hello everyone, I found there are many question mark(""?"") in geotrellis source code, for example in .../geotrellis-master/spark-etl/src/main/scala/geotrellis/spark/etl/Etl.scala file",5/13/16,1
lsycool,"`code`   def save[    K: SpatialComponent: TypeTag,    V <: CellGrid: TypeTag: ? => TileMergeMethods[V]: ? => TilePrototypeMethods[V]  ](id: LayerId, rdd: RDD[(K, V)] with Metadata[TileLayerMetadata[K]], method: KeyIndexMethod[K]): Unit = {",5/13/16,1
lsycool,},5/13/16,1
lsycool,"but i don't really understand what it means, can anyone explain it to me, thank you very much",5/13/16,1
lossyrob,That is from the compiler plugin called kind-projector,5/13/16,1
lsycool,"Hi everyone, when I using the following code to save my tile result, there comes the problem:"" could not find implicit value for evidence parameter of type spray.json.JsonFormat[geotrellis.spark.SpatialKey]      writer.write[SpatialKey, MultibandTile, TileLayerMetadata[SpatialKey]](layerId, rdd, ZCurveKeyIndexMethod)"",  can anyone knows how to solve it, thank you very much!'''    val attributeStore = FileAttributeStore(outputPath)    val writer = FileLayerWriter(attributeStore)    writer.write[SpatialKey, MultibandTile, TileLayerMetadata[SpatialKey]](layerId, rdd, ZCurveKeyIndexMethod)'''",5/13/16,1
lossyrob,`import geotrellis.spark.io._`,5/13/16,1
lossyrob,Need to import io packages if missing json implicits,5/13/16,1
SDeathLeaDer,"@lossyrob, I used ""HadoopValueReader"" class but the service doesn't recover the tiles, what more classes i need to use for that service works in cluster mode. I am using your example of landsat but using spark-etl for ingesting. Please can you give me a example ?.     ",5/13/16,1
lossyrob,"@SDeathLeaDer sorry, not very clear on the question. Can you clarify? What do you mean by recover tiles?",5/13/16,1
lossyrob,This is interesting_Ñ_.final case class sematics...,5/13/16,1
lossyrob,"```scalascala> class Outer { case class Inner(x: Int) }defined class Outerscala> val x1 = new Outerx1: Outer = Outer@37baf35dscala> val x2 = new Outerx2: Outer = Outer@1e835f05scala> def f(x: Any) = x match {     |   case x1.Inner(v) => v     |   case _ => -1     | }f: (x: Any)Intscala> f(x1.Inner(5))res0: Int = 5scala> f(x2.Inner(5))res1: Int = -1// As opposed to...scala> class Outer { final case class Inner(x: Int) }warning: there was one unchecked warning; re-run with -unchecked for detailsdefined class Outerscala> val x1 = new Outerx1: Outer = Outer@60f2f242scala> val x2 = new Outerx2: Outer = Outer@18156ddbscala> def f(x: Any) = x match {     |   case x1.Inner(v) => v     |   case _ => -1     | }warning: there was one unchecked warning; re-run with -unchecked for detailsf: (x: Any)Intscala> f(x1.Inner(5))res0: Int = 5scala> f(x2.Inner(5))res1: Int = 5```",5/13/16,1
SDeathLeaDer,"@lossyrob  When i mentioned ""recover tiles"", i refer to ""ServeNDVI.reader(LayerId(""nlcd-tms"",zoom)).read(x, y)"".",5/13/16,1
lossyrob,so read tiles?,5/13/16,1
lossyrob,are you not able to read tiles out of hadoop? what is the error you are getting?,5/13/16,1
SDeathLeaDer,"@lossyrob when i compile my code i don't get error, https://gist.github.com/SDeathLeaDer/727734af3324d00767c6520d3edccb95.",5/13/16,1
lossyrob,"@SDeathLeaDer sorry, still unclear about the issue. So it compiles, but what is the issue your having with that code? that the tile is not found?",5/14/16,1
rshirochenko,"Is it possible to move ingested data with FileLayerWriter and saved on the local disk to Accumulo database directly, without new ingesting operation?",5/14/16,1
pomadchin,"@rshirochenko right now we don_Ñét have an abstraction to move layers between backends; but oviously you can read ingested data using layer reader, and to write it to a new backend using layer writer:```scalaval fileLayerReader = ???val accumuloLayerWriter = ???accumuloLayerWriter.write[SpatialKey, Tile, TileLayerMetadata[SpatialKey]](  layerId,  fileLayerReader.read[SpatialKey, Tile, TileLayerMetadata[SpatialKey]](layerId),   keyIndexMethod)```",5/14/16,1
rshirochenko,Whats advantages for Accumulo option compare to HDFS files writing/reading?,5/14/16,1
pomadchin,"That_Ñés definitely speed, you need smth like Accumulo (big table) when you need a near real time response, Accumulo has it_Ñés own mr, faster scans accross tables, etcThough hadoop is better (to be honest hadoop again is slower) for some background jobs",5/14/16,1
pomadchin,"Accumulo is a big table, a database that stores data in hdfs (like mongodb stores data in some more common fs).that_Ñés my opinion though; \+ more flexible schema definition \+ better mr \+  in case of accumulo better security etc, etc, etc O: i haven_Ñét described even a part of advantages of using Accumulo instead of using hadoop",5/14/16,1
rshirochenko,thx,5/14/16,1
SDeathLeaDer,"@lossyrob, thanks for response sorry if it was not clear. I want to say that problem is that the service doesn't found the tiles, i  was ingesting by spark-etl, the code of the service and script for ingesting are: https://gist.github.com/SDeathLeaDer/727734af3324d00767c6520d3edccb95. I think  could be that the structure of files when was ingested not is according with the service.  ",5/16/16,1
lossyrob,"@SDeathLeaDer if you could specify what you man by ""service doesn't found the tiles_Ñù, that would help. e.g. is it just a 404? Does it say the resource is not found? You_Ñéve tried hitting the service with a tile address directly and it shows _ÑÒinside complete_Ñù but then gives a 404?",5/16/16,1
singhkorner_twitter,"Hi, I was able to successfully ingest tiffs into HDFS.  Question: If the input raster or tile size is higher than the HDFS blocksize (128mb),  are they split?. what is the strategy? thanks",5/16/16,1
echeipesh,"@singhkorner_twitter The input rasters are tiled into a grid that you specify the tile size for 256x256 being the default. Those are the tiles that are parked into HDFS records. If you specify a layout with tile size larger than the HDFS block they will not be split, they will overrun the block size. But again, that doesn_Ñét have to do with your input raster size",5/16/16,1
SDeathLeaDer,"@lossyrob, the error that show me in the web page is 500 and error in console is ""empty collection"", the full error is in the gist. ",5/16/16,1
lossyrob,Can you find the json metadata for the layers (in the attribute folder) and put those in the gist,5/16/16,1
SDeathLeaDer,@lossyrob  i put in the gist.,5/16/16,1
lossyrob,"look like there werent tiles ingested, `{""keyBounds"":{""minKey"":{""col"":0,""row"":0},""maxKey"":{""col"":0,""row"":0}}}}`",5/16/16,1
SDeathLeaDer,,5/16/16,1
lossyrob,ah. that was zoom 0. whats the highest zoom metadata?,5/16/16,1
SDeathLeaDer,"ok,  for *nlcd-tms___15___metadata.json* is `{""keyBounds"":{""minKey"":{""col"":9898,""row"":17843},""maxKey"":{""col"":9920,""row"":17865}}}`",5/16/16,1
lossyrob,what are the URLs of the tiles that are throwing the errors?,5/17/16,1
lossyrob,"should be `z/x/y` paths where z is zoom and x,y are the spatial key",5/17/16,1
SDeathLeaDer,"The service doesn't find any tiles, all requests return error 500.",5/17/16,1
lossyrob,are the request keys within the bounds?,5/17/16,1
echeipesh,"@SDeathLeaDer `500` is service error, it should have thrown something on console or log, what does it say there ?",5/17/16,1
SDeathLeaDer,"@echeipesh, in ",5/17/16,1
SDeathLeaDer,I put the log of the console in the gist: https://gist.github.com/SDeathLeaDer/727734af3324d00767c6520d3edccb95,5/17/16,1
echeipesh,"@SDeathLeaDer Ah thanks, catching up to the conversation. Something weird about that gist. The second JSON has metadata for `{""name"":""nlcd-tms"",""zoom"":10}`, zoom 10, but you_Ñére saying it came from ` nlcd-tms___15___metadata.json`, I assume that was a type-o.",5/17/16,1
echeipesh,with that service endpoint you should be able to get `/10/309/557` based on that metadata.,5/17/16,1
SDeathLeaDer,"@lossyrob, the request keys are within the bounds.",5/17/16,1
SDeathLeaDer,"@echeipesh sorry the put the wrong content , i update the gist.",5/17/16,1
SDeathLeaDer,"The test with  	`10/309/557` and returns 500.",5/17/16,1
singhkorner_twitter,@echeipesh  Thank you. ,5/17/16,1
mchendricks1,I'm looking into digital globe WV-3 image processing because a heckler in a presentation I was giving basically asserted that if the Geotrillis RDD didn't in effect automatically tile say 2TB for distribution over several say m3.xlarge then it was doing anything useful. For whatever reasons we couldn't come to an agreement about sub-tiling before ingesting into Geotrellis. ,5/18/16,1
mchendricks1,So so I thought I would ask: what is the recommended model(s) for dealing with say terabyte images and bands?,5/18/16,1
mchendricks1,(That should be: wasn't doing anything useful),5/18/16,1
echeipesh,"@mchendricks1 Maybe I misunderstand the question: but we have to tile large rasters in order to distributed them, thats something that happens during ingest.",5/18/16,1
echeipesh,"We_Ñére not great at ingesting a huge image all by itself, so I think you what you mean by sub-tiling is haveing some kind process to chunk it out into a bunch of smaller images before ingesting it. Usually we use something like a pyspark job leaning on gdal/rasterio to do that.",5/18/16,1
echeipesh,"What would be best way to handle that gracefully is to have a streaming image reader, something similar to gdal vsicurl, but we haven_Ñét written that yet.",5/18/16,1
mchendricks1,@echeipesh Excellent! That points me in the right direction! Thank you!,5/18/16,1
sfosdal,Does anyone know where to find some docs on how PixelIsArea vs PixelIsPoint are supposed to work when used in Rasterizer.foreachCellByPolygon? The behavior I'm seeing is non-intuitive.,5/18/16,1
jamesmcclain,"At a high level, the intent is distinguish between pixels treated as points and pixels treated as extents",5/18/16,1
jamesmcclain,"For example, in the `foreachCellByPolygon` case, if the pixels are points, then they are reported iff the corresponding point is inside of the polygon",5/18/16,1
jamesmcclain,When pixels are treated as extents (if memory serves) we report the pixel if the area that it represents intersects with the polygon,5/18/16,1
dwins,you mean the center of the cell?,5/18/16,1
jamesmcclain,"Yes, the corresponding point is the center of the cell",5/18/16,1
sfosdal,I'll work up a gist for my example.  I think it's not working this way...,5/18/16,1
jamesmcclain,"Okay, if there is a bug we will address it",5/18/16,1
sfosdal,"This is what I'm seeing:https://gist.github.com/sfosdal/02436a234dda81d520d4d939f6a1fbd7",5/18/16,1
lossyrob,"That is a strange case, and it looks like a bug. Thanks for stating it in a gist like that, it_Ñéll make it easier to test and fix",5/18/16,1
sfosdal,@lossyrob no problem you know if there is (should be) a github issue for it I can create one if it's preferred.,5/18/16,1
lossyrob,"@sfosdal creating one would be great, thank you",5/18/16,1
sfosdal,https://github.com/geotrellis/geotrellis/issues/1482,5/18/16,1
sfosdal,Thanks!,5/18/16,1
jamesmcclain,"@sfosdal  Please give this branch a try:  https://github.com/geotrellis/geotrellis/pull/1483  .  This change resolve the particular issue that you posted, but please let us know if it does or does not fix the original issue that you experienced",5/18/16,1
jamesmcclain,"Thanks for catching/reporting this bug, as well!",5/18/16,1
sfosdal,"sure, I can give that a try tonight",5/18/16,1
jamesmcclain,Sounds good,5/18/16,1
sfosdal,"a question or two. (I_Ñém new to building this project)Is this test failure because of something I_Ñém doing wrong?",5/18/16,1
sfosdal,"[info] ProjectedSpec:[info] Exception encountered when attempting to run a suite with class name: geotrellis.slick.ProjectedSpec *** ABORTED ***[info]   com.typesafe.config.ConfigException$Missing: No configuration setting found for key 'db'[info]   at com.typesafe.config.impl.SimpleConfig.findKey(SimpleConfig.java:124)[info]   at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:147)[info]   at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:159)[info]   at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:164)[info]   at com.typesafe.config.impl.SimpleConfig.getString(SimpleConfig.java:206)[info]   at geotrellis.slick.TestDatabase$.newInstance(TestDatabase.scala:11)[info]   at geotrellis.slick.TestDatabase$class.beforeAll(TestDatabase.scala:30)[info]   at geotrellis.slick.ProjectedSpec.beforeAll(ProjectedSpec.scala:27)[info]   at org.scalatest.BeforeAndAfterAll$class.beforeAll(BeforeAndAfterAll.scala:187)[info]   at geotrellis.slick.ProjectedSpec.beforeAll(ProjectedSpec.scala:27)[info]   ...[info] PostgisSpec:[info] Exception encountered when attempting to run a suite with class name: geotrellis.slick.PostgisSpec *** ABORTED ***[info]   com.typesafe.config.ConfigException$Missing: No configuration setting found for key 'db'[info]   at com.typesafe.config.impl.SimpleConfig.findKey(SimpleConfig.java:124)[info]   at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:147)[info]   at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:159)[info]   at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:164)[info]   at com.typesafe.config.impl.SimpleConfig.getString(SimpleConfig.java:206)[info]   at geotrellis.slick.TestDatabase$.newInstance(TestDatabase.scala:11)[info]   at geotrellis.slick.TestDatabase$class.beforeAll(TestDatabase.scala:30)[info]   at geotrellis.slick.PostgisSpec.beforeAll(PostgisSpec.scala:30)[info]   at org.scalatest.BeforeAndAfterAll$class.beforeAll(BeforeAndAfterAll.scala:187)[info]   at geotrellis.slick.PostgisSpec.beforeAll(PostgisSpec.scala:30)",5/18/16,1
sfosdal,also to build it_Ñés as simple as publishLocal?,5/18/16,1
jamesmcclain,I think that normally one uses the `scripts/buildall.sh` script to build,5/18/16,1
jamesmcclain,And the `scripts/publish-local.sh` to do a local publish,5/18/16,1
sfosdal,"awesome, thanks",5/18/16,1
jamesmcclain,You are welcome :smile: ,5/18/16,1
sfosdal,That branch fixed my issue.,5/18/16,1
sfosdal,totally cool,5/18/16,1
sfosdal,My gist was really my attempt to understand PixelIsPoint vs PixelIsArea. so I don_Ñét really have any other use cases.,5/18/16,1
sfosdal,Anything I can do to help get this merged in and released?,5/18/16,1
sfosdal,The border differences are what I was expecting,5/18/16,1
lossyrob,"We_Ñéll get this fixed merged in to 0.10.1, which I want to put out soon (next week)",5/19/16,1
timothymschier,"Hey guys, @echeipesh mentioned that you use ""something like a pyspark job leaning on gdal/rasterio"" to slice up large raster images before ingest. Do you have sample code or a project somewhere that we could look at? I guess there'd be a few interested in doing this in a distributed way using pyspark.",5/19/16,1
lossyrob,@timothymschier https://github.com/hotosm/oam-server-tiler/blob/master/chunk/chunk.py,5/19/16,1
owcm,"I have just updated my code the 0.10.0 and modified my code to use the new Pyramid z/x/y tiling scheme.  Same approach that is used in the geotrellis-landsat-tutorial project IngestImage.scala code. With some raster data I end up with a very blocked result after performing:```val (zoom, reprojected): (Int, RDD[(SpatialKey, Tile)] with Metadata[TileLayerMetadata[SpatialKey]]) =      TileLayerRDD(tiled, tileLayerMetadata).reproject(WebMercator, layoutScheme, Bilinear)```will attach and image of what I am seeing.   Prior to upgrading I did not have this problem.  ",5/19/16,1
owcm,[![stitchedre.png](https://files.gitter.im/geotrellis/geotrellis/Vwb4/thumb/stitchedre.png)](https://files.gitter.im/geotrellis/geotrellis/Vwb4/stitchedre.png),5/19/16,1
owcm,,5/19/16,1
owcm,Created a Gist with my code: https://gist.github.com/owcm/0270dcfe21c1c3db4ed89b9accf537e7,5/19/16,1
timothymschier,Thanks @lossyrob I'll check it out.,5/19/16,1
SDeathLeaDer,"Sorry for my insistence @lossyrob  and @echeipesh, but  could see what could be the error ?, because i am  stuck with this  :(. ",5/19/16,1
lossyrob,@SDeathLeaDer the best way to allow us to help is to get a repeatable environment we can observe the error in. Is there a project you can set up and put on github that will show this error?,5/19/16,1
lossyrob,"@owcm thanks for supplying the project, I ran it and found the issue",5/20/16,1
lossyrob,"```scala    val (zoom, reprojected): (Int, RDD[(SpatialKey, Tile)] with Metadata[TileLayerMetadata[SpatialKey]]) =      TileLayerRDD(tiled, tileLayerMetadata)        .convert(FloatConstantNoDataCellType)        .reproject(WebMercator, layoutScheme, Bilinear)```",5/20/16,1
lossyrob,the incoming geotiffs do not have a NoData declared,5/20/16,1
lossyrob,"so when it tiles, it sees Float.NaN and considers it valid data, and doesn_Ñét overwrite. This isn_Ñét good behavior, and I think should behave differently. I_Ñéll put a fix into 0.10.1 for it, but until then, if you do the convert before the reproject, all is well",5/20/16,1
owcm,"@lossyrob Ok, I am thick what I am converting?  Loop though the cells and if Float.Nan  set to NoData?",5/20/16,1
lossyrob,calling convert just changes the cell type to declare Float.NaN is actually representative of NoData values,5/20/16,1
owcm,@lossyrob Never mind the example is in the code snippet  you sent me,5/20/16,1
SDeathLeaDer,"@lossyrob   for now there is no project and environment. I am going to create and configure one, thanks for advice. ",5/20/16,1
lossyrob,"ok, thanks @SDeathLeaDer, that will make helping a lot easier for us!",5/20/16,1
owcm,"@lossyrob Thanks, that worked like a charm.",5/20/16,1
dwins,"Hey all, tried to deploy locally with `scripts/publish-local-cross-version.sh` and got a build error on `geotrellis-engine````[error] /Users/dwins/Projects/radiantblue/geotrellis/engine/src/main/scala/geotrellis/engine/op/focal/FocalRasterSourceMethods.scala:31: type mismatch;[error]  found   : (geotrellis.raster.Tile, geotrellis.raster.mapalgebra.focal.Neighborhood, Option[geotrellis.raster.GridBounds], geotrellis.raster.mapalgebra.focal.TargetCell) => geotrellis.raster.Tile[error]  required: (geotrellis.raster.Tile, geotrellis.raster.mapalgebra.focal.Neighborhood, Option[geotrellis.raster.GridBounds]) => geotrellis.raster.Tile[error]   def focalStandardDeviation(n: Neighborhood) = focal(n)(StandardDeviation.apply)[error]                                                                            ^[warn] one warning found[error] 7 errors found[info] No documentation generated with unsuccessful compiler run[warn] one warning found[error] 7 errors found[error] (engine/compile:doc) Scaladoc generation failed[error] (engine/compile:compileIncremental) Compilation failed```",5/20/16,1
dwins,I'll just comment out geotrellis-engine from the script for now but it seems like this might be worth correcting,5/20/16,1
dwins,oh ok this is related to changes on my branch. sorry for the noise.,5/20/16,1
mbertrand,"Hi, I'm wondering if there's a way to specify a max zoom level when ingesting geotiff's.  I've been playing with the landsat-tutorial and chatta demo projects, replacing some of the provided geotiff's with my own, and noticed that the max zoom level available seems to depend on the resolution of the input geotiff.  Ideally I'd like to be able to force a raster layer to be viewable up to zoom level 14 instead of 9 for example.",5/20/16,1
fosskers,"That's the default behaviour, yeah. The nearest zoom level is projected to depending on the input resolution",5/20/16,1
fosskers,Lower (re: farther zoomed out) layers are then interpolated from that original projected layer,5/20/16,1
lossyrob,"@mbertrand we actually don_Ñét have that ability right now. Which is something we should definitely have, it wouldn_Ñét be too difficult to implement. Care to write a github issue for it?",5/20/16,1
mbertrand,"Sure, will do.  I might take a stab at implementing it too, once I get more familiar with the code base, if no one beats me to it.",5/20/16,1
lossyrob,"Ok, that sounds great! If you wanted to get that into the 0.10.1 release, I was hoping to get that out next week.",5/20/16,1
lossyrob,"Let me know when you make the github issue, I can put some pointers on the issue of where the code would need to change",5/20/16,1
mbertrand,https://github.com/geotrellis/geotrellis/issues/1484,5/20/16,1
mbertrand,Thanks @lossyrob and @fosskers. ,5/20/16,1
owcm,"@lossyrob  I declared victory a little early.   I tried loading multiple files, same area, but 50cm resolution instead of 1 meter.  So 12 files processed instead of 1.   Ended up with the block look again.  This was with build 0.10.0-31e8edc.   I updated by git project and added a zip with the multiple files if you want to try it.  Need to remove the original data file and copy the zip file contents into the data directory.  Zip file is https://drive.google.com/file/d/0B1YZbN2gCsmScGlJd1F0YW1zNlk/view?usp=sharing",5/20/16,1
owcm,[![Screen Shot 2016-05-20 at 10.46.57 AM.png](https://files.gitter.im/geotrellis/geotrellis/PiJg/thumb/Screen-Shot-2016-05-20-at-10.46.57-AM.png)](https://files.gitter.im/geotrellis/geotrellis/PiJg/Screen-Shot-2016-05-20-at-10.46.57-AM.png),5/20/16,1
lossyrob,You have to do the convert beforehand,5/20/16,1
lossyrob,Another way to do it is to change the cell type of the TileLayerMetadata that you start with,5/20/16,1
lossyrob,before that first tileToLayout call,5/20/16,1
lossyrob,something like,5/20/16,1
lossyrob,"```scalaval tileLayerMetadata = {  val (_, md) = TileLayerMetadata.fromRdd(inputTiles, FloatingLayoutScheme(512))  md.copy(cellType = FloatConstantNoDataCellType)}```",5/20/16,1
lossyrob,instead of,5/20/16,1
lossyrob,"```scala    println(""Processing Set FloatingLayoutScheme"")    val (_, tileLayerMetadata ) = TileLayerMetadata.fromRdd(inputTiles, FloatingLayoutScheme(512))```",5/20/16,1
lossyrob,@moradology this is a good pain point to think about going back on a choice we made about _ÑÒraw_Ñù Double and Float cell types,5/20/16,1
lossyrob,"although I_Ñém not entirely sure why the merging is behaving the way that it is, I_Ñéll have to look further into it",5/20/16,1
owcm,@lossyrob I will give it a try,5/20/16,1
lossyrob,"also could convert the inputTiles via a map,",5/20/16,1
lossyrob,"```scala    val inputTiles = BuildRdd.tilePrepRoute( sc).map { tile => tile.convert(FloatConstantNoDataCellType) }```",5/20/16,1
lossyrob,"@mbertrand I made some comments on that issue. It might be a bit tricky, mostly because I don_Ñét think we ever really nailed the LayoutScheme API completely. Let me know if you have questions, either on the github issue (I_Ñéll get notifications) or here.",5/20/16,1
mbertrand,"Thanks, I'll look it over",5/20/16,1
owcm,@lossyrob the first option worked for me.   ,5/20/16,1
lossyrob,"@owcm you are dealing with Float32 geotiffs that have no NoData value defined. But they seem to contain NaN values, and those represent NoData values. Is that a common thing you_Ñéve encountered?",5/20/16,1
owcm,@lossyrob Well the problems seems to re-occur from the same source I am receiving the data from.  But I also am using GDAL to translate the data to WGS84 (LatLng) and then cutting the tiles into smaller sizes.   So I could be introducing the process.     Should gdalinfo tell me if a NoData value is defined.,5/20/16,1
lossyrob,yeah it_Ñéll say it under the band info,5/20/16,1
lossyrob,but gdal would translate the nodata values by default,5/20/16,1
lossyrob,so if your source hand a ND value it would end up in the output tiff,5/20/16,1
lossyrob,e.g.,5/20/16,1
lossyrob,"```consoleCorner Coordinates:Upper Left  (-112.0005556,  38.0005556) (112d 0' 2.00""W, 38d 0' 2.00""N)Lower Left  (-112.0005556,  36.9994444) (112d 0' 2.00""W, 36d59'58.00""N)Upper Right (-110.9994444,  38.0005556) (110d59'58.00""W, 38d 0' 2.00""N)Lower Right (-110.9994444,  36.9994444) (110d59'58.00""W, 36d59'58.00""N)Center      (-111.5000000,  37.5000000) (111d30' 0.00""W, 37d30' 0.00""N)Band 1 Block=256x256 Type=Float32, ColorInterp=Gray  Description = Layer_1  NoData Value=-3.4028234663852886e+38  Metadata:    LAYER_TYPE=athematic```",5/20/16,1
owcm,"The raw data I am getting in this case:```Image Structure Metadata:  INTERLEAVE=BANDCorner Coordinates:Upper Left  (  395990.500, 4088009.500) ( 76d10' 4.46""W, 36d55'56.61""N)Lower Left  (  395990.500, 4081996.500) ( 76d10' 1.48""W, 36d52'41.52""N)Upper Right (  402481.500, 4088009.500) ( 76d 5'42.11""W, 36d55'59.11""N)Lower Right (  402481.500, 4081996.500) ( 76d 5'39.32""W, 36d52'44.01""N)Center      (  399236.000, 4085003.000) ( 76d 7'51.84""W, 36d54'20.33""N)Band 1 Block=6491x1 Type=Float32, ColorInterp=Gray  Min=-1.300 Max=41.750  Minimum=-1.300, Maximum=41.750, Mean=6.321, StdDev=8.063  Metadata:    STATISTICS_MAXIMUM=41.75    STATISTICS_MEAN=6.3210937014136    STATISTICS_MINIMUM=-1.2999877929688    STATISTICS_STDDEV=8.0626231023727```",5/20/16,1
owcm,@ lossyrob There must be a way in gdal to give NaN data a NoData Value,5/20/16,1
lossyrob,`-a_srs nan`,5/20/16,1
lossyrob,or something similar,5/20/16,1
owcm,"ok, well I will put that on my data validation list.",5/20/16,1
lossyrob,"```consolefrom gdal_translate docs:-a_nodata value:Assign a specified nodata value to output bands. Starting with GDAL 1.8.0, can be set to none to avoid setting a nodata value to the output file if one exists for the source file. Note that, if the input dataset has a nodata value, this does not cause pixel values that are equal to that nodata value to be changed to the value specified with this option.```",5/20/16,1
lossyrob,that option is everywhere,5/20/16,1
lossyrob,pretty certain that it_Ñés `nan` that is the string representing what you_Ñéd want,5/20/16,1
lossyrob,"but even so, I think we need to work with Float and Double types that have no nodata value as still considering NaN as nodata by default",5/20/16,1
lossyrob,it seems pretty intuitive to me,5/20/16,1
lossyrob,"I wonder what GDAL does with it, e.g. if you rendered as a PNG with gdal, would it render the NaN..or if you had a VRT, would it read through NaN values as NoData. Probably should much around with it a bit and see",5/20/16,1
owcm,@lossyrob Well thats what I am going to do this afternoon.  Change by pipeline processing scripts.  You have me wondering now.,5/20/16,1
owcm,@lossyrob Funny thing is pre-0.10.0 I was getting away with it.   But I agree I need to have the nodata value defined in the data.,5/20/16,1
lossyrob,"yeah that_Ñés because we weren_Ñét thinking as hard as we did right before the release, and decided on the necessity to declare NoData values. But I_Ñém actually going back on that line of reasoning_Ñ_I_Ñéll show you where in code it matters",5/20/16,1
lossyrob,https://github.com/geotrellis/geotrellis/blob/master/raster/src/main/scala/geotrellis/raster/FloatArrayTile.scala#L263,5/20/16,1
lossyrob,"if you do a `FloatArrayTile.empty(FloatCellType, cols, rows)`, where FloatCellType is the one without any NoData declared",5/20/16,1
lossyrob,it fills it full of 0_Ñés,5/20/16,1
lossyrob,"so it considers `0` the sort of, base value. Which really turns out to mean, 0 is the default no-data-ish value",5/20/16,1
lossyrob,"in our merge logic, we do a `prototype` call",5/20/16,1
lossyrob,https://github.com/geotrellis/geotrellis/blob/master/raster/src/main/scala/geotrellis/raster/prototype/SinglebandTilePrototypeMethods.scala#L17,5/20/16,1
lossyrob,"which ends up calling that `FloatArrayTile.empty` call I mentioned, so you end up with the prototype tile (the tile which we will be burning values to when we do `tileToLayout`) full of 0_Ñés",5/20/16,1
lossyrob,"and then when we are merging tiles,",5/20/16,1
lossyrob,https://github.com/geotrellis/geotrellis/blob/master/raster/src/main/scala/geotrellis/raster/merge/SinglebandTileMergeMethods.scala#L51,5/20/16,1
lossyrob,you can see we assume 0.0 to be the transparent value,5/20/16,1
lossyrob,"so once a `NaN` ends up in a tile somewhere, it is considered good data and sticks around",5/20/16,1
lossyrob,"if we determine that NaN should be the default NoData even for Float or Double tiles, we should change the `empty` method and the merge methods to consider NaN instead of 0",5/20/16,1
lossyrob,"so if your GDAL tests show that yes, GDAL will consider NaN as NoData even if it_Ñés not explicitly declared, I would want to make that change",5/20/16,1
owcm,"Okay, then I will let you know what I find.",5/20/16,1
lossyrob,"cool, thanks @owcm !",5/20/16,1
lossyrob,@jamesmcclain looking over your gridcoverage2d PR now,5/20/16,1
lossyrob,"```scala  val policy = AbstractGridFormat.OVERVIEW_POLICY.createValue  policy.setValue(OverviewPolicy.IGNORE)  val gridSize = AbstractGridFormat.SUGGESTED_TILE_SIZE.createValue  gridSize.setValue(""1024,1024"")  val useJaiRead = AbstractGridFormat.USE_JAI_IMAGEREAD.createValue  useJaiRead.setValue(true)```",5/20/16,1
lossyrob,that_Ñés in the initial commands for the geotools project. can you talk a bit about what that does?,5/20/16,1
jamesmcclain,"Yes, I can try",5/20/16,1
jamesmcclain,"Ah, I think that came from the initial commands for the console in the geotools project",5/20/16,1
jamesmcclain,"it isn't clear to me what an overview policy is, so that parameter is not used in the code itself",5/20/16,1
dwins,"Hm, I wonder why they would put `OverviewPolicy.IGNORE` in the example. Seems like it would be a performance hit",5/20/16,1
jamesmcclain,What is an overview policy?,5/20/16,1
dwins,"Some raster file formats support ""overviews"" as in downsampled copies of the dataset embedded in a single file",5/20/16,1
jamesmcclain,"Ah, gotcha",5/20/16,1
jamesmcclain,Maybe the attempt is to create the simplest possible behavior in the context of the example,5/20/16,1
dwins,"Are there other policies besides ""IGNORE"" and ""DONT IGNORE"" ?",5/20/16,1
jamesmcclain,http://docs.geotools.org/stable/javadocs/org/geotools/coverage/grid/io/OverviewPolicy.html,5/20/16,1
jamesmcclain,It looks like there are about four different possible policies,5/20/16,1
jamesmcclain,"""ignore"", ""nearest"", ""quality"" and ""speed""",5/20/16,1
dwins,"Oh I see. That makes sense, I wasn't considering cases where the client needs a resolution different from any of the overviews.",5/20/16,1
jamesmcclain,"Yes, I think that makes sense to me, as well",5/20/16,1
jamesmcclain,I believe that the JAI image read is just another optimization?  I wasn't able to detect any substantial difference in behavior when I did or did not supply it,5/20/16,1
jamesmcclain,"And finally, the suggested tile size seems to be another optimization to limit the amount of data read from striped tiles",5/20/16,1
dwins,In fact JAI image read is the default https://github.com/geotools/geotools/blob/42c4cc0c3d850b1dea50c07108f171aace35a798/modules/plugin/coverage-multidim/coverage-api/src/main/java/org/geotools/coverage/io/ReadType.java#L168-L170,5/20/16,1
jamesmcclain,"Okay, that makes sense",5/20/16,1
lossyrob,"ok, thanks for the explanation. I think putting a comment saying, _ÑÒInitialization code taken from geotools_Ñù would make sense",5/20/16,1
lossyrob,"I_Ñém messing around on the branch as part of the code review, I could make that change",5/20/16,1
jamesmcclain,"I am about to push some changes to the branch, I will add a comment",5/20/16,1
lossyrob,ok cool,5/20/16,1
lossyrob,"I_Ñém going to be making some changes around the API, I_Ñéll tag you in a PR when I push those up",5/20/16,1
jamesmcclain,"Okay, sounds good",5/20/16,1
jamesmcclain,I have just pushed up the changes,5/20/16,1
lossyrob,"@echeipesh I_Ñém concerned about the extensability of the TileCode, that is union codec of known types https://github.com/geotrellis/geotrellis/blob/master/spark/src/main/scala/geotrellis/spark/io/avro/codecs/Implicits.scala#L12",5/21/16,1
lossyrob,how should this support GeoTiffTiles? Other tile types that are defined outside of known types to geotrellis.spark?,5/21/16,1
echeipesh,"Yes, that_Ñés a little messy. You_Ñéd have to instantiate another version of `AvroUnionCodec[Tile]`, but using it would be tricky because you wouldn_Ñét be able to use the `geotrellis.spark._` import at risk of getting duplicate implicits. We should shift that implicit instance into `AvroRecordCodec` companion.",5/23/16,1
kevinmhinson,"I hate asking, but I'm at a bit of a loss...  if I have a Spark cluster on AWS...   on Yarn, with Zookeeper/Accumulo...  what's my best shot at getting Geotrellis going from that starting point?  is there a fat jar I could use, or a shell script as a starting point, or is deployment much more involved than that?",5/23/16,1
lossyrob,no worries! The deployment at that point is the same as most spark deployments,5/23/16,1
lossyrob,so you would bring in GeoTrellis as a library dependnecy,5/23/16,1
lossyrob,"write your code, and then `sbt assembly` it so that you can run it through spark-submit",5/23/16,1
lossyrob,do you have some spark code your trying to execute with it? or will you be making a project from scratch,5/23/16,1
lossyrob,"This is very unpolished, but an example of what a project might look like that would be run on a spark cluster",5/23/16,1
lossyrob,https://github.com/lossyrob/treecanopy-ingest,5/23/16,1
lossyrob,"you can see here we_Ñére pulling in GeoTrellis as a dependency (a SNAPSHOT version that is published locally, so you_Ñéd want to use the release version) https://github.com/lossyrob/treecanopy-ingest/blob/master/build.sbt#L26",5/23/16,1
lossyrob,"and then the main is just the spark job, which uses GeoTrellis to perform an ingest https://github.com/lossyrob/treecanopy-ingest/blob/master/src/main/scala/treecanopy/Main.scala",5/23/16,1
kevinmhinson,"ok, great!  Thank you Rob!  that helps!  one follow up...  is there a way I can import Geotrellis into spark-shell to interactively duff my way through my first few attempts?  [edit] sorry...  I guess I could just build a jar that does the imports and use that in spark-shell, huh?",5/23/16,1
lossyrob,"yes, you_Ñéd want to set up an sbt project that inludes GeoTrellis,even if it doesn_Ñét do anything. https://github.com/geotrellis/geotrellis-sbt-template is a good starting place for that",5/23/16,1
lossyrob,"then you would do `./sbt assembly` on that to create the fat jar, which will pack in all the dependencies",5/23/16,1
lossyrob,push that jar up to your master node that you_Ñéll be running spark-shell on,5/23/16,1
lossyrob,and then there is an option to include a jar in the spark-shell startup,5/23/16,1
kevinmhinson,"perfect!  thank you, Rob!  I'm excited to get up and running with Geotrellis!",5/23/16,1
lossyrob,ah it_Ñés `_ÑÓjars` http://spark.apache.org/docs/latest/programming-guide.html#using-the-shell,5/23/16,1
lossyrob,"no problem! feel free to hit us up here, talking through things helps us as well too so don_Ñét hesitate",5/23/16,1
sfosdal,got time for another raster question? This time about resampling?,5/23/16,1
sfosdal,I_Ñém seeing strange artifacts for three of the five resample methods.,5/23/16,1
sfosdal,https://gist.github.com/sfosdal/026ee3beb9a63b98a65efffe0d56109f,5/23/16,1
lossyrob,Very interesting. And great test. Ill have to look into that...is the code to reproduce available anywhere?,5/23/16,1
sfosdal,Resample.scala should reproduce it,5/23/16,1
lossyrob,"ah sorry, I scrolled down to the results and spaced on the code above, my bad!",5/23/16,1
lossyrob,thanks,5/23/16,1
sfosdal,no prob,5/23/16,1
sfosdal,@lossyrob if you think it's really a bug I'd be happy to create an issue ,5/23/16,1
lossyrob,"Not sure; I would guess so_Ñ_those resample types are the least tested for sure, so I_Ñém suspecting that it is",5/23/16,1
sfosdal,"I was under the assumption that (maybe is mistaken,  but would work just for me)  that the NoData cells and the cells outside the extent would be treated the same.  Which I think would make it a bug,  but I'm not sure that is the intent ",5/23/16,1
sfosdal,I'd be happy to run other tests ,5/23/16,1
lossyrob,"your right, nodata and data outside cells should be treated the same",5/23/16,1
sfosdal,Or issue PRs for them if I know what's expected ,5/23/16,1
lossyrob,that would be great. I think your on the right path about what the correct behavior is.,5/23/16,1
sfosdal,I'll try to write that test up tonight ,5/23/16,1
sfosdal,Cool,5/23/16,1
lossyrob,thanks!,5/23/16,1
bichocj,,5/24/16,1
pljplj,"Not a scala expert but I'm having trouble doing some JTS stuff.  I want to do a difference between a Polygon and a LineString .  What is the right way to do it?  I tried      val poly = read(""POLYGON ( (255 75 , 255 0, 120 0, 255 75) )"")    val line = read(""LINESTRING (255 1,255 254)"")    val diff = poly.jtsGeom.difference(line)",5/25/16,1
pljplj,It doesn't think line is the correct type,5/25/16,1
dwins,@pljplj for most uses we hope you won't need `jtsGeom`,5/25/16,1
pljplj,"ok, what is the correct way",5/25/16,1
dwins,I think it will work with just `poly difference line` or even `poly - line`,5/25/16,1
dwins,(on my phone right now so hard to verify code ,5/25/16,1
pljplj,Compiler doesn't like either of those,5/25/16,1
dwins,"Oh I see. In your code the geometries are typed as `Geometry`rather than the specific geometry types, but geotrellis does not offer the difference operation on generic geometries. You could use `as[Polygon]` to assert that the geometry is a polygon",5/25/16,1
dwins,Or you can just go with the JTS geometry: `poly.jtsGeom.difference(line.jtsGeom)`,5/25/16,1
pljplj,If I want to do it the proper scala way.   poly.as[Polygon] - line.as[Line] doesn't work.  What should try,5/25/16,1
dwins,`for (p <- poly.as[Polygon]; l <- line.as[Line]) yield p - l`,5/25/16,1
dwins,Gives an Option that is none if the types aren't what you expect ,5/25/16,1
pljplj,thanks -- it compiles now.  I must admit scala is pretty opaque.  That is bizarre for no purpose that I can see ,5/25/16,1
dwins,The idea is to model geometry operation types more precisely than just Geometry Õ‹ Geometry __ç Geometry ,5/25/16,1
dwins,"So the compiler will verify that your codes doesn't expect anything other than a point when you take the intersection of a point and a polygon, etc ",5/25/16,1
pljplj,Well that sounds like a noble goal.  This example doesn't show it AFAIK.  p - l  could be anything.  I would have to dig into the '-' overload to find out what it might return.  In this case it is something totally different from anything I've ever seen before.  Maybe as I use scala more I'll be convinced,5/25/16,1
dwins,"Well, it is an example of how GeoTrellis has solved this problem - rather than returning a Geometry, `Polygon.difference` returns a `PolygonAtMostOneDimensionDifferenceResult`and that result type reflects the range of types that might be returned by taking a difference with a polygon",5/25/16,1
dwins,i agree this API is somewhat cumbersome,5/25/16,1
pljplj,thanks for the help.  ,5/25/16,1
dwins,@pljplj however you don't need to look at the code to find the return type. you can either ask the REPL with `:type` or prompt a compiler error that indicates the type by ascribing `Nothing` as the type,5/25/16,1
pljplj,"More for me to learn.       val diffLeft  = for (p <- left.as[Polygon]; l <- rightLine.as[Line]) yield p & ldiffLeft:type  -- doesn't work.  How would I use :type",5/25/16,1
dwins,@pljplj in a scala console session you can write `:type (any expression)`  to find the type of the expression,5/25/16,1
dwins,so `:type 0` would print `Int`,5/25/16,1
pljplj,ok -- thank again,5/25/16,1
fosskers,"Thoughts on naming the current `geotrellis-admin` project. Is it more appropriate to be called `admin`, `viewer`, or something else entirely?",5/26/16,1
echeipesh,":+1: to expanding to `geotrellis-` but not sure if it_Ñés `admin` or `viewer`. I_Ñéd lean towards limiting it to viewer. Most of the admin line functions would require a cluster to perform on layer of any size, which would complicate the project significantly.",5/26/16,1
fosskers,`*-tms-viewer`?,5/26/16,1
lossyrob,"@pljplj I think our WKT reader should have a strongly typed option, so it could have just been something like```scalaval polygon = WKT.read[Polygon](""POLYGON ( (255 75 , 255 0, 120 0, 255 75) )_Ñù)val line = WKT.read[Line](""LINESTRING (255 1,255 254)_Ñù)// You can handle the difference in a couple of different ways.// If you want just a geometry back, you could do_Ñ_val resultOption: Option[Geometry] = (polygon - line).toGeometry// which gives None if it ends up with an empty geometry, or Some(geometry) if there was a result//If you want to handle the types of the results specifically, you can _Ñ_(polygon - line) match {  case LineResult(line) => // Handle where we got a line: Line result_Ñ_  case PolygonResult(polygon) => // Handle where we got a polygon: Polygon result _Ñ_  case _ => // Handle other cases }```we don_Ñét have that type of `read` function, just one that returns Geometry; I_Ñéll put in an issue to create those methods",5/26/16,1
pomadchin,@echeipesh @fosskers mb geotrellis-dashboard ?,5/27/16,1
pljplj,  I'm trying to reach the coordinates of the result of an intersection between a polyon and a line.  With the  ,5/27/16,1
pljplj,"    val diffLeft  = for (p <- left.as[Polygon]; l <- rightLine.as[Line]) yield p & l how would I get the coordinates of diffLeft ?",5/27/16,1
dwins,@pljplj in this example `diffLeft` has type `Option[geotrellis.vector.OneDimensionAtLeastOneDimensionIntersectionResult]`,5/27/16,1
dwins,"you could use pattern matching to test for specific geometry types, or just treat it as a generic Geometry using `toGeometry`. But toGeometry actually gives an Option (because in Geotrellis we want to assume that Geometries are never empty, so we use None when the intersection is empty)",5/27/16,1
dwins,anyway i think you'd want something along the lines of `diffLeft.flatMap(_.toGeometry).get.jtsGeom.getCoordinates`,5/27/16,1
dwins,note because I called `Option.get` in this snippet that an unhandled exception may be thrown.,5/27/16,1
pljplj,This is what I came up with after playing awhile.  I'm not checking for a empty intersect yet.  ,5/27/16,1
pljplj,"        diffV.asMultiLine.foreach(ln => {          println(""ln "" + ln)          ln.vertices.foreach(pt =>{            println(""pt "" + pt + "" vert "" + vert)          })        } )",5/27/16,1
pljplj,sorry left off 1st line,5/27/16,1
pljplj,"      for ( diffV <- diffLeft){       diffV.asMultiLine.foreach(ln => {          println(""ln "" + ln)          ln.vertices.foreach(pt =>{            println(""pt "" + pt + "" vert "" + vert)          })        } )        }",5/27/16,1
dwins,I see. yep if you just want to print it out you can use foreach this way,5/27/16,1
pljplj,well I don't just want to print it out.  I need to look at the points and save them.  This is the way I figure it out.  Programming scala sort of like java.  I printed out for debuggin so I could see what was going on,5/27/16,1
dwins,I see. Foreach is only useful when you only want to perform some side-effect like printing. if you want a value that can be used as a variable in other parts of the program you need other methods like map and flatMap,5/27/16,1
pljplj,So I can't put add to a collection when I find a point that I want in a foreach?,5/27/16,1
dwins,"adding to a mutable collection is considered a side-effect, it's reasonable to do this in a foreach. but the idiom in Scala is to prefer immutable collections",5/27/16,1
fosskers,"@pomadchin `dashboard` could work. Yesterday @moradology has some suggestions for extended behaviour that could make it feel more ""dashboardy""",5/27/16,1
pljplj,"Ok, I see the point.  I think for now I'll learn slowly.  Maybe in awhile i'll a be a true Scala believer.",5/27/16,1
dwins,"@pljplj if you squint a little you can see some Java best practices here ;)  Minimizing shared mutable state is common in multithreaded applications, pattern matching is closely related to the Visitor pattern",5/27/16,1
lossyrob,"@pljplj @dwins I was looking at polygon line differences yesterday, and I think there might be some bad results_Ñ_ this is from the console history```scalascala> val p =Polygon((0, 0), (1, 0), (1, 1), (0, 1), (0, 0))p: geotrellis.vector.Polygon = POLYGON ((0 0, 1 0, 1 1, 0 1, 0 0))scala> p - Line( (0, 0), (1, 0) )res2: geotrellis.vector.PolygonAtMostOneDimensionDifferenceResult = PolygonResult(POLYGON ((1 0, 0 0, 0 1, 1 1, 1 0)))``` ",5/27/16,1
lossyrob,"Shouldn_Ñét that subtract the polygon line?",5/27/16,1
lossyrob,Also looking at the code it looks like `PolygonAtMostOneDimensionDifferenceResult` expects a `Polygon` or nothing. This might be a bug.,5/27/16,1
dwins,hm should it? a line has zero area,5/27/16,1
lossyrob,"it seems like what @pljplj is doing with the difference makes me think s/he_Ñés expecting a multiline out of that operation",5/27/16,1
lossyrob,"@pljplj also I think you can write that bit of code you pasted as this, if you wanted to collect all the points:```scalaval points = for (diffV <- diffLeft;      ln <- diffV.asMultiLine;      pt <- ln.verticies) yield { pt }```",5/27/16,1
dwins,"subtracting one of the edges from a polygon in JTS without the GeoTrellis wrappers doesn't change the polygon (semantically, it does change the edge order interestingly enough)",5/27/16,1
dwins,however if you turn it into a linear geometry with `.getExteriorRing` then subtracting does modify that result,5/27/16,1
lossyrob,"yeah that_Ñés what I noticed, that_Ñés not geotrellis behavior, it_Ñés JTS behavior. So I guess it is correct_Ñ_though if I did want to get a MultiLine out of it, I_Ñém not sure how to do that",5/27/16,1
lossyrob,ah,5/27/16,1
lossyrob,nice,5/27/16,1
moradology,"'Most of the admin line functions would require a cluster to perform on layer of any size, which would complicate the project significantly' - i think there's not really a good reason that you couldn't launch the viewer portion as a cluster",5/27/16,1
moradology,it's currently just running in local mode,5/27/16,1
moradology,@echeipesh @fosskers ,5/27/16,1
moradology,thanks to @dwins work on getting the backbone in place,5/27/16,1
fosskers,you mean the server?,5/27/16,1
pljplj,@lossyrob  when I tried your suggestions the compiler complains ,5/27/16,1
pljplj,"Error:(32, 14) type mismatch; found   : Array[geotrellis.vector.Point] required: Option[?]          pt <- ln.vertices) yield { pt }             ^",5/27/16,1
pljplj,(aside -- I identify as a male bodied person and as a male bodied person),5/27/16,1
dwins,"```val points = for (diffV <- diffLeft.toList;      ln <- diffV.asMultiLine.toList;      pt <- ln.vertices) yield { pt }```",5/27/16,1
dwins,this should work better,5/27/16,1
pljplj,thanx -- that works,5/27/16,1
pljplj,I am expecting diffV to be either a line or a multiline.  When I gave it a line.asMultiLine it works so I'm happy,5/27/16,1
lossyrob,"Good. `asMultiLine` was developed to basically _ÑÒupcast_Ñù a Line to a MultiLine, or just pass through a MultiLine, so if you know it_Ñés one of the two you can just use a MultiLine to capture both cases",5/27/16,1
lossyrob,@pomadchin cassandra PR hasn_Ñét built successfully in travis for a bit_Ñ_ was the full pass a fluke?,5/31/16,1
pomadchin,"@lossyrob yep, it's a openjdk 7 bug, with problems getting hostname",5/31/16,1
pomadchin,there is a solution (these lines in travis ci  making magic with a host,5/31/16,1
pomadchin,**file,5/31/16,1
lossyrob,is the solution in the PR?,5/31/16,1
pomadchin,"but for some reasons it's always working, especially in pushs with changings in travis.yml",5/31/16,1
pomadchin,yep in pr,5/31/16,1
lossyrob,ok...,5/31/16,1
lossyrob,can_Ñét really merge the PR if it_Ñés going to cause a build errored all the time,5/31/16,1
pomadchin,https://github.com/geotrellis/geotrellis/pull/1452/files#diff-354f30a63fb0907d4ad57269548329e3R17,5/31/16,1
pomadchin,https://github.com/travis-ci/travis-ci/issues/5227,5/31/16,1
pomadchin,@lossyrob can you restart build?,5/31/16,1
pomadchin,or I need to push smth,5/31/16,1
lossyrob,restarted,5/31/16,1
pomadchin,+ an important thing that can probably be: https://github.com/travis-ci/travis-ci/issues/5669,5/31/16,1
pomadchin,So in theory there should not be such problems in master branch; i'll check it in my fork,5/31/16,1
pomadchin,"@lossyrob fixed, after passing ci tests ready to merge; completely fixed this openjdk issue",5/31/16,1
pomadchin,https://travis-ci.org/pomadchin/geotrellis/builds/134217479,5/31/16,1
pomadchin,woooh,5/31/16,1
pomadchin,praying there would be no _ÑÒjai_core_Ñù dep issues :D ,5/31/16,1
lossyrob,I_Ñém going to cancel the one commit build so that the latest one kicks off,5/31/16,1
uuuuq,"dear all, I'm new to geotrellis. I am trying to read geotiff files using is but it does not work with some certain files.",6/2/16,1
uuuuq,i use `val tile = MultibandGeotiff(Path)` read the files and i got errors. All the files are generated by GDAL with no nodata value and in EPSG:4326.,6/2/16,1
pomadchin,@uuuuq Hi! Can you provide more information? error and tiff,6/2/16,1
uuuuq,"@pomadchin hi, i know why this error happens. i test the code in my laptop and these files i want to read are too large(1.5GB) to read into the memory(16GB).",6/3/16,1
uuuuq,"These files can be read to memory as numpy double array,  does scala/geotrellis need more memory to store data? ",6/3/16,1
lossyrob,Sounds like you need to up your java memory. The JVM has a memory limit that has a default set,6/3/16,1
lossyrob,"so it won_Ñét try to use all your 16GB, unless you specify to give it that much (don_Ñét do that much_Ñ_just up it)",6/3/16,1
lossyrob,"can you give me the error message, I can tell more with a stack trace and exception",6/3/16,1
jamesmcclain,"@lossyrob In terms of creating a GeoWave subproject, what is the best way to satisfy the GeoWave dependency given the fact we are probably going to have to rely post-0.9.1 versions?",6/3/16,1
jamesmcclain,Do they have builds on bintray or similar?,6/3/16,1
lossyrob,good question,6/3/16,1
lossyrob,how are you doing it now?,6/3/16,1
jamesmcclain,Right now I am just building a jar and putting it in the `lib` directory of my test project,6/3/16,1
jamesmcclain,"But of course, we don't want to check a binary blob into the Geotrellis tree",6/3/16,1
lossyrob,There should be a way to pulbish to a local maven repo,6/3/16,1
lossyrob,and then add the local maven repo to the search for the build,6/3/16,1
dwins,"yeah in maven you can ""install"" to the local repo. or ""deploy"" if you just have a jar and not the source tree",6/3/16,1
dwins,and then i believe there's a builtin in SBT that you can add to your resolvers to pull from the local maven repo,6/3/16,1
dwins,`resolvers += Resolver.mavenLocal` shown on http://www.scala-sbt.org/0.13/docs/Library-Dependencies.html,6/3/16,1
mshareghi,"hi, is anyone familiar with setting up geodocker-cluster (https://github.com/geotrellis/geodocker-cluster). It appears to be out-of-date and doesn't install properly when I try to run docker-compose. It is looking for certain images that no longer exist (e.g.  daunnc/geodocker-hadoop-base )",6/3/16,1
pljplj,I have an Array of Coordinates.  How do I create a Polygon?,6/3/16,1
dwins,@pljplj Coordinate is a JTS class so you can consult the JTS API: http://tsusiatsoftware.net/jts/javadoc/com/vividsolutions/jts/geom/GeometryFactory.html,6/3/16,1
dwins,Here are the methods available for constructing a `geotrellis.vector.Polygon`: http://geotrellis.github.io/scaladocs/0.10/#geotrellis.vector.Polygon$ ,6/3/16,1
jamesmcclain,"@dwins It looks as though GeoWave is going to start publishing snapshots of master.  Thanks for the info, though, I will file that away.",6/3/16,1
dwins,@jamesmcclain definitely nicer if there are published snapshots managed by the GeoWave team,6/3/16,1
jamesmcclain,Definitely,6/3/16,1
pljplj,@dwins -- Thanx,6/3/16,1
fosskers,Can anyone explain the difference between Tile size and cell size?,6/3/16,1
dwins,"@fosskers well conceptually a raster dataset is just a grid of cells, tiles are an implementation detail",6/3/16,1
dwins,"typically you'd talk about cells having some real world size (""30 cm resolution"") but for tiles it's more about how many cells you are chunking into a tile (256x256 cell tiles)",6/3/16,1
pomadchin,@mshareghi what are you trying to do?,6/3/16,1
pomadchin,"@mshareghi to build you can use:```docker-compose build```it is not necessary in fact; everything is published to docker hub ",6/3/16,1
pomadchin,oh i see that bug; thx!,6/3/16,1
pomadchin,@mshareghi fixed! thx for that comment,6/3/16,1
jamesmcclain,"@fosskers Someone correct me if I am wrong, but the former is the extent of a tile and the latter the extent of a pixel",6/3/16,1
fosskers,"Ok, that's clearer now",6/3/16,1
fosskers,"at a high level, cell == pixel, and cell size is going to be some distance, or relative to a CRS?",6/3/16,1
fosskers,"Extents are always in the context of a CRS, yeah?",6/3/16,1
jamesmcclain,"Yes, I think that is true (for both of your questions)",6/3/16,1
lossyrob,"cell == pixel, pixels make up a tile, Tile size is the number of cols x rows in the grid space, cell size is the pixel size in the CRS (either implicitly the CRS we are working with [implicit in english not in scala], because we don_Ñét carry around CRS info but may have an extent, or explicitly in the case of something like `ProjectedRaster(Raster(tile, extent), crs)`)",6/3/16,1
lossyrob,"a `TileLayout` talks about the tile size, the grid space pixel cols and rows, but also Layout dimensions, which are the number of tiles heigh and wide. `TileLayout(layoutCols, layoutRows, tileCols, tileRows)`",6/3/16,1
mshareghi,@pomadchin thanks! ,6/3/16,1
fosskers,After much code diving: `--output s3 -O encoding=png` seems to be what would force a PNG output during ingest. Is that right?,6/3/16,1
echeipesh,@fosskers that sounds right,6/3/16,1
jportway,Hi - is there a simple example of reading in a vector shapefile and projecting it into a 2D map projection? I can't find such a thing anywhere,6/4/16,1
pomadchin,@lossyrob Yo! Don_Ñét forget (when you_Ñéll have time) to move gt integrations tests to gt repo :D ,6/6/16,1
lossyrob,@pomadchin the transfer link expired. can you do it again?,6/6/16,1
pomadchin,@lossyrob done,6/6/16,1
lossyrob,got it,6/6/16,1
lossyrob,tansferred to the geotrellis org,6/6/16,1
pomadchin,"cool, thanks!",6/6/16,1
akarmas,"Hi all,Here is a question! I want to read a multiband geotiff and create a SpaceTimeKey for it.I am creating the RDD with the following way:```val conf: Configuration = sc.hadoopConfiguration.withInputPath(inputPath)TemporalGeoTiffInputFormat.setTimeTag(conf, ""TIFFTAG_DATETIME"")TemporalGeoTiffInputFormat.setTimeFormat(conf, ""YYYY-MM-dd HH:mm:ss"")val source: RDD[(TemporalProjectedExtent, MultibandTile)] =    sc.newAPIHadoopRDD(conf, classOf[TemporalMultibandGeoTiffInputFormat],        classOf[TemporalProjectedExtent],        classOf[MultibandTile])```and everything works fine! But when i try to get the metadata for the multiband tile in order to reproject it and ultimately create pyramids and store it the following error appears:```val (_, rasterMetaData) =      TileLayerMetadata.fromRdd(source, FloatingLayoutScheme(512))ambiguous implicit values: both method withProjectedExtentTemporalTilerKeyMethods in package spark of type [K](self: K)(implicit evidence$11: geotrellis.util.Component[K,geotrellis.vector.ProjectedExtent], implicit evidence$12: geotrellis.util.Component[K,geotrellis.spark.TemporalKey])geotrellis.spark.withProjectedExtentTemporalTilerKeyMethods[K] and method withProjectedExtentTilerKeyMethods in package spark of type [K](self: K)(implicit evidence$13: geotrellis.util.Component[K,geotrellis.vector.ProjectedExtent])geotrellis.spark.withProjectedExtentTilerKeyMethods[K] match expected type geotrellis.spark.TemporalProjectedExtent => geotrellis.spark.tiling.TilerKeyMethods[geotrellis.spark.TemporalProjectedExtent,K2]             TileLayerMetadata.fromRdd(source, FloatingLayoutScheme(512))```Any ideas how to resolve this issue?Thank you very much!",6/6/16,1
dwins,@akarmas it will be a bit easier to read if you can surround code blocks with ` ``` ` before and after,6/6/16,1
lossyrob,@akarmas try explicitly stating the type params in the method call,6/6/16,1
dwins,"@akarmas you may need to provide a type hint so that the compiler can determine which kind of metadata to read. the error message is basically saying that for this type of source, we don't know whether to read temporal or non-temporal metadata",6/6/16,1
echeipesh,"@akarmas Usually when that type of thing happens scala type inference has gone wrong, the thing to do is to provide the type params",6/6/16,1
echeipesh,"```scalaval (_, rasterMetaData) =      TileLayerMetadata.fromRdd[TemporalProjectedExtent, MultibandTile, SpaceTimeKey](source, FloatingLayoutScheme(512))```",6/6/16,1
lossyrob,It needs the type params because it doesn_Ñét know what your target key type is. it can sometimes figure it out but it needs help with SpaceTimeKey,6/6/16,1
akarmas,I cannot thank you enough for the immediate response!,6/6/16,1
fosskers,"@echeipesh hmm disappointing... I was hoping that `--output hadoop -O path=file:///home/colin/tiles encoding=png` would pump out a handful of PNG files, but it didn't",6/6/16,1
fosskers,I got the same `part-XXX` folders with generic `data` files inside them,6/6/16,1
echeipesh,@fosskers I might have given you bad info: try the `_ÑÓoutput render`. https://github.com/geotrellis/geotrellis/blob/master/spark-etl/src/main/scala/geotrellis/spark/etl/hadoop/SpatialRenderOutput.scala#L21,6/6/16,1
echeipesh,"What it needs to do is select the correct output plugin and picking an output plugin is the only mechanism we have to switch the pipeline, which is render rather than save using GeoTrellis index.",6/6/16,1
fosskers,`--output render -O path=file:///home/colin/tiles encoding=png`?,6/6/16,1
fosskers,"Also, [new PR for removing the `cellType` dep on the `layoutExtent` option in ETL](https://github.com/geotrellis/geotrellis/pull/1510) <- new PR for removing the `cellType` dep on the `layoutExtent` option in ETL",6/6/16,1
echeipesh,@fosskers that looks right,6/6/16,1
fosskers,"If the `tiles` directory already exists, it exists. It seems to want the path to be the thing it's writing to directly.",6/6/16,1
fosskers,So in trying: `--output render -O path=file:///home/colin/tiles/tiles encoding=png`,6/6/16,1
fosskers,"I have spark spin for a while, and in the end it outputs a single 8kb file named ""tiles"" (no extension)",6/6/16,1
fosskers,"`feh` (image viewer) reads it fine, but it's entirely black (or just empty)",6/6/16,1
fosskers,"Odd, considering I ingested 790 input tiffs ;)",6/6/16,1
echeipesh,"oh right, the path needs to be a template with `{x}`, `{y}`, `{z}`so they can be substituted.",6/6/16,1
echeipesh,So basically it was writing all tiles to the same file,6/6/16,1
echeipesh,"One of them was black, thats probably not going to change. I_Ñém guessing the dataset is going to need color breaks to be rendered properly",6/6/16,1
echeipesh,https://github.com/geotrellis/geotrellis/blob/master/docs/spark-etl/spark-etl-intro.md#rendering-a-layer,6/6/16,1
echeipesh,although _ÑÒencodering_Ñù in that section is an ovbious type-o :facepalm:,6/6/16,1
lossyrob,"@shiraeeshi what were the motivations behind the 3 pending PR_Ñés for the `avro`, `python-avro-json-serializer`, and `spec` projects? Are these bug fixes that we couldn_Ñét do without, or enhancements? It_Ñés unfortunate that we would become dependant on those projects merging the PRs in order for the `geotrellis-python` projects to work, just because it_Ñés unclear when that would happen.",6/6/16,1
fosskers,High level explanation of vector tiles: https://www.youtube.com/watch?v=MEhskF2JnnY,6/6/16,1
pomadchin,@lossyrob how do you feel about this warn message (cassandra test)?:,6/6/16,1
pomadchin,,6/6/16,1
pomadchin,[![Screen Shot 2016-06-07 at 00.41.12.png](https://files.gitter.im/geotrellis/geotrellis/2xyu/thumb/Screen-Shot-2016-06-07-at-00.41.12.png)](https://files.gitter.im/geotrellis/geotrellis/2xyu/Screen-Shot-2016-06-07-at-00.41.12.png),6/6/16,1
akarmas,"Hi,I have created a layer (with HadoopLayerWriter) from a RDD[(SpaceTimeKey, MultibandTile)] with Metadata[TileLayerMetadata[SpaceTimeKey]] that contains 2 bands of a Landsat image for a specific AOI at a given date.Then i am reading another  2-band image of the same AOI at a different date as a RDD[(SpaceTimeKey, MultibandTile)] with Metadata[TileLayerMetadata[SpaceTimeKey]] and i want to perform an update operation to the previously created layer.I am using HadoopLayerUpdater  to achieve this ```updater.update(layerId,reprojected)```but the following error occurs```geotrellis.spark.io.package$LayerOutOfKeyBoundsError: Updating rdd is out of the key index space for Layer(name = ""landsat_temp"", zoom = 13): KeyBounds(SpaceTimeKey(2398,3083,1462104000000),SpaceTimeKey(2423,3103,1462104000000)). You must reindex this layer with large enough key bounds for this update.```Since the spatial extent of the two images is excactly the same i guess that the problem lies with the temporal bounds of the SpaceTimeKey.What is the proper way to reindex the layer to hold the new temporal bounds so as to perform the layer update?Thanks!!",6/7/16,1
fosskers,"Thanks @echeipesh , looking into that",6/7/16,1
lossyrob,@akarmas that is a little complicated. In a meeting now but I will document that process when in a bit,6/7/16,1
fosskers,"Within the `render` output module, there's no sanity check on the formatting of the template string. should there be?",6/7/16,1
fosskers,It currently won't barf at you if you don't properly specify `.../{name}/{z}-{x}-{y}.png`,6/7/16,1
lossyrob,"is there any reason where we would want to not have the full x,y,z template? name should be optional for sure",6/7/16,1
lossyrob,zoom should be too,6/7/16,1
fosskers,We could arbitrate that format if they havent provided a valid one,6/7/16,1
fosskers,"Update: it worked, although the values are all grayscale and semitransparent",6/7/16,1
fosskers,"before, the black background of my image viewer was overpowering it",6/7/16,1
lossyrob,what_Ñés the color ramp that it_Ñés using,6/7/16,1
lossyrob,it might just be writing the pngs as if they were already colored,6/7/16,1
fosskers,"For render, the options are: `path, encoding=(geotiff or png), breaks='{limit}:{RGBA};{limit}:{RGBA};...'`",6/7/16,1
fosskers,"I didn't set `breaks`, and it pumped them all out grayscale",6/7/16,1
lossyrob,"you_Ñéll have to dive into the code about how it actually colors the raster; I don_Ñét know how it does, and this is why I normally write custom code to color the raster explicitly",6/7/16,1
fosskers,"yeah, `RDD.renderPng(colorMap)` is way easier",6/7/16,1
fosskers,"but the breaks option exists, so I'll figure out how it's used",6/7/16,1
fosskers,It's not elaborated on in the current ETL docs,6/7/16,1
fosskers,"Here: `@param color map in ""BREAK:COLOR"" format, ex: ""23:cc00ccff;30:aa00aaff;120->ff0000ff""`",6/7/16,1
fosskers,Although that `->` in the last one is probably a typo,6/7/16,1
fosskers,"Is it trivial to reverse engineer the ""break string"" if you already had a ColorMap?",6/7/16,1
echeipesh,You can go from string to colormap easily via: https://github.com/geotrellis/geotrellis/blob/master/raster/src/main/scala/geotrellis/raster/render/ColorMap.scala#L125-L133,6/7/16,1
fosskers,"https://github.com/geotrellis/geotrellis/blob/master/spark-etl/src/main/scala/geotrellis/spark/etl/hadoop/SpatialRenderOutput.scala#L36It expects Integers here, although `ColorMap` can be instantiated with `Int` or `Double`",6/7/16,1
fosskers,Yeah @echeipesh that code is copied almost verbatim in what I just linked to,6/7/16,1
echeipesh,"I wonder if using Double color map always, even for int tiles, would produce sensible results.",6/7/16,1
fosskers,"My tests are currently throwing, because the `SpatialRenderOutput` expects Int",6/7/16,1
fosskers,and I found a break string with Doubles in the landsat tutorial,6/7/16,1
fosskers,"Interestingly it throws with `throw new BadFormatException(s""Unable to parse classifications, expected '{limit}:{RGBA};{limit}:{RGBA};...' got: '$blob'"")`",6/7/16,1
fosskers,"Alluding to a formatting error in your break string, which in this case it isn't, it's a number casting exception",6/7/16,1
lossyrob,got `$blob` lol,6/7/16,1
lossyrob,hate seeing those,6/7/16,1
fosskers,so useful lol,6/7/16,1
lossyrob,what_Ñés that facepalm emoji,6/7/16,1
fosskers,:facepalm: ,6/7/16,1
fosskers,"It's Picard, in Slack",6/7/16,1
lossyrob,yeah it_Ñés nothing in gitter,6/7/16,1
lossyrob,but that_Ñés the expression I always make whenever I see one of those because I forgot to put an `s`,6/7/16,1
echeipesh,"Hey, it_Ñés important to call a rose a rose and a blob a blob. Otherwise you go off believing that strings provided by user always contain useful information.",6/7/16,1
fosskers,"Proposed patch: catch on the number formatting exception as well, and/or move away from requiring `Int` here",6/7/16,1
echeipesh," Yes, and to be honest there should be a way to get that string from a ColorMap. I don_Ñét htink overwriting toString is justified, but something.",6/7/16,1
fosskers,I can do both,6/7/16,1
fosskers,"The `ColorMap => String` is a convenience. Say in `gt-admin` I have an established `ColorMap` that I like for a layer, and want to know its break string",6/7/16,1
fosskers,Then I can borrow that and fire it into an ingestion script,6/7/16,1
fosskers,"In general, when one rendersto PNG some dataset they have, is the break string they'd need something they'd trivially know? Is me thinking ""no"" my inexperience?",6/7/16,1
lossyrob,nope. It_Ñés something that is usually calculated,6/7/16,1
fosskers,"thought so. I guess yeah, you'd get your RDD and call `.colorBreaks`",6/7/16,1
fosskers,"take that, pass it one of our ramp strings (""blue-to-orange"") to generate a `ColorMap`, and there you go",6/7/16,1
fosskers,"From within Scala it's just a matter of calling `renderPng` after that, but we're outside Scala here",6/7/16,1
fosskers,hence as @echeipesh said it would be good to have `ColorMap => String`,6/7/16,1
fosskers,"although this does suffer from chicken-and-egg a bit. You need to know your break string before you can process/render, but you can't get the break string until you process ;)",6/7/16,1
lossyrob,that_Ñés why PNG generation jobs written in code usually just do the histogram calculation on the tiles and then renders them through the zoom levels using that histogram,6/7/16,1
lossyrob,really instead of breaks we should be passing in numBreaks and a color ramp,6/7/16,1
lossyrob,"so that you calculate the histogram in ETL if it_Ñés rendering png or jpg and colors it by computing the histogram, getting the breaks, and then rendering it with the passed in color ramp",6/7/16,1
fosskers,Want me to change it?,6/7/16,1
fosskers,"a la:> really instead of breaks we should be passing in numBreaks and a color ramp",6/7/16,1
fosskers,"looking at the code here, that's very possible",6/7/16,1
echeipesh,"@lossyrob @fosskers The reason that is breaks is because the very first case was to render NCLD, a categorical dataset. That is a pretty legitimate usecase. so we_Ñéd have to support both, not either.",6/7/16,1
fosskers,good point,6/7/16,1
echeipesh,"@akarmas Partial answer is that when a layer is saved it creates an index for transforming a key to some long via a space filing curve. The use of SFC is the core of our query mechanism. The restriction with SFCs is that they must index a bounded space. What happens is that when you write a layer the default behavior is to take the bounds of the data that is being written as the whole space. If an udpate happens outside of those bounds it can_Ñét be addressed, thus your error. The way to deal with this is to specify a wider bound when you initially write, forcing you to consider what kind of key ranges may be addressable.",6/7/16,1
echeipesh,"@akarmas Coding from the hip it would look something like this:```scala// maximum spatial extent we plan to addressval maxExtent: Extent = ???val minDate: DateTime = ???val maxDate: DateTime = ???// Use our layout to transform an extent to tile col/rows boundsval GridBounds(colMin, rowMin, colMax, rowMax) = rdd.metadata.layout.mapTransform(maxExtent)// Build the space we need to be able to addressval updateBounds: KeyBounds[SpaceTimeKey] =  KeyBounds(SpaceTimeKey(colMin, rowMin, minDate, SpaceTimeKey(colMax, rowMax, maxDate))// Choose a method to decompose addressable space into series of sub-spaces indexed by an SFCval keyIndex: KeyIndex[SpaceTimeKey] = ZSpaceTimeKeyIndex.byMonth(updateBounds) //..byMonthish, does not break on month endlayerWriter.write(layerId, rdd, keyIndex)```",6/7/16,1
echeipesh,"You don_Ñét have to worry overly about collisions in the index. Any `(K,V)` that share the same index will be written together and will be read together and post-filetered for the query. So the shouldn_Ñét be TOO many for obvious reasons, but some are acceptable.",6/7/16,1
lossyrob,^- this has to be on the initial write of the layer,6/7/16,1
lossyrob,"if you have a previously ingested layer that you want to change the key index bounds for, you have to use the LayerCopier ",6/7/16,1
lossyrob,"scratch that, I don_Ñét think LayerCopier does that",6/7/16,1
lossyrob,It would be the LayerReindexer,6/7/16,1
lossyrob,It_Ñés not great that context bounds are not documented nicely in ScalaDocs,6/7/16,1
lossyrob,e.g. https://geotrellis.github.io/scaladocs/latest/#geotrellis.spark.reproject.TileRDDReproject$,6/7/16,1
echeipesh,:wolf: ,6/7/16,1
lossyrob,https://github.com/geotrellis/geotrellis/blob/master/spark/src/main/scala/geotrellis/spark/reproject/TileRDDReproject.scala#L38,6/7/16,1
lossyrob,"I wonder if that would look better like```scaladef apply[    K   : SpatialComponent          : Boundable          : ClassTag,    V <: CellGrid          : ClassTag          : Stitcher          : (? => TileReprojectMethods[V])          : (? => CropMethods[V])          : (? => TileMergeMethods[V])          : (? => TilePrototypeMethods[V])  ]```",6/7/16,1
echeipesh,Wonder if there is a way to extend the doc generation. Really it should only need to keep the signature that is there.,6/7/16,1
lossyrob,That_Ñés not lining up like it does in the markdown text. but generally,6/7/16,1
lossyrob,Probably would take hacking on the scaladoc engine,6/7/16,1
echeipesh,"Sure, that_Ñés more readable.",6/7/16,1
lossyrob,"@akarmas I have a PR with some docs additions, please let me know if that answers your question! https://github.com/geotrellis/geotrellis/pull/1513",6/7/16,1
rajadain,"(Sorry if this is a dumb question, but) how can I convert a MultiPolygon to a GeoJSON string? I know I can do `String.parseGeoJson[MultiPolygon]` to go from GeoJSON to MultiPolygon, but I don't know how to go the other way",6/7/16,1
echeipesh,Should be able to do `mp.toGeoJson()`,6/7/16,1
rajadain,"D'oh, thanks",6/7/16,1
rajadain,I was looking at https://github.com/geotrellis/geotrellis/blob/master/vector/src/main/scala/geotrellis/vector/io/json/GeometryFormats.scala,6/7/16,1
echeipesh,https://github.com/geotrellis/geotrellis/blob/master/vector/src/main/scala/geotrellis/vector/io/json/Implicits.scala#L32-L36,6/7/16,1
rajadain,"Ah got it, thanks!",6/7/16,1
echeipesh,"Yeah, I wonder we should develop some kind of marker for intermediate API.",6/7/16,1
echeipesh,"Like API that is there, but is meant to be more of a building block, with something prettier availble.",6/7/16,1
rajadain,"So, I see that in GeometryFormats there's a lot of RootJsonFormats for various Vector items",6/7/16,1
rajadain,"I'm trying to support an endpoint that takes `{ rasters: Seq[String], multiPolygon: MultiPolygon }`",6/7/16,1
rajadain,"Currently I'm doing the conversion by hand, which is to say `case Seq(JsString(multiPolygon)) => new MyArgs(multiPolygon.parseGeoJson[MultiPolygon])`, but I wonder if there's a way to incorporate [`MultiPolygonFormat`](https://github.com/geotrellis/geotrellis/blob/master/vector/src/main/scala/geotrellis/vector/io/json/GeometryFormats.scala#L150-L163)",6/7/16,1
rajadain,"So that I don't have to represent the GeoJSON as a string, but can work with an object in the API. I think that would be somewhat faster than using `parseGeoJson`. But no biggie",6/7/16,1
echeipesh,"You should be able to just declare it as a case class and make a format for it, the definition of the format will pull in the formats for the multipolygon and the sequence of strings automatically:```scalacase class RasterRequest(rasters: Vector[String], multiPolygon: MultiPolygon)// have this instance be available in the service. Maybe define in companion and import it in or define in the service itselfimplicit val rasterRequestFormat = jsonFormat2(RasterRequest.apply)```",6/7/16,1
echeipesh,At that point you can just start using `entity` with that type for spray to parse it for you: http://spray.io/documentation/1.2.2/spray-routing/marshalling-directives/entity/,6/7/16,1
fosskers,Good to go: https://github.com/geotrellis/geotrellis/pull/1512,6/7/16,1
fosskers,"That PR opens up the discussion for reworking how ETL input works, but I'm not going to tackle that just yet",6/7/16,1
fosskers,"I've done enough to enable me to generate a decent break string for my test data, enough to confirm the behaviour of the `--output render ...` stuff (re: ADH docs)",6/7/16,1
rajadain,@echeipesh sweet!,6/7/16,1
fosskers,"Anyone ever seen this before?*Exception in thread ""main"" org.rogach.scallop.exceptions.IncompleteBuildException: It seems you tried to get option value before you constructed all options (maybe you forgot to call .verify method?). Please, move all extraction of values to after 'verify' method in ScallopConf.*",6/7/16,1
akarmas,"@lossyrob  @echeipesh   Thanks a lot! They answer my question for sure! The use of LayerReindexer is a bit unclear to me but i ll work on your comments and come back later if i am stuck!Best regards!",6/8/16,1
wsf1990,"I use tile.renderPng().bytes to render tile at the webbrowser. When I request the tile, it jsut show  a digit array not a png image.why?can somebody help me ?",6/8/16,1
pomadchin,@wsf1990 can you provide more context? where do you use it? do you use spray? ,6/8/16,1
wsf1990,"respondWithMediaType(MediaTypes.`image/png`) {        val result =          timedCreate(            ""tms"",            ""ChattaServiceActor(211)::result start"",            ""ChattaServiceActor(211)::result end"") {            val bytes = tile.renderPng().bytes            println(""show bytes:"" + bytes.length)            //bytes.foreach(println)            bytes          }        printBuffer(""tms"")        complete(result)      }",6/8/16,1
pomadchin,"so you use chatta demo, or you modified smth?",6/8/16,1
wsf1990,"nothing,just chatta demo",6/8/16,1
pomadchin,sounds weird like there are some additional json imports in that call scope,6/8/16,1
wsf1990,"i think i just get the tile data not png the response content-type is image/png; charset=UTF-8",6/8/16,1
pomadchin,"hm i can double check, but i have no problems with it; how do you check results?",6/8/16,1
pomadchin,"can you provide url to check, to repeat your case?",6/8/16,1
wsf1990,"sorry,it's in inner net,dose the content-type made this error?",6/8/16,1
pomadchin,"not a problem, i can rise a local / somewhere chatta demo ",6/8/16,1
pomadchin,"just give me any url to repeat your error, to see what you see",6/8/16,1
pomadchin,and steps you done to repeat it,6/8/16,1
wsf1990,cant request from out net,6/8/16,1
pomadchin,._.,6/8/16,1
pomadchin,url without hostname,6/8/16,1
pomadchin,smth like `/tms/z/x/y/?...`,6/8/16,1
pomadchin,and screenshot and / or copypaste output of your result,6/8/16,1
pomadchin,"and steps you done to start chatta demo, have you ingested all data, or only some part?",6/8/16,1
wsf1990,"http://192.168.1.201:8777/gt/tms/10/721/400?breaks=-4,0,1,2,3,4,5,6,7,13&colorRamp=yellow-to-red-heatmap&mask=",6/8/16,1
pomadchin,emm,6/8/16,1
pomadchin,using your url i can get only `Request is missing required query parameter 'layers_Ñé` msg,6/8/16,1
pomadchin,"however, even trying: `http://<hostname>:8777/gt/tms/10/721/400?layers=ImperviousSurfacesBarrenLandsOpenWater,DevelopedLand,Wetlands,ForestedLands,NonWorkingProtectedOrPublicLands,PublicallyOwnedWorkingLands,PrivatelyOwnedWorkingLandsWithEasements,FarmlandWithoutPrimeAgriculturalSoils,FarmlandOrForestedLandsWithPrimeAgriculturalSoils&weights=-5,-4,-2,1,-1,2,4,3,5&breaks=-4,0,1,2,3,4,5,6,7,13&colorRamp=yellow-to-red-heatmap&mask=` would give you not found exception",6/8/16,1
pomadchin,"you can try that link: `http://<hostname>:8777/gt/tms/9/134/203?layers=ImperviousSurfacesBarrenLandsOpenWater,DevelopedLand,Wetlands,ForestedLands,NonWorkingProtectedOrPublicLands,PublicallyOwnedWorkingLands,PrivatelyOwnedWorkingLandsWithEasements,FarmlandWithoutPrimeAgriculturalSoils,FarmlandOrForestedLandsWithPrimeAgriculturalSoils&weights=-5,-4,-2,1,-1,2,4,3,5&breaks=-4,0,1,2,3,4,5,6,7,13&colorRamp=yellow-to-red-heatmap&mask=`",6/8/16,1
pomadchin,why you can try to use `http://<hostname>:8777/`?,6/8/16,1
pomadchin,have you ingested demo tiles?,6/8/16,1
wsf1990,"yes,I removed some parameters",6/8/16,1
pomadchin,eh ._. you said that you havent modified your code,6/8/16,1
wsf1990,i means the code of generate tile ,6/8/16,1
pomadchin,"so probably you have serialisation problems, be carefull with spray json import",6/8/16,1
wsf1990,i use temp.renderPng().write(path) can get the png file correct,6/8/16,1
pomadchin,"if you have smth like global `import spray.json.DefaultJsonProtocol._` make it local,that causes incorrect tile serialisation",6/8/16,1
pomadchin,it tries to convert Array[Byte] to Json,6/8/16,1
wsf1990,"ou,thank you very much,yes i make spray.json.DefaultJsonProtocol._  global",6/8/16,1
wsf1990,very thanks @pomadchin,6/8/16,1
pomadchin,@wsf1990 we are always glad to see any questions here :) ,6/8/16,1
wsf1990," geotrellis is an excellent frame of gos,thanks",6/8/16,1
akarmas,"Now i have created a layer with keyIndex as previously discussed. The layer initially contains a 2-band image. I want to update the layer with another 2-band image at a different date. So the resulting layer will have 4 bands. Correct? And each time a new image arrives 2 new bands will be stacked in the layer.Which should be the mergeFunc in the update function of the LayerUpdater object in order for this update to work as expected? ",6/8/16,1
echeipesh,"@akarmas Not quite. The udpate is either going to update/overwrite a record/tile with a key that exists, or if it there is no such key it will create a new record. Suppose it_Ñés same location but new date, that means it_Ñés a new key, so you_Ñére going to have two tiles with two bands each. ",6/8/16,1
echeipesh,"You can do the udpate without the merge func at first, it only matters when you_Ñére updating keys that already exist, which it doesn_Ñét sound like what you_Ñére doing. The default behavior is to replace old pixels with new pixels anyway so it_Ñés probably what you want.",6/8/16,1
fosskers,"@echeipesh re: `run-local` dying. I've checked the ""Blame"" view on github for both `EtlConf` and `SpatialRenderOutput`. The only thing that has changed in months in `EtlConf` is my latest removal of the dep on `cellType` for the `layoutExtent` flag",6/8/16,1
fosskers,And nothing has changed in SRO except a local patch that I've made to make the module accept doubles and ints,6/8/16,1
fosskers,as opposed to just ints,6/8/16,1
fosskers,"now `run-local` throws the exception I posted above, even when I don't try and specify a breaks string",6/8/16,1
fosskers,"`--output render -O path=""file:///home/colin/tiles/{name}/{z}-{x}-{y}.png""  encoding=png` <- this dies",6/8/16,1
fosskers,and that's what was successfully rendering my grayscale tiles previously,6/8/16,1
echeipesh,"Master dies, right ?",6/8/16,1
echeipesh,"Hmm, hard to imagine how https://github.com/geotrellis/geotrellis/pull/1510 would have caused that change.",6/8/16,1
echeipesh,@fosskers does that scallop error have a stack trace? It should at least point to where the early init is happening.,6/8/16,1
fosskers,"It's basically master, with #1512 and one more tiny patch on it",6/8/16,1
fosskers,"The trace, in all it's glory:```Exception in thread ""main"" org.rogach.scallop.exceptions.IncompleteBuildException: It seems you tried to get option value before you constructed all options (maybe you forgot to call .verify method?). Please, move all extraction of values to after 'verify' method in ScallopConf.	at org.rogach.scallop.ScallopConf.assertVerified(ScallopConf.scala:409)	at org.rogach.scallop.ScallopConf$$anon$1$$anonfun$fn$1.apply(ScallopConf.scala:136)	at org.rogach.scallop.ScallopConf$$anon$1$$anonfun$fn$1.apply(ScallopConf.scala:135)	at org.rogach.scallop.ScallopOption.value$lzycompute(ScallopOption.scala:20)	at org.rogach.scallop.ScallopOption.value(ScallopOption.scala:20)	at org.rogach.scallop.ScallopOption.get(ScallopOption.scala:23)	at org.rogach.scallop.ScallopOption.apply(ScallopOption.scala:26)	at geotrellis.spark.etl.Etl$$anonfun$2.apply(Etl.scala:81)	at geotrellis.spark.etl.Etl$$anonfun$2.apply(Etl.scala:81)	at scala.collection.LinearSeqOptimized$class.find(LinearSeqOptimized.scala:113)	at scala.collection.immutable.List.find(List.scala:84)	at geotrellis.spark.etl.Etl.load(Etl.scala:81)	at com.azavea.datahub.etl.IngestApp$class.main(IngestApp.scala:23)	at com.azavea.datahub.etl.Ingest$.main(Ingest.scala:6)	at com.azavea.datahub.etl.Ingest.main(Ingest.scala)	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)	at java.lang.reflect.Method.invoke(Method.java:498)	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:731)	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181)	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206)	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121)	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)```",6/8/16,1
echeipesh,:sounds of head scratching:,6/8/16,1
echeipesh,@fosskers Could you double check for me that your datahub repo doesn_Ñét have some notable changes to `Ingest` or `IngestApp` ?,6/8/16,1
fosskers,"My `docs-overhaul` branch is current the furthest in term of commits, and is based on master",6/8/16,1
fosskers,Neither `Ingest` nor `IngestApp` are touched,6/8/16,1
akarmas,"I have already tried the update without the mergeFunc.  I performed the update but the resulting layer seems to be the original one i.e. the image at the first date even though the second image has different SpaceTimeKey as i observe from the RDD's metadata. I want to have both images in the layer so as to find for example the max for a pixel over time.""Suppose it_Ñés same location but new date, that means it_Ñés a new key, so you_Ñére going to have two tiles with two bands each.""And if i want to take the for example the tiles of the second date i  ll do it with FilteringLayerReader? ",6/8/16,1
echeipesh,"@akarmas ""And if i want to take the for example the tiles of the second date i ll do it with FilteringLayerReader?_Ñù correct. If you did an update you should with different time you should have more records in the layer. The original would remain unchanged but in addition you would see new records for the new date. Is that what is happening?",6/8/16,1
pomadchin,layer key space allows new time? ,6/8/16,1
akarmas," @echeipesh  After testing i think that this is NOT happening as i use filtering layer readers with date as a filter to isolate the two images. The initial image returns fine but the second one seems to be empty. is there a function to count the records within a layer just to be sure? @pomadchin I 'm creating the layer key space according to documentation that Rob created earlier : ",6/8/16,1
akarmas,"```// Create the key index with our date rangeval minDate: DateTime = new DateTime(2013, 1, 1, 0, 0)val maxDate: DateTime = new DateTime(2017, 1, 1, 0, 0)val indexKeyBounds: KeyBounds[SpaceTimeKey] = {    val KeyBounds(minKey, maxKey) = reprojected.metadata.bounds.get // assuming non-empty layer        KeyBounds(            minKey.setComponent[TemporalKey](minDate),            maxKey.setComponent[TemporalKey](maxDate)        )    }val keyIndex = ZCurveKeyIndexMethod.byDay.createIndex(indexKeyBounds)val attributeStore = new HadoopAttributeStore(outputPath, sp.hadoopConfiguration)val writer = new HadoopLayerWriter(outputPath, attributeStore)Pyramid.upLevels(reprojected, layoutScheme, zoom) { (rdd, z) =>    val layerId = LayerId(layerName, z)    // If the layer exists already, delete it out before writing    if(attributeStore.layerExists(layerId)) {        new HadoopLayerManager(attributeStore).delete(layerId)    }    writer.write(layerId, rdd, keyIndex)}```",6/8/16,1
echeipesh,"@akarmas Yes, you should use: ```scala// count the number of tiles in the layerlayerReader.read(LayerId(_ÑÒname_Ñù, zoom)).count// collect the keys and print themlayerReader.read(LayerId(_ÑÒname_Ñù, zoom)).map(_._1).collect.foreach(println)```",6/8/16,1
echeipesh,@akarmas what does your update code look like ?,6/8/16,1
akarmas,"Initially i create a  reprojected:RDD[(SpaceTimeKey, MultibandTile)] with Metadata[TileLayerMetadata[SpaceTimeKey]] for the new image and then i just do the following for the update:```val attributeStore = new HadoopAttributeStore(outputPath, sp.hadoopConfiguration)val reader = new HadoopLayerReader(attributeStore)val writer = new HadoopLayerWriter(outputPath, attributeStore)val conf: Configuration = sp.hadoopConfigurationval deleter = new HadoopLayerDeleter(attributeStore, conf)val copier = new HadoopLayerCopier(outputPath, attributeStore)val updater = new HadoopLayerUpdater(attributeStore, reader, writer, deleter, copier)val layerId = LayerId(layerName, 13)updater.update(layerId, reprojected)```",6/8/16,1
akarmas,i also counted the layers and there are the same number of tiles before and after the update.,6/8/16,1
echeipesh,Is this project on github perchance?,6/8/16,1
pomadchin,\+,6/8/16,1
pomadchin,"I am curious, spatial key index space allows to update?._. ",6/8/16,1
pomadchin,"Oh, there would be thrown an exception",6/8/16,1
echeipesh,"@pomadchin yes, it was created with wide margin, so updating should be no problem.",6/8/16,1
akarmas,"@echeipesh  No. I ll create a project and upload it later! @pomadchin  Also the images have exactly the same extent  and no exception is thrown!",6/8/16,1
pomadchin,@echeipesh @akarmas so code sample with data to ingest could help us to repeat that behaviour,6/8/16,1
pomadchin,sounds really weird ):,6/8/16,1
echeipesh,"sounds like the data is just landsat 8, so we_Ñéll just have to have a look-see and maybe even run it : )",6/8/16,1
akarmas,"Guys i set up this gist so you can see the code. In the comments i have a link to the two images.The code is the code i use in the spark shell to perform all the steps we are discussing.Thanks a lot!https://gist.github.com/akarmas/131bab6b1b1c61ed59425f386a3be4ae",6/8/16,1
echeipesh,@akarmas could you make sure that the update `zoom` is actually `13`? You could be updating wrong layer: https://gist.github.com/akarmas/131bab6b1b1c61ed59425f386a3be4ae#file-sample-scala-L147,6/8/16,1
echeipesh,"going to run through this myself, thank you for the paste",6/8/16,1
pljplj,"I'm running into trouble with JTS again.  I want to union 2 polygons (or maybe multipolygon, I don't know).  But I was have problems with JTS percision so I do the following .  I end up with a JTS polygon, but I want a GeoTrellis polygon.  What should I be doing?",6/8/16,1
pljplj,"    val prec = new PrecisionModel(PrecisionModel.FLOATING_SINGLE)    val polyNow = GeometryPrecisionReducer.reduce(poly.jtsGeom,prec)    val transposePolyNow = GeometryPrecisionReducer.reduce(transposePoly.jtsGeom,prec)    val union = polyNow.union(transposePolyNow)",6/8/16,1
echeipesh,_ª__,6/8/16,1
echeipesh,"@pljplj You can always rewrap jtsGeom in a polygon again, doing something like this:```scalaval prec = new PrecisionModel(PrecisionModel.FLOATING_SINGLE)def reducePolyPrecision(poly: Polygon, pm: PrecisionModel): Polygon = {  Polygon(GeometryPrecisionReducer.reduce(poly.jtsGeom, prec))}reducePolyPrecision(poly, prec) union reducePolyPrecision(transposePoly, prec) match {  case PolygonResult(p) => ???  case MultiPolygonResult(mp) => ???}```It seems somewhat of an oversight that we don_Ñét have a way to reduce precision through GeoTrellis geometries since it seems inevitable that it would be required at some point.",6/8/16,1
echeipesh,"@akarmas Turns out it_Ñés a bone-headed mistake in the `HadoopLayerUpadater`: https://github.com/geotrellis/geotrellis/blob/master/spark/src/main/scala/geotrellis/spark/io/hadoop/HadoopLayerUpdater.scala#L51-L61The leftOuterJoin will not let any new rows be entered, of course. Thank you a lot for flushing this out.",6/8/16,1
pomadchin,fix pr is in process,6/8/16,1
akarmas,"Ok i get it now. From what i saw the same mistake spans all updaters like AccumuloLayerUpdater and FileLayerUpdater.Is there any prediction of when this issue will be fixed ? :) Thanks!",6/9/16,1
pomadchin,@akarmas think today (: ,6/9/16,1
pomadchin,but that would require you to use master branch ):,6/9/16,1
pomadchin,until `.1` version would be published,6/9/16,1
schBen,HiI have raster data (GeoTiff) and want to get an Isodata clustering. In Spark's MLlib I found a KMeans.scala that looks helpful. It expects an RDD[org.apache.spark.mllib.linalg.Vector]. Can I use GeoTrellis to bring the GeoTiff into this format?,6/9/16,1
lossyrob,"hey @schBen, yeah you can get the raster into an RDD using GeoTrellis",6/9/16,1
lossyrob,converting it to a set of linalg.Vector after that is possible,6/9/16,1
lossyrob,are you trying to pull point data out of the geotiffs and then run the kmeans?,6/9/16,1
lossyrob,"I_Ñém not very familiar with isodata clustering, looks like a pixel based algorithm. it might actually be more convenient/faster to write the algorithm against the raster tiles instead of trying to convert to vectors",6/9/16,1
lossyrob,"the RDD[org.apache.spark.mllib.linalg.Vector] element type (Vector) represents a N-D point, so you could represent pixels as `[col, row, value]` vectors, or even `[lat, lng, value]` vectors if there_Ñés any benefit in being in the map space instead of grid space, and run the kmeans over those elements",6/9/16,1
lossyrob,"probably would be better to be in map space, the cluster algo outputs could then be interpolated back onto grid space",6/9/16,1
lossyrob,more precision,6/9/16,1
lossyrob,"but yeah if kmeans is just a starting point, it might be interesting to imagine the isodata clustering algorithm on an `RDD[(SpatialKey, Tile)]` or `RDD[(SpatialKey, MultibandTile)]` instead of doing the conversion. But if MLLib has what you need, you_Ñéd be able to convert. I can point you to the relavent code for reading geotiffs into RDDs off of the local file system, HDFS or S3, and the convert logic. Is it singleband or multiband raster data?",6/9/16,1
schBen,"@lossyrob It's multiband raster data. I start with multiband GeoTiffs for now. I think in the k means vectors, the components are spectral, so that one vector represents the different bands from the multiband raster. So my question is how to map the bands of the GeoTiff to the vector components. I'm still not sure how the spatial information is preserved. Of course it would be better to have an isodata algorithm for RDD[(SpatialKey, Tile)] already :-)",6/9/16,1
owcm,"General question: If  I populate a RDD of elevation Geotiffs, where some of the geotiffs have overlapping extents, and I run a slope operation on the RDD collection how are overlapping cell elevation values interpreted?   Are the cell values averaged,  resulting slope cell value set to no Data? ",6/9/16,1
lossyrob,"@owcm is this a tiled RDD or raw geotiff RDD[(ProjectedExtent, Tile)]?",6/9/16,1
lossyrob,"If you tile it, and it_Ñés a SpatialKey RDD, those pixel will be merged in a first-come-first-serve basis",6/9/16,1
pomadchin,@lossyrob @echeipesh @akarmas https://github.com/geotrellis/geotrellis/pull/1529,6/9/16,1
lossyrob,@pomadchin danke,6/9/16,1
pomadchin,@lossyrob not a WIP O: (locally works),6/9/16,1
pomadchin,appreciate your comments,6/9/16,1
owcm,"Well everything I have been doing up to this point is a tiled RDD, taking advantage of the Spatialkey.",6/9/16,1
lossyrob,"gotcha. yeah when the geotiff RDD is tiled up, the overlapping areas are merged",6/9/16,1
lossyrob,"@pomadchin nice, I just made a stylistic comment, not sure that_Ñés better, but otherwise look like good changes to me",6/9/16,1
lossyrob,so the accumulo thing got worked out?,6/9/16,1
pomadchin,@lossyrob I like that comment; yep everything works perfect ._. cleared accumulo subproject ,6/9/16,1
lossyrob,gotcha,6/9/16,1
owcm,@lossyrob thanks,6/9/16,1
lossyrob,"@owcm would it be desirable to have a custom merging function in there? I_Ñéve thought about that before, it_Ñés not implemented though",6/9/16,1
lossyrob,"@schBen there_Ñés a couple ways to go about this. One path forward is, you would first need to get to an `RDD[(ProjectedExtent, MultibandTile)]` using some geotrellis methods for reading in the geotiff tile. You can do that by reading in the geotiff just like how this is doing: https://github.com/geotrellis/geotrellis-landsat-tutorial/blob/master/src/main/scala/tutorial/IngestImage.scala#L60",6/9/16,1
lossyrob,At that point you_Ñéd just be making dense vectors from the pixel values,6/9/16,1
lossyrob,I_Ñéll cowboy some code that maybe won_Ñét compile but will be the general idea and paste it in in a min,6/9/16,1
lossyrob,"warning - this is not pretty scala code (mutability) but that_Ñés for reasons of wanting it to be fast, iterating over rasters using nice functional scala collections stuff is super slow",6/9/16,1
lossyrob,"```scalaval geotiffPath: String = ???val inputRdd: RDD[(ProjectedExtent, MultibandTile)] =      sc.hadoopMultibandGeoTiffRDD(geotiffPath)val vectors: RDD[org.apache.spark.mllib.linalg.Vector] =  inputRdd    .flatMap { case (_, tile) =>      // Mutability is faster      val vectors = Array.ofDim[org.apache.spark.mllib.linalg.Vector](tile.size)      // There's not actually a nice method for this, I wrote an issue to add this      // to the API.      //      // var i = 0      // tile.foreachDouble { bandValues =>      //   vectors(i) = org.apache.spark.mllib.linalg.Vector.dense(bandValues)      //   i += 1      // }      // So instead we'll do this.      val bands = tile.bands      var i = 0      import spire.syntax.cfor._   // We are performance nuts, so use a macro for to iterate over cols and rows as fast as possible      cfor(0)(_ < tile.cols, _ + 1) { col =>        cfor(0)(_ < tile.rows, _ + 1) { row =>          val bandValues = Array.ofDim[Double](tile.bandCount)          cfor(0)(_ < tile.bandCount, _ + 1) { band =>            bandValues(band) = bands(band).getDouble(col, row)          }          vectors(i) = org.apache.spark.mllib.linalg.Vector.dense(bandValues)          i += 1        }      }      vectors    }```",6/9/16,1
owcm,"@lossyrob right now the current approach would work from us.   We are ingesting data that has been collected at different times, same resolution, I was going to order the files by time and create  a  ""raw geotiff RDD[(ProjectedExtent, Tile)"" then tile them.  So the files would be loaded in sequence from newest to oldest time wise.   So I am assuming if I load and process like this the pixels in the data loaded first would take precedence.",6/9/16,1
pomadchin,"@lossyrob Cassandra PR has a stable fix right now, https://github.com/travis-ci/travis-ci/issues/5669 is closed; let_Ñés wait ci finish",6/9/16,1
pomadchin,@lossyrob https://travis-ci.org/geotrellis/geotrellis/builds/136488436 so openjdk 7 bug fixed (: :tada: :tada: :tada: :tada: :tada: :tada: :tada: ,6/9/16,1
lossyrob,"RDDs are unordered, so you would lose the sequence in that case",6/9/16,1
owcm,@lossyrob  Okay my results would be random then,6/9/16,1
lossyrob,"yes. there_Ñés some logic that I wrote to do a smarter merge of imagery, that_Ñés a bunch more complicated",6/9/16,1
lossyrob,is part of it,6/9/16,1
lossyrob,but that project basically merges overlapping imagery based on a sequence ordering ,6/9/16,1
lossyrob,so that images listed first will override pixels of images listed later,6/9/16,1
lossyrob,@pomadchin did we put README info about how to run the unit tests with the docker container?,6/9/16,1
lossyrob,we should add what ports need to be forwarded if your using docker-machine,6/9/16,1
pomadchin,"hm, no; I can add it",6/9/16,1
pomadchin,\+ we require postgres,6/9/16,1
pomadchin,"i can describe both situations, some more generic doc, to describe the problem in general and what do we need to run tests",6/9/16,1
pomadchin,or that can be a separate pr,6/9/16,1
lossyrob,"I think doing it for the cassandra PR would be good, we can mimic it for the slick unit tests in another PR",6/9/16,1
lossyrob,we need to cut down on build time. we_Ñéve hit the travis limits,6/9/16,1
owcm,I know I can render my results as PNG's or JPEG's but can I render them out in GeoTiff format. Was thinking of having the Geotiffs exported out and served up by GeoServer.    ,6/9/16,1
lossyrob,"yes, there_Ñés a function for that",6/10/16,1
lossyrob,https://github.com/geotrellis/geotrellis/blob/master/spark/src/main/scala/geotrellis/spark/render/SpatialTileLayerRDDRenderMethods.scala#L17,6/10/16,1
uuuuq,"hi there,",6/10/16,1
uuuuq,"i wrote a demo, it works with  SparkConf.setMaster(""local[*]"")«_ÔI built a spark standalone cluster with 1 master and 2 slaves, and i use  SparkConf.setMaster(""spark://192.168.1.7:7077"") and it raises:```[error] 16/06/10 15:16:03 INFO AppClient$ClientEndpoint: Connecting to master spark://192.168.1.7:7077...[error] 16/06/10 15:16:23 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[appclient-registration-retry-thread,5,main][error] java.util.concurrent.RejectedExecutionException: Task java.util.concurrent.FutureTask@fa0cd65 rejected from java.util.concurrent.ThreadPoolExecutor@6c854f61[Running, pool size = 1, active threads = 0, queued tasks = 0, completed tasks = 1][error] 	at java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2047)[error] 	at java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:823)[error] 	at java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1369)[error] 	at java.util.concurrent.AbstractExecutorService.submit(AbstractExecutorService.java:112)[error] 	at org.apache.spark.deploy.client.AppClient$ClientEndpoint$$anonfun$tryRegisterAllMasters$1.apply(AppClient.scala:96)[error] 	at org.apache.spark.deploy.client.AppClient$ClientEndpoint$$anonfun$tryRegisterAllMasters$1.apply(AppClient.scala:95)[error] 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)[error] 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)[error] 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)[error] 	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108)[error] 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:244)[error] 	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:108)[error] 	at org.apache.spark.deploy.client.AppClient$ClientEndpoint.tryRegisterAllMasters(AppClient.scala:95)[error] 	at org.apache.spark.deploy.client.AppClient$ClientEndpoint.org$apache$spark$deploy$client$AppClient$ClientEndpoint$$registerWithMaster(AppClient.scala:121)[error] 	at org.apache.spark.deploy.client.AppClient$ClientEndpoint$$anon$2$$anonfun$run$1.apply$mcV$sp(AppClient.scala:132)[error] 	at org.apache.spark.util.Utils$.tryOrExit(Utils.scala:1119)[error] 	at org.apache.spark.deploy.client.AppClient$ClientEndpoint$$anon$2.run(AppClient.scala:124)[error] 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)[error] 	at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)[error] 	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)[error] 	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)[error] 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)[error] 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)[error] 	at java.lang.Thread.run(Thread.java:745)[error] 16/06/10 15:16:23 INFO DiskBlockManager: Shutdown hook called[error] 16/06/10 15:16:23 INFO ShutdownHookManager: Shutdown hook called[error] 16/06/10 15:16:23 INFO ShutdownHookManager: Deleting directory /tmp/spark-51da9afa-40ff-4dc0-93a2-d01cceeb838a/httpd-5cc8d338-5a1d-4c91-8582-0bfe4036bbcc[error] 16/06/10 15:16:23 INFO ShutdownHookManager: Deleting directory /tmp/spark-51da9afa-40ff-4dc0-93a2-d01cceeb838ajava.lang.RuntimeException: Nonzero exit code returned from runner: 50	at scala.sys.package$.error(package.scala:27)[trace] Stack trace suppressed: run last compile:run for the full output.[error] (compile:run) Nonzero exit code returned from runner: 50[error] Total time: 27 s, completed Jun 10, 2016 3:16:24 PM```so how to deal with this problem? thanks.",6/10/16,1
pomadchin,@uuuuq how do you start it? Using spark submit?,6/10/16,1
schBen,"@lossyrob Alright, I understand the idea and will try to use it that way. Thank you for adding the API issue, that would be nice.",6/10/16,1
pomadchin,"@lossyrob `monocole 1.2.2` is out, you know what i mean ;)",6/10/16,1
akarmas,"@pomadchin Thanks, i tested and now the updater works fine!One stupid question. I have one project set up with the following dependencies:```libraryDependencies ++= Seq(  ""com.azavea.geotrellis"" %% ""geotrellis-spark"" % ""0.10.0"",  ""org.apache.spark"" %% ""spark-core"" % ""1.5.2"",  Dependencies.sprayRouting,  Dependencies.sprayCan,  ""org.scalatest""       %%  ""scalatest""      % ""2.2.0"" % ""test"")```How can i use the master branch in the dependencies?",6/10/16,1
pomadchin,@akarmas thanks for you bug report ;),6/10/16,1
pomadchin,"@akarmas first of all you need to build, and to publish artefacts to your local repo: https://github.com/geotrellis/geotrellis/tree/master/scripts ```bash./scripts/buildall.sh # if you use scala 2.10./scripts/buildall-2.11.sh # if you use scala 2.11```After that you can add your locally published jars as a dep into your project: ```scalalibraryDependencies ++= Seq(  ""com.azavea.geotrellis"" %% ""geotrellis-spark"" % ""1.0.0-SNAPSHOT"",  ""org.apache.spark"" %% ""spark-core"" % ""1.5.2"",  Dependencies.sprayRouting,  Dependencies.sprayCan,  ""org.scalatest""       %%  ""scalatest""      % ""2.2.0"" % ""test"")```",6/10/16,1
pomadchin,"@akarmas sry, fixed version: `1.0.0-SNAPSHOT`, not `0.10.0-SNAPSHOT`",6/10/16,1
pomadchin,https://github.com/geotrellis/geotrellis/blob/master/project/Version.scala#L18 and https://github.com/geotrellis/geotrellis/blob/master/project/Environment.scala#L25,6/10/16,1
echeipesh,"@akarmas We also publish artifacts for every PR merged with SHA suffix. So you can change your `geotrellis-spark` dep to `""com.azavea.geotrellis"" %% ""geotrellis-spark"" % _ÑÒ1.0.0-033d5c9_Ñù`. You can see that the tag (https://bintray.com/azavea/geotrellis/geotrellis-spark/view) matches first seven digits of PR merge commit: https://github.com/geotrellis/geotrellis/commit/033d5c97d241b5bf47043c1c0a8175b95bdc5fff",6/10/16,1
echeipesh,That can save you the trouble of building the GT on your local machine and doing publish-local.,6/10/16,1
pomadchin,@echeipesh \+ // a good point about gt bintray repo ,6/10/16,1
pomadchin,probably we need to add repos info into out README ._.,6/10/16,1
akarmas,"@echeipesh That is a great!But i must also set the  https://dl.bintray.com/azavea/geotrellis/com/azavea/geotrellis/ repo in the build.sbt? How do i do this? Because if i simply change the dep it does not work!",6/10/16,1
echeipesh,"@akarmas ah, that_Ñés good point, you have to add it to resolvers like so: https://github.com/echeipesh/landsat-demo/blob/emr/build.sbt#L22",6/10/16,1
pomadchin,"@akarmas `resolvers += Resolver.bintrayRepo(""azavea"", ""geotrellis"")`",6/10/16,1
pomadchin,"btw, we have an sbt template O: https://github.com/geotrellis/geotrellis-sbt-template",6/10/16,1
akarmas,@echeipesh  @pomadchin Thank you!! everything works perfect now!,6/10/16,1
echeipesh,Very glad to hear : ),6/10/16,1
lossyrob,https://twitter.com/GeoTrellis/status/741308215456108544,6/10/16,1
pomadchin,:tada: :tada: :tada: :tada: :tada: :tada: GT Cassandra Support released (ok in a master branch :D),6/10/16,1
mshareghi,@pomadchin Does the data node / name node wait script you have in geodocker-cluster work in centos? It doesn't appear to have nc installed,6/10/16,1
pomadchin,@mshareghi can you force pull all the 0.2.0 containers?,6/10/16,1
mshareghi,grisha; i was playing around with building my own from the scripts/dockerfiles instead just to learn / validate the scripts,6/10/16,1
pomadchin,"```docker pull daunnc/geodocker-accumulo:latestdocker pull daunnc/geodocker-hadoop:latestdocker pull daunnc/geodocker-spark:latestdocker pull daunnc/geodocker-zookeeper:latestdocker pull daunnc/geodocker-accumulo:0.2.0docker pull daunnc/geodocker-hadoop:0.2.0docker pull daunnc/geodocker-spark:0.2.0docker pull daunnc/geodocker-zookeeper:0.2.0```",6/10/16,1
pomadchin,"great, then have a look there: https://github.com/geotrellis/geodocker-cluster/blob/master/base/centos.dockerfile#L30-L32",6/10/16,1
mshareghi,thanks,6/10/16,1
pomadchin,"@mshareghi you may have a question, why just not to install using `yum`; the answer is that for some reason (definitely bug) it just not works (ed);",6/10/16,1
mshareghi,"I'm trying to build them container by container, starting with zookeeper, then hadoop name / data, and then accumulo and spark.. so far I have gotten zookeeper/hadoop running but i have to restart the hadoop data node because it doesn't wait for the name node to connect. It says the following (and I need to manually restart, which then works)Waiting for TCP connection to geodocker-hadoop-name:50070..............Ok.copyFromLocal: Call From 95203515632a/172.18.0.4 to geodocker-hadoop-name:8020 failed on connection exception: java.net.ConnectException: Connection refused; ",6/10/16,1
pomadchin,you tried to go inside the container?,6/10/16,1
pomadchin,how do you start it? using docker-compose?,6/10/16,1
mshareghi,@pomadchin yes i noticed that yum has an issue too.. apparently it has to do with the /etc/resolv.conf   nameserver config? ,6/10/16,1
pomadchin,@mshareghi i decided to install just from the official repo ,6/10/16,1
mshareghi,I created my own compose file with a subset of the -dev containers and used docker-compose,6/10/16,1
pomadchin,be carefull with startup commands ,6/10/16,1
mshareghi,"I dont intend to check any of that stuff in, just trying to learn what's going on by tinkering with it",6/10/16,1
pomadchin,"that_Ñés the only thing i can help you with;or you can throw there your docker-compose file and i can check how it works",6/10/16,1
mshareghi,How do you paste code as above ?,6/10/16,1
pomadchin,wrap it with \```,6/10/16,1
pomadchin,"\+ you can specify syntax \```json\```bash\```scala.. etc",6/10/16,1
pomadchin,"or even publish everything into your own github repo, if it is possible",6/10/16,1
mshareghi,"```version: '2'services:    geodocker-pbase:        build:          context: ./base          # choose one of images (centos.dockerfile / ubuntu.dockerfile)          dockerfile: centos.dockerfile        image: geodocker-pbase:latest    geodocker-zookeeper:        build: ./zookeeper        image: local/geodocker-zookeeper:latest        volumes:          - '/data/gt/hdfs:/data/hdfs'        ports:          - 2181:2181        depends_on:                   - geodocker-pbase      geodocker-hadoop-name:        build: ./hadoop        command: name dev        image: local/geodocker-hadoop:latest        environment:          HADOOP_MASTER_ADDRESS: geodocker-hadoop-name        ports:          - 50070:50070          - 8020:8020        volumes:          - '/data/gt/hdfs:/data/hdfs'        depends_on:          - geodocker-zookeeper    geodocker-hadoop-data:        build: ./hadoop        image: local/geodocker-hadoop:latest        command: data dev        environment:          HADOOP_MASTER_ADDRESS: geodocker-hadoop-name        volumes:          - '/data/gt/hdfs:/data/hdfs'        depends_on:          - geodocker-hadoop-name",6/10/16,1
mshareghi,"its pretty much the same, it works aside from having to restart the name node ... gotta add in some sleep commands =)",6/10/16,1
pomadchin,ok would check it out,6/10/16,1
pomadchin,"Try to wait on that port in addition, what do you think?",6/10/16,1
mshareghi,"yeah on 8020. but that code's already in the name node shell script, right? (using nc)",6/10/16,1
mshareghi,data node rather,6/10/16,1
moradology,hey @mshareghi just saw this - how are things going with gd-cluster,6/10/16,1
moradology,i've been messing with it a lot the past few days and can probably be of some assistance,6/10/16,1
mshareghi,"@moradology Ah, just trying to get it working off a virtual box / windows machine. I've had a couple issues, for instance the centos config doesn't seem to run the wget properly (cannot find vault.centos.org ... Is this a DNS name issue in  /etc/resolv.conf file ? ",6/10/16,1
mshareghi,"(The wget to download  nc, for the sleep/wait code in the hadoop name  and data shell scripts)",6/10/16,1
mshareghi,"I haven't actually set up Geotrellis yet and I'm looking forward to finally doing so.. I was wondering, is there currently Hive / Impala integration with geo UDFs so that your HQL statements get automatically converted to Accumulo iterators and pushed to the tablet servers for calculation? What's the recommended way to perform geographic operations in Geotrellis, via spark submit jobs? I'm very new at all of this tech. ",6/10/16,1
mshareghi,I was able to get it working by adding nameserver 8.8.8.8 to my /etc/resolv.conf file in my docker machine instance. Not sure if this will screw up my container IP mappings though? ,6/10/16,1
uuuuq,@pomadchin I start the spark cluster by running ./sbin/start_all.sh,6/11/16,1
pomadchin,@uuuuq and how do you deploy your app?,6/11/16,1
mshareghi,"Is type of log normal in geodocker-cluster zookeeper? ""EndOfStreamException: Unable to read additional data from client sessionid 0x1554135f6350000, likely client has closed socket""",6/11/16,1
mshareghi,finally got geowave ingest working.. woohoo!,6/12/16,1
wsf1990,can ingest shp data to the accumulo and cut to tiles?,6/12/16,1
wsf1990,And how to convert the shape data to raster by geotrellis?,6/12/16,1
wsf1990,Dose anybody know this? *Thanks*!,6/12/16,1
uuuuq,,6/13/16,1
uuuuq,"the code works fine with spark-shell --master:"" "" and spark-submit, but failed when i use sbt run,",6/13/16,1
uuuuq,"for example, i modified the  IngestImage.scala of geotrellis-landsat-demo and run the code with ./sbt run*work```    val conf =      new SparkConf()        .setMaster(""local[*]"")        .setAppName(""Spark Tiler"")        .set(""spark.serializer"", ""org.apache.spark.serializer.KryoSerializer"")        .set(""spark.kryo.registrator"", ""geotrellis.spark.io.kryo.KryoRegistrator"")```*fail```    val conf =      new SparkConf()        .setMaster(""spark://xxxx:7077"")        .setAppName(""Spark Tiler"")        .set(""spark.serializer"", ""org.apache.spark.serializer.KryoSerializer"")        .set(""spark.kryo.registrator"", ""geotrellis.spark.io.kryo.KryoRegistrator"")```",6/13/16,1
pomadchin,"@uuuuq it is not possible to start app using sbt run on a spark cluster, you need to have an assembly jar to do it",6/13/16,1
pomadchin,To deliver jar on every spark worker node,6/13/16,1
wsf1990,@pomadchin How to convert the shape data to raster by geotrellis?,6/13/16,1
uuuuq,@pomadchin thx,6/13/16,1
pomadchin,"@wsf1990 yep, you can convert Feature to raster: https://github.com/geotrellis/geotrellis/blob/master/raster/src/main/scala/geotrellis/raster/rasterize/Rasterizer.scala https://github.com/geotrellis/geotrellis/blob/master/raster/src/main/scala/geotrellis/raster/VectorToRaster.scalabut that depends on your data ._. what does it mean _ÑÒconvert"" in your case?",6/13/16,1
wsf1990,"I means convert the shapefile to geotif,like what gdal do.",6/13/16,1
wsf1990,The better is can ingest the shape file to accumulo by Geotrellis directly like what ETL do to raster,6/13/16,1
wsf1990,"And why I use ```geotrellis.shapefile.ShapeFileReader.readSimpleFeatures(path)``` to read shp,the JVM made an error like this:",6/13/16,1
wsf1990,"```The following locker still has a lock: read on file:..shp by org.geotools.data.shapefile.shp.ShapefileReaderThe following locker still has a lock: read on file:..shx by org.geotools.data.shapefile.shp.IndexFileThe following locker still has a lock: read on file:...dbf by org.geotools.data.shapefile.dbf.DbaseFileReaderException in thread ""main"" java.lang.IllegalArgumentException: Expected requestor org.geotools.data.shapefile.dbf.DbaseFileReader@4ea5b703 to have locked the url but it does not hold the lock for the URL```",6/13/16,1
wsf1990,@pomadchin Can merge some polygons to one by geotrellis?,6/13/16,1
pomadchin,"@wsf1990 we have geometries union function, however not sure that what you want",6/13/16,1
pomadchin,I am not very familiar with our Vector API; a bit later @dwins and / or @lossyrob can help you,6/13/16,1
dwins,reading,6/13/16,1
dwins,"i think union would work for this, but it will not always produce a polygon (for example, the union of two polygons that do not intersect is just a MultiPolygon containing those polygons.)",6/13/16,1
wsf1990,"yes,I am trying intersect now. I means merge some multipolygon to one multipolygon.",6/13/16,1
lossyrob,"```scalaval polygons: Seq[Polygon] = ???polygons. unionGeometries match {  case MultipolygonResult(mp) => // mp is a multi polygon  case PolygonResult(p) => // p is a polygon}```",6/13/16,1
dwins,"@wsf1990 intersect isn't what i would think of when you say ""merge"". maybe you can describe what you're trying to accomplish?",6/13/16,1
wsf1990,"I have a shp file which has many multipolygon record, Then I want to Rasterizer the shp file to raster, So I think I must first merge these multipolygon to one, Then use ```Rasterizer.rasterizeWithValue``` to make it to raster",6/13/16,1
wsf1990,maybe union,6/13/16,1
dwins,Union makes more sense here for sure.,6/13/16,1
owcm,"Using etl can I read files from HDFS, I see accumulo, hadoop,s3 in geotrellis.spark.etl.  Should I be reading from HDFS using the Hadoop geotrellis.spark.etl.hadoop package?",6/13/16,1
pomadchin,"@owcm you can read using etl object, or just natively using `spark.io.hadoop.hadoop(Temporal)GeoTiffRDD`",6/13/16,1
pomadchin,"I like more the second way, but you also can do the following (using etl): ```scalaval loadParams = Map(""path"" -> ""some path to tiles on hdfs or locally"")val hadoopInput = new GeoTiffHadoopInput()val loadTiles: RDD[(ProjectedExtent, Tile)] = hadoopInput(loadParams)```",6/13/16,1
owcm,@pomadchin  Thanks for the pointer.  The example should be very helpful.,6/13/16,1
pomadchin,@owcm np,6/13/16,1
pomadchin,"to read using hadoopGeoTiffRDD looks prettier though: ```sc.hadoopGeoTiffRDD(inputPath)```",6/13/16,1
akarmas,"Hi all, i have a question!i am reading in a layer with two Int bands like this```val irdd1: RDD[(SpaceTimeKey, MultibandTile)] with Metadata[TileLayerMetadata[SpaceTimeKey]] =        reader.query[SpaceTimeKey, MultibandTile, TileLayerMetadata[SpaceTimeKey]](layerId).where(Between(time1, time2)).result```Then i want to perform a simple processing to the bands of the RDD i.e. to calculate the NDVI for the layer and produce a new RDD[(SpaceTimeKey, Tile)] with Metadata[TileLayerMetadata[SpaceTimeKey]]  with the same metadata as the original but with the NDVI values as the the tile values. I am performing the processing like this:```val test = MultibandTileLayerRDD(irdd1, irdd1.metadata)val i = test map { case(key, tile) =>  tile.convert(DoubleCellType).combineDouble(0, 1) {              (r, ir) => if(isData(r) && isData(ir)) {               (ir - r) / (ir + r)             } else {                 Double.NaN             }}  } collect()```But it is not working. The conversion to double does not work and as a consequence the resulting tiles have zero values everywhere.What am i doing wrong?Thanks! ",6/13/16,1
owcm,"Another question.  I am pyramiding using HadoopLayerWriter and HadoopLayerManager.  Code snippet:```      val writeOp =      Pyramid.upLevels(elevation, targetLayoutScheme, zoom) { (rdd, z) =>        val layerId = LayerId(jobId, z)        // If the layer exists already, delete it out before writing        if(attributeStore.layerExists(layerId)) {          new HadoopLayerManager(attributeStore).delete(layerId)        }        writer.write(layerId, rdd, ZCurveKeyIndexMethod)      }```Using sbt and when I build I get the following error:```SurgePipelines.scala:888: could not find implicit value for evidence parameter of type spray.json.JsonFormat[geotrellis.spark.SpatialKey][error]         writer.write(layerId, rdd, ZCurveKeyIndexMethod)```Is there a package I need to be including?  Not obvious to me.  Thanks in advance.",6/13/16,1
pomadchin,just `import geotrellis.spark.io._`,6/13/16,1
owcm,@pomadchin Thanks again that saved me,6/13/16,1
lossyrob,@akarmas so when you iterate through `i` all the tile values have 0 data? or are you stitching?,6/13/16,1
lossyrob,@akarmas you can try converting at the top level as well - `test.convert(DoubleConstantNoDataCellType)`,6/13/16,1
aeffrig,I'm working with the Landsat data.  I've ingested each of the single band images into Accumulo with ETL.  Once the data is in Acculumo is there any way to join it together to  run the NDVI calculation and then treated it as multiband without reconstituting it as a single multiband image?,6/13/16,1
lossyrob,I_Ñém having trouble seeing why that wouldn_Ñét work,6/13/16,1
lossyrob,"@akarmas can you write out the initial collection of multiband tiles as a sequence of geotiffs and then pass them to us somehow? I_Ñém reading through the code and I can_Ñét figure out what would be causing that, would like to test locally",6/13/16,1
echeipesh,"@aeffrig sure you_Ñéd have to query each band seperatly, since they_Ñére each their own layer, and then join the NIR and VIS (docs on joins: https://github.com/geotrellis/geotrellis/blob/master/docs/spark/spark-joins.md) then you can mape over the result and do something like `rdd.mapValues{ case (nir, vis) => nir.combineDouble(vis){ (pixelNir, pixelVis) => (pixelNir - pixelVix) / (pixelNir + pixelVis) } }` ",6/13/16,1
lossyrob,"```scalaval crs: CRS = ??? // whatever the CRS isvar i = 0MultibandTileLayerRDD(irdd1, irdd1.metadata).asRasters.collect.foreach { case (_, raster) => GeoTiff(raster, crs).write(s_ÑÒ/some/local/path/${i}.tif_Ñù) ; i += 1 }```",6/13/16,1
akarmas,"@lossyrob Ok, this confusion is my mistake. Everything works fine. I was using asciiDraw() to inspect tile values instead of asciiDrawDouble() and that is why i thought the conversion was not working.I 'm really sorry for that!!From the above code snippet i is of type Array[geotrellis.raster.Tile]. How can i transform it to RDD[SpaceTimeKey, Tile] where the SpaceTimeKey is the same as the original key of each tile in irdd1?",6/14/16,1
akarmas,"I think i did it like this:```val i = test map { case(key, tile) =>  (key, tile.convert(DoubleCellType).combineDouble(0, 1) {  (r, ir) => if(isData(r) && isData(ir)) {               (ir - r) / (ir + r)             } else {                 Double.NaN             }})  } collect()val s = sp.parallelize(i)val d = TileLayerRDD(s, irdd1.metadata)```I have managed to construct a layer that contains the NDVI values for a specific AOI at two different dates. The layer is of type RDD[(SpaceTimeKey, Tile] with Metadata[TileLayerMetadata[SpaceTimeKey]]. I want calculate the maximum value across the two dates for each pixel. I am trying to do it as follows in the code(with temporalMAX) but it does not work. Each date has 546 tiles that belong to the image of that date. So i think that the result must contain also 546 tiles that hold the max values for each pixel but this is not the case as the way i am doing it the result has 1092 tiles which does not make sense. Also i cannot understand how to use the windowSize, unit and partitioner arguments of temporalMax function```val irdd: RDD[(SpaceTimeKey, Tile)] with Metadata[TileLayerMetadata[SpaceTimeKey]] =        reader.read[SpaceTimeKey, Tile, TileLayerMetadata[SpaceTimeKey]](layerId)import geotrellis.spark.mapalgebra.local.temporal._val m = LocalTemporalStatistics.temporalMax[SpaceTimeKey](irdd, 1, 1, time1, time3) val c = m.collect```",6/14/16,1
lossyrob,"@akarmas you don_Ñét want to collect if you don_Ñét have to. You can just stay in RDD land. When you collect, you are shipping all of your tiles back to the driver program. This will never scale. Instead, if you just map the values of the tile, and maintain the context, you_Ñéll already have a TileLayerRDD",6/14/16,1
lossyrob,"```scalatest.withContext(_.rdd.mapValues { tile =>  tile.convert(DoubleCellType).combineDouble(0, 1) { (r, ir) =>    if(isData(r) && isData(ir)) {      (ir - r) / (ir + r)    } else {      Double.NaN    }}})```",6/14/16,1
lossyrob,(formatting is weird),6/14/16,1
lossyrob,"It seems like you_Ñéd have to translate the `SpaceTimeKey` to a `SpatialKey`, since the pixels would be potentially of different dates",6/14/16,1
lossyrob,let me see how i_Ñéd code that,6/14/16,1
lossyrob,"didn_Ñét compile this, but this is the idea",6/14/16,1
lossyrob,"```scala// Note: type TileLayerRDD[K] = RDD[(K, Tile)] with Metadata[TileLayerMetadata[K]]val irdd: TileLayerRDD[SpaceTimeKey] =  reader.read(layerId) // Should infer the type params via the specified return typeval maxValues: RDD[(SpatialKey, Tile)] =   irdd    .map { case (key, tile) =>      // Get the spatial component of the SpaceTimeKey, which turns it into SpatialKey      (key.getComponent[SpatialKey], tile)    }    // Now we have all the tiles that cover the same area with the same key.    // Simply reduce by the key with a localMax    .reduceByKey(_.localMax(_))```",6/14/16,1
lossyrob,@akarmas see https://github.com/geotrellis/geotrellis/pull/1549,6/14/16,1
lossyrob,"Also devs, I think we should start doing that_Ñ_when there_Ñés an example that we show in the Gitter or somewhere else, we should try to mark it down in the docs as a short example in an examples file",6/14/16,1
fosskers,I'll elaborate on my protobuf trials here,6/14/16,1
fosskers,"Here's an update. I used [mapbox-vector-tile](https://github.com/mapzen/mapbox-vector-tile), a mapzen tool in Python, to encode a simple object:```python vt = {     ""name"": ""water"",     ""features"": [         {             ""geometry"":""POINT(5 5)"",             ""properties"": {}         }     ] }```",6/14/16,1
fosskers,"I then wrote the encoded bytes to a file, and decoded them with both my hand-written VT schema, and the autogenerated one",6/14/16,1
fosskers,"*Both* report the point to be at (5, 4041) instead",6/14/16,1
fosskers,"Here's what's curious. A point at `(5,5)` would be encoded as `[9 10 10]`. `9` says ""move relative to where you are (`(0,0)` at the start), and expect 1 pair of x,y coords""",6/14/16,1
fosskers,10 is the z-encoded form of 5,6/14/16,1
fosskers,10 in binary is `.... 0000 1010` (these are uint32),6/14/16,1
fosskers,"the mistaken coordinate read by both my backends above, 4041, would be encoded as 8182",6/14/16,1
fosskers,8182 in binary is `.... 0001 1111 1111 0110`,6/14/16,1
lossyrob,could this be an endian problem,6/14/16,1
fosskers,the binary for 10 isn't found within the binary for 8182,6/14/16,1
fosskers,"so it's not like my backends were ""reading more bits than they were supposed to"" etc",6/14/16,1
fosskers,"> endian problemMy code and the python code agreed on everything else except this one y-coord",6/14/16,1
lossyrob,"yeah, speghetti at the wall",6/14/16,1
fosskers,"and since it's all just ints, an endian problem would (should) blow the whole thing apart",6/14/16,1
lossyrob,is it a y axis flip problem,6/14/16,1
lossyrob,upper or lower based 0 value,6/14/16,1
fosskers,doubt it,6/14/16,1
fosskers,"my next step is to write out some of my mock instances via my protobuf code, and see if the python can read them",6/14/16,1
fosskers,"if it can't, then I think their tool is to blame in this case",6/14/16,1
fosskers,and I'd have to restart my search for a working 3rd party codec,6/14/16,1
lossyrob,dang,6/14/16,1
fosskers,"> Both report the point to be at (5, 4041) insteadMakes me suspect the python code",6/14/16,1
fosskers,"So I took a test instance of some protobuf data, encoded it, and read it with the python tool",6/14/16,1
fosskers,"The test instance encodes a single point at `(25,17)`",6/14/16,1
fosskers,"the python tool read it as being at `(25,4079)`",6/14/16,1
fosskers,waiiiiit a minute,6/14/16,1
fosskers,The default extent for these tiles is 4096,6/14/16,1
fosskers,`4096 - 17 = 4079`,6/14/16,1
fosskers,"In fact, the range of points is supposed to be `[0,4096)`",6/14/16,1
fosskers,No,6/14/16,1
fosskers,"> A point at (0,10) or (4096,10) is on the edge of the extent. A point at (-1,10) or (4097,10) is outside the extent of the tile.",6/14/16,1
fosskers,"So the range *is* `[0,4096]`",6/14/16,1
fosskers,interesting that that requires 4079 points,6/14/16,1
fosskers,"From the python tool's README:> geometry: representation of the feature geometry in WKT, WKB, or a shapely geometry. Coordinates are relative to the tile, scaled in the range [0, 4096).",6/14/16,1
lossyrob,you sure it_Ñés not a Y up-or-down problem? Similar to https://github.com/geotrellis/geotrellis/blob/master/raster/src/main/scala/geotrellis/raster/RasterExtent.scala#L32 . The math of `4096 - 17 = 4079` makes me even more suspicious,6/14/16,1
fosskers,you're right,6/14/16,1
fosskers,https://github.com/mapzen/mapbox-vector-tile/blob/1c9d86945c9d581f36ebcf997c889c44cb605111/README.md#coordinate-transformations-for-encoding,6/14/16,1
fosskers,"> It is possible to control whether the tile is in a ""y down"" coordinate system by setting the parameter y_coord_down=True on the call to encode(). The default is ""y up"".",6/14/16,1
fosskers,ok! got it,6/14/16,1
lossyrob,well that_Ñés at least one part,6/14/16,1
lossyrob,wonder why that would crash the reader on that large complex tile,6/14/16,1
fosskers,my protobuf codec reads the correct coords from `onepoint.mvt` now,6/14/16,1
lossyrob,nice,6/14/16,1
fosskers,(the hand-written one),6/14/16,1
fosskers,but they still aren't the same binary,6/14/16,1
fosskers,"ok, the python tool can also decode my encoded mock instance",6/14/16,1
fosskers,"After a lot of pencil, paper, and base-2 math, I've discovered the problem with https://github.com/mapzen/mapbox-vector-tile is how it handles polygons",6/14/16,1
fosskers,"V2 of the spec clarified how polygon winding order is supposed to work. Namely, given some list of `uint32` defining a polygon geometry:```[9 4 4 18 6 4 5 4 15]```",6/14/16,1
fosskers,"Those ints encode ""move"" commands and coordinate diffs from some ""cursor"" location",6/14/16,1
fosskers,"These ints encode the polygon `(2,2) -> (5,4) -> (2,6) -> (2,2)`",6/14/16,1
fosskers,"however, with that y-flippage business we found before, it seems the python tool confuses winding order",6/14/16,1
fosskers,encoding the above poly as `[9 4 4 18 0 8 6 3 15]`,6/14/16,1
fosskers,"which, when read by my code, gives the oppositely wound poly: `(2,2) -> (2,6) -> (5,4) -> (2,2)`",6/14/16,1
fosskers,"that poly gives a negative area by the surveyor's formula, and so should be an interior polygon (except it isn't)",6/14/16,1
lossyrob,so in v1 it didn_Ñét matter or was unspecified behavior? which is why the python lib is still at v1?,6/14/16,1
fosskers,"wow, okay, I just took a look at the v1 spec. It's about a quarter the length of the v2",6/14/16,1
fosskers,"and yeah, there's no mention of winding order at all",6/14/16,1
fosskers,meaning winding order / results of SF for determing interior rings is fair game,6/14/16,1
fosskers,https://github.com/mapzen/mapbox-vector-tile/issues/57,6/14/16,1
lossyrob,_ÑÒThe Puzzle of Winding Order_Ñù ha I like that. sounds like an adventure tale,6/14/16,1
fosskers,Who says we can't have fun at work? ;),6/14/16,1
fosskers,I'm going to sit on that over night and see what happens,6/14/16,1
fosskers,"The bug is fairly clear, so that's almost out of the way. A bigger issue is why my codecs wouldn't read the ""official"" tile",6/14/16,1
lossyrob,does the python lib read the offical one?,6/14/16,1
fosskers,Can't believe I haven't tried that yet,6/14/16,1
fosskers,Hahah I've been pretty deep in the weeds today,6/14/16,1
fosskers,! no it doesn't,6/14/16,1
fosskers,"```Traceback (most recent call last):  File ""vt2.py"", line 6, in <module>    decoded = mapbox_vector_tile.decode(data, y_coord_down=True)  File ""/usr/lib/python3.5/site-packages/mapbox_vector_tile/__init__.py"", line 7, in decode    message = vector_tile.getMessage(tile, y_coord_down)  File ""/usr/lib/python3.5/site-packages/mapbox_vector_tile/decoder.py"", line 31, in getMessage    self.tile.ParseFromString(pbf_data)  File ""/usr/lib/python3.5/site-packages/google/protobuf/message.py"", line 185, in ParseFromString    self.MergeFromString(serialized)  File ""/usr/lib/python3.5/site-packages/google/protobuf/internal/python_message.py"", line 1091, in MergeFromString    if self._InternalParse(serialized, 0, length) != length:  File ""/usr/lib/python3.5/site-packages/google/protobuf/internal/python_message.py"", line 1117, in InternalParse    new_pos = local_SkipField(buffer, new_pos, end, tag_bytes)  File ""/usr/lib/python3.5/site-packages/google/protobuf/internal/decoder.py"", line 850, in SkipField    return WIRETYPE_TO_SKIPPER[wire_type](buffer, pos, end)  File ""/usr/lib/python3.5/site-packages/google/protobuf/internal/decoder.py"", line 820, in _RaiseInvalidWireType    raise _DecodeError('Tag had invalid wire type.')google.protobuf.message.DecodeError: Tag had invalid wire type.```",6/14/16,1
lossyrob,wire type. huh,6/14/16,1
fosskers,so the tile might've been bad afterall,6/14/16,1
fosskers,"No! The tile wasn't bad, it was gzipped!",6/15/16,1
fosskers,My hand-rolled schema can read the tile :),6/15/16,1
akarmas,"@lossyrob Many thanks for all this info!Some questions...1) Does localMax work with negative values?..because i am inspecting for example two tiles and then their max calculated with the example code and especially when negative values exist the result is not the expected...i will perform some more testing and come back with more info.. 2)i was wondering  what is the difference between the way you proposed (localMax) and temporalMax.3)Moreover in the same use case if i want to calculate the variance of the same pixels, how am i supposed to do it. Using temporalVariance or with  TemporalWindow class from geotrellis.spark.mapalgebra.local.temporal.Implicits ?",6/15/16,1
lossyrob,"temporal is basically for _ÑÒper X amount of time_Ñù statistics. Are you computing variance _ÑÒper day_Ñù or _ÑÒper month_Ñù, etc? Are you calcalating max _ÑÒper X_Ñù, or just total max? The way I gave you computes total max",6/15/16,1
lossyrob,localMax should work with negative values,6/15/16,1
fosskers,Protobuf update: hand-written schema decodes a 40kb vector tile in 4.7ms,6/15/16,1
pomadchin,@fosskers :tada: :tada: ,6/15/16,1
lossyrob,do you have any timing info on the python lib decoding time?,6/15/16,1
fosskers,"that would be helpful, wouldn't it",6/15/16,1
fosskers,not sure how one does python benchmarks in a rigorous way,6/15/16,1
lossyrob,"sure it_Ñés possible, not sure rigor is worth the time",6/15/16,1
lossyrob,some system time calls and printouts will probably give a rough idea,6/15/16,1
fosskers,The python takes about 70ms,6/15/16,1
lossyrob,nice,6/15/16,1
lossyrob,sounds like a win,6/15/16,1
fosskers,"actually, that's not entirely fair",6/15/16,1
fosskers,"because their ""decode"" function parses the bytes, and morphs it into a pseudo-JSON that's useable",6/15/16,1
fosskers,"so to be fair I'll write my conversion code to the higher type, then try again",6/15/16,1
lossyrob,yeah,6/15/16,1
lossyrob,"also this is pre-micro optimization. Usually there_Ñés some things you can flush out to speed things up; at some point in the review process we can pair on that, I have a lot of experience maticulously sweating over benchmarks and profile results to squeak out faster times",6/15/16,1
fosskers,"yeah, I imagine there's clever concurrent stuff that could occur here",6/15/16,1
fosskers,"but since this is the prototype, I'm going to avoid all fanciness",6/15/16,1
lossyrob,"yeah, exactly. these are really good prelim timing results tho",6/15/16,1
fosskers,"should also mention that these are haskell results, I don't know what Scala will do",6/15/16,1
lossyrob,good one,6/15/16,1
fosskers,"But I can't imagine it will be much worse, if we're smart",6/15/16,1
lossyrob,wait_Ñ_did you really code it in haskell,6/15/16,1
fosskers,"yes, apologies for not mentioning that sooner",6/15/16,1
fosskers,"being the prototype, I could work much faster this way",6/15/16,1
fosskers,"Given a complete prototype, the function from this code to the scala should be little effort",6/15/16,1
fosskers,Most of the hurdles have been learning about the nuances of protobuf/vector tiles,6/15/16,1
dwins,"Hey guys, I put up #1551 which rearranges things in the proj4 module to better support alternative projection handling libraries - didn't allow swapping backends yet but I'll try and make that happen now.",6/15/16,1
lossyrob,"nice, thanks",6/15/16,1
dwins,"I noticed the GeoTiff parsing code is constructing proj.4 parameter strings, that's going to be a problem since I'm not aware of any libs outside the proj family that can interpret those.  But it seems sort of inevitable that parsing geotiffs will involve converting to some parameter notation so I'm not sure what can be done.",6/15/16,1
lossyrob,"yeah, we did a port of that code from GDAL. The kicker though is that the WKT version in GDAL is the one it actually uses, and the one that is complete",6/15/16,1
lossyrob,"the GDAL geotiff tag -> proj.4 string parser is incomplete, so our is as well",6/15/16,1
lossyrob,but we need that since we can_Ñét parse WKT,6/15/16,1
dwins,"i see, so SIS can help after all",6/15/16,1
lossyrob,"so if we had something that parsed WKT, we could write that in.",6/15/16,1
lossyrob,yeah,6/15/16,1
wsf1990,"If I have a tile that just have two data like 1 and NODATA, how can I use renderPng to render a beautiful png,like make the 1 to red color,ND to transparent.",6/16/16,1
lossyrob,"```scalaimport geotrellis.raster._import geotrellis.raster.render.ColorMapval colorMap = ColorMap(Map( 1 -> 0xFF0000FF))val tile: Tile = ??? // The tile you describedval png = tile.renderPng(colorMap)png.write(_ÑÒ/path/to/file.png_Ñù)```",6/16/16,1
lossyrob,(not compiled),6/16/16,1
wsf1990,thanks«_ÅI got it in the `rendering.md`,6/16/16,1
akarmas,"@lossyrob Many thanks! i understand now the difference...I tried to apply localMean to find the total mean for every pixel following your demonstration of how to calculate the totalMax. I have tried to do it like this but i it is not working. Any suggestions?```val meanValues=  irdd.map { case (key, tile) =>    (key.getComponent[SpatialKey], tile)  }.groupBy(_._1)  .map{ case(key,tiles) =>     val t=tiles.to[ scala.collection.Traversable];    (key, new withLocalTileRDDSeqMethods(t).localMean) }```In the same way i am looking to calculate total variance for each pixel but i am not able to decide which is the right function from all the functions that are to be used for variance calculation .",6/16/16,1
lossyrob,the thing inside the `map` is not working with RDDs,6/16/16,1
lossyrob,so not `withLocalTileRDDSeqMethods`,6/16/16,1
lossyrob,you_Ñéll want the `geotrellis.raster` version,6/16/16,1
lossyrob,which should be implicitly available after `import geotrellis.raster._`,6/16/16,1
lossyrob,so you should be able to call `t.localMean`,6/16/16,1
lossyrob,maybe `t.toSeq.localMean`,6/16/16,1
mchendricks1,"Geodocker cluster question: I'm testing geodocker cluster under a Marathon/Mesos deployment model. The ""runner"" scripts create a complexity though with the ""fully meshed"" host names required for service startup. E.g., Accumulo startup appears to need to (if",6/16/16,1
mchendricks1,Memory serves) all the nodes in the cluster. ,6/16/16,1
mchendricks1,Is it possible ,6/16/16,1
mchendricks1,To deploy it such that a node only needs to know itself and its leader - and figure out the membership later on startup?,6/16/16,1
pomadchin,@mchendricks1 we were investigating possibility to launch dockerized accumulo / spark / etc on dc/os ,6/17/16,1
pomadchin,"So the answer is yes, it is possible, but you have to use Calico for network between containers, as you have to forward all necessary ports manually to make possible talks between containers",6/17/16,1
pomadchin,manually right now** without Calico,6/17/16,1
pomadchin,In a couple of minutes will throw you some links (: and with json task to launch accumulo // but with max tablets amount only equaled to mesos nodes number ):,6/17/16,1
pomadchin,@mchendricks1 https://github.com/geotrellis/geodocker-cluster/blob/master/docs/arch/adr-0001-deployment.md,6/17/16,1
pomadchin,job example: https://gist.github.com/pomadchin/2193ed3a10808e9368d326a0cebe393f,6/17/16,1
pomadchin,"but i have a feeling, that that_Ñés a deprecated spec",6/17/16,1
pomadchin,we had some significant changes in geodocker cluster images,6/17/16,1
mchendricks1,@pomadchin alright I will have a look. Thanks for the info. Hopefully the project calico isn't strictly required. That could create some headaches. I have a couple of tricks to try now that I looked at the runner scripts more closely. ,6/17/16,1
pomadchin,"so for sure, that would be updated soon ;) but that can give you just a general idea",6/17/16,1
pomadchin,"your thoughts are appreciated :) \+ you can throw here your thoughts and i hope that can give you some help to reduce your time o: \+ we all need some sort of an easy deployment solution for geodocker cluster",6/17/16,1
pomadchin,): ,6/17/16,1
mchendricks1,"So one hack I'm considering is a two stage marathon geodocker deployment with special ""leader"" and ""follower"" startup sequences. Basically, the leader marathon app just deploys one node (on AWS EC2 environment use the metadata services) to figure out where (hostname/ip address). The leader container calls marathon app deploy to launch ""follower"" node with the leader ip/hostname as a marathon environment variable.",6/17/16,1
mchendricks1,"Basically, I'm looking some service registration/discovery pattern for follower nodes to find additional information of ""where to go"" after they come up. ",6/17/16,1
pomadchin,@mchendricks1 yep that_Ñés definitely what needs to be done,6/17/16,1
pomadchin,"you can look inside their official cassandra package, probably that makes sense to look how cassandra can be deployed",6/17/16,1
pomadchin,"but honestly i havent looked into it too close, so probbaly its not possible to have multiple cassandra nodes on one mesos node",6/17/16,1
pomadchin,:\,6/17/16,1
mchendricks1,Well,6/17/16,1
mchendricks1,Why would multiple Cassandra nodes be necessary/useful?,6/17/16,1
mchendricks1,"(On one Mesos node, that is)",6/17/16,1
akarmas,"@lossyrob My approach towards calculating total mean is not working```val meanValues=  irdd.map { case (key, tile) =>    (key.getComponent[SpatialKey], tile)  }.groupBy(_._1)  .map{ case(key,tiles) =>     val t=tiles.to[ scala.collection.Traversable];    (key, t.toSeq.localMean) }```That is because the result of groupBy is of type: RDD[(SpatialKey, Iterable[(SpatialKey, Tile)])]and t is of type RDD[Traversable[(SpatialKey, Tile)]]For localMean to work i think that i need to tranform t from RDD[Traversable[(SpatialKey, Tile)]] to Traversable[Tile].I am banging my head for hours but i cannot seem to find the correct way to achieve this! Any suggestions?? Thanks!",6/17/16,1
pomadchin,"@mchendricks1 just an example of ports exposing problem, you can_Ñét have same ports forwarded on one mesos nodeyep it_Ñés not a problem in some cases, but in others can be a problemanother fact, that you have to forward ports into internal network manually provides some restrictions on marathon usage as well",6/17/16,1
pomadchin,"@mchendricks1 starting Accumulo (with one tablet per node) is not a hard task (to start is as a group), and tservers autoscale guaranteed ;) so that_Ñés a good variant of geodocker cluster deployment; i_Ñéll update scripts today / tomorrow ",6/17/16,1
owcm,"General question.  I am interested in reading the Layer data I created during a ETL Avro data process and generating Geotiffs which will be written to HDFS.   If I load at zoom levels up through 12 all works well.  At zoom levels 16 and greater I blow the KryoSerializer when trying to create the Geotiff.    My thought is at the higher zoom levels to perform some kind of quad-tree algorithm, query in smaller segments of the Layer and geotiff them individually.   Can I learn the extent of a stored Layer without having to load it up and filter the Layer query?  Thanks in advance.",6/17/16,1
shiraeeshi,"> @shiraeeshi what were the motivations behind the 3 pending PR_Ñés for the `avro`, `python-avro-json-serializer`, and `spec` projects? Are these bug fixes that we couldn_Ñét do without, or enhancements? It_Ñés unfortunate that we would become dependant on those projects merging the PRs in order for the `geotrellis-python` projects to work, just because it_Ñés unclear when that would happen.",6/17/16,1
aeffrig,"I'm trying to run a Geotrellis job using 0.10.0 under Spark 1.6.1, but I keep seeing java.lang.NoSuchMethodError: org.apache.spark.rdd.ShuffledRDD.<init>(Lorg/apache/spark/rdd/RDD;Lorg/apache/spark/Partitioner;)V.  Is this familiar to anyone? Is Geotrellis known to work under Spark 1.6.1?",6/17/16,1
shiraeeshi,"> @shiraeeshi what were the motivations behind the 3 pending PR_Ñés for the `avro`, `python-avro-json-serializer`, and `spec` projects? Are these bug fixes that we couldn_Ñét do without, or enhancements? It_Ñés unfortunate that we would become dependant on those projects merging the PRs in order for the `geotrellis-python` projects to work, just because it_Ñés unclear when that would happen.The changes in the `avro` and `python-avro-json-serializer` projects are the enhancements for the authors, but for me these are bug fixes because it makes the logic more correct. There is an [issue](https://issues.apache.org/jira/browse/AVRO-1777) in the jira board for the `avro` project that my PR closes. The change in `python-avro-json-serializer` is as simple as changing `schema.name` to `schema.fullname` in some single line in the code.The changes in the `spec` are enhancements, we could do without them, but there would be a lot more repetitions in the code just to make the `spec` happy.I think these changes belong to those projects, not to `geotrellis-python`, so changing that projects makes the code in `geotrellis-python` more clean and neat than if we tried to monkey-patch the changes without creating the PRs.There is some progress in [`spec`](https://github.com/bitprophet/spec/pull/40) and [`python-avro-json-serializer`](https://github.com/linkedin/python-avro-json-serializer/pull/2). (sorry for the delay with an answer, I have been checking the chat in the PR and forgot about this chat)",6/17/16,1
shiraeeshi,"> It_Ñés unfortunate that we would become dependant on those projects merging the PRs in order for the geotrellis-python projects to work, just because it_Ñés unclear when that would happen.What if we publish temporary versions to pypi? The user will be able to install the dependencies with commands like `pip install temp-gt-avro` or `pip install temp-gt-spec` without having to download my branch.",6/17/16,1
aeffrig,"The following error occurs when compiling Geotrellis from master with Spark 1.6.1.```[error] /opt/dfc/github/geotrellis/spark/src/main/scala/geotrellis/spark/partition/SpacePartitioner.scala:65: No ClassTag available for K[error]           new ShuffledRDD(rdd.filter(r => containsKey(r._1)), this).asInstanceOf[RDD[(K, V)]],[error]           ^[error] /opt/dfc/github/geotrellis/spark/src/main/scala/geotrellis/spark/partition/SpacePartitioner.scala:65: No ClassTag available for K[error]           new ShuffledRDD(rdd.filter(r => containsKey(r._1)), this).asInstanceOf[RDD[(K, V)]],[error]           ^[error] one error found```",6/17/16,1
pomadchin,"@aeffrig we still don't have geotrellis built against spark 1.6.x, btw 1.6.x has backwards compatibility with 1.5.x , so there would be no problems in running geotrellis built with 1.5.x jars on spark 1.6.x ",6/17/16,1
pomadchin,,6/17/16,1
fosskers,VectorTile benchmarking results:,6/17/16,1
aeffrig,"@pomadchin Thanks.  I had some trouble with RejecteExecutionExceptions on 1.5.2 when using the SparkContext directly, so I may change to a spark-submit based approach.",6/17/16,1
fosskers,"* Python (bytes -> protobuf object -> pseudo-json): 74ms* Haskell (bytes -> protobuf object): 4.7ms* Haskell (bytes -> protobuf object -> high-level `VectorTile`): 14.7ms",6/17/16,1
pomadchin,"@aeffrig very interesting, can you throw here gist with example to reproduce your error?",6/17/16,1
owcm,"Trying RDD 'toRasters' call and I am getting ``` could not find implicit value for evidence parameter of type geotrellis.util.Component[geotrellis.spark.SpatialKey,geotrellis.vector.ProjectedExtent]  ```",6/17/16,1
aeffrig,"@pomadchin I can post a gist once I isolate the issue a bit more from my larger app. Currently my app submits and successfully runs multiple ETL jobs programmatically with SparkLauncher against our 1.6.1 cluster.  I then attempt to use SparkContext in my app after those ETL jobs complete and that's when I see ```java.util.concurrent.RejectedExecutionException: Task java.util.concurrent.FutureTask@4bd409ba rejected from java.util.concurrent.ThreadPoolExecutor@20b1f599[Running, pool size = 1, active threads = 0, queued tasks = 0, completed tasks = 1]        at java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2047)```I suspect a bad interaction launching a new SparkContext after the previous ones complete, but I need to investigate farther.",6/17/16,1
wsf1990,"When I use Etl to ingest data,Can I change the `Pyramid.Options`? I saw this code in the source code",6/20/16,1
wsf1990,"```val (nextLevel, nextRdd) = Pyramid.up(rdd, s, zoom)```this used the default options.",6/20/16,1
pomadchin,"@wsf1990 nope, we have an etl refactor pr https://github.com/geotrellis/geotrellis/pull/1553 , if you have some wishes you can write them there or in the pr, however I_Ñéll add these options settings",6/20/16,1
wsf1990,"@pomadchin So I want to rewrite the Etl, But it told me that `not found: type ?`",6/20/16,1
pomadchin,can you throw some code samples of what you are trying to do?,6/20/16,1
wsf1990,"```def save[      K: SpatialComponent: TypeTag,      V <: CellGrid: TypeTag: ? => TileMergeMethods[V]: ? => TilePrototypeMethods[V]      ](id: LayerId, rdd: RDD[(K, V)] with Metadata[TileLayerMetadata[K]], method: KeyIndexMethod[K])```",6/20/16,1
pomadchin,oh,6/20/16,1
pomadchin,we use kind projector ,6/20/16,1
pomadchin,https://github.com/non/kind-projector,6/20/16,1
pomadchin,"add into your build.sbt: ```scalaresolvers += Resolver.sonatypeRepo(""releases"")addCompilerPlugin(""org.spire-math"" % ""kind-projector"" % ""0.8.0"" cross CrossVersion.binary)```",6/20/16,1
pomadchin,should help,6/20/16,1
wsf1990,"OK,Thanks!",6/20/16,1
pomadchin,"@wsf1990 thank you ;) strogly recommend you to look into that pr, probably you have some thoughts about it",6/20/16,1
pomadchin,main point of it that we are moving from cli arguments to json defined etl args,6/20/16,1
wsf1990,"@pomadchin OK. I will,Thanks.",6/20/16,1
wsf1990,"@pomadchin can we use this json format now, And how to ingest the data by this version.",6/20/16,1
pomadchin,"@wsf1990 that stuff is in process; you can pull my pr, and build it locally; but i believe this etl refactor would be only in our next major release (as it is an api change)",6/20/16,1
wsf1990,"Got it, I will try",6/20/16,1
pomadchin,@wsf1990 thanks!,6/20/16,1
wsf1990,thanks for your hard work!,6/20/16,1
mshareghi,I'm using geodocker but I'm having problems getting the instances to start up correctly after they have been stopped.  I get the following error from my hadoop data node: copyFromLocal: Cannot create file/hdfsready._COPYING_. Name node is in safe mode.     -- any advice? ,6/20/16,1
pomadchin,@mshareghi can you checkout if datanode is availble?,6/20/16,1
mshareghi,"its stopped, and when I restart it it stops again after that message",6/20/16,1
mshareghi,How can I get the name node out of safe mode? Restart? Basically everything was working fine but my system shut down accidentally and now it's all hosed,6/20/16,1
pomadchin,you can try to force it to leave safe mode,6/20/16,1
pomadchin,`hdfs dfsadmin -safemode leave`,6/20/16,1
mshareghi,"Should I start the data node first, and then the name node ?",6/20/16,1
pomadchin,i think you can just try to stop data node,6/20/16,1
pomadchin,after that restart name node,6/20/16,1
pomadchin,"after that, check out `localhost:50070` there would be current namenode state displayed",6/20/16,1
mshareghi,"ok, thanks",6/20/16,1
pomadchin,"yep, if name node still would be in a safe mode, just log into the namenode container and run `hdfs dfsadmin -safemode leave`",6/20/16,1
pomadchin,after that start your data node,6/20/16,1
mshareghi,"@pomadchin awesome, works now. thanks!",6/20/16,1
pomadchin,@mshareghi glad to hear that!,6/20/16,1
aeffrig,"@pomadchin So I'm still trying to run a simple app compiled against 1.5.2, but running on a Spark 1.6.1 cluster, which you mentioned should be compatible with Geotrellis I believe.  However, when performing a spatial join I see my old friend ShuffledRDD back again```java.lang.reflect.InvocationTargetException        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)        at java.lang.reflect.Method.invoke(Method.java:497)        at org.apache.spark.deploy.worker.DriverWrapper$.main(DriverWrapper.scala:58)        at org.apache.spark.deploy.worker.DriverWrapper.main(DriverWrapper.scala)Caused by: java.lang.NoSuchMethodError: org.apache.spark.rdd.ShuffledRDD.<init>(Lorg/apache/spark/rdd/RDD;Lorg/apache/spark/Partitioner;)V        at geotrellis.spark.partition.SpacePartitioner.apply(SpacePartitioner.scala:65)        at geotrellis.spark.join.SpatialJoin$.join(SpatialJoin.scala:53)        at geotrellis.spark.join.SpatialJoinMethods.spatialJoin(SpatialJoinMethods.scala:20)        at ...```Spark-9043 added ClassTags to the ShuffledRDD constructor which might have something to do with it.  Any known workarounds?",6/20/16,1
lossyrob,"@ owcm it looks like you were trying to call `.toRasters` on an `RDD[(SpatialKey, ProjectedExtent)]`.",6/21/16,1
lossyrob,You can_Ñét convert ProjectedExtents to Rasters,6/21/16,1
lossyrob,As far as the error trying to create high zoom level geotiffs_Ñ_sounds like the geotiff you are trying to create is too big to fit into memory,6/21/16,1
lossyrob,"@akarmas right, you have to transform t (no need to cast) to tiles, instead of a sequence of (key, tile)",6/21/16,1
lossyrob,"```scalaval meanValues=  irdd.map { case (key, tile) =>    (key.getComponent[SpatialKey], tile)  }.groupBy(_._1)  .map{ case(key,tiles) =>     val localMean =         tiles           .map(_._2) // Maps to the 2nd element of the tuple, i.e. the tiles           .toSeq           .localMean    (key, localMean)   }```",6/21/16,1
owcm,@lossyrob Thanks for answering.,6/21/16,1
pomadchin,"@aeffrig hm, can you throw there some code sample, to repeat exception you have?",6/21/16,1
wsf1990,"I want to rasterize a geom which the feature type is Line,So dose geotrellis has some methods can do `buffer analysis`? thanks!",6/21/16,1
wsf1990,I had got the answer at `https://gitter.im/?at=56ca917de1786831677c6807` thanks!,6/21/16,1
akarmas,"@lossyrob Many thanks! Tested it and works perfect! I also tested with localVariance and everything runs smoothly!",6/21/16,1
lossyrob,:tada:,6/21/16,1
lossyrob,"very cool, glad it all is working for you now!",6/21/16,1
wsf1990,Dose the method of regionGroup has some bug? The compare of tile's png and use `tile.regionGroup.tile`'s png just like this,6/22/16,1
wsf1990,[![0.png](https://files.gitter.im/geotrellis/geotrellis/cUgF/thumb/0.png)](https://files.gitter.im/geotrellis/geotrellis/cUgF/0.png),6/22/16,1
wsf1990,[![1.png](https://files.gitter.im/geotrellis/geotrellis/TMTQ/thumb/1.png)](https://files.gitter.im/geotrellis/geotrellis/TMTQ/1.png),6/22/16,1
wsf1990,Why the `tile.regionGroup.tile`'s png just have some data at the top of the pic?,6/22/16,1
wsf1990,"I got it,It's caused by the render type.",6/22/16,1
pljplj,JTS/Scala question.  I'm trying to get Vector of Points from a MultiPoint.  I can't do it.  This is what I am trying,6/22/16,1
pljplj,"          case t: MultiPoint => {      // only keep points that intersect the edge            val top = t & topLine            val bot = t & bottomLine            val rit = t & rightLine            val lef = t & leftLine            val st = Set(top,bot,rit,lef) - NoResult   // need to unwind the set and turn into a Vector            Vector(t.points)          }",6/22/16,1
lossyrob,"I would say just do `t.intersection(MultiLine(topLine, bottomLine, rightLine, leftLine)) match { /// Handle result }`",6/22/16,1
aaron-santos,"Hi, I'm trying to build https://github.com/geotrellis/geotrellis-gdal. My build steps are git cloning the repo, cd-ing to geotrellis-gdal,  and then running ./sbt compile",6/22/16,1
lossyrob,"that should work, though I feel like it needs a bit of an update",6/22/16,1
aaron-santos,"haha, agreed",6/22/16,1
aaron-santos,"but I'm getting ```sbt.ResolveException: unresolved dependency: com.azavea.geotrellis#geotrellis-raster_2.10;0.10.0-SNAPSHOT: not foundunresolved dependency: com.azavea.geotrellis#geotrellis-spark_2.10;0.10.0-SNAPSHOT: not foundunresolved dependency: com.azavea.geotrellis#geotrellis-spark-etl_2.10;0.10.0-SNAPSHOT: not found```",6/22/16,1
aaron-santos,Is there a step that I'm missing that relates to these deps?,6/22/16,1
lossyrob,"well, whenever it is pointing at SNAPSHOT, that means it is based on geotrellis components that would have been published locally",6/22/16,1
lossyrob,"i would recommend though trying to change the version to `0.10.1`, and seeing how that goes",6/22/16,1
aaron-santos,Thx @lossyrob ! That worked :),6/22/16,1
lossyrob,sweet,6/22/16,1
wsf1990,why I convert a tile to ByteConstantNoDataCellType use `tile.convert(ByteConstantNoDataCellType)` then the result tile's NODATA value is 128 not -128?,6/23/16,1
wsf1990,"When I use `tile.asciiDraw()` to see it shows -128 ,but when I  use `GeoTiff(tile, extent, WebMercator).write(""path"")` to write a tif,it change to 128.Why?",6/23/16,1
pljplj,"@lossyrob  t.intersection(MultiLine(topLine, bottomLine, rightLine, leftLine)) doesn't compile. Nor are my scala skills sufficient to make it compile.  Assume that I have val st = Set(top,bot,rit,lef) - NoResult.  st contains a set with 2 PointResult.  How do I get the 2 entries as Point?",6/23/16,1
lossyrob,"```scalascala> import geotrellis.vector._import geotrellis.vector._scala> val s = Set(PointResult(Point(1,2)), PointResult(Point(2,3)))s: scala.collection.immutable.Set[geotrellis.vector.PointResult] = Set(PointResult(POINT (1 2)), PointResult(POINT (2 3)))scala> s.map { case PointResult(p) => p }res0: scala.collection.immutable.Set[geotrellis.vector.Point] = Set(POINT (1 2), POINT (2 3))```",6/23/16,1
lossyrob,@wsf1990 what was the tile type that you are converting from?,6/23/16,1
wsf1990,"Maybe ByteConstantNoDataCellType,When I use tile.asciiDraw() to see it shows many -128 ,after convert it shows many ND, and when I use GeoTiff(tile, extent, WebMercator).write(""path"") to write a tif,nodata value change to 128,and type change to ubyte",6/23/16,1
lossyrob,tile.convert is not a mutable method,6/23/16,1
lossyrob,you are capturing it into a new var yeah?,6/23/16,1
wsf1990,yes,6/23/16,1
lossyrob,ok,6/23/16,1
lossyrob,hmm,6/23/16,1
lossyrob,"so your saying if you do `gdalinfo` on the GeoTiff, it says it_Ñés NoData is 128?",6/23/16,1
wsf1990,yes,6/23/16,1
lossyrob,well that_Ñés concerning,6/23/16,1
lossyrob,"if there_Ñés a bug in the GEoTiff writer, it means it won_Ñét set the proper _ÑÒPIXELTYPE=SIGNEDBYTE_Ñù flag on the geotiff",6/23/16,1
lossyrob,and it would be interpreted as a unsigned byte,6/23/16,1
lossyrob,and then the nodata value of Byte.MinValue would be interpreted as 128,6/23/16,1
lossyrob,"if you could get it in a reproducable test case, I_Ñéll take a closer look into it",6/23/16,1
wsf1990,"ok,i'll find it",6/23/16,1
lossyrob,thanks!,6/23/16,1
wsf1990,"```val tile = ByteArrayTile.fill(-10, 12, 12)    GeoTiff(tile, Extent(10, 10, 11, 11), LatLng).write(""data\\1.tif"")```",6/23/16,1
wsf1990,just try this,6/23/16,1
lossyrob,"Perfect, thanks",6/23/16,1
wsf1990,maybe it's my mistake,6/23/16,1
wsf1990,I use qgis to show this tif's properties it showns that,6/23/16,1
wsf1990,but when I use gdalinfo it's correct,6/23/16,1
wsf1990,I'm sorrey to waste your time,6/23/16,1
pomadchin,hm o: and what_Ñés using `(t: Tile).findMinMax` ?,6/23/16,1
wsf1990,What is the meaning of `ND` in the `println(tile.asciiDraw)`,6/23/16,1
echeipesh,"NoData, it reads the tile as Int and anything that gets converted to int nodata gets _ÑÒND""",6/23/16,1
aeffrig,"So I [now have a gist](https://gist.github.com/aeffrig/f49cefdb5326e0917263a926cd71c14b) to reproduce the error with ShuffledRDD against Spark 1.6.1.  I also [have a branch with a possible but untested fix](https://github.com/geotrellis/geotrellis/compare/master...aeffrig:spark_1.6.1).  ",6/23/16,1
aeffrig,"For reference running the above gist results in```Exception in thread ""main"" java.lang.NoSuchMethodError: org.apache.spark.rdd.ShuffledRDD.<init>(Lorg/apache/spark/rdd/RDD;Lorg/apache/spark/Partitioner;)V        at geotrellis.spark.partition.SpacePartitioner.apply(SpacePartitioner.scala:65)        at geotrellis.spark.join.SpatialJoin$.join(SpatialJoin.scala:53)        at geotrellis.spark.join.SpatialJoinMethods.spatialJoin(SpatialJoinMethods.scala:20)        at name.aeffrig.SpatialSparkExample$.run(SpatialSparkExample.scala:66)        at name.aeffrig.SpatialSparkExample$.main(SpatialSparkExample.scala:32)        at name.aeffrig.SpatialSparkExample.main(SpatialSparkExample.scala)```",6/23/16,1
echeipesh,"oh yeah, I just ran into that last night. The problem is that we build against spark 1.5 where shuffle RDD does not class ClassTag params, but in spark 1.6 they actually changed the ShuffleRDD constructor to requrie ClassTags",6/23/16,1
echeipesh,https://github.com/apache/spark/blob/branch-1.5/core/src/main/scala/org/apache/spark/rdd/ShuffledRDD.scala#L40 vs https://github.com/apache/spark/blob/branch-1.6/core/src/main/scala/org/apache/spark/rdd/ShuffledRDD.scala#L42,6/23/16,1
echeipesh,sorry if I_Ñém missing a discussion that happened above me : ),6/23/16,1
echeipesh,@aeffrig is that the only thing you had to change to build vs spark 1.6 ?,6/23/16,1
pomadchin,@echeipesh mb we can make a separate branch with 1.6 support?,6/23/16,1
echeipesh,"The plan was to upgrade to 1.6 after _ÑÒ0.10.0_Ñù release, we just haven_Ñét looked at that yet. If that_Ñés all that takes there is no problem with merging that into master and bumping our support to 1.6",6/23/16,1
echeipesh,"@aeffrig the fix looks good, could you make a PR against geotrellis so we see what Travis makes of it ?",6/23/16,1
aeffrig,"Sure, I'll do that soon.",6/23/16,1
echeipesh,"awesome, thank you :)",6/23/16,1
aeffrig,pull request #1557 is out to address Spark 1.6 changes.,6/24/16,1
pomadchin,@aeffrig thanks for your pr,6/24/16,1
wsf1990,"I found that the Byte type in geotiff it's range is [0, 255] but in the scala it's range is [-128, 127],It's made the bug of ByteArrayTile.",6/27/16,1
wsf1990,The UByte type in the scala corresponding to the Byte type in the geotiff.  @pomadchin,6/27/16,1
pomadchin,"@wsf1990 hi, can you throw an example tiff?",6/27/16,1
wsf1990,"@pomadchin I use geotrellis write a tiff the tile's type is ByteTile,then ingest it to accumulo,but when read the tile form accumulo the minmax data change to (-128, some) not (-127, some)",6/27/16,1
wsf1990,"I got the reason, I ingest the data which the type is Byte and the nodata value is -128,But when I ingest it to the accumulo,Then read it from accumulo,the celltype change to int8raw and havn't nodata value.Why? @pomadchin ",6/27/16,1
wsf1990,"I guess the class of `Etl` read the tile and havn't read the nodata value,So My data changed because when cut tile it do resample,And the result of the data havn't nodata value",6/27/16,1
wsf1990,[![temp_tile.tif](https://files.gitter.im/geotrellis/geotrellis/0V9c/thumb/temp_tile.tiff)](https://files.gitter.im/geotrellis/geotrellis/0V9c/temp_tile.tif),6/27/16,1
wsf1990,this is the tile which read from accumulo,6/27/16,1
pomadchin,@wsf1990 i believe we have -128 nd value by default for a byte type,6/27/16,1
wsf1990,"yes,but why the ingested tile havn't the nodata value",6/27/16,1
wsf1990,when ingest the tiff dose need the conf of celltype?,6/27/16,1
lossyrob,that might be a problem in the codec,6/27/16,1
lossyrob,"If the problem is having a ByteConstantNoData tile, writing it to the Avro format, reading it back, and the cell type changing, that would be pretty easy to spot in a unit test",6/27/16,1
lossyrob,,6/27/16,1
lossyrob,"> I found that the Byte type in geotiff it's range is [0, 255]That is only true of Byte type without the `PIXELTYPE=SIGNEDBYTE` geotiffs. GeoTiffs don_Ñét have a signed byte concept, so there_Ñés a special tag that says it_Ñés to be interpreted as a signed byte. Java bytes are signed by default, so we have a special `UByte` type to state that it_Ñés unsigned",6/27/16,1
wsf1990,"Yes,I found that,Maybe it's my mistake, But I use `Etl` to ingest a tiff which type is byte and  with `PIXELTYPE=SIGNEDBYTE` and nodata value is -128.But when read from accumulo, the celltype change to int8raw and havn't nodata value.",6/27/16,1
owcm,"I am trying to to render aspect for a elevation layer I have stored in Avro format. Code is as follows:```  def aspect = handleExceptions(missingTileHandler) {    pathPrefix(Segment / IntNumber / IntNumber / IntNumber) { (layer, zoom, x, y) =>      val key = SpatialKey(x, y)      val layerId = LayerId(layer, zoom)      get {        import geotrellis.raster._        respondWithMediaType(MediaTypes.`image/png`) {          complete {            getTile(layerId, key).flatMap { tile =>              val aspectColorBreaks = ConfigFactory.load().getString(""aspect.colorbreaks"")              getColorMapDouble(aspectColorBreaks).map { colorMap =>                val cs = new CellSize(1.0, 1.0)                val reclassed =                  tile.aspect( cs )                Some(reclassed.renderPng(colorMap).bytes)              }            }          }        }      }    }  }```Question is operations like slope, and aspect for tile take a Cellsize value.   What is the best way to obtain or derive the CellSize?Thanks in advance.   ",6/27/16,1
echeipesh,"@owcm You need to grab the the layer metadata from the attribute store it would look something like: ```scalaval md = attributeStore.readMetadata[TileLayerMetadata[SpatialKey]](layerId)val cs: CellSize = md.layout.cellSize```the metadata access is cached in attribute store so you don_Ñét need to worry too much about calling that often.",6/27/16,1
owcm,"@echeipesh  Thanks, so I will not be taking much of a performance hit",6/27/16,1
echeipesh,"@owcm: yup, you_Ñéll just be paying the decoding cost from JSON to the case class per call, but that wouldn_Ñét be my first concern on performance : )",6/27/16,1
redarma,"hello guys, I was reading about Space-filling curves applied to satellite images optimization in query and compression, and I found implementations in geotrellis about of Hilbert curves, ZCurves. Exist another implementation of spatial indexation, compression methods, or another project, I want to know for take a reference for my study and contribute, thanks a lot for your help.",6/28/16,1
owcm,"@echeipesh Interesting, what would be in your first concern.  Something in code segment I should be concerned about. :)",6/28/16,1
echeipesh," @owcm: heh, I just meant that IO will almost always dominate code like this, so parsing JSON AST into a case class instance would barely register. But as far as code above there is nothing wrong with it. You could move the `aspectColorBreaks` var outside of the service, basically everything that is inside a spray directive that is parametarized will be re-calculated on every request. Not totally sure if typesafe config caches in the manner that we do, so could be splitting hairs there. Also you will eventually want to return a future on a different execution context than the spray actors sit on, unless thats what `getTile` already does. That would allow spray to remain responsive under heavier load.",6/28/16,1
echeipesh,@redarma glad you found us! Let us know if you have any questions. I am curious what you are studying.,6/28/16,1
owcm,"@echeipesh Thanks, after looking at the code I moved the  aspectColorBreaks outside.  The getTile is setup as a future already. Thanks for the pointers.",6/28/16,1
wsf1990,"I read a tile from accumulo.when use this code to show celltype:```val tile = tileReader.reader[SpatialKey, Tile](LayerId(name, zoom)).read(key)println(""metadata  "" + attributeStore.read[TileLayerMetadata[SpatialKey]](LayerId(name, zoom), Fields.metadata).cellType)println(""tile  "" + tile.cellType)```it showd me this:```bashmetadata  int8tile  int8ud0```Why use metadata and use tile.cellType get the different type?",6/28/16,1
echeipesh,"@wsf1990 there is nothing that forces metadata for the layer to match the tiles when they are written, generally care must be taken to make sure the metadata stays valid as you do the transformations to the layer. Since it_Ñés pretty tough to get `ud0`, user defined no-data, without knowing that you_Ñére doing it, I would guess that `TileLayerMetadata` is modified at some point before the layer was written.",6/28/16,1
wsf1990,"@echeipesh  I ingest the tiff which is byte type and nodata value is -128 to accumulo and add --cellType int8 to conf,then read it,Why the tile's cellType is int8ud0",6/28/16,1
redarma,"@owcm:,  Well I'm studying about indexation methods of satellite images, the common methods are Peano, Hilbert, Zorder, etc.   I would like to implement or improve a method of indexing in their representation, compression or query, mix algorithms, for example I was watching some way to represent through Space-filling tree as the R-tree or quadtree, if you have any recomendation or idea thanks.",6/28/16,1
wsf1990,"How dose this code in class of `AvroEncoder` know what is the cellType of Tile or dose other code determain the tile's cellType when read from accumulo?```def fromBinary[T: AvroRecordCodec](writerSchema: Schema, bytes: Array[Byte]): T = {    val format = implicitly[AvroRecordCodec[T]]    val schema = format.schema    val reader = new GenericDatumReader[GenericRecord](writerSchema, schema)    val decoder = DecoderFactory.get().binaryDecoder(decompress(bytes), null)    val rec = reader.read(null.asInstanceOf[GenericRecord], decoder)    format.decode(rec)  }```",6/28/16,1
pomadchin,"@wsf1990 you see there a context bound in the function definition: `AvroRecordCodec`, definition `def fromBinary[T: AvroRecordCodec]` eqls to `def fromBinary[T](implicit ev1: AvroRecordCodec[T])` that means that it requires AvroRecordCodec for a type T: https://github.com/geotrellis/geotrellis/blob/master/spark/src/main/scala/geotrellis/spark/io/avro/codecs/TileCodecs.scala",6/28/16,1
pomadchin,mb you can throw there an example tile you are trying to ingest?,6/28/16,1
wsf1990,[raster0.zip](https://files.gitter.im/geotrellis/geotrellis/Vmqz/raster0.zip),6/28/16,1
wsf1990,Tanks!,6/28/16,1
wsf1990,I sue this conf to ingest this tiff `--input hadoop --format geotiff --cache NONE -I path=file:path               --output accumulo -O instance=instance table=table user=root password=pass zookeeper=host               --layer road --pyramid --crs EPSG:3857 --layoutScheme tms --cellType int8`,6/28/16,1
wsf1990,if don't use `--cellType int8` the cellType change to int8raw,6/28/16,1
wsf1990,When use `--cellType int8` and read from accumulo the cellType is 'int8ud0',6/28/16,1
wsf1990,@pomadchin Have you try? Dose it's a bug or my mistake?,6/28/16,1
pomadchin,"@wsf1990 sry, would test it in a couple of minutes ",6/28/16,1
wsf1990,"OK,Thanks!",6/28/16,1
pomadchin,@wsf1990 thank you for your reports!,6/28/16,1
pomadchin,"@wsf1990 checked on master branch on modifyed etl refactor branch (json based) and had no problems; a bit later would check on older gt version, can you try it using current master branch?",6/28/16,1
wsf1990,I use 0.10.0-RC4,6/28/16,1
pomadchin,w0w; can you try 0.10.0 or 0.10.1 ?,6/28/16,1
wsf1990,ok,6/28/16,1
wsf1990,"@pomadchin  I changed the version to 0.10.1,But when I ingest the tiff,It took me this error:",6/28/16,1
wsf1990,"```Exception in thread ""main"" org.rogach.scallop.exceptions.IncompleteBuildException: It seems you tried to get option value before you constructed all options (maybe you forgot to call .verify method?). Please, move all extraction of values to after 'verify' method in ScallopConf.```",6/28/16,1
wsf1990,"@pomadchin when you ingest the data,Dose you add the conf of `--cellType int8`",6/28/16,1
pomadchin,@wsf1990 no,6/28/16,1
wsf1990,Why I got the int8raw again,6/28/16,1
wsf1990,And it told me need call the function of verify,6/28/16,1
wsf1990,"@pomadchin Dose it has different between the dir path or the every tiff path in conf of input?Because I have some small tiff which everyone is  a part of region,So I want to ingest them once.",6/28/16,1
wsf1990,@pomadchin Maybe the question is how to stitch tiff in the same layer when cut tile,6/28/16,1
owcm,@redarma I think Eugene or someone else on the Geotrellis team would be more qualified to answer.,6/28/16,1
schBen,"I have processed a raster image and got an RDD[(ProjectedExtent, Tile)]. What is the fastest and most straightforward way to write that out to a local single band geotiff?",6/28/16,1
lossyrob,you want one geotiff per tile?,6/28/16,1
pomadchin,"@wsf1990 can you throw there codesample, or smth, to follow your steps?",6/28/16,1
lossyrob,"@schBen if so, something like```scalaval rdd: RDD[(ProjectedExtent, Tile)] = ???rdd.collect.foreach { case (projectedExtent, tile) =>  GeoTiff(tile, projectedExtent.extent, projectedExtent.crs).write(s_ÑÒ/to/some/path/${projectedExtent.extent.xmin}.tif_Ñù)}```",6/28/16,1
lossyrob,"the file name being unique is a question, I just put in the extent xmin for the name",6/28/16,1
lossyrob,"otherwise you should tile them, stitch them and write them out",6/28/16,1
lossyrob,if you want one big one,6/28/16,1
lossyrob,"@redarma we implement Z order and Hilbert only (well, row major, but that_Ñés no fun); I don_Ñét know of another library that implements another curve_Ñ_actually google_Ñés s2 might, but I think those are just ideas on top of hilbert",6/28/16,1
lossyrob,"there_Ñés a difference between a sort of _ÑÒstored_Ñù index like an R-tree and a computed index like Z order and hilbert. We have a spatial index (which is a wrapper around JTS STRtree) that implements an R-tree, specifically an Sort-tile-recursive R tree",6/28/16,1
lossyrob,"as far as ""compression or query, mix algorithms_Ñù, there_Ñés a lot in GeoTrellis, so you_Ñéd have to maybe ask more specifically what you_Ñére looking for",6/28/16,1
schBen,@lossyrob One big one at the moment. How do I do the tiling best?,6/28/16,1
wsf1990,,6/28/16,1
wsf1990,"@pomadchin I have 20 tifs, whoes name from ranster0.tif to raster19.tif```...raster4raster0 raster1 raster2 raster3 ..```they can be stitched like that,So I want to ingest them to one layer.If I ingest them one by one, the position at the edge of them will be empty without data,because the tile will be override by their neibourhood.My english is poor.Do you understand what I want to say?",6/28/16,1
pomadchin,"ye, definitely",6/28/16,1
pomadchin,"so, to ingest them one by one you need to use `LayerUpdater`",6/28/16,1
wsf1990,Can you tell me how to use this?,6/28/16,1
pomadchin,"@wsf1990 sure, sec",6/28/16,1
lossyrob,"@schBen let me write it up as an example for the docs, will let you know when I make the PR",6/28/16,1
lossyrob,@pomadchin might be good to do the same with yours,6/28/16,1
pomadchin,"@wsf1990 sorry for the delay, had to go out of the laptop ._.",6/28/16,1
pomadchin,@lossyrob ? One by one tile ingest example?,6/28/16,1
wsf1990,Thanks! @pomadchin ,6/28/16,1
lossyrob,@pomadchin does it have to be one by one ingest? or one by one merge? Sorry I wasn_Ñét following the question too closely,6/28/16,1
lossyrob,but if it_Ñés a common use case it would be good to have a code example we could refer someone to later/someone could discover and learn from in the docs,6/28/16,1
pomadchin,"@lossyrob gotcha, one by one layer update, that could be useful in strong mem limits cases",6/28/16,1
lossyrob,"@pomadchin @echeipesh this actually is a use case for a streaming ingest as well, I think",6/28/16,1
echeipesh,"Hmm, yeah, sounds like it. Essentially if I follow correctly the source data is on multiple regions/locations so you can ingest it at the same time ?",6/28/16,1
lossyrob,needs merging,6/28/16,1
lossyrob,"@schBen check this out, see if it explains how to do what you_Ñére looking for https://github.com/geotrellis/geotrellis/pull/1564/files#diff-246024fc933a9b8274f9ca524e1feaf4R25",6/28/16,1
pomadchin,"@wsf1990 that would be a manuall example, with explaining steps; so if you want to make a stream / modify this example feel free to throw your ideas ```scala    val source = sc.hadoopTemporalGeoTiffRDD(""path to file"")    val (_, md) = TileLayerMetadata.fromRdd[TemporalProjectedExtent, Tile, SpaceTimeKey](source, FloatingLayoutScheme(256))    // Keep the same number of partitions after tiling.    val tilerOptions = Tiler.Options(resampleMethod = NearestNeighbor)    val tiled = ContextRDD(source.tileToLayout[SpaceTimeKey](md, tilerOptions), md)    val (zoom, reprojected) = tiled.reproject(WebMercator, ZoomedLayoutScheme(WebMercator), NearestNeighbor)    val writer: LayerWriter[LayerId] = ???    // let_Ñés say we want an everyday index, but loading one tile, we have limited keyIndex space by tiles metadata information    val keyIndex: KeyIndexMethod[SpaceTimeKey] = ZCurveKeyIndexMethod.byDay()    // we increased in this case date time range, but you can modify anything in your _ÑÒpreset_Ñù key bounds    val updatedKeyIndex = keyIndex.createIndex(md.bounds match {      case kb: KeyBounds[SpaceTimeKey] => KeyBounds(        kb.minKey.copy(instant = DateTime.parse(""2000-01-01"").getMillis),        kb.maxKey.copy(instant = DateTime.parse(""2020-01-01"").getMillis)    })    // writing a layer with larger than default keyIndex space    writer.write[SpaceTimeKey, Tile, TileLayerMetadata[SpaceTimeKey]](LayerId(""layer to be updated"", zoom), reprojected, updatedKeyIndex)    // now we can just update layer     val source2 = ???    // same steps there, to read, retile tiles    val updater: LayerUpdater[LayerId] = ???    updater.update[SpaceTimeKey, Tile, TileLayerMetadata[SpaceTimeKey]](LayerId(""layer to be updated"", zoom))```",6/28/16,1
lossyrob,Roadmap Discussion issue: https://github.com/geotrellis/geotrellis/issues/1565,6/28/16,1
lossyrob,"@/all if you could take a look at that roadmap issue (https://github.com/geotrellis/geotrellis/issues/1565), and give your thoughts/ideas/feedback, it would be greatly appreciated.",6/28/16,1
wsf1990,@pomadchin Tanks«_ÅBut it told me `Cannot find AvroRecordCodec for SpatialKey. Try to import geotrellis.spark.io.avro.codecs.Implicits._`.Dose I need add some import?,6/29/16,1
pomadchin,"@wsf1990 ```scalaimport geotrellis.spark._import geotrellis.spark.io._import geotrellis.spark.io.{file / hadoop / accumulo / cassandra / s3}._```",6/29/16,1
wsf1990,"@pomadchin I'm sorry,It show the same error when I add these import code.",6/29/16,1
pomadchin,"hm, import geotrellis.spark.io._ should be enough",6/29/16,1
pomadchin,have you tried to import geotrellis.spark.io.avro.codecs.Implicits._ ?,6/29/16,1
wsf1990,yes,6/29/16,1
pomadchin,hm sounds weird; sec,6/29/16,1
wsf1990,"this is my code```def saveOrUpdate  [  SpatialKey,//K: AvroRecordCodec: Boundable: JsonFormat: ClassTag,  Tile,//V: AvroRecordCodec: ClassTag,  TileLayerMetadata[SpatialKey]//M: JsonFormat: GetComponent[?, Bounds[SpatialKey]]: Mergable  ]  (id: LayerId, rdd: RDD[(SpatialKey, Tile)] with Metadata[TileLayerMetadata[SpatialKey]])= {    implicit val sc = rdd.sparkContext    def getInstance(props: Map[String, String]): AccumuloInstance =      AccumuloInstance(props(""instance""), props(""zookeeper""), props(""user""), new PasswordToken(props(""password"")))    val updater: AccumuloLayerUpdater = AccumuloLayerUpdater(getInstance(conf.outputProps))    import geotrellis.spark.io._    updater.update(id, rdd)  }```",6/29/16,1
pomadchin,"@wsf1990 https://gist.github.com/pomadchin/c2c8346ab0ef1ece11cf4432a87d14ec",6/29/16,1
pomadchin,"@wsf1990 ```scalaobject updateOrWrite {  import geotrellis.raster._  import geotrellis.spark._  import geotrellis.spark.io._  import geotrellis.spark.io.index._  import geotrellis.spark.io.accumulo._  import geotrellis.spark.merge._  import geotrellis.spark.io.avro._  import geotrellis.util._  import org.apache.spark.rdd.RDD  import spray.json.JsonFormat  import scala.reflect.ClassTag    def apply[    K: AvroRecordCodec: Boundable: JsonFormat: ClassTag,    V: AvroRecordCodec: ClassTag,    M: JsonFormat: GetComponent[?, Bounds[K]]: Mergable  ](id: LayerId, rdd: RDD[(K, V)] with Metadata[M], keyIndex: KeyIndex[K])= {    implicit val sc = rdd.sparkContext    val updater: AccumuloLayerUpdater = ???    val writer: AccumuloLayerWriter = ???    try { updater.update(id, rdd) } catch {      case e: LayerNotFoundError => {        writer.write(id, rdd, keyIndex)      }    }  }}```",6/29/16,1
wsf1990,"Thanks! But the error go on when I call `UpdateOrWrite(...)`.I'm so confused!",6/29/16,1
pomadchin,pass implicits from gist,6/29/16,1
pomadchin,examples of imports for spacetime key / spatial key,6/29/16,1
wsf1990,"I add all import of yours in gist,But the error go on.",6/29/16,1
wsf1990,"```Error:(216, 20) not enough arguments for method apply: (implicit evidence$1: geotrellis.spark.io.avro.AvroRecordCodec[K], implicit evidence$2: geotrellis.spark.Boundable[K], implicit evidence$3: spray.json.JsonFormat[K], implicit evidence$4: scala.reflect.ClassTag[K], implicit evidence$5: geotrellis.spark.io.avro.AvroRecordCodec[V], implicit evidence$6: scala.reflect.ClassTag[V], implicit evidence$7: spray.json.JsonFormat[geotrellis.spark.TileLayerMetadata[K]], implicit evidence$8: geotrellis.util.GetComponent[geotrellis.spark.TileLayerMetadata[K],geotrellis.spark.Bounds[K]], implicit evidence$9: geotrellis.spark.merge.Mergable[geotrellis.spark.TileLayerMetadata[K]])Unit in object UpdateOrWrite.Unspecified value parameters evidence$1, evidence$2, evidence$3, ...      UpdateOrWrite(id, rdd, getInstance(conf.outputProps))                   ^```",6/29/16,1
wsf1990,When I buid it show this:,6/29/16,1
pomadchin,"```scalaobject Test {  import org.apache.spark.rdd.RDD  import geotrellis.raster._  import geotrellis.spark._  import geotrellis.spark.io._  import geotrellis.spark.io.index._  val id: LayerId = ???  val rdd: RDD[(SpatialKey, Tile)] with Metadata[TileLayerMetadata[SpatialKey]] = ???  val keyIndex: KeyIndex[SpatialKey] = ???  UpdateOrWrite(id, rdd, keyIndex)}```",6/29/16,1
pomadchin,it works for me,6/29/16,1
pomadchin,"I am not sure what means `UpdateOrWrite(id, rdd, getInstance(conf.outputProps))` in your code",6/29/16,1
pomadchin,there should be passed KeyIndex with predefined keyIndex space and not accumulo instance,6/29/16,1
pomadchin,check you function arguments,6/29/16,1
wsf1990,"I just change the code of `outputPlugin(currentId, rdd, method, conf.outputProps)` in `Etl` to `UpdateOrWrite(currentId, rdd, getInstance(conf.outputProps))`",6/29/16,1
wsf1990,The `UpdateOrWrite` I changed to:,6/29/16,1
wsf1990,"```object UpdateOrWrite {  import geotrellis.raster._  import geotrellis.spark._  import geotrellis.spark.io._  import geotrellis.spark.io.index._  import geotrellis.spark.io.accumulo._  import geotrellis.spark.merge._  import geotrellis.spark.io.avro._  import geotrellis.util._  import org.apache.spark.rdd.RDD  import spray.json.JsonFormat  import scala.reflect.ClassTag  def apply[  K: AvroRecordCodec: Boundable: JsonFormat: ClassTag,  V: AvroRecordCodec: ClassTag,  M: JsonFormat: GetComponent[?, Bounds[K]]: Mergable  ](id: LayerId, rdd: RDD[(K, V)] with Metadata[M]/*, keyIndex: KeyIndex[K]*/, instance: AccumuloInstance)= {    implicit val sc = rdd.sparkContext    val updater: AccumuloLayerUpdater = AccumuloLayerUpdater(instance)//    val writer: AccumuloLayerWriter = AccumuloLayerWriter(instance, )    try { updater.update(id, rdd) } catch {      case e: LayerNotFoundError => {//        writer.write(id, rdd, keyIndex)      }    }  }}```",6/29/16,1
wsf1990,Just for test Update,6/29/16,1
pomadchin,should work,6/29/16,1
pomadchin,sent you compiling examples,6/29/16,1
wsf1990,[LayerEtl.scala](https://files.gitter.im/geotrellis/geotrellis/NIRR/LayerEtl.scala),6/29/16,1
wsf1990,[UpdateOrWrite.scala](https://files.gitter.im/geotrellis/geotrellis/NIRR/UpdateOrWrite.scala),6/29/16,1
pomadchin,@wsf1990 https://gist.github.com/pomadchin/c2c8346ab0ef1ece11cf4432a87d14ec#file-updateorwrite-scala,6/29/16,1
pomadchin,updated example,6/29/16,1
pomadchin,it compiles,6/29/16,1
pomadchin,._.,6/29/16,1
pomadchin,checkout implicits / generic implicits carefully,6/29/16,1
wsf1990," :worried:  My scala version:`Scala compiler version 2.9.2 -- Copyright 2002-2011, LAMP/EPFL ` Dose it caused by this?",6/29/16,1
pomadchin,i believe in your build you use 2.10 version,6/29/16,1
pomadchin,what_Ñés your build.sbt scala version?,6/29/16,1
wsf1990,"scalaVersion := Properties.propOrElse(""scala.version"", ""2.10.6"")crossScalaVersions := Seq(""2.11.8"", ""2.10.6"")",6/29/16,1
pomadchin,then it_Ñés ok,6/29/16,1
pomadchin,sbt downloads scala compiler you provided in your build file,6/29/16,1
wsf1990,"sure,Why it dosen't work for my code.I'm so confused.",6/29/16,1
wsf1990,Can the code which I uploaded work well in your envirment?,6/29/16,1
pomadchin,you missed implicits in your save function definition @wsf1990,6/29/16,1
pomadchin,"```scaladef save[  K: SpatialComponent: TypeTag: AvroRecordCodec: Boundable: JsonFormat,  V <: CellGrid: TypeTag: ? => TileMergeMethods[V]: ? => TilePrototypeMethods[V]: AvroRecordCodec](id: LayerId, rdd: RDD[(K, V)] with Metadata[TileLayerMetadata[K]], method: KeyIndexMethod[K]): Unit = ???```",6/29/16,1
wsf1990,What dose I need implicits ?,6/29/16,1
pomadchin,"just replace your save function definition with save function definition i throwed above",6/29/16,1
pomadchin,"Context Bounds _are_ implicits ```scaladef f[T: M] = ??? // same as def f[T](implicit ev0: M[T]) = ???```",6/29/16,1
pomadchin,that_Ñés what i compiled: https://gist.github.com/pomadchin/22d34853130ad17294af7e90f63de481,6/29/16,1
pomadchin,it works ,6/29/16,1
pomadchin,._. so just be careful with context bounds / implicits in functions scope,6/29/16,1
wsf1990,Thanks! it work well.I was use C# in older. this year changed to scala just for use Geotrellis.,6/29/16,1
lossyrob,@wsf1990 :) I made a similar switch. I hope you end up enjoying Scala as much as I now do!,6/29/16,1
pomadchin,@wsf1990 ye we appreciate your questions ;),6/29/16,1
wsf1990,"@lossyrob @pomadchin  Yes,In some features scala is similar to C#.I'm enjoying scala and very like Geotrellis which is an excellent opensource frame.Thanks!",6/29/16,1
jamesmcclain,What is the easiest way to project a SpaceTime rdd into a Spatial rdd (I am not concerned about how collisions are handled)?,6/29/16,1
lossyrob,https://github.com/geotrellis/geotrellis/blob/master/spark/src/main/scala/geotrellis/spark/filter/SpaceTimeRasterToSpatialMethods.scala#L31,6/29/16,1
wsf1990,"I have succeed build the update program,But it told me this error:",6/30/16,1
wsf1990,"```Exception in thread ""main"" geotrellis.spark.io.package$LayerOutOfKeyBoundsError: Updating rdd is out of the key index space for Layer(name = ""road"", zoom = 12): KeyBounds(SpatialKey(2911,1637),SpatialKey(2951,1677)). You must reindex this layer with large enough key bounds for this update```",6/30/16,1
wsf1990,I think it's caused by rdd extent,6/30/16,1
pomadchin,"@wsf1990 definitely, that_Ñés why you need to pass your KeyIndex, with wider keyBounds than your rdd",6/30/16,1
pljplj,"JTS/Scala question again.  I get a compile error saying ""type mismatch; found : Uint required: geotrellis.vector.Geometry"".  It seems that it doesn't like my Point or MultiPoint in the following code.  How do I make this work?",6/30/16,1
pljplj,"  def normalize(tile : Geometry): Geometry = {    val coords = tile match {      case t: GeometryCollection => {        t.normalized()      }      case t: MultiPolygon => {        t.normalized()      }      case t: Polygon => t.normalized()      case t: Line => {        t.normalized()      }      case t: MultiLine => {        t.normalized()      }      case t: Point => {        t      }      case t: MultiPoint => {        t      }    }  }",6/30/16,1
pljplj,Actually I just figured it out.  I made the last line coords.asInstanceOf[Geometry];  All is well,6/30/16,1
lossyrob,"you could also just drop the `val coords =`, and make the value of the match statement expression the return value of the method",6/30/16,1
tomauer,Is this a good place to ask some general questions about geotrellis?,6/30/16,1
lossyrob,for sure! this is the place.,6/30/16,1
enjdev,"Hi, I've been working on a proof of concept using GeoTrellis off and on for a while now.  It's gone quite well, but I have a few questions.  I don't know how much info you need, so this could get long...I tried ingesting data from a tif file using spark-etl 0.10.1 and scala 2.11, and got the same error a couple other people reported:```Exception in thread ""main"" org.rogach.scallop.exceptions.IncompleteBuildException: It seems you tried to get option value before you constructed all options (maybe you forgot to call .verify method?). Please, move all extraction of values to after 'verify' method in ScallopConf.```To get around this, I changed scallop back to version 0.9.5 from 1.0.0 - should I have just changed my command somehow?  I used:`--input hadoop --format geotiff --cache NONE -I path=""hdfs://path"" --output accumulo -O instance=""instance"" table=""SmallTest"" user=""root"" password=""password"" zookeeper=""zookeeper"" strategy=""socket"" --layer test1 --crs EPSG:3857 --layoutScheme floating`",6/30/16,1
enjdev,"Then I tried to get the value of a point, but it seems like the data was shifted by half a pixel.  From gdalinfo, the extents of the file are: Lower Left  (   0.5000000,   0.5000000)Upper Right (  10.5000000,  10.5000000)And from the GeoTrellis metadata the extent appears to be shifted:Extent(1.0, 0.0, 11.0, 10.0)This picture shows the upper left corner of a small test input tif file and four corner points output from GeoTrellis that appear to be shifted half a pixel from the pixel centers.  It also shows a point that I sampled using the NearestNeighbor resample method.  It returned the top left pixel value instead of the value from the pixel it is on.",6/30/16,1
enjdev,[![test.png](https://files.gitter.im/geotrellis/geotrellis/bMFk/thumb/test.png)](https://files.gitter.im/geotrellis/geotrellis/bMFk/test.png),6/30/16,1
enjdev,"Am I doing something wrong, or is this a bug?  Maybe the 'pixel as point' property wasn't accounted for properly?",6/30/16,1
enjdev,"Oh, and here's the tif file:",6/30/16,1
enjdev,[![test.tif](https://files.gitter.im/geotrellis/geotrellis/H1M2/thumb/test.tiff)](https://files.gitter.im/geotrellis/geotrellis/H1M2/test.tif),6/30/16,1
lossyrob,"@enjdev the scallop bug you mentioned is running into the bug this PR fixes, I think (https://github.com/geotrellis/geotrellis/pull/1571)",6/30/16,1
lossyrob,I_Ñém reading in the GeoTiff and it is giving me the shifted extent,6/30/16,1
lossyrob,where did this geotiff come from?,6/30/16,1
lossyrob,"when I do a gdal_translate on it, I get a weird warning",6/30/16,1
lossyrob,"```bash~/proj/lt/cq gdal_translate ~/Downloads/test.tif ~/Downloads/test2.tifInput file size is 10, 100Warning 1: Mercator projection should not define both StdParallel1 and ScaleAtNatOrigin.Using StdParallel1 and ignoring ScaleAtNatOrigin....10...20...30...40...50...60...70...80...90...100 - done.```",6/30/16,1
lossyrob,seems like it might have a geotiff property that we aren_Ñét reading like GDAL is,6/30/16,1
enjdev,"I made a bil file as a hypothetical example using random numbers in Matlab, then I used GlobalMapper to turn it into a tif file.",6/30/16,1
pomadchin,"@enjdev @lossyrob yep, #1571 fixes that scallop bug",6/30/16,1
enjdev,@pomadchin excellent - thanks!,6/30/16,1
lossyrob,@enjdev not sure what_Ñés going on with the offset - created an issue with a failing unit test: https://github.com/geotrellis/geotrellis/issues/1574,6/30/16,1
enjdev,"@lossyrob  thanks, I look forward to the fix!",7/1/16,1
lossyrob,any help you can give tracking it down would make the fix happen faster for sure ;),7/1/16,1
tomauer,"Geotrellis looks amazing and I'm thinking of using it for a web GIS app. However, I have no experience with Scala. Is it possible to deploy our own instance of Geotrellis and primarily use the REST API? Load in some rasters and hit it with REST calls?",7/1/16,1
tomauer,"The basic functions are passing in polygons and getting summary statistics on raster subsets. Maybe eventually, doing some simple raster algebra.",7/1/16,1
pomadchin,"@tomauer hi! yes, you can; so geotrellis is a library you may use to build your own applications: * https://github.com/geotrellis/geotrellis-chatta-demo* https://github.com/geotrellis/geotrellis-landsat-emr-demo* https://github.com/lossyrob/geotrellis-simple-tile-server",7/1/16,1
pomadchin,"if you are planing to have smth like a server to request information related to ingested tiles, your app would consist of two logical parts: ingest process (first you need to ingest your tiles in a prefered backend), and server",7/1/16,1
tomauer,That's great. Thanks for the demo git links.,7/1/16,1
pomadchin,"no problems, any questions are appreciated ;)",7/1/16,1
tomauer,Are there any examples of documentation for the REST ends for the geotrellis functions? Or does one have to write their own REST ends?,7/1/16,1
pomadchin,"we implemented tile server (rgb, ndvi, other indexes, just basing on ingested layers and making calculations on request) with some additional routes, to get neccesary information about tiles (like histogram, summary), so probably you can find nessesary examples on links ",7/1/16,1
lossyrob,"@tomauer but to answer your question I think, there is no server that you can just stand up and then access through a REST interface, without getting your hands a bit dirty with Scala :) those links are to examples of REST services made with GeoTrellis, and you_Ñéll have to work with a project like that in order to get a server up running the endpoints that you_Ñéd like to hit",7/1/16,1
tomauer,"Thanks, to the both of you. Getting dirty with Scala and these examples is a more promising route than writing my own services from scratch in Python, I think.",7/1/16,1
tomauer,"I've spent the week fighting with WPS and parameterized SQL views in Geoserver and it's just not the right solution. While there's more programming overhead for me here, this is a much more promising, modern solution.",7/1/16,1
lossyrob,Sound like you_Ñére headed down the right path to me :) but I may be biased ,7/1/16,1
tomauer,":) I think you are, but that's reasonable, I think you've got a good thing on your hands. I'm sure I'll be back with more questions, once I jump in. This gitter is great too.",7/1/16,1
lossyrob,for sure. just hit us up here when you have questions or get stuck,7/1/16,1
owcm,"I have been using a piece of code to create layers for me in HDFS.  So I have been using HadoopLayerWriter and HadoopAttributeStore.  Code worked great for me when I ingested over 500+  Geotiffs with 1M resolution, 1 meg each file.   Created the Avro store no problem and I was able to read from it.   I tried creating a new later using Geotiffs with 50cm resolution, 2 files 271 megs each.  The Avro Store created as zoom levels 0 through 18 but when I try retrieving data using my service layer nothing is rendered at levels 0 through 10 and then I start getting random tiles showing up as zoom down to 18.I created a Gist at https://gist.github.com/owcm/6ef74c60e9b077886db7ad4559c73207   This was run in a Spark environment.  I don't see any obvious errors in the logs.   Is there anyway I can validate the integrity of the Avro layer created?",7/1/16,1
pomadchin,@owcm and what gt version do you use?,7/1/16,1
owcm,@pomadchin Currently using 0.10.1,7/1/16,1
owcm,@pomadchin Is there something newer?,7/1/16,1
pomadchin,@owcm believe no ):,7/1/16,1
pomadchin,just to try your codesample,7/1/16,1
owcm,@pomadchin Sorry?  You said your trying my code sample?   Would making the Geotiffs I used be helpful?,7/1/16,1
pomadchin,"ye, mb you can make a min example with tiffs you ingesting to repeat your case?",7/1/16,1
owcm,@pomadchin  I will try to create a min example,7/1/16,1
jamesmcclain,"Given an RDD of `SpaceTimeKey`s,  it appears that the [toSpatial method](https://github.com/geotrellis/geotrellis/blob/master/spark/src/main/scala/geotrellis/spark/filter/SpaceTimeRasterToSpatialMethods.scala#L31) returns an RDD whose associated metadata is `TileLayerMetadata[SpaceTimeKey]`.  Is that correct?",7/1/16,1
pomadchin,@jamesmcclain yep,7/1/16,1
jamesmcclain,Is that desired?,7/1/16,1
jamesmcclain,Or should it be `Spatial`?,7/1/16,1
pomadchin,"not sure, but we have too generic metadata type to reproject it",7/1/16,1
pomadchin,sounds like it shoud be spatial ._.,7/1/16,1
pomadchin,but not all user defined metadata types can depend on key type,7/1/16,1
jamesmcclain,I see,7/1/16,1
moradology,@tomauer the landsat EMR demo is actually doing some quick summary statistics with polygons right now,7/1/16,1
moradology,"It might be a good place to start and a few of us have worked with it recently, so we could probably be of assistance",7/1/16,1
lossyrob,@jamesmcclain that seems wrong,7/1/16,1
jamesmcclain,"Okay, if we want to change it, I can address it in this PR",7/1/16,1
lossyrob,"well, it_Ñés more complicated than that",7/1/16,1
lossyrob,"or, I guess more complicatd then the easy solution I have in my head",7/1/16,1
lossyrob,because M is generic,7/1/16,1
lossyrob,"M: Component[?, Bounds[K]]",7/1/16,1
lossyrob,"we need an `(M, Bounds[SpatialKey]) => M2`",7/1/16,1
lossyrob,which would turn e.g. `TileLayerMetadata[SpaceTimeKey] => TileLayerMetadata[SpatialKey]`,7/1/16,1
jamesmcclain,I see,7/1/16,1
jamesmcclain,That might be out of scope,7/1/16,1
jamesmcclain,Perhaps we can create an issue for it,7/1/16,1
lossyrob,yeah,7/1/16,1
jamesmcclain,"Okay, will create one",7/1/16,1
timothymschier,hey all,7/4/16,1
timothymschier,I'm having issues with what I thought would be an out-of-the-box thing to try,7/4/16,1
timothymschier,"git clone https://github.com/geotrellis/geotrellisgit checkout v0.10.1./sbt assembly",7/4/16,1
timothymschier,I get merge errors,7/4/16,1
timothymschier,deduplicating errors,7/4/16,1
timothymschier,"```[error] 2 errors were encountered during mergejava.lang.RuntimeException: deduplicate: different file contents found in the following:/home/ubuntu/.ivy2/cache/com.esotericsoftware.kryo/kryo/bundles/kryo-2.21.jar:com/esotericsoftware/minlog/Log$Logger.class/home/ubuntu/.ivy2/cache/com.esotericsoftware.minlog/minlog/jars/minlog-1.2.jar:com/esotericsoftware/minlog/Log$Logger.classdeduplicate: different file contents found in the following:/home/ubuntu/.ivy2/cache/com.esotericsoftware.kryo/kryo/bundles/kryo-2.21.jar:com/esotericsoftware/minlog/Log.class/home/ubuntu/.ivy2/cache/com.esotericsoftware.minlog/minlog/jars/minlog-1.2.jar:com/esotericsoftware/minlog/Log.class        at sbtassembly.Assembly$.applyStrategies(Assembly.scala:140)        at sbtassembly.Assembly$.x$1$lzycompute$1(Assembly.scala:25)        at sbtassembly.Assembly$.x$1$1(Assembly.scala:23)        at sbtassembly.Assembly$.stratMapping$lzycompute$1(Assembly.scala:23)        at sbtassembly.Assembly$.stratMapping$1(Assembly.scala:23)        at sbtassembly.Assembly$.inputs$lzycompute$1(Assembly.scala:67)        at sbtassembly.Assembly$.inputs$1(Assembly.scala:57)        at sbtassembly.Assembly$.apply(Assembly.scala:83)        at sbtassembly.Assembly$$anonfun$assemblyTask$1.apply(Assembly.scala:241)        at sbtassembly.Assembly$$anonfun$assemblyTask$1.apply(Assembly.scala:238)        at scala.Function1$$anonfun$compose$1.apply(Function1.scala:47)        at sbt.$tilde$greater$$anonfun$$u2219$1.apply(TypeFunctions.scala:40)        at sbt.std.Transform$$anon$4.work(System.scala:63)        at sbt.Execute$$anonfun$submit$1$$anonfun$apply$1.apply(Execute.scala:228)        at sbt.Execute$$anonfun$submit$1$$anonfun$apply$1.apply(Execute.scala:228)        at sbt.ErrorHandling$.wideConvert(ErrorHandling.scala:17)        at sbt.Execute.work(Execute.scala:237)        at sbt.Execute$$anonfun$submit$1.apply(Execute.scala:228)        at sbt.Execute$$anonfun$submit$1.apply(Execute.scala:228)        at sbt.ConcurrentRestrictions$$anon$4$$anonfun$1.apply(ConcurrentRestrictions.scala:159)        at sbt.CompletionService$$anon$2.call(CompletionService.scala:28)        at java.util.concurrent.FutureTask.run(FutureTask.java:262)        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)        at java.util.concurrent.FutureTask.run(FutureTask.java:262)        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)        at java.lang.Thread.run(Thread.java:745)java.lang.RuntimeException: deduplicate: different file contents found in the following:/home/ubuntu/.ivy2/cache/commons-beanutils/commons-beanutils/jars/commons-beanutils-1.7.0.jar:org/apache/commons/beanutils/BasicDynaBean.class/home/ubuntu/.ivy2/cache/commons-beanutils/commons-beanutils-core/jars/commons-beanutils-core-1.8.0.jar:org/apache/commons/beanutils/BasicDynaBean.class```",7/4/16,1
timothymschier,and many more of the dedplucates,7/4/16,1
lossyrob,"what are you trying to make an assembly of? if you do that, you_Ñére trying to make an assembly out of the aggregate root project, which is simply a aggregate sbt project that combines all the project (useful only for testing and compiling everything)",7/4/16,1
timothymschier,ah,7/4/16,1
timothymschier,spark-etl is what I'm after,7/4/16,1
lossyrob,`./sbt _ÑÒproject spark-etl_Ñù assembly`,7/4/16,1
timothymschier,thank you kindly sir :),7/4/16,1
timothymschier,my notes to myself were missing that bit - I remember it now,7/4/16,1
lossyrob,no problem,7/4/16,1
timothymschier,next silly question actually running the ingest,7/4/16,1
timothymschier,running using ```spark-submit --class geotrellis.spark.etl.MultibandIngest --conf spark.master=mesos://mesos-leader.service.geotrellis-spark.internal:5050 --conf spark.serializer=org.apache.spark.serializer.KryoSerializer --conf spark.kryo.registrator=geotrellis.spark.io.kryo.KryoRegistrator geotrellis-spark-etl-assembly-0.10.1.jar --input s3 --format temporal-geotiff -I bucket=bucket.name key=raster_staging/blah --output s3 -O bucket=bucket.name key=asdf --layer blah --crs EPSG:3857 --pyramid --layoutScheme tms```,7/4/16,1
timothymschier,getting,7/4/16,1
timothymschier,"```Exception in thread ""main"" org.rogach.scallop.exceptions.IncompleteBuildException: It seems you tried to get option value before you constructed all options (maybe you forgot to call .verify method?). Please, move all extraction of values to after 'verify' method in ScallopConf.[ec2-54-165-108-2.compute-1.amazonaws.com] out:         at org.rogach.scallop.ScallopConf.assertVerified(ScallopConf.scala:409)[ec2-54-165-108-2.compute-1.amazonaws.com] out:         at org.rogach.scallop.ScallopConf$$anon$1$$anonfun$fn$1.apply(ScallopConf.scala:136)[ec2-54-165-108-2.compute-1.amazonaws.com] out:         at org.rogach.scallop.ScallopConf$$anon$1$$anonfun$fn$1.apply(ScallopConf.scala:135)[ec2-54-165-108-2.compute-1.amazonaws.com] out:         at org.rogach.scallop.ScallopOption.value$lzycompute(ScallopOption.scala:20)[ec2-54-165-108-2.compute-1.amazonaws.com] out:         at org.rogach.scallop.ScallopOption.value(ScallopOption.scala:20)[ec2-54-165-108-2.compute-1.amazonaws.com] out:         at org.rogach.scallop.ScallopOption.get(ScallopOption.scala:23)[ec2-54-165-108-2.compute-1.amazonaws.com] out:         at org.rogach.scallop.ScallopOption.apply(ScallopOption.scala:26)[ec2-54-165-108-2.compute-1.amazonaws.com] out:         at geotrellis.spark.etl.Etl$$anonfun$2.apply(Etl.scala:81)[ec2-54-165-108-2.compute-1.amazonaws.com] out:         at geotrellis.spark.etl.Etl$$anonfun$2.apply(Etl.scala:81)[ec2-54-165-108-2.compute-1.amazonaws.com] out:         at scala.collection.LinearSeqOptimized$class.find(LinearSeqOptimized.scala:100)[ec2-54-165-108-2.compute-1.amazonaws.com] out:         at scala.collection.immutable.List.find(List.scala:84)[ec2-54-165-108-2.compute-1.amazonaws.com] out:         at geotrellis.spark.etl.Etl.load(Etl.scala:81)[ec2-54-165-108-2.compute-1.amazonaws.com] out:         at geotrellis.spark.etl.Etl$.ingest(Etl.scala:42)[ec2-54-165-108-2.compute-1.amazonaws.com] out:         at geotrellis.spark.etl.MultibandIngest$.main(MultibandIngest.scala:15)[ec2-54-165-108-2.compute-1.amazonaws.com] out:         at geotrellis.spark.etl.MultibandIngest.main(MultibandIngest.scala)[ec2-54-165-108-2.compute-1.amazonaws.com] out:         at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)[ec2-54-165-108-2.compute-1.amazonaws.com] out:         at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)[ec2-54-165-108-2.compute-1.amazonaws.com] out:         at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)[ec2-54-165-108-2.compute-1.amazonaws.com] out:         at java.lang.reflect.Method.invoke(Method.java:606)[ec2-54-165-108-2.compute-1.amazonaws.com] out:         at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:674)[ec2-54-165-108-2.compute-1.amazonaws.com] out:         at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180)[ec2-54-165-108-2.compute-1.amazonaws.com] out:         at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205)[ec2-54-165-108-2.compute-1.amazonaws.com] out:         at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:120)[ec2-54-165-108-2.compute-1.amazonaws.com] out:         at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)```",7/4/16,1
pomadchin,"@timothymschier #v10.1 has a scopt update up to 1.0.0, since 1.0.0 scopt has no autovalidation ): we fogot to call verify function manually, checkout this fix to resolve your problem (etl would be fixed in 10.2) https://github.com/geotrellis/geotrellis/pull/1572",7/4/16,1
timothymschier,thanks @pomadchin. Any word on a release date for 10.2?,7/4/16,1
wsf1990,When I calc a tile's slope the result tile's edge has some value which isn't nodata but it must be nodata.Dose geotrellis has some method to fix the edge of data?,7/5/16,1
wsf1990,or dose the method of `tile.slope` has some parameter to resolve this?,7/5/16,1
uuuuq,hi,7/5/16,1
uuuuq,"i want to ingest SRTM DEM to HDFS, the DEM files are pre tiled to 1degree*1degree tiles.",7/5/16,1
uuuuq,"and here is my code:``` def main(args: Array[String]): Unit = {    val conf =      new SparkConf()        .setMaster(""local[*]"")        .setAppName(""GeoTrellis DEM Ingest"")        .set(""spark.serializer"", ""org.apache.spark.serializer.KryoSerializer"")        .set(""spark.kryo.registrator"", ""geotrellis.spark.io.kryo.KryoRegistrator"")        .set(""spark.executor.memory"", ""16g"")    val sc = new SparkContext(conf)    try {      run(sc)      println(""Hit enter to exit."")      readLine()    } finally {      sc.stop()    }  }  def run(implicit sc: SparkContext) = {    val fileIter = getFilePath(fullPath(inputPath))    val resampleMethod = Bilinear    while (fileIter.hasNext) {      val filePath = fileIter.next.getAbsolutePath      val inputRdd: RDD[(ProjectedExtent, Tile)] =        sc.hadoopGeoTiffRDD(filePath)      val (_, rasterMetaData) =        TileLayerMetadata.fromRdd(inputRdd, FloatingLayoutScheme(512))      val tiled: RDD[(SpatialKey, Tile)] =        inputRdd          .tileToLayout(rasterMetaData.cellType, rasterMetaData.layout, resampleMethod)          .repartition(100)      val layoutScheme = ZoomedLayoutScheme(WebMercator, tileSize = 256)      val (zoom, reprojected): (Int, RDD[(SpatialKey, Tile)] with Metadata[TileLayerMetadata[SpatialKey]]) =        TileLayerRDD(tiled, rasterMetaData)          .reproject(WebMercator, layoutScheme, Bilinear)//      val rdd = new ContextRDD(reprojected, rasterMetaData)      val attributeStore = HadoopAttributeStore(outputPath)      val writer = HadoopLayerWriter(outputPath)      val deleter = HadoopLayerDeleter(outputPath)      val copier = HadoopLayerCopier(outputPath)      val reader = HadoopLayerReader(outputPath)      val updater = new HadoopLayerUpdater(attributeStore, reader, writer, deleter, copier)      Pyramid.upLevels(reprojected, layoutScheme, zoom, 1, resampleMethod) { (rdd, z) =>        val layerId = LayerId(""DEM"", z)        if(attributeStore.layerExists(layerId)) {//          val manager = new HadoopLayerManager(attributeStore)//          manager.reindex(layerId, HilbertKeyIndexMethod)          updater.update(layerId, rdd)        }        writer.write(layerId, rdd, ZCurveKeyIndexMethod)      }    }  }```i run it and it raised ```[error] Exception in thread ""main"" geotrellis.spark.io.package$LayerOutOfKeyBoundsError: Updating rdd is out of the key index space for Layer(name = ""DEM"", zoom = 13): KeyBounds(SpatialKey(5939,2881),SpatialKey(5965,2918)). You must reindex this layer with large enough key bounds for this update.[error] 	at geotrellis.spark.io.hadoop.HadoopLayerUpdater._update(HadoopLayerUpdater.scala:42)[error] 	at geotrellis.spark.io.hadoop.HadoopLayerUpdater._update(HadoopLayerUpdater.scala:21)[error] 	at geotrellis.spark.io.LayerUpdater.update(LayerUpdater.scala:52)[error] 	at test.IngestImage$$anonfun$run$1.apply(IngestImage.scala:83)[error] 	at test.IngestImage$$anonfun$run$1.apply(IngestImage.scala:78)[error] 	at geotrellis.spark.pyramid.Pyramid$.runLevel$1(Pyramid.scala:180)[error] 	at geotrellis.spark.pyramid.Pyramid$.upLevels(Pyramid.scala:188)[error] 	at test.IngestImage$.run(IngestImage.scala:78)[error] 	at test.IngestImage$.main(IngestImage.scala:43)[error] 	at test.IngestImage.main(IngestImage.scala)[error] 16/07/05 12:40:44 INFO ShutdownHookManager: Shutdown hook called[error] 16/07/05 12:40:44 INFO ShutdownHookManager: Deleting directory /tmp/spark-9b00e683-45b9-4c4b-b567-57904a3acdc8[error] 16/07/05 12:40:44 INFO RemoteActorRefProvider$RemotingTerminator: Shutting down remote daemon.[error] 16/07/05 12:40:44 INFO RemoteActorRefProvider$RemotingTerminator: Remote daemon shut down; proceeding with flushing remote transports.[error] 16/07/05 12:40:44 INFO RemoteActorRefProvider$RemotingTerminator: Remoting shut down.```",7/5/16,1
uuuuq,"my question is how to reindex the layer or is there any better way to do this thing?another question:the tiled images may located on two dem files, i need to merge two tiles, so how to check if the tile already exists?",7/5/16,1
pomadchin,"@uuuuq yes, we have LayerReindexer",7/5/16,1
pomadchin,"@timothymschier after fixing significnt bugs i believe, cant say when exactly",7/5/16,1
wsf1990,"@pomadchin Has some method can deal the edge of the tile?",7/5/16,1
wsf1990,[![srtm_slope .tif](https://files.gitter.im/geotrellis/geotrellis/bNPj/thumb/srtm_slope-.tiff)](https://files.gitter.im/geotrellis/geotrellis/bNPj/srtm_slope-.tif),7/5/16,1
wsf1990,this is my slope tile,7/5/16,1
lossyrob,"@wsf1990 can you describe the edge result, what you expected and what was the actual? There migh be some choices around what we thing the edge of a slope operation should look like (i.e. what do we do when the neighborhood has 3 pixels outside of the bounds of the raster), vs what you are expecting",7/5/16,1
wsf1990," @lossyrob You can look the srtm_slope.tif which I upload, this is a slope tile that I write to tiff.You can see at the edge of the tile has a pixel of value, but at this position it should be nodata.",7/6/16,1
aaron-santos,Hi guys. I'm trying to ingest sentinel2 basetiles and I could use a little guidance,7/6/16,1
aaron-santos,I'm also trying to do it in < ~8GB of ram so I can test it on my workstation before deploying :D,7/6/16,1
aaron-santos,"The input is a set of differently sized jp2 images, one for each of the 13 bands",7/6/16,1
aaron-santos,"The workflow goes like this:use java gdal bindings to read the raster dimensions and find the max (width, height) of all the bands",7/6/16,1
aaron-santos,"use gdalwarp to resize them to maxwidth, maxheight and also convert to geotiff",7/6/16,1
aaron-santos,use gdal_retile.py to slice each of the bands into 256x256 chunks,7/6/16,1
aaron-santos,transpose the sets of chunks so that I have a sequence of stacks of these chunks,7/6/16,1
aaron-santos,then read each band in the stack as a singe band geotiff and collapse them into a multiband geotiff,7/6/16,1
aaron-santos,so the end result is a set of ~1800 multiband geotiffs. That seems to work alright so far,7/6/16,1
aaron-santos,Now I'd like to parallelize them into an RDD and ship them off to MultibandIngest,7/6/16,1
aaron-santos,"My thought was that by splitting up the basetile into small enough pieces, I could stay in my memory budget",7/6/16,1
aaron-santos,So I'm stuck at an out of memory exception when spark's calling serializeWithDependencies,7/6/16,1
aaron-santos,Am I on the right track? Would splitting up my basetile have a positive impact on memory consumption?,7/6/16,1
aaron-santos,"One more thing, I'm persisting my RDD to disk before passing it to MultibandIngest but it doesn't appear to have an impact one way or another",7/6/16,1
aaron-santos,"After adjusting -Xmx and switching to `-XX:+UseConcMarkSweepGC`, I get a bit farther, but fail with```[info]  WARN [sparkDriver-akka.actor.default-dispatcher-2] TaskSetManager - Stage 0 contains a task of very large size (1567693 KB). The maximum recommended task size is 100 KB.[error] [Stage 0:>                                                          (0 + 0) / 2][error] [Stage 0:>                                                          (0 + 1) / 2]Java HotSpot(TM) 64-Bit Server VM warning: INFO: os::commit_memory(0x0000000780090000, 1073152000, 0) failed; error='Cannot allocate memory' (errno=12)```",7/6/16,1
aaron-santos,The task size of 1567693 KB is a bit disconcerting,7/6/16,1
lossyrob,@aaron-santos perhaps a problem with the number of partitions,7/6/16,1
lossyrob,how many partitions are you working with?,7/6/16,1
lossyrob,if you look at the spark UI it would be the number of tasks for a stage,7/6/16,1
aaron-santos,"Hey @lossyrob . Ahh, I haven't modified that so it's set to default. I'll check",7/6/16,1
aaron-santos,thx!,7/6/16,1
lossyrob,https://github.com/lossyrob/geotrellis-ned-example/blob/master/src/main/scala/elevation/Main.scala#L176,7/6/16,1
lossyrob,@aaron-santos here_Ñés an example of doing some splitting in GeoTrellis and then repartitioning ,7/6/16,1
shaystrong,can't quite ascertain if geotrellis can parse mbtile files as rasters...,7/7/16,1
shaystrong,hoping for some insight,7/7/16,1
pomadchin,@shaystrong right now we support only tiffs,7/7/16,1
lossyrob,"@shaystrong @fosskers is currently working on VectorTile support, he might be able to better give an updated on where we are at with mbtile support",7/7/16,1
fosskers,"As far as I'm aware, `yourfile.mbtile.pb` is the older format of vector tiles, and is what was output by mapbox tools",7/7/16,1
fosskers,Any tool taken from them as of April or so should produce version 2 tiles (with `.mvt` extension),7/7/16,1
fosskers,"Although I believe there was a period of transition, where the previous files were ""version 2"" compatible, while still marked with the old file extensions",7/7/16,1
fosskers,"@shaystrong , how are you coming about your tiles?",7/7/16,1
lossyrob,@wsf1990 why do you say that the slope calculation should have NODATA at it_Ñés edges? Is that based on an implementation of slope calculation that you are assuming makes choices around NODATA for neighborhoods where there are 3 pixels missing data (due to edges)? ,7/7/16,1
aaron-santos,@lossyrob thanks for the partition sizing help. Looks like that was the ticket :),7/7/16,1
wsf1990,"@lossyrob Yeah,You can see at some position of the edge just have one pixel data.",7/8/16,1
lossyrob,I see what you are talking about. can I get the elevation tile as well?,7/8/16,1
wsf1990,[![3`06@Y0D]MKQG$Y1ZVT_%XU.png](https://files.gitter.im/geotrellis/geotrellis/VyyW/thumb/3_06_Y0D_MKQG_Y1ZVT__XU.png)](https://files.gitter.im/geotrellis/geotrellis/VyyW/3_06_Y0D_MKQG_Y1ZVT__XU.png),7/8/16,1
wsf1990,@lossyrob you can see this,7/8/16,1
wsf1990,@lossyrob every tile's edge has one pixel data,7/8/16,1
wsf1990,my elevation data is SRTM data,7/8/16,1
uuuuq,"I follow this guide : https://github.com/geotrellis/geotrellis/blob/master/docs/spark-etl/spark-etl-run-examples.md```#!/bin/shexport JAR=""~/Project/geotrellis/spark-etl/target/scala-2.10/geotrellis-spark-etl-assembly-1.0.0-SNAPSHOT.jar""spark-submit \--class geotrellis.spark.etl.SinglebandIngest \--master local[*] \--driver-memory 8G \$JAR \--input hadoop --format geotiff --cache NONE -I path=""hdfs://192.168.1.7:9000/dem/tif"" \--output accumulo -O instance=""gis"" table=""dem"" user=""test"" password=""test"" zookeeper=""192.168.1.7:2181,192.168.1.20:2181,192.168.1.21:2181"" \--layer dem --crs EPSG:3857 --pyramid --layoutScheme tms```but it shows:```[scallop] Error: Bad arguments for option 'O': 'instance=gis table=dem user=test password=test zookeeper=192.168.1.7:2181,192.168.1.20:2181,192.168.1.21:2181' - wrong arguments format```so what is the right arguments format?",7/8/16,1
pomadchin,@uuuuq pass only one zookeeper; not sure what it can be connected with,7/8/16,1
schBen,"Hi!In local filesystems, I can read in a single geotiff file as an RDD like this:`val myGeotiff: RDD[(ProjectedExtent, MultibandTile)] = sc.hadoopMultibandGeoTiffRDD(""C:\\Test\\myGeo.tif"")`I tried something similar for S3:`val myGeotiff: RDD[(ProjectedExtent, MultibandTile)] = sc.hadoopMultibandGeoTiffRDD(""s3n://my-bucket/myGeo.tif"")`It works, but it takes very long. Then I tried`sc.newAPIHadoopRDD[ProjectedExtent, MultibandTile, MultibandGeoTiffS3InputFormat]( ... )`and it worked fine. Questions: Why does the shorthand method take so long on S3, and is there a method which I can use on both local FS and S3 in the same way, by just exchanging the path?",7/8/16,1
schBen,So I would like to have an API which lets me select the input source in the easiest way possible.,7/8/16,1
lossyrob,"@wsf1990 are you doing slope per-tile? You can_Ñét do that, since slope is a focal operation, and the tiles need their neighboring tile_Ñés information for doing the slope calculation correctly",7/8/16,1
lossyrob,the thing to do there would be to calculate the slope in an RDD,7/8/16,1
lossyrob,"if you have the tiles as a TileLayerRDD[SpatialKey], you can call `slope` on it",7/8/16,1
wsf1990,Thanks!,7/8/16,1
lossyrob,are you serving those tiles out dynamically (are you calculating slope dynamically off of the elevation tiles)?,7/8/16,1
wsf1990,"@lossyrob OK,I'll try",7/8/16,1
lossyrob,"it would be best to calculate slope in a batch process, and save that off as a separate layer",7/8/16,1
lossyrob,"that and hillshade, aspect_Ñ_any operation that is a focal operation, since the border pixel calculation will need information from the neighboring tiles",7/8/16,1
wsf1990,"Got it,Many Thanks!",7/8/16,1
lossyrob,"no problem! Sorry it took so long to get to that one, let me know how it goes",7/8/16,1
wsf1990,OK!,7/8/16,1
wsf1990,have a nice weekend,7/8/16,1
lossyrob,you too!,7/8/16,1
akarmas,"@lossyrob  @echeipesh I have noticed that when a layer is updated (with HadoopLayerUpdater for example) all the already stored tiles in the layer are examined in order to be merged with the new data so that the new updated layer can be created.The problem is that as the layer grows the amount of HDFS storage needed as temporary storage for the update to be performed grows exponentially. With my hadoop conf for instance in order to update a 3GB layer with new data of 600MB almost 15GB of free  HDFS space are required.I was wondering if there is a point in further extending the update function with a (sort of) append mode, so that if the user is certain that there is no overlapping between the keys of the tiles already stored in a layer and in the keys of the tiles that are going to update the layer (for example when a layer that holds all L8 images of a specific path/row is updated with a new acquistion) the update process will result in a simple writing of the new tiles in the existing layer instead of re-reading the whole layer.What are your thoughts on this issue?Thanks!",7/8/16,1
pomadchin,"@akarmas think there is a problem in updating Hadoop stored layer, as hdfs is a copy on write fs; if you want to update smth, you need to copy old tiles into memory, merge with new tiles, and write into a new direcotry; it_Ñés not possible to update hadoop map files ",7/8/16,1
pomadchin,):,7/8/16,1
pomadchin,if you are planning to have a layer and update it during its lifecycle it is better to use Accumulo / Cassandra (still in beta) backend,7/8/16,1
lossyrob,the problem boils down to MapFiles - you can_Ñét update HDFS MapFiles really,7/8/16,1
lossyrob,"if your trying to do CRUD on HDFS, it doesn_Ñét support that very well - but Accumulo solves that issue well",7/8/16,1
pomadchin,@lossyrob _cassandra (may be )`:)!_ :D,7/8/16,1
lossyrob,once we work out all the bugs ;),7/8/16,1
pomadchin,)`````:,7/8/16,1
lossyrob,but I_Ñém excited for cassandra to be competing with accumulo,7/8/16,1
lossyrob,for our recommended option in this case,7/8/16,1
echeipesh,"@akarmas I_Ñéve been doing some work on our HDFS backend lately and actually I think there is a smart way to update a subset of the mapfiles. I wasn_Ñét planning to address that issue directly because its getting very quickly into  realm of pushing very hard on abstractions of HDFS and it seems some external process that controls an index really starts paying off here, like Accumulo or Cassandra.",7/8/16,1
echeipesh,@akarmas what are your thoughts on the deployment? Is HDFS something you_Ñére committed in your architecture or something you_Ñére using for the first pass?,7/8/16,1
wsf1990,"@lossyrob  I have resolved the edge question. But I found when use tile.findMinMax to  show a nodata tile's minmax it will show me this `(-2147483648,-2147483648) `.Dose this an bug? The tile's celltype is int16.Thanks",7/11/16,1
wsf1990,I found that -2147483648 is the Int.MinValue,7/11/16,1
wsf1990,"And when I use tile.get(0, 0) I got -2147483648 not -32768,Dose this is a bug?",7/11/16,1
pomadchin,@wsf1990 can you try findMinMaxDouble?,7/11/16,1
wsf1990,"use findMinMaxDouble get   (NaN,NaN)",7/11/16,1
uuuuq,"i use the geotrellis-spark-etl-assembly-1.0.0-SNAPSHOT.jar to ingest tiles from hdfs into accumulogetting```16/07/11 18:42:26 INFO Executor: Running task 4.0 in stage 17.0 (TID 211)16/07/11 18:42:26 ERROR TaskSetManager: Task 0 in stage 17.0 failed 1 times; aborting job16/07/11 18:42:26 WARN TaskSetManager: Lost task 2.0 in stage 17.0 (TID 209, localhost): java.io.IOException: Mkdirs failed to create /geotrellis-ingest/43700da1-da3c-4e82-a7a8-2297c0850b4f-Xrzu0i1hST/_temporary/0/_temporary/attempt_201607111842_0026_r_000002_0 (exists=false, cwd=file:/home/geo)```so I add write rights to `/` and submit the job again, it works and writes file in `/geotrellis-ingest/89c80d30-144e-4d65-abd2-01180a32eab4-8wf3sMHx7E/_temporary/0/task_201607111907_0026_r_000000`",7/11/16,1
uuuuq,"but it fails again:```Exception in thread ""main"" geotrellis.spark.io.package$LayerWriteError: Failed to write Layer(name = ""dem"", zoom = 13)	at geotrellis.spark.io.accumulo.AccumuloLayerWriter._write(AccumuloLayerWriter.scala:50)	at geotrellis.spark.io.accumulo.AccumuloLayerWriter._write(AccumuloLayerWriter.scala:15)	at geotrellis.spark.io.LayerWriter$class.write(LayerWriter.scala:43)	at geotrellis.spark.io.accumulo.AccumuloLayerWriter.write(AccumuloLayerWriter.scala:15)	at geotrellis.spark.io.LayerWriter$$anon$1.write(LayerWriter.scala:55)	at geotrellis.spark.io.LayerWriter$$anon$1.write(LayerWriter.scala:53)	at geotrellis.spark.etl.OutputPlugin$class.apply(OutputPlugin.scala:21)	at geotrellis.spark.etl.accumulo.SpatialAccumuloOutput.apply(SpatialAccumuloOutput.scala:11)	at geotrellis.spark.etl.Etl.savePyramid$1(Etl.scala:173)	at geotrellis.spark.etl.Etl.save(Etl.scala:187)	at geotrellis.spark.etl.Etl$.ingest(Etl.scala:46)	at geotrellis.spark.etl.SinglebandIngest$.main(SinglebandIngest.scala:15)	at geotrellis.spark.etl.SinglebandIngest.main(SinglebandIngest.scala)	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)	at java.lang.reflect.Method.invoke(Method.java:498)	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:731)	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181)	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206)	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121)	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)Caused by: org.apache.accumulo.core.client.AccumuloException: file:/geotrellis-ingest/89c80d30-144e-4d65-abd2-01180a32eab4-8wf3sMHx7E: java.io.IOException: file:/geotrellis-ingest/89c80d30-144e-4d65-abd2-01180a32eab4-8wf3sMHx7E is not in a volume configured for Accumulo	at org.apache.accumulo.core.client.impl.TableOperationsImpl.doFateOperation(TableOperationsImpl.java:300)	at org.apache.accumulo.core.client.impl.TableOperationsImpl.doFateOperation(TableOperationsImpl.java:261)	at org.apache.accumulo.core.client.impl.TableOperationsImpl.doTableFateOperation(TableOperationsImpl.java:1427)	at org.apache.accumulo.core.client.impl.TableOperationsImpl.importDirectory(TableOperationsImpl.java:1046)	at geotrellis.spark.io.accumulo.HdfsWriteStrategy.write(AccumuloWriteStrategy.scala:67)	at geotrellis.spark.io.accumulo.AccumuloRDDWriter$.write(AccumuloRDDWriter.scala:41)	at geotrellis.spark.io.accumulo.AccumuloLayerWriter._write(AccumuloLayerWriter.scala:52)	... 21 more```",7/11/16,1
uuuuq,what does 'not in a volume configured for Accumulo' mean? :worried: ,7/11/16,1
uuuuq,"well i read the source code,   i do not set `fs.defaultFS` property in `core-site.xml`.",7/11/16,1
akarmas,"Thank you all for the info!! @echeipesh I am not committed to HDFS. I just used it because i thought it was simpler to do so than use Accumulo. But i have also installed Accumulo so i ll just change the function calls in my code from io.hadoop to io.accumulo. I suppose that the process will be pretty straightforward.One question though in  ```AccumuloAttributeStore(connector: Connector, attributeTable: String) ```what should i put as connector and attributeTable?Moreover, in AccumuloLayerWriter what is the correct practice in setting the table that the data are to be written?Should i use one table per layer? and tables are to be created from accumulo shell or AccumuloLayerWriter is responsible for creating the table if the table does not exist?",7/11/16,1
pomadchin,"@akarmas ```scalaval instance = AccumuloInstance(_ÑÒinstance"", _ÑÒzookeeper_Ñù, _ÑÒuser"", new PasswordToken(_ÑÒpassword_Ñù))instance.connector // that_Ñés what you need```yes, all tables would be created if there are no such",7/11/16,1
echeipesh,"@schBen: The hadoop S3 implemintation is somewhat of a mess. `s3n` came first and it has multiple problems. `s3a` uses amazon SDK to implement the connection and in some ways it is better but still has issues. Sorry for the really vaigue answer, it was a while since I looked at it, but AFAIR somehow the hadoop `FileSystem` object will end up listing the whole bucket to get just one file and that leads to silly slow lookups you_Ñére seeing. To side step this ugly issue we created an `s3` project to read files from S3 directly using aws SDK: https://github.com/geotrellis/geotrellis/tree/master/s3/src/main/scala/geotrellis/spark/io/s3 I don_Ñét think we have created convinience methods for them, so you would have to lean on using the `sc.newAPIHadoopFile` and one of the InputFormats.",7/11/16,1
echeipesh,"@akarmas: One cool trick with Accumulo is that if you store multiple layers in the same table (they have to share KeyBounds and KeyIndexMethod though) when you query across them they will come up _ÑÒlineup_Ñù across the cluster. This happens because Accumulo is a columnar store and a single row will never get split across multiple machines. Other than that there is no hard guidance. Generally you shouldn_Ñét store more than _ÑÒa dozen_Ñù of layers per table, you would see a performance decrease from that. By specifying the table in the writer it will be created for you.",7/11/16,1
echeipesh,@akarmas what kind of application are you making? Is it going to support interactive analsysis or is it basically going to be running batch jobs of varying scope ?,7/11/16,1
echeipesh,@uuuuq: You can also side-step the problem by giving full `hdfs://` path to the `HdfsWriteStrategy`.,7/11/16,1
akarmas,"@pomadchin @echeipesh Thanks for the immediate response!As of now i am working on the latter! ",7/11/16,1
lossyrob,@wsf1990 what you are seeing are the GeoTrellis constants that represent NoData,7/11/16,1
lossyrob,"for integer values, this is a constant defined as `NODATA`, which is equal to `Int.MinValue`",7/11/16,1
lossyrob,"for Doubles, NoData is `Double.NaN`",7/11/16,1
lossyrob,this is how we represent NoData as far as interacting with Tiles through the interface,7/11/16,1
lossyrob,Tiles can have underlying values that represent NoData in different ways,7/11/16,1
lossyrob,"for instance, `ByteUserDefinedNoDataCellType(-9.toByte)` would internally be a Byte array that had values of -9 representing NoData",7/11/16,1
lossyrob,"however, if you were to do a `tile.get(0, 0)` on the tile, which produces an integer value, and internally that value was -9, you would get a NODATA returned from the `get` call",7/11/16,1
lossyrob,"if you did `tile.getDouble(0, 0)` in the same scenario, you_Ñéd get `Double.NaN`",7/11/16,1
lossyrob,the fact you get NODATA from ` tile.findMinMax` makes me think your tile is empty,7/11/16,1
lossyrob,https://github.com/geotrellis/geotrellis/blob/master/raster/src/main/scala/geotrellis/raster/Tile.scala#L313,7/11/16,1
lossyrob,"if you look at the implementation, that will only happen if all your cells are NODATA values",7/11/16,1
longcao,"hey there... i was pointed this way by erik osheim for some geotrellis questions, namely if geotrellis is appropriate for my possible use-case: making geoqueries and moving data from Shapefiles in Spark and possibly outputting to PostgreSQL/PostGIS",7/11/16,1
lossyrob,nice Erik was one of two original GeoTrellis developers,7/11/16,1
longcao,i'm not entirely sure of the extent of what kind of queries I'd need (it might just need to preserve geometries to be moved into PostgreSQL) but at the very least I'd like to know if geotrellis would be capable of handling that over Spark?,7/11/16,1
lossyrob,might be a good fit: if you_Ñére doing geospatial in Scala then I_Ñém sure we have something for you,7/11/16,1
lossyrob,"mainly we deal with raster data, but are currently implementin vector data operations in Spark as well",7/11/16,1
lossyrob,"when you say _ÑÒfrom shapefiles in spark_Ñù, what do you mean? what would be the RDD type?",7/11/16,1
longcao,sorry - let me be clearer,7/11/16,1
longcao,my concrete example is: http://www1.nyc.gov/site/planning/data-maps/open-data.page#pluto,7/11/16,1
longcao,i'd have this MapPLUTO Shapefile data sitting in S3,7/11/16,1
longcao,so I'm wondering if there's an `RDD` type that Geotrellis would support,7/11/16,1
longcao,(i would dig through source but admittedly i got a little lost :)),7/11/16,1
lossyrob,"so your taking a CSV file of feature data (points? lines? polygons? a mix?) and trying to do some operations on them, and then dump results into PostGis?",7/11/16,1
lossyrob,oh sorry MapPLUTO,7/11/16,1
lossyrob,"we have a Scala Shapefile reader, we can also read GeoJSON",7/11/16,1
lossyrob,turn that into feature data types,7/11/16,1
lossyrob,"and then you would have an `RDD[Feature[Geometry, Map[String, Object]]]` that you could work with",7/11/16,1
lossyrob,we also have a set of Slick funtionality for interacting with PostGIS,7/11/16,1
lossyrob,"so if it_Ñés a matter of reading shapefiles then writing data into PostGIS, that could be accomplished in Spark..but there_Ñés not a lot that Spark is doing in this case (just reading from S3 and piping into PostGIS)",7/11/16,1
lossyrob,"what we don_Ñét currently have is a Shapefile s3 RDD loader that is nice, like we do with GeoTiffs, so that would be a bit of a manual process of reading the bytes via a S3Client and reading that as a shapefile.",7/11/16,1
longcao,"ah - ok, that's fine to me",7/11/16,1
longcao,i'm really more getting a feel for if these things are possible :D,7/11/16,1
longcao,before i head down a rabbit hole,7/11/16,1
echeipesh,"Actually it should be pretty straight forward to extend `S3InputFormat` like we do for GeoTiff case, so you wouldn_Ñét even have to muck around with the s3 clients: https://github.com/geotrellis/geotrellis/blob/master/s3/src/main/scala/geotrellis/spark/io/s3/GeoTiffS3InputFormat.scala",7/11/16,1
longcao,great - thanks for the help,7/11/16,1
longcao,y'all are super responsive,7/11/16,1
lossyrob,"really what you_Ñéd want is https://github.com/geotrellis/geotrellis/blob/master/s3/src/main/scala/geotrellis/spark/io/s3/S3InputFormat.scala#L20, but that requires Key Value RDDs",7/11/16,1
uuuuq,"hi, what does temporal geotiff mean? do i need to add time tag to each geotiff file manually? ",7/13/16,1
pomadchin,"@uuuuq yes, you need to set tiff time and tiff time format tags (if you want some custom date format support)",7/13/16,1
lossyrob,"@uuuuq you can also supply the temporal aspect via some other method. For instance, in landsat images, we use the MTL file to get the time of the image",7/13/16,1
lossyrob,"it just takes some custom handling, where we have some nicities built up around dealing with GeoTiffs that have a time tiff tag",7/13/16,1
akarmas,"Hi all. I hope you are doing well!One (hopefully quick) question!I have two RDDs of type RDD[(SpatialKey, Tile)] namely maxValues and in that cover the same areal extent and i want to perform a comparison between the two RDDs and output a new RDD. I am doing it like this:```val doy: Double =  some_constantval combined = maxValues.cogroup(in)val new: RDD[(SpatialKey, Tile)] =      combined.map{ case(key,tiles) =>        val c = tiles._1.head;        val d = tiles._2.head;        (key, IfCell(c, d, { (x: Double, y: Double) => x == y }, doy, 0.0))      }```but it throws the following error:org.apache.spark.SparkException: Task not serializableAny thoughts on what i am doing wrong?",7/14/16,1
echeipesh,@akarmas Likely something in the closure does not extend java `Serializable` interface or refers to an object that doesn_Ñét. I would look at `IfCell` method. Usually you get a nice stack trace for the serialization that is a little helpful,7/14/16,1
echeipesh,Generally the remedy to this type of situation is to put as much code into `object`s as possible.,7/14/16,1
akarmas,"the stack trace is not so nice```org.apache.spark.SparkException: Task not serializable        at org.apache.spark.util.ClosureCleaner$.ensureSerializable(ClosureCleaner.scala:304)        at org.apache.spark.util.ClosureCleaner$.org$apache$spark$util$ClosureCleaner$$clean(ClosureCleaner.scala:294)        at org.apache.spark.util.ClosureCleaner$.clean(ClosureCleaner.scala:122)        at org.apache.spark.SparkContext.clean(SparkContext.scala:2055)        at org.apache.spark.rdd.RDD$$anonfun$map$1.apply(RDD.scala:324)        at org.apache.spark.rdd.RDD$$anonfun$map$1.apply(RDD.scala:323)        at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)        at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)        at org.apache.spark.rdd.RDD.withScope(RDD.scala:316)        at org.apache.spark.rdd.RDD.map(RDD.scala:323)        at $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.......```and continues with more ```Caused by: java.io.NotSerializableException: org.apache.spark.SparkContextSerialization stack:        - object not serializable (class: org.apache.spark.SparkContext, value: org.apache.spark.SparkContext@5d9d01d)        - field (class: $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$$$$$c57ec8bf9b0d5f6161b97741d596ff0$$$$wC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC, name: sp, type: class org.apache.spark.SparkContext)```So the issue can be restricted to the ```IfCell(c, d, { (x: Double, y: Double) => x == y }, doy, 0.0)```statement?I did not quite understand what you mean by ""the remedy to this type of situation is to put as much code into objects as possible."" What can i do in the current situation??Sorry the code blocks got really messed up!",7/14/16,1
echeipesh,Is the code by any chance public ?,7/14/16,1
echeipesh,"@akarmas ah, kind of looks like something is hooking into the spark-shell and pulling in the spark context object.",7/14/16,1
echeipesh,So a way to proof against that is to move your functions into objects like so: https://github.com/geotrellis/geotrellis/blob/master/spark/src/main/scala/geotrellis/spark/tiling/CutTiles.scala,7/14/16,1
echeipesh,Basically that makes them static and impossible to refer to state/objects/fields outside of itself.,7/14/16,1
pljplj,I have a scala question.  I create a vector of tuple2 and I want to reduceByKey.  How should I do it?  Not the way I'm trying,7/14/16,1
pljplj,"    val mapped = tiles.map(t => {       val key = t._1 + ""_"" + findColumn(t._2).toString()      (key,new tileInfo(t,256,256))    })    val leftRight = mapped.reduceByKey((a,b) => {",7/14/16,1
echeipesh,@pljplj if `tiles` is a `Seq` there is no `reduceByKey` in scala collections. If `tiles` is and `RDD` than likely you need to `import org.apache.spark._` to get extension methods from `PairRDDMethods` to get `reduceByKey`.,7/14/16,1
pljplj,@echeipesh thanks.  I had just come to the conclusion that I needed spark,7/14/16,1
lossyrob,"you don_Ñét necessarily need Spark and for it to be an RDD. reduceByKey is just not written for scala collections, you_Ñéd have to write it yourself",7/14/16,1
lossyrob,will have something in a sec,7/14/16,1
lossyrob,"```scalascala> object KeyValueSeqMethods {     |   def reduceByKey[K, V](items: Seq[(K, V)])(f: (V, V) => V): Seq[(K, V)] =     |     items     |       .groupBy(_._1)     |       .map { case (key, values) =>      |         (key, values.map(_._2).reduce(f))     |       }     |       .toSeq     | }defined object KeyValueSeqMethodsscala> implicit class withKeyValueSeqMethods[K, V](val items: Seq[(K, V)]) {     |   def reduceByKey(f: (V, V) => V): Seq[(K, V)] =     |     KeyValueSeqMethods.reduceByKey(items)(f)     | }defined class withKeyValueSeqMethodsscala> val v = Vector((""one"", 1), (""one"", 1), (""two"", 2), (""three"", 3), (""three"", 3))v: scala.collection.immutable.Vector[(String, Int)] = Vector((one,1), (one,1), (two,2), (three,3), (three,3))scala> v.reduceByKey(_ + _)res3: Seq[(String, Int)] = ArrayBuffer((one,2), (three,6), (two,2))```",7/14/16,1
lossyrob,"This is the version that actually keeps the type as a Vector, using the scala collection_Ñés madness of `CanBuildFrom` pattern. ",7/14/16,1
lossyrob,"```scala  import scala.collection.generic.CanBuildFrom  import scala.collection.GenTraversableLike  object KeyValueSeqMethods {    def reduceByKey[Self, K, V](items: Self)(f: (V, V) => V)      (implicit ev: Self <:< GenTraversableLike[(K, V), Self], b1: CanBuildFrom[Self, (K, V), Self], b2: CanBuildFrom[Self, V, Seq[V]]): Self = {      val builder = b1.apply(items)      items        .groupBy(_._1)        .map { case (key, values) =>          (key, values.map(_._2).reduce(f))      }        .foreach { builder += _ }      builder.result    }  }  implicit class withKeyValueSeqMethods[Self, K, V](val items: Self)(implicit ev: Self <:< GenTraversableLike[(K, V), Self]) {    def works: Int = 4    def reduceByKey(f: (V, V) => V)(implicit b1: CanBuildFrom[Self, (K, V), Self], b2: CanBuildFrom[Self, V, Seq[V]]): Self =      KeyValueSeqMethods.reduceByKey(items)(f)  }  val v = Vector((""one"", 1), (""one"", 1), (""two"", 2), (""three"", 3), (""three"", 3))  v.reduceByKey(_ + _)```",7/14/16,1
lossyrob,in gist form: https://gist.github.com/lossyrob/b96a0903aa6313f985f33bc1a2a4c1bb,7/14/16,1
lossyrob,@pomadchin this might be useful for the Collections API,7/14/16,1
ardilgulez,does anyone know how i can combine landsat's red green and blue bands,7/15/16,1
akarmas,"@echeipesh I identified the error! The problem is that doy was defined outside the closure and as a result spark cannot serialize it. Once i moved it inside the closure it worked perfectly!Thanks for your help!!```val doy: Double =  some_constant             //outside closure causes the error!!val combined = maxValues.cogroup(in)val new: RDD[(SpatialKey, Tile)] =      combined.map{ case(key,tiles) =>        val c = tiles._1.head;        val d = tiles._2.head;        (key, IfCell(c, d, { (x: Double, y: Double) => x == y }, doy, 0.0))      }```",7/15/16,1
ardilgulez,http://stackoverflow.com/questions/38398180/combining-rgb-channels-on-landsat-and-converting-to-png-gives-a-horrible-result,7/15/16,1
lossyrob,"Hey @ardilgulez, check out our landsat tutorial, I think it will help: https://github.com/geotrellis/geotrellis-landsat-tutorial",7/15/16,1
lossyrob,The code is commented and you should be able to follow along. Let us know if there_Ñés anything that_Ñés unclear so we can make it better,7/15/16,1
ardilgulez,"Thanks for the response Rob, I've been reading it for the past 2 days",7/15/16,1
ardilgulez,Tried to implement my own example,7/15/16,1
lossyrob,"@akarmas ah, serialization issues_Ñ_you get better and better at detecting them as you do more and more Spark, but they always seem to bite",7/15/16,1
ardilgulez,"Simple stuff, just getting channels 2, 3 and 4, combining them and rendering a png",7/15/16,1
ardilgulez,png just turns out to be horrible,7/15/16,1
ardilgulez,as in the stackoverflow question i just posted :(,7/15/16,1
lossyrob,do you have your code up on github,7/15/16,1
ardilgulez,"no, but i can put it in seconds",7/15/16,1
lossyrob,"```scala if(r + g + b == 0) 0                else {                    val color = ( r + g + b ) / 3                    ((color & 0xFF) << 24) | ((color & 0xFF) << 16) | ((color & 0xFF) << 8) | 0xFF                }```",7/15/16,1
lossyrob,what is this code doing,7/15/16,1
lossyrob,it takes the average color of the 3 bands and sets a grayscale color to the pixel,7/15/16,1
ardilgulez,yeah,7/15/16,1
ardilgulez,is it wrong?,7/15/16,1
lossyrob,I feel like it might be. Put the code up on github so I can run it and I_Ñéll investigate ,7/15/16,1
ardilgulez,ok,7/15/16,1
ardilgulez,https://github.com/ardilgulez/gt-1,7/15/16,1
ardilgulez,this is my github,7/15/16,1
lossyrob,how can I get the data,7/15/16,1
ardilgulez,oh sry,7/15/16,1
ardilgulez,https://libra.developmentseed.org/,7/15/16,1
lossyrob,any way you can get me the specific files you were working with?,7/15/16,1
ardilgulez,let me try making a bash script for that,7/15/16,1
ardilgulez,nvm i can't ,7/15/16,1
ardilgulez,it requires my aws s3 auth,7/15/16,1
ardilgulez,i downloaded landsat images of my hometown from aws,7/15/16,1
lossyrob,from the public landsat images hosted on S3?,7/15/16,1
ardilgulez,yes,7/15/16,1
lossyrob,"ok, if you give me the S3 paths, I can download them",7/15/16,1
ardilgulez,LC81750342016185LGN00,7/15/16,1
lossyrob,you don_Ñét have the full s3 path by any chance do you?,7/15/16,1
lossyrob,"if not that_Ñés ok, I_Ñéll just have to figure it out",7/15/16,1
ardilgulez,landsat-pds/L8/175/034/,7/15/16,1
lossyrob,"ok, thanks",7/15/16,1
ardilgulez,thank you,7/15/16,1
lossyrob,the output tif looks fine,7/15/16,1
lossyrob,what are you trying to have the PNG look like? I think your averaging logic is causing the weirdness,7/15/16,1
lossyrob,do you just want a false color image?,7/15/16,1
ardilgulez,I'm trying to get one regular rgb image and a grayscale image,7/15/16,1
lossyrob,oh I see. it_Ñés not rescaled,7/15/16,1
lossyrob,"so, the landsat pixels are coming in as UInt values",7/15/16,1
ardilgulez,rescaled? nah,7/15/16,1
lossyrob,"and the logic to convert it to an rgb, or grayscale, is doing that `0xFF` bitmasking",7/15/16,1
ardilgulez,if rescaling = converting 256x256 image into 128x128,7/15/16,1
lossyrob,so you_Ñére downcasting a large int to a byte,7/15/16,1
lossyrob,no,7/15/16,1
lossyrob,converting the pixel values to byte values on a 0 to 255 scale,7/15/16,1
ardilgulez,oh,7/15/16,1
lossyrob,one sec I_Ñéll mess with it a bit,7/15/16,1
lossyrob,rescaling landsat is tough because you_Ñére basically doing color correction,7/15/16,1
ardilgulez,you can't just divide by the coefficient?,7/15/16,1
lossyrob,how do you mean,7/15/16,1
ardilgulez,"Like, you can just convert #FFFFFF into #FFF by adding one, dividing by 16 and subtracting one on each color",7/15/16,1
lossyrob,"I_Ñém not sure, you could try that",7/15/16,1
lossyrob,"the thing with the values though, is that if you just normalize them to the min and max, the image comes out all white. So I usually use some values I_Ñéve poked around and arrived at",7/15/16,1
ardilgulez,how can I fix my stuff,7/15/16,1
lossyrob,https://github.com/ardilgulez/gt-1/pull/1,7/15/16,1
owcm,"I have a utility that use to write out Avro Layers using HadoopLayerWriter.   Running this in EMR Spark.   I was writing out layers out successfully for a while during by job and then experienced a ""Failed to write Layer(name = ""7300a7ba-3963-4b5a-af1a-91bcd57b106a-ccm-AVLB_M60-pl"", zoom = 18)"" error.  Here is my utility:",7/15/16,1
owcm,"```def writeAvroLayer( jobId: String, maxZoom: Int,  dataRdd: RDD[(SpatialKey, Tile)]    with Metadata[TileLayerMetadata[SpatialKey]] ) (implicit sc: SparkContext): Unit = {    var avroDataStore = ""/"" + SupportUtils.getAppConfigValue( ""ingest-hdfs-catalog-storage-dest"" )    val targetLayoutScheme = ZoomedLayoutScheme(WebMercator, 512)    val gtTilesFSPath: Path = new org.apache.hadoop.fs.Path(avroDataStore)    val conf = SparkUtils.hadoopConfiguration    // Create the attributes store that will tell us information about our catalog.    val attributeStore = HadoopAttributeStore( gtTilesFSPath, conf )    // Create the writer that we will use to store the tiles in the local catalog.    val writer = HadoopLayerWriter(gtTilesFSPath, attributeStore)    val writeOp =      Pyramid.upLevels( dataRdd, targetLayoutScheme, maxZoom) { (rdd, z) =>        val layerId = LayerId(jobId, z)        // If the layer exists already, delete it out before writing        if ( attributeStore.layerExists(layerId )) {          new HadoopLayerManager(attributeStore).delete(layerId)        }        writer.write(layerId, rdd, ZCurveKeyIndexMethod)      }  }```Should there be handles I should be closing after I  am done using a HadoopLayerWriter.  Wondered if anything like this has been reported before.Thanks",7/15/16,1
lossyrob,"looks right, is there more exception info we can look at?",7/15/16,1
owcm,"Trying to tease that out of the spark logs, Was a long running job on 6 nodes.",7/15/16,1
owcm,"@lossyrob  The only exception around the time that this error occurred is captured in the following gist https://gist.github.com/owcm/5353e6558ed143e0b11a658eeb35a02fI don't see anything traceable to file write activity.   But a underlying channel io issue.",7/15/16,1
echeipesh,"@owcm the original exception you showed, `Failed to write layer` should have a `Caused dy` section some ways down. Is it truncated or just plain not there ?",7/15/16,1
owcm,"@echeipesh  The ""Failed to write Layer(name = ""d514e69b-2729-4355-874c-0066370ab8cb-ccm-Dismounted-pl"", zoom = 18)"" was simply the exception handler message I received on my try catch. I will start dumping the stack trace in case it happens again.",7/15/16,1
akarmas,"@lossyrob haha thats for sure..but it still bugs me...how can you pass a variable to a closure....i mean in my example the trueval in IfCell depends on the input..so how can i  pass it and avoid the serialization issue?",7/15/16,1
echeipesh," @akarmas there is a bag of tricks to work around. I think in your case you_Ñére refering to a field of a class, which grabs that class? In that case if you re-assign the field to a local `val`, you_Ñére ok (ex: https://github.com/geotrellis/geotrellis/blob/master/spark/src/main/scala/geotrellis/spark/io/slippy/FileSlippyTileReader.scala#L37)",7/15/16,1
akarmas,"@echeipesh I think my case is different. I have this (just to get the picture)```object  app {  var doy = 0.0  def main(args: Array[String]): Unit = {    //init spark context etc    // read input    val image_time = from input args    doy = image_time.getDayOfYear().toDouble   try {      run(sc)} finally {      sc.stop()    }  }  def run(implicit sc: SparkContext) = {     val test: RDD[(SpatialKey, Tile)] =      combined.map{ case(key,tiles) =>        val c = tiles._1.head;        val d = tiles._2.head;        (key, IfCell(c, d, { (x: Double, y: Double) => x == y }, doy , 0.0))      }}}```",7/15/16,1
echeipesh,@akarmas I think if you just made it an argument to `run` method it would get serialized fine,7/15/16,1
mchendricks1,"Hello. I'm having a hard time running Cassandra (Debain) in a certain container configuration (Docker is choking it seems). My question, is Cassandra core to the geodocker stack? (E.g., I know it won't work without Zookeeper instances)",7/15/16,1
akarmas,"@echeipesh Thanks a lot!!!Tested it and your suggestion did the trick!! :)",7/15/16,1
echeipesh,@mchendricks1 We_Ñére just recently started testing against Cassandra and like you are seeing some instances where Cassandra containers are just choking and and dropping out under heavy writes. (https://github.com/geotrellis/geotrellis-landsat-emr-demo/pull/6) @pomadchin is hacking on it to figure out the cause and the solution.,7/15/16,1
echeipesh,"@mchendricks1 But to answer your qeustion Cassandra support is pretty important, it_Ñés a nice deployment and simplifies the stack, so we_Ñére going to keep working on it and I consider it to be part of geodocker.",7/15/16,1
echeipesh,Also I don_Ñét think cassandra requires zookeeper I believe it has it_Ñés own implemintation of paxos.,7/15/16,1
pomadchin,"@mchendricks1 so yes, as @echeipesh mentioned cassandra is in a _ÑÒbeta_Ñù stage right now",7/15/16,1
mchendricks1,@echeipesh Well I meant that most of the rest of the geodocker stack won't rn.,7/15/16,1
mchendricks1,run,7/15/16,1
mchendricks1,without zk,7/15/16,1
mchendricks1,"But if I don't deploy Cassandra, it's not clear what I lose by doing that.",7/15/16,1
echeipesh,Not sure I follow the exact phrasing but the idea is that you could could the way you want to persist data. You need zookeeper to run Accumulo and HBase but not Cassandra and HDFS.,7/15/16,1
echeipesh,"Both GeoWave and GeoMesa have Cassandra backends in some form, but it_Ñés new addition for them as well.",7/15/16,1
mchendricks1,"I see. Yes, that's what I needed to know. I'm going to backburner Cassandra issues for now then.",7/15/16,1
mchendricks1,Thanks for the feedback!,7/15/16,1
echeipesh,"No worries, thanks for dropping by  :) Out of curiousity what use case are you chasing ?",7/15/16,1
pomadchin,"@mchendricks1 good to hear that you are interested in cassandra support; however on _small_ datasets cassandra definitely should work _ÑÒas is_Ñù without hacking on deployment configuration / connection settings / etc",7/15/16,1
mchendricks1,"But if I had to guess it seems like the jessie:backports that Cassandra Debian Docker container is using is not the most stable (if I've understood ""backports"" correctly)",7/15/16,1
mchendricks1,"As for use case, I'm trying to deploy this through Marathon. I should check the Cassandra Marathon containers perhaps. (if they are any different)",7/15/16,1
echeipesh,"Yeah, I guess it_Ñés just a step above _ÑÒstable_Ñù from what I understand (https://backports.debian.org/) not sure what component they need updated for the container, as-in why wouldn_Ñét just just one jessie",7/15/16,1
mchendricks1,Exactly.,7/15/16,1
lossyrob,@owcm sorry had to step away for a while. My hunch is that there_Ñés some memory issues killing off your executors. Hard to say though from what you have. Is it a large dataset? Would mucking with the number of partitions change anything?,7/15/16,1
spereirag,"Hey guys, quick question about docker. When I try to run zookeeper (as `./1-zookeeper.sh -t=latest -zi=1 -zs1=GeoServer1 -zs2=GeoServer2 -zs3=GeoServer3`), I get a docker container constantly restarting. I then edited the 1-zookeeper.sh file to get the output of the container execution and I get `/sbin/entrypoint.sh: line 6: /data/zookeeper/myid: Permission denied`. I further edited the script to change the mount point from '/data/zookeeper' to 'data/zookeeper2' and now the container is stable, though I guess this is not the proper solution. I have the same problem with the hadoop-name script. Any ideas on how to fix this?",7/18/16,1
spereirag,"And btw once I get the container running, I can attach and modify the `/data/zookeeper/myid` file with no problems",7/18/16,1
spereirag,I figured it out. I was the SELinux problem. I had fixed that in every machine but this one. I'll leave the reference link in case someone else runs into the same problem in the future. http://www.projectatomic.io/blog/2015/06/using-volumes-with-docker-can-cause-problems-with-selinux/,7/18/16,1
pomadchin,@spereirag sorry for late response; great news that everything works! can you make an issue with that solution? or some pr to add this information into README.md ?,7/18/16,1
schBen,"Hi! I want to read in landsat geotiffs and work with them as multiband RDDs. However, at AWS they are stored as singleband tiffs (e.g. http://landsat-pds.s3.amazonaws.com/L8/139/045/LC81390452014295LGN00/index.html). Also, there are other files with the same prefix that I don't need (I guess). So how should I get multi(bandTile) RDDs efficiently if I just need all the bands? Should I read in every file as a single band RDD and then join them all? (In the end I want to do this for multiple scenes in parallel)",7/19/16,1
ardilgulez,I am on a similar thing myself and that's kinda what's bugging me. I need different combinations for different types of analyses so what I'm going to will be to get some sort of utility to download bands for me and trigger a re-tiling operation.,7/19/16,1
echeipesh,"@schBen @ardilgulez I_Ñém not sure if this was mentioned but we wrote a util library to consume the landsat pds images specifically: https://github.com/azavea/scala-landsat-util, you can see it being used here: https://github.com/geotrellis/geotrellis-landsat-emr-demo/blob/master/ingest/src/main/scala/demo/LandsatIngest.scala#L139-L170",7/19/16,1
echeipesh,Does that cover what you need?,7/19/16,1
echeipesh,That landsat-demo project also fails over to getting images from google cloud. I did some spot checks and it appears that not all scenes are stored on landsat-pds bucket.,7/19/16,1
ardilgulez,Mostly. I still have to implement some kind of a queue system on top of that but this saves me from the burden of coding a utility to download the most recent landsat images,7/19/16,1
ardilgulez,Failover is a nice touch too. I didn't even know I could resort to some other place than aws if aws s3 failed to deliver,7/19/16,1
echeipesh,"Yup, it_Ñés just query and fetch. How are you getting the notifications of the latest landsat images? ",7/19/16,1
ardilgulez,I don't. Ifit's ,7/19/16,1
ardilgulez,"If it's been 16 days over the last image, i go get the new one",7/19/16,1
schBen,"@echeipesh I will try it out, it looks quite useful. Thank you!",7/19/16,1
aaron-santos,"Hi, I've got a question about structuring layers. I'm ingesting Sentinel2 imagery and my use case requires ingesting imagery as it's being made available to the public",7/21/16,1
aaron-santos,"I'm creating one spatial layer per image captured by the satellite, but that results in a lot of layers in my catalog. I'm wondering if I can/should move to creating one layer indexed by space-time and if that would even be possible",7/21/16,1
aaron-santos,I don't exactly know the space-time extent upfront and I'm not sure how that would play into things or if that's even possible,7/21/16,1
ardilgulez,You need to do the indexing yourself,7/21/16,1
ardilgulez,It's really not that difficult though,7/21/16,1
echeipesh,"@aaron-santos Sure, basically you just need to provide some way to generate a `SpaceTimeKey`. You can do that either by providing `TemporalProjectedExtent` instead of `ProjectedExtent` before your ingest or to map over keys and map from `SpatialKey` to `SpaceTimeKey` after the ingest. The result would be pretty much the same.",7/21/16,1
echeipesh,"The only thing to be aware of is that you should specifify the max space-time bounds when creating the layer, covering the anticipated range of time, otherwise it_Ñés going to use the bounds of the first layer written and that_Ñés not going to be useful as it will reject updates out of those bounds.",7/21/16,1
aaron-santos,"gotcha. It sounds like if I establish those upfront, then I should be add additional data as it comes in as long as it has a `SpaceTimeKey`",7/21/16,1
echeipesh,"yep, exactly right.",7/21/16,1
aaron-santos,Cool. Once I have some data ingested will I be able to query for available times given a `ProjectedExtent`?,7/21/16,1
echeipesh,"Also worth noting is that since time does not have a set resolution you_Ñére going to have to define it when creating your `KeyIndexMethod`, it should be about related to frequency of your imagery updates.",7/21/16,1
aaron-santos,"If I set it too wide, then my `SpaceTimeKey`s might collide?",7/21/16,1
echeipesh,@aaron-santos You will be able to use the query syntax like so: https://github.com/geotrellis/geotrellis/blob/master/spark/src/test/scala/geotrellis/spark/io/LayerQuerySpec.scala#L36 but you_Ñéll be able to also use `Between` caluse to specify a time range.,7/21/16,1
echeipesh,"They will collide, but what that means is that collisions will be stored in the same block. Very similar logic and trade-off to binning HashTable",7/21/16,1
echeipesh,"so if its 2-5 collisions it_Ñés mostly _ÑÒmeh_Ñù, if it_Ñés couple of hunrdred you_Ñére probably going to be doing a lot more IO than your queries require.",7/21/16,1
aaron-santos,Nice. That's way better than what I was thinking was going to happen,7/21/16,1
echeipesh,"It makes writes a little expensive because we have to force a shuffle to detect collisions, but its WAY more useful than just randomly overwriting and losing imagery. At least that was our opinion.",7/21/16,1
aaron-santos,Thanks for mentioning that. That totally changes everything. :),7/21/16,1
aaron-santos,I'm going to check out https://github.com/geotrellis/geotrellis/blob/88ed5a56afefc0e34ed998ed9fa6830853d1728a/spark/src/main/scala/geotrellis/spark/io/LayerFilter.scala too. Thanks for the help @echeipesh :),7/21/16,1
echeipesh,No problemo :) Let us know how it works out for you and if you have more questions.,7/21/16,1
akarmas,"Hi all,I have created a pyramided layer with  Pyramid.upLevels function. Although the layer displays as expected for its native zoom level (i.e. 13) it does not behave nice with the other zoom levels as most tiles are filled completely with NoData values.I cannot understand why this is happening. I am suspecting that i have to change the resampling function. Is this a step to the correct direction? And if yes how am i supposed to pass another resampling function to  Pyramid.upLevels function??Thanks in advance!!",7/22/16,1
echeipesh,"@akarmas That_Ñés a pretty strange failure, having trouble imagning how it could have happened. You can set the resample method as a param to `upLevels` function (ex: https://github.com/geotrellis/geotrellis-landsat-emr-demo/blob/master/ingest/src/main/scala/demo/LandsatIngest.scala#L111) but it defaults to NearestNeighbor, which is completly safe and well tested. I would not expect to find a problem there.",7/22/16,1
echeipesh,@akarmas I would start looking to see if there is something strange going on with the `cellType` of the tiles. Could you show the snippet of the code in question to help me generate more ideas ?,7/22/16,1
spereirag,Is there a simple way to create a geodocker-cluster with scala 2.11?,7/22/16,1
echeipesh,Actually I think most of it doesn_Ñét really concern scala. It would only  effect `spark` builds at the moment. You would need to build spark for 2.11 (there isn_Ñét a pre-build binary from Spark) and then rebuild the container using those artifacts.,7/22/16,1
spereirag,"Ok, thanks!",7/22/16,1
lossyrob,@akarmas what resampling method are you using?,7/25/16,1
lossyrob,"that sounds like a NoData problem. If the incoming rasters do not have an explicit nodata value attached, and you try to use something like Bilinear, it will be a problem, since the Bilinear and merging knows to avoid NoData values. What is the cell type of the rasters?",7/25/16,1
uuuuq,ingest temporal multiband geotiff from hdfs to accumulo got:,7/26/16,1
uuuuq,"```Unable to find input module of type 'hadoop' for format `temporal-geotiff```",7/26/16,1
uuuuq,"i check the source code of `geotrellis.spark.etl`,  `TemporalMultibandGeoTiffHadoopInput` was registered in `HadoopModule`, so why i got this?",7/26/16,1
uuuuq,,7/26/16,1
uuuuq,"and here is the shell script i use```#!/bin/shexport JAR=/home/test/geotrellis-spark-etl-assembly-1.0.0-SNAPSHOT.jar/usr/local/spark/spark-1.6.1-bin-hadoop2.6/bin/spark-submit \--class geotrellis.spark.etl.MultibandIngest \--master spark://master:7077 \--driver-memory 12G \$JAR \--input hadoop --format temporal-geotiff --cache NONE -I path=""hdfs://master/data"" \--output accumulo -O instance=""gis"" table=""data"" user=""tester"" password=""test"" zookeeper=""192.168.1.7:2181"" \--layer test --crs EPSG:3857 --pyramid --layoutScheme tms```",7/26/16,1
pomadchin,"@uuuuq touch; intersting, what gt version do you use?",7/26/16,1
pomadchin,`0.10.2` ?,7/26/16,1
ardilgulez,I understand nothing from reading the codes or examples,7/26/16,1
ardilgulez,source codes*,7/26/16,1
ardilgulez,what should I know in advance so that I understand this stuff,7/26/16,1
ardilgulez,other than scala ofc,7/26/16,1
pomadchin,"@ardilgulez Hi, think you already viewed [docs](https://github.com/geotrellis/geotrellis/tree/master/docs) if not, that would be rather usefulthat would be good for you to be familiar (basics) with spark and with hdfs / s3 / accumulo / cassandra ",7/26/16,1
ardilgulez,I kinda know what s3 is,7/26/16,1
pomadchin,"we can help you if you have more concrete questions o:what do you want to do finally? what are you planning to do?what are you trying to do?",7/26/16,1
ardilgulez,I'm trying to do all types of on the fly raster analysis,7/26/16,1
pomadchin,mmm; before making any analysis on geo raster / vector data you need to ingest it into backend you want (s3 in your case); after that you would have an api to query ingested data and to make different geo / etc operations on it,7/26/16,1
ardilgulez,"what does ""ingest"" mean",7/26/16,1
ardilgulez,i'm that new to this thing,7/26/16,1
pomadchin,"ingest means, insert data into you database, to have quick access to it (as data would be indexed (z3 / hilbert curves: https://github.com/locationtech/sfcurve))",7/26/16,1
pomadchin,"say you have a set of tiffs with geo metadata; ingest process would take your tiles, reproject it into desired projection (crs you want to store your tiles) and save into desired backend (s3 in your case)",7/26/16,1
ardilgulez,"I kinda got the basic workflow: step 1 - I obtain the raw raster data somehow (this is the easy part)step 2 - Turn the raw data into ??? that is servable to the user (I think this is what we say ingestion and I have absolutely no idea what this thing is) step 3 - Serve the data to the userstep 4 - Make calculations depending on the user input",7/26/16,1
ardilgulez,step 1 is easy and can be done without the help of gt,7/26/16,1
pomadchin,step2 its the ingest process,7/26/16,1
ardilgulez,step 2 is where i lose it,7/26/16,1
pomadchin,https://github.com/geotrellis/geotrellis/blob/master/docs/spark-etl/spark-etl-intro.md,7/26/16,1
ardilgulez,"val sourceTiles = etl.load[ProjectedExtent, Tile]",7/26/16,1
ardilgulez,"the ""Tile"" in this is the input singleband or multiband tile, right?",7/26/16,1
pomadchin,"\+ we have couple of demos: https://github.com/geotrellis/geotrellis-chatta-demohttps://github.com/geotrellis/geotrellis-landsat-emr-demo",7/26/16,1
pomadchin,"yes, definitely",7/26/16,1
ardilgulez,I've been checking the ins and outs of landsat-emr-demo lately,7/26/16,1
pomadchin,"you can try with geotrellis chatta demo, though it has a bit outdated docs ): ",7/26/16,1
akarmas,"@echeipesh  @lossyrob Well the cell type is DoubleCellType and i am using the default resampling method (i.e. Nearest Neighbour).Incoming rasters do not have an explicitly NoData value attached. Do you think that this is the root of the issue?ï_he layer's values are float numbers that indicate different categories. Is it possible that for example the 0.0 value is interpreted as NoData? If so how can i set the NoData value when i write a new layer?Thanks guys!!",7/26/16,1
lossyrob,"@akarmas there is a `convert` method on Tile, and also on TileLayerRDD[K], that will convert the cell type",7/26/16,1
lossyrob,Convert to `DoubleConstantNoDataCellType`,7/26/16,1
lossyrob,it_Ñés potentailly the cause. A lot of problems I_Ñéve seen similar to that have been because mechanisms that are expected to do one thing do something else because what we think it should treat as NoData and what we explicitly tell it is NoData are two different things,7/26/16,1
lossyrob,@ardilgulez I_Ñém curious about how your having difficulty reading the code. What parts are hard to read? Have you used Scala before this?,7/26/16,1
lossyrob,"Any help you could give on improving our documentation, by pointing out places we can improve, or giving us an understanding of how someone who doesn_Ñét get how the project works looks at the project, woud be appreciated.",7/26/16,1
ardilgulez,"Matter of fact, I haven't worked with Scala before until Geotrellis",7/26/16,1
ardilgulez,"Still, code itself isn't unreadable",7/26/16,1
lossyrob,ah. how are you finding it?,7/26/16,1
ardilgulez,"elegant, to say the least",7/26/16,1
lossyrob,I agree :),7/26/16,1
ardilgulez,https://github.com/geotrellis/geotrellis/tree/master/docs,7/26/16,1
lossyrob,"the implicits, particularly the context bounds, usually trip people up",7/26/16,1
ardilgulez,docs here are quite good too,7/26/16,1
ardilgulez,made a lot of progress just reading through this thing,7/26/16,1
lossyrob,glad that helped. I think we need a bit more on the initial README to guide the user through the topics,7/26/16,1
ardilgulez,I'm actually new to this gis thing entirely,7/26/16,1
ardilgulez,just joined this new company less than 3 months ago,7/26/16,1
lossyrob,that_Ñés a bit to learn all at once then,7/26/16,1
lossyrob,"I had a similar experience_Ñ_learned Scala to work on GeoTrellis, learned GIS the same time - a lot of coming up to speed",7/26/16,1
ardilgulez,that's a mouthful,7/26/16,1
ardilgulez,that i'm trying to chew now,7/26/16,1
lossyrob,"well if there_Ñés any GIS questions, we_Ñére GIS devs so not GIS experts, but feel free to talk about that stuff here too",7/26/16,1
ardilgulez,no gis questions for now,7/26/16,1
lossyrob,"imagery processing is a whole other topic in GIS, I mainly did raster processing like suitability maps and hydrology operations over elevation at first. got into imagery afterwards",7/26/16,1
lossyrob,it_Ñés a really cool place to be right now,7/26/16,1
ardilgulez,"as for docs, spark related stuff just doesn't fit into my head",7/26/16,1
ardilgulez,and docs about it lack some meat tbh,7/26/16,1
lossyrob,do you have a pretty solid understanding of Apache Spark itself?,7/26/16,1
ardilgulez,nothing on spark itself,7/26/16,1
lossyrob,"good to note, we_Ñéll have to add some meat",7/26/16,1
ardilgulez,gotta work on that too,7/26/16,1
lossyrob,that would probably be good - we are just enhancing Spark to deal with raster layers. Most of the heavy lifting is done by Spark,7/26/16,1
ardilgulez,yeah,7/26/16,1
ardilgulez,the queries on raster layers and stuff,7/26/16,1
lossyrob,"The querying is pretty complicated, in that it goes through some code that is quite awesome but hard to grok. very scala type-level stuff",7/26/16,1
lossyrob,"but it all comes down to taking a 2D or 3D bounding box, and then turning it into KeyBounds",7/26/16,1
lossyrob,and turning the KeyBounds into a set of Space Filling Curve index ranges,7/26/16,1
lossyrob,and querying the backend according to that,7/26/16,1
ardilgulez,where do i learn what Keybounds and Space Filling Curve index ranges are,7/26/16,1
lossyrob,good question...,7/26/16,1
lossyrob,Space Filling Curves is a whole thing. I_Ñéll get some links together,7/26/16,1
lossyrob,"but that is how we take 2D and 3D queries (N-Dimensionsal in theory) and turn them into a set of 1-D range queries, which can be applied to a backend",7/26/16,1
lossyrob,Both GeoWave and GeoMesa use a similar technique,7/26/16,1
lossyrob,http://ngageoint.github.io/geowave/documentation.html#theory,7/26/16,1
lossyrob,That should explain SFC_Ñés to some detail.,7/26/16,1
lossyrob,"We have 2 types of keys: `SpatialKey` and `SpaceTimeKey`. Both have a tile `col` and `row`, and `SpaceTimeKey` has a time element to it. KeyBounds are simply a `minKey` and `maxKey` bounds in the key space. So really representing a bounding box, but in tile coordinates (and also time if we_Ñére dealing with SpaceTimeKey)",7/26/16,1
ardilgulez,tile coordinates = epsg 3857?,7/26/16,1
lossyrob,tile coordinates are according to some tile layout,7/26/16,1
lossyrob,`TileLayout`,7/26/16,1
lossyrob,"so `SpatialKey(5, 6)` represents the tile at column 5, row 6",7/26/16,1
ardilgulez,uh huh,7/26/16,1
lossyrob,"so there_Ñés a translation between map coordinates (somehting like EPSG 3857), and tile coordinates",7/26/16,1
ardilgulez,aha,7/26/16,1
lossyrob,these are used in web maps accordint to specific tile layouts,7/26/16,1
lossyrob,often called a `TMS` layout,7/26/16,1
ardilgulez,"so SpatialKey(5, 6) means {z}/5/6",7/26/16,1
lossyrob,"although that is slightly a misnomer, I won_Ñét get too deep into that",7/26/16,1
lossyrob,yeah,7/26/16,1
lossyrob,z/x/y,7/26/16,1
lossyrob,"so the zoom, which we actually hold in the `LayerId`",7/26/16,1
lossyrob,"and the `x/y` would translate to `SpatialKey(x, y)`",7/26/16,1
lossyrob,"so you could think of a query being like, from `4/3/5` to `4/5/9`",7/26/16,1
lossyrob,"i.e. `KeyBounds(SpatialKey(3, 4), SpatialKey(5, 9))`",7/26/16,1
ardilgulez,what about queries within a single tile?,7/26/16,1
lossyrob,"and those KeyBounds could be derived from some Extent (bounding box in map coordinates), and the metadata about the tile layout",7/26/16,1
lossyrob,You use a `ValueReader` for that,7/26/16,1
lossyrob,and just query according to the single key,7/26/16,1
lossyrob,"```scalaval reader = S3ValueReader(bucket, prefix).reader(LayerId(_ÑÒsome-name_Ñù, zoomeLevel))reader.read(SpatialKey(3,4))```",7/26/16,1
lossyrob,for instance,7/26/16,1
lossyrob,"that reader call needs type parameters, but that_Ñés the basic idea",7/26/16,1
ardilgulez,"reader.read returns a single tile, right",7/26/16,1
ardilgulez,?,7/26/16,1
lossyrob,yes,7/26/16,1
lossyrob,the reader is for a specific LayerId,7/26/16,1
lossyrob,"the `ValueReader` is actually somethig that produces those types of readers, given an LayerId",7/26/16,1
ardilgulez,"which gets layer name and zoom level, and using spatial key 3,4",7/26/16,1
ardilgulez, i get a single tile,7/26/16,1
lossyrob,yup,7/26/16,1
ardilgulez,what about querying on extents,7/26/16,1
lossyrob,"You use a LayerReader, e.g. `S3LayerReader`",7/26/16,1
lossyrob,and use the query syntax,7/26/16,1
ardilgulez,oh yeah,7/26/16,1
ardilgulez,that's what happens in landsat-emr-demo example,7/26/16,1
ardilgulez,lol,7/26/16,1
lossyrob,"```scalaval reader: S3LayerReader = ???val layer: MultibandTileLayerRDD[SpaceTimeKey] =  reader.query(LayerId(_ÑÒsome-layer_Ñù, zoomLevel))    .where(Intersects(Extent(xmin, ymin, xmax, max)))    .result```",7/26/16,1
lossyrob,yeah,7/26/16,1
ardilgulez,,7/26/16,1
ardilgulez,damn this looks bad,7/26/16,1
ardilgulez,can you please help me understand this thing?,7/26/16,1
lossyrob,"sure, but I have to step away for a bit, be back in a couple of hours",7/26/16,1
ardilgulez,ok,7/26/16,1
pomadchin,"@ardilgulez p/s/ markdown there is used like on github \```scala<space><code><space> \```",7/26/16,1
ardilgulez,,7/26/16,1
ardilgulez,ugh,7/26/16,1
ardilgulez,"```scalaval (_, rasterMetaData) =TileLayerMetadata.fromRdd(inputRdd, FloatingLayoutScheme(512))val tiled: RDD[(SpatialKey, MultibandTile)] =  inputRdd    .tileToLayout(rasterMetaData.cellType, rasterMetaData.layout, Bilinear)    .repartition(100)val layoutScheme = ZoomedLayoutScheme(WebMercator, tileSize = 256)val (zoom, reprojected): (Int, RDD[(SpatialKey, MultibandTile)] with Metadata[TileLayerMetadata[SpatialKey]]) =  MultibandTileLayerRDD(tiled, rasterMetaData)    .reproject(WebMercator, layoutScheme, Bilinear)val attributeStore = FileAttributeStore(outputPath)val writer = FileLayerWriter(attributeStore)Pyramid.upLevels(reprojected, layoutScheme, zoom, Bilinear) { (rdd, z) =>  val layerId = LayerId(""landsat"", z)  if(attributeStore.layerExists(layerId)) {    new FileLayerManager(attributeStore).delete(layerId)  }  writer.write(layerId, rdd, ZCurveKeyIndexMethod)}```",7/26/16,1
ardilgulez,better now,7/26/16,1
ardilgulez,"sorry for flooding, please delete all prior code blocks",7/26/16,1
pomadchin,;) ,7/26/16,1
pomadchin,"@ardilgulez ```scala// getting metadata for input tiles, in native projectionval (_, rasterMetaData) =TileLayerMetadata.fromRdd(inputRdd, FloatingLayoutScheme(512))// inputRdd is RDD[(ProjectedExtent, MultibandTile)]// pairs of tiles projected extent, we use extents as keys and of tiles// we need to cut them to native tiles layout (from tiles metadata)// what the F* rdd is?// you can think about rdd (very very BAD comparison) as a distributed array// in our case distributed array of tuplesval tiled: RDD[(SpatialKey, MultibandTile)] =  inputRdd    .tileToLayout(rasterMetaData.cellType, rasterMetaData.layout, Bilinear)    .repartition(100)// destination layout scheme we want our tiles reproject intoval layoutScheme = ZoomedLayoutScheme(WebMercator, tileSize = 256)// reprojecting tiled tiles into web mercator in tiles 256x256 (see layoutScheme definition above)val (zoom, reprojected): (Int, RDD[(SpatialKey, MultibandTile)] with Metadata[TileLayerMetadata[SpatialKey]]) =  MultibandTileLayerRDD(tiled, rasterMetaData)    .reproject(WebMercator, layoutScheme, Bilinear)// in geotrellis we have at least two logically devided things// tiles itself (array of bytes / ints / doubles)// and metadata with tiles metadata information for the entire layer (crs, keybounds, etc)// AttributeStore abstraction provides an api to have access to read / write layer metadata information// in constructo we need to point path (in this case) to attribuetStore pathval attributeStore = FileAttributeStore(outputPath)// layer writer for AttribtueStoreval writer = FileLayerWriter(attributeStore)// when we are talking about maps, usually we talk about maps on //zoomed layout scheme; that sceheme allows us to persist resampled //tiles for different zoom levels: from max to 0 Pyramid.upLevels(reprojected, layoutScheme, zoom, Bilinear) { (rdd, z) =>  // we get there rdd for zoom level z  val layerId = LayerId(""landsat"", z)  if(attributeStore.layerExists(layerId)) {    new FileLayerManager(attributeStore).delete(layerId)  } // we write layer with name layerId and tiles in rdd using ZCuriveKeyIndex method  writer.write(layerId, rdd, ZCurveKeyIndexMethod)}```",7/26/16,1
pomadchin,sry for probably not complete comments; any questions are appreciated as well; ,7/26/16,1
ardilgulez,actually i deleted the comments in my code block (that i rewrote 5 times or something),7/26/16,1
ardilgulez,thanks for plain english comments,7/26/16,1
ardilgulez,helps noobs like me,7/26/16,1
cj2001,"hi, all.  please forgive my n00bness, but i was hoping someone could point me to an example with geotrellis of kriging based on sensor measurements taken at specific gps coordinates?  i have a large data set containing time, lon, lat, sensor_measurement and i am trying to make sense of it all, but am having a hard time getting started with geotrellis.  will it even work for my non-raster data?",7/26/16,1
lossyrob,"What format is your data in? Shapefile, GeoJSON, ...",7/26/16,1
cj2001,"csv (i know, i know...).  i have it read in to a sparksql dataframe just fine.  i would really like to perform universal kriging on it, but am coming up blank.",7/26/16,1
cj2001,"it is more than 37M rows, hence me teaching myself spark quickly (although not well)!",7/26/16,1
lossyrob,"hmm_Ñ_we dont_Ñé have an RDD form of Kriging, although my gears are spinning on how to do a distributed form of it",7/26/16,1
lossyrob,but basicall you would turn your DataFrame into an RDD of `PointFeature[Double]` (where the feature data was sensor measurement),7/26/16,1
lossyrob,"and then from there, be able to call Kriging on it",7/26/16,1
lossyrob,"well, if you `.collect`ed it",7/26/16,1
cj2001,making an rdd out of it is simple enough.  here is the code i currently have:,7/26/16,1
lossyrob,https://github.com/geotrellis/geotrellis/blob/master/docs/raster/kriging-interpolation.md,7/26/16,1
cj2001,"'''import org.apache.spark.sql.Rowimport org.apache.spark.sql.types._import org.apache.spark.sql.functions._import org.apache.spark.sql.functions.udf",7/26/16,1
lossyrob,backticks,7/26/16,1
cj2001,doh!,7/26/16,1
lossyrob,"_ÑÒ` ``scala  _Ñ_  ` ``""",7/26/16,1
lossyrob,hmm darn it won_Ñét not format it,7/26/16,1
lossyrob,https://help.github.com/articles/creating-and-highlighting-code-blocks/,7/26/16,1
cj2001,"```import org.apache.spark.sql.Rowimport org.apache.spark.sql.types._import org.apache.spark.sql.functions._import org.apache.spark.sql.functions.udfval file = ""s3://mybucket/data.csv""val df = sqlContext.read.format(""com.databricks.spark.csv"").option(""header"",""true"").option(""inferSchema"",""true"").load(file)val df2 = df.filter($""Grosscount"" > 0).filter($""lat"" > 40.109028).filter($""lat"" < 40.116430).filter($""lon"" < -88.222401).filter($""lon"" > -88.230338)df2.describe().show()```",7/26/16,1
cj2001,dang...i made that way too hard!  ;),7/26/16,1
lossyrob,I have an idea about how this would work,7/26/16,1
lossyrob,it_Ñés a little complicated though,7/26/16,1
lossyrob,let me spitball the idea,7/26/16,1
lossyrob,"if you can get an `RDD[PointFeature[Double]]`, which wouldn_Ñét be hard to do",7/26/16,1
lossyrob,then we can set the LayoutDefinition of what you want the resulting raster tiles to look like (defines the tile layout of the raster layer that will be the result of the kriging),7/26/16,1
cj2001,i can do df2.rdd,7/26/16,1
lossyrob,"well, we need them as `geotrellis.vector.Feature[G, D]` types with G = Point and D = Double",7/26/16,1
lossyrob,,7/26/16,1
lossyrob,"```scaladf2.rdd.map { row =>  val (lat, lng): (Double, Double) = ???  val data: Double = ??? // The measurement data  PointFeature(Point(lng, lat), data)}```",7/26/16,1
lossyrob,"using the tile layout, we assign a key to each point. We then group by key_Ñ_hmmm I need to think it through a bit",7/26/16,1
lossyrob,To pull this off we need a better version of BufferTiles,7/26/16,1
lossyrob,"or a different version, that can work with more generic data",7/26/16,1
lossyrob,sorry I_Ñém just thinking out loud here...,7/26/16,1
cj2001,ok...,7/26/16,1
lossyrob,let me write out some mock code of what I_Ñém thinking,7/26/16,1
cj2001,"thank you so much!  in the mean time, is there are manual for geotrellis i can be reading or is it all just in  the scala api doc?",7/26/16,1
cj2001,(seeking some more human-readable form since i don't yet understand the search terms with which to put together a reasonable bit of spark code),7/26/16,1
cj2001,nvm...i found the html markdown one <facepalm>,7/26/16,1
lossyrob,"```scalaval pointFeatures: RDD[PointFeature[Double]] = ???val layoutDefinition: LayoutDefinition = ???  // The target layout of raster tilesval keyed: RDD[(SpatialKey, PointFeature[Double])] =  pointFeatures.flatMap { pf =>    val centerKey @ SpatialKey(col, row) = layoutDefinition.mapTransform(pf.geom)    Seq(      (centerKey, pf),      (SpatialKey(col - 1, row), pf), // Left      (SpatialKey(col + 1, row), pf), // Right      (SpatialKey(col, row - 1), pf), // Upper      (SpatialKey(col, row + 1), pf), // Upper      (SpatialKey(col - 1, row - 1), pf), // Upper Left      (SpatialKey(col + 1, row - 1), pf), // Upper Right      (SpatialKey(col - 1, row + 1), pf), // Lower Left      (SpatialKey(col + 1, row + 1), pf)  // Lower Right    )  }val grouped: RDD[(SpatialKey, Iterable[PointFeature[Double]])] = keyed.groupByKeyval result: RDD[(SpatialKey, Tile)] =  grouped    .map { case (key, points) =>      val extent = layoutDefinition.mapTransform(key)      val (cols, rows) = layoutDefinition.tileLayout.tileDimensions      val rasterExtent = RasterExtent(extent, cols, rows)      // Perform kriging (taken from docs, not sure how to best do this TBH)      val sv: Semivariogram = NonLinearSemivariogram(points, 30000, 0, Spherical)      val krigingTile: Tile =        points.simpleKriging(rasterExtent, 5000, sv)      (key, krigingTile)    }```",7/26/16,1
lossyrob,"at that point you have a kriging tile for each tile coordinate, that takes into account any points contained in that tile, or contained in a tile that surrounds that tile",7/26/16,1
lossyrob,"and then what you do with that, I_Ñém not sure. What are you looking to do with this?",7/26/16,1
cj2001,"thank you for your thoughts on this!  i have to catch my bus, but then i will give it a try this evening and report back.  as far as what i am looking to do with this, i just need to try and make some sense out of this big sensor network data set.  and right now i can't visualize anything about the data beyond the basic statistics i can get out of the data frame.  so this will get me to a good start!  ",7/26/16,1
lossyrob,I would say that the next thing you might want to do is save these off as web map tiles so you can visualize it on a web map,7/26/16,1
cj2001,sounds like a good idea!,7/26/16,1
lossyrob,"it_Ñés a fun problem, thanks for bringing it to the room",7/26/16,1
cj2001,"definitely!  in a gps-enabled iot world, it would seem that there are plenty of opportunities to work on such problems!",7/26/16,1
cj2001,more this evening...,7/26/16,1
lossyrob,cheers,7/26/16,1
uuuuq,,7/27/16,1
uuuuq,"@pomadchin how to check the geotrellis version? the latest i think, it was just cloned from github.",7/27/16,1
ardilgulez,"/home/ardil/Desktop/dev/altitude/src/main/scala/Runner.scala:93: could not find implicit value for evidence parameter of type spray.json.JsonFormat[geotrellis.spark.SpatialKey]writer.write(layerId, rdd, ZCurveKeyIndexMethod)",7/27/16,1
ardilgulez,no clue why this happens,7/27/16,1
ardilgulez,copied the code from geotrellis-landsat-tutorial,7/27/16,1
ardilgulez,which was (practically) identical to the one in the gitter code block I wrote earlier,7/27/16,1
pomadchin,@ardilgulez missing implicits: add `import geotrellis.spark.io._`,7/27/16,1
julienrf,"Hi, I have a bunch of information associated to geographic locations. I_Ñéd like to krige the data, but it is not clear to me in which coordinate system my points should be, when defining my `PointFeature`. (my locations are expressed as latlng)",7/27/16,1
lossyrob,"Should be `Point(lng, lat)` (we always use x, y order regardless of CRS)",7/27/16,1
julienrf,Thanks!,7/27/16,1
julienrf,There is no need to configure the CRS somewhere?,7/27/16,1
lossyrob,"CRS_Ñés are implicit, unless you explicitly need them for operations",7/27/16,1
lossyrob,"for instance, if you are reprojecting",7/27/16,1
lossyrob,"```scalaval p: Point = Point(x, y)val reprojected = p.reproject(LatLng, WebMercator)```",7/27/16,1
lossyrob,the first CRS is the source CRS,7/27/16,1
julienrf,OK,7/27/16,1
walleyyang,"Hi, I_Ñém trying to install GeoTrellis locally. When using vagrant up I get the following: ERROR: An attempt to install Ansible role dependencies failed.https://github.com/geotrellis/geotrellis-ec2-clusterVagrant 1.8.5VirtualBox 5.0.26 r108824",7/27/16,1
cj2001,"Hi, all.  So I have this RDD of sensor data structured as: Array[(String, Double, Double, Int, Int)] which corresponds respectively to sensorID, lon, lat, sensorValue, timestamp.  My end state goal would be to create a pretty plot using universal kriging on this data. I am a complete newbie to geotrellis and am having a hard time getting started.  Can someone help point me in the right direction?",7/27/16,1
lossyrob,have you included geotrellis dependencies as part of the your project? (i.e. does `import geotrellis.spark._` compile)?,7/27/16,1
cj2001,"yes, it works (at least in spark-shell)",7/27/16,1
lossyrob,"ok, then you want to create point features out of that data",7/27/16,1
cj2001,done,7/27/16,1
lossyrob,"you have `RDD[PointFeature[Double]]`?",7/27/16,1
cj2001,hmm...not so much.  i have an array of points created with magellan.  give me a sec to fix that...,7/27/16,1
cj2001,where is PointFeature found?,7/27/16,1
lossyrob,`import geotrellis.vector._`,7/27/16,1
lossyrob,"```scalardd.map { point => PointFeature(Point(point.x, point.y), data) }```",7/27/16,1
lossyrob,not sure where data comes from,7/27/16,1
lossyrob,at that point you_Ñéd have the start to this https://gist.github.com/lossyrob/d4a3cbb27c068279f868b287af60cebb#file-kriging-on-rdd-point-features-scala-L1,7/27/16,1
cj2001,"man, github is slow today.",7/27/16,1
cj2001,brb...looks like i need a reboot...,7/27/16,1
cj2001,"```scalaval points = data.map { line => PointFeature(Point(line(2), line(1)), line(3)) }<console>:46: error: not found: value PointFeature```",7/27/16,1
cj2001,"here is the whole code...```scalaimport org.apache.spark.sql.Rowimport org.apache.spark.sql.types._import org.apache.spark.SparkContext._import java.io.StringReaderimport au.com.bytecode.opencsv.CSVReaderimport geotrellis.spark._import geotrellis.vector._val file = ""s3://cjsbucket/trialData.csv""val data = sc.textFile(file).map( line => line.split("","") ).map( line => (line(0), line(2).toDouble, line(1).toDouble, line(3).toInt, line(4).toInt) )val points = data.map { line => PointFeature(Point(line(2), line(1)), line(3)) }```",7/27/16,1
cj2001,It would seem that I do not have Geometry.scala because Point is not being recognized either.,7/27/16,1
lossyrob,interesting_Ñ_not sure how you are importing vector and not having Point exposed,7/27/16,1
lossyrob,"```scalaval points = data.map { line => geotrellis.vector.Feature(geotrellis.vector.Point(line(2), line(1)), line(3)) }```",7/27/16,1
lossyrob,try that,7/27/16,1
cj2001,"```scala<console>:47: error: object Feature is not a member of package geotrellis.vector         val points = data.map { line => geotrellis.vector.Feature(geotrellis.vector.Point(line(2), line(1)), line(3)) }                                                           ^```",7/27/16,1
cj2001,"ok...here is a dumb question.  i installed geotrellis via git (git clone https://github.com/geotrellis/geotrellis.git).  i am not a user of sbt or maven or anything since i am a total n00b, but did i not then build something i was supposed to build?  are there any install instructions that i missed?",7/27/16,1
lossyrob,how are you running the code you pasted?,7/27/16,1
cj2001,in spark-shell ... i have it saved as a .scala file that i run in the shell via :load ,7/27/16,1
lossyrob,ok. You would need to create a fat jar with geotrellis and load it in the shell,7/27/16,1
lossyrob,http://spark.apache.org/docs/latest/programming-guide.html#using-the-shell,7/27/16,1
lossyrob,"in your geotrellis clone, you can run",7/27/16,1
cj2001,"ok...are there any install instructions for geotrellis i can read?  i feel like this is a rtfm moment for me, but am having a hard time finding the manual.",7/27/16,1
lossyrob,`./sbt _ÑÒproject spark-etl_Ñù assembly`,7/27/16,1
lossyrob,"well, geotrellis is a library",7/27/16,1
lossyrob,so _ÑÒinstall_Ñù is not really a thing,7/27/16,1
lossyrob,it needs to be a dependency that_Ñés included,7/27/16,1
lossyrob,"so if you_Ñére building an application, you would list geotrellis as a dependency",7/27/16,1
lossyrob,"if you want to have access to it on the spark-shell, you have to jump through some hoops",7/27/16,1
lossyrob,the hoops being creating a JAR file with all of the geotrellis code and it_Ñés dependencies,7/27/16,1
lossyrob,and including it with the `_ÑÓjars` option,7/27/16,1
cj2001,"oy.  i think i am officially over my head now.  i thought this would be the right tool for me, but it looks like not.",7/27/16,1
lossyrob,"that_Ñés unfortunate. I know it_Ñés a pain to wrap heads around getting into using it (especially coming in not knowing the JVM ecosystem, Spark, Scala) - if there_Ñés anything else we could help with let us know. It_Ñés not easy stuff though :/",7/27/16,1
cj2001,"i know a bit of spark and a bit of scala, but not java (more of a python gal myself).  but for research purposes using the shell is my preference.  might you know of any other tools out there for kriging?  i have 37M rows to analyze and ph33r doing this in python.",7/27/16,1
cj2001,(even pyspark),7/27/16,1
lossyrob,"Not sure of other tools that do kriging, at that scale at least",7/27/16,1
lossyrob,you might want to check in with the GeoMesa or GeoWave folk,7/27/16,1
cj2001,yeah.  i had my hopes up.,7/27/16,1
cj2001,will do.  thank you for your help.!,7/27/16,1
lossyrob,is that data your working with open?,7/27/16,1
lossyrob,would like to get my hands on it if I could :),7/27/16,1
lossyrob,maybe give it a go in a demo project,7/27/16,1
cj2001,sort of yes and sort of no.  my group has been collecting it.  the s3 bucket above (s3://cjsbucket) should be public and the data file is a sample of our data.,7/27/16,1
cj2001,"""Grosscount"" is the sensor measurement and the final column is in Unix time.  the first column is just the sensor id.",7/27/16,1
lossyrob,"doesn_Ñét look like it_Ñés public, can_Ñét list the bucket with aws-cli",7/27/16,1
cj2001,hang on...let me fix that...,7/27/16,1
cj2001,"worse comes to worse, this file is only a few megs...",7/27/16,1
lossyrob,"as far as working with external libraries with spark-shell, any dependency will have to go through a similar process of the `_ÑÓjars` hoop, just fyi",7/27/16,1
lossyrob,"ah, cool. if you know of a bigger open dataset that would be similar, would be nice to point me to that",7/27/16,1
cj2001,see pm,7/27/16,1
lossyrob,"cool, thank you.",7/27/16,1
cj2001,np!  thank you!,7/27/16,1
ardilgulez,Cannot find JsonReader or JsonFormat type class for geotrellis.vector.Geometry,7/28/16,1
ardilgulez,which import am I missing,7/28/16,1
ardilgulez,I think i found it,7/28/16,1
ardilgulez,Geotrellis.vector.io._,7/28/16,1
ardilgulez,I'm guessing,7/28/16,1
ardilgulez,"Didn't ab test, just randomly added imports until i no longer saw red",7/28/16,1
ardilgulez,how can i obtain tiles only for an extent,7/28/16,1
lossyrob,obtain tiles from where?,7/28/16,1
lossyrob,what do you have to draw tiles from?,7/28/16,1
ardilgulez,I have tiles ready set and go,7/28/16,1
ardilgulez,I just want to serve only the parts that are enclosed by some geometry,7/28/16,1
lossyrob,"tiles as in - geotiffs, or as in an ingested layer",7/28/16,1
ardilgulez,ingested layer,7/28/16,1
lossyrob,nice,7/28/16,1
lossyrob,so you use the query syntax,7/28/16,1
lossyrob,and a LayerReader,7/28/16,1
lossyrob,I_Ñéll point you at an example,7/28/16,1
lossyrob,https://github.com/geotrellis/geotrellis-landsat-emr-demo/blob/master/server/src/main/scala/demo/DemoServiceActor.scala#L144,7/28/16,1
lossyrob,"instead of `catalog` you_Ñéd have your `LayerReader`, e.g. `S3LayerReader`",7/28/16,1
lossyrob,which you_Ñéll have to create an instance of,7/28/16,1
ardilgulez,I'm using the same example,7/28/16,1
lossyrob,"ok. so instead of saying `Intersects(extent)`, you would give it some polygon",7/28/16,1
ardilgulez,yeah,7/28/16,1
ardilgulez,and I want to serve it as a tile layer,7/28/16,1
ardilgulez,like the tile route in the same class,7/28/16,1
lossyrob,well tile routes are served per-tile,7/28/16,1
lossyrob,"so is it jus that you want a tile layer, that only shows tiles that intersect with some geom?",7/28/16,1
ardilgulez,yes,7/28/16,1
ardilgulez,I think I can use tile.mask for that thing,7/28/16,1
lossyrob,well that will mask it,7/28/16,1
lossyrob,but you want to avoid fetching any tile that doesn_Ñét intersect with the geo,7/28/16,1
lossyrob,"so if a tile request is made for a non-intersecting tile, it_Ñéll just return a 404",7/28/16,1
lossyrob,"in that case, you want to check the incoming spatial key, see if it intersects the geom, if so, fetch the tile, then mask by the geom",7/28/16,1
lossyrob,you_Ñéll need to grab the layer_Ñés metadata to get the `mapTransform` to translate from keys to map coordinates,7/28/16,1
ardilgulez,tile.mask asks for both an extent and a geometry,7/28/16,1
ardilgulez,why,7/28/16,1
lossyrob,because tile_Ñés don_Ñét carry their position info,7/28/16,1
lossyrob,"`Raster[Tile]` does. it_Ñés a composition of an extent and a tile. You should be able to call `mask` on a `Raster[Tile]` with just the geom. but for a tile, you have to pass in that tile_Ñés extent",7/28/16,1
ardilgulez,so I can't give a random extent for that ,7/28/16,1
lossyrob,You_Ñéll need to pull the `TileLayerMetadata[K]` from the `AttributeStore` in order to get the `MapKeyTransform` that will tell you what the extent of a tile is at a specific `z/x/y` coordinate ,7/28/16,1
lossyrob,nope,7/28/16,1
lossyrob,think of a Tile as just the col/row data,7/28/16,1
lossyrob,it needs to be placed over a map (via an extent) to mean something that can be combined with a gemoetry,7/28/16,1
ardilgulez,"val metadata : TileLayerMetadata[SpatialKey] = attributeStore.readMetadata(layerId)							val extent = metadata.mapTransform.extent",7/28/16,1
walleyyang,"I thought the point of the repository was to help install the dependencies. I'm going to try to manually install the listed technologies that it was going to install.                                     @lossyrob > have you included geotrellis dependencies as part of the your project? (i.e. does `import geotrellis.spark._` compile)? ",7/28/16,1
ardilgulez,diverging implicit expansion for type spray.json.JsonFormat[M] starting with method setFormat in trait CollectionFormats  val metadata : TileLayerMetadata[SpatialKey] = attributeStore.readMetadata(layerId),7/28/16,1
lossyrob,@WalleyYang I_Ñém not quite following,7/28/16,1
lossyrob,@ardilgulez means that implicits are not being resolved correctly; put in type parameters explicitly,7/28/16,1
lossyrob,https://github.com/geotrellis/geotrellis/blob/master/spark/src/main/scala/geotrellis/spark/io/AttributeStore.scala#L47,7/28/16,1
lossyrob,hm actually,7/28/16,1
lossyrob,giving the result type explicitly should have done that,7/28/16,1
lossyrob,"```scalaval metadata = attributeStore.readMetadata[TileLayoutMetadata[SpatialKey]](layerId)```",7/28/16,1
lossyrob,"is waht i was thinking, but would be suprised if that worked and the other didnt",7/28/16,1
julienrf,"Hi, when I_Ñém applying the kriging prediction I get the following error:~~~org.apache.commons.math3.linear.NonPositiveDefiniteMatrixException: -197,970.652 is smaller than, or equal to, the minimum (0): not positive definite matrix: value -197,970.652 at index 0~~~",7/28/16,1
julienrf,"I don_Ñét really understand what it means, can you point me to some explanation?",7/28/16,1
julienrf,"My feature points have latitude and longitude coordinates (between [-180, 180) for longitude, and [-90, 90) for latitude)",7/28/16,1
julienrf,"And the locations to predict are just a regular grid made of points within the same range ( [-180, 180) for longitude, and [-90, 90) for latitude)",7/28/16,1
lossyrob,@julienrf what_Ñés the stack trace?,7/28/16,1
lossyrob,"@vishalanand wrote that feature, in case he_Ñés around to help...",7/28/16,1
julienrf,"Hi @lossyrob, sorry for the delay, here is the stacktrace:~~~org.apache.commons.math3.linear.NonPositiveDefiniteMatrixException: -197,970.652 is smaller than, or equal to, the minimum (0): not positive definite matrix: value -197,970.652 at index 0        at org.apache.commons.math3.linear.CholeskyDecomposition.<init>(CholeskyDecomposition.java:141)        at org.apache.commons.math3.linear.CholeskyDecomposition.<init>(CholeskyDecomposition.java:84)        at geotrellis.vector.interpolation.UniversalKriging.createPredictorInit(UniversalKriging.scala:144)        at geotrellis.vector.interpolation.Kriging$class.createPredictor(Kriging.scala:78)        at geotrellis.vector.interpolation.Kriging$class.predict(Kriging.scala:93)        at geotrellis.vector.interpolation.UniversalKriging.predict(UniversalKriging.scala:76)~~~",7/29/16,1
julienrf,It does the same if I use `GeoKriging` instead of `UniversalKriging`,7/29/16,1
ardilgulez,@lossyrob indeed what you said worked and what i did didn't,7/29/16,1
ardilgulez,"val extent = attributeStore.readMetadata[TileLayerMetadata[SpatialKey]](layerId).mapTransform.extenttile.mask(extent, geom).renderPng(colorMap1).bytes",7/29/16,1
ardilgulez,this gives me all blank tiles,7/29/16,1
ardilgulez,"and my geom is:geoJsonString.parseJson.convertTo[Geometry].reproject(LatLng, WebMercator)",7/29/16,1
ardilgulez,"My extent is : Extent(-2.0037508342789244E7, -2.0037508342789244E7, 2.0037508342789244E7, 2.0037508342789244E7)",7/29/16,1
ardilgulez,"The box that my tile covers is (in Webmercator): 3242420.5213, 5022830.0027; 3257784.3640, 5035518.5494",7/29/16,1
lossyrob,@ardilgulez is there a NoData defined on the tile_Ñés cell type?,7/29/16,1
ardilgulez,What do you mean by that,7/29/16,1
ardilgulez,tile.convert(IntConstantNoDataCellType) ?,7/29/16,1
lossyrob,@julienrf it looks like this matrix is turning out not positive definite _Ñ_ I_Ñém not sure why tbh. Will have to look deeper https://github.com/geotrellis/geotrellis/blob/master/vector/src/main/scala/geotrellis/vector/interpolation/UniversalKriging.scala#L123,7/29/16,1
lossyrob,"@ardilgulez the cell type, does it have a defined NoData. I_Ñém thinking the mask might be blanking out the tile if there_Ñés no nodata defined, which would be a bug, but it_Ñés a guess",7/29/16,1
lossyrob,"if you do that convert, do you still get blank tiles?",7/29/16,1
lossyrob,and you sure that `geom.intersect(extent)` will produce true for tiles that are coming out blank?,7/29/16,1
ardilgulez,I didn't check geom.intersect(extent) but it will prolly come true. Extent is from -180 -85 180 85,7/29/16,1
ardilgulez,Which is something that's really close to the entire world,7/29/16,1
ardilgulez,"and my geom is a polygon like 40, 29; 41, 29; 41, 30; 40, 30; 40, 29",7/29/16,1
ardilgulez,"GEOM: POLYGON ((29.191017150878903 41.1084593385229, 29.191017150878903 41.13716677674901, 29.25298690795898 41.13716677674901, 29.25298690795898 41.1084593385229, 29.191017150878903 41.1084593385229))EXTENT: Extent(-180.0, -85.0511287798066, 180.0, 85.0511287798066)INTERSECTS: PolygonResult(POLYGON ((29.191017150878903 41.1084593385229, 29.191017150878903 41.13716677674901, 29.25298690795898 41.13716677674901, 29.25298690795898 41.1084593385229, 29.191017150878903 41.1084593385229)))",7/29/16,1
ardilgulez,"``` scalaval tileOpt: Option[Tile] =								try {									Some(Server.reader(layerId).read(x, y))								} catch {									case _: TileNotFoundError =>										None								}							println(""GEOM: "" + geom.toString());							println(""EXTENT: "" + extent.toString());							println(""INTERSECTS: "" + geom.intersection(extent).toString())							tileOpt.map { tile =>								val colorMap1 =									ColorMap(										Map(											0 -> RGB(0, 255, 0),											50 -> RGB(63, 255, 51),											100 -> RGB(102, 255, 102),											150 -> RGB(178, 255, 102),											200 -> RGB(255, 255, 0),											250 -> RGB(255, 255, 51),											300 -> RGB(255, 153, 51),											350 -> RGB(255, 128, 0),											400 -> RGB(255, 51, 51),											450 -> RGB(255, 0, 0)										)									)								tile									.convert(IntConstantNoDataCellType)									.mask(extent, geom)									.renderPng(colorMap1)									.bytes							}```",7/29/16,1
ardilgulez,:(,7/29/16,1
ardilgulez,still blank tiles,7/29/16,1
lossyrob,and the tiles are not blank if you comment out the mask?,7/29/16,1
ardilgulez,yes indeed,7/29/16,1
lossyrob,that extent is huge,7/29/16,1
lossyrob,ok I see what happened,7/29/16,1
lossyrob,where are you getting the extent. for the whole layer?,7/29/16,1
lossyrob,are you using the extent of the layer,7/29/16,1
ardilgulez,"val extent = attributeStore.readMetadata[TileLayerMetadata[SpatialKey]](layerId).mapTransform.extent.reproject(WebMercator, LatLng)",7/29/16,1
ardilgulez,I'm guessing yes,7/29/16,1
lossyrob,yeah,7/29/16,1
lossyrob,so the extent has to be the extent of hte individual tile,7/29/16,1
lossyrob,"or else your saying, here_Ñés this 256x256 (or similar) tile, and it covers the whole world. so when you mask by a small geom, it doesn_Ñét even cover any cells",7/29/16,1
lossyrob,mapTransform is what is used to turn tile keys into extents,7/29/16,1
ardilgulez,how can i get the extent of the tile,7/29/16,1
lossyrob,"```scalaval extent =  attributeStore.readMetadata[TileLayerMetadata[SpatialKey]](layerId).mapTransform(SpatialKey(x, y)).reproject(WebMercator, LatLng)```",7/29/16,1
ardilgulez,how do i get the spatial key of a given tile,7/29/16,1
lossyrob,"mapTransform can be though of as a funciton (it has a series of apply functions). When you apply to a spatial key, you get an extent. apply to an extent, you get `GridBounds`, apply to a point you get a `SpatialKey`",7/29/16,1
lossyrob,"You already have it, it_Ñés the x, y you_Ñére using to fetch the tile",7/29/16,1
lossyrob,"`SpatialKey(x, y)`",7/29/16,1
lossyrob,"(col, row)",7/29/16,1
ardilgulez,got it,7/29/16,1
ardilgulez,yeah,7/29/16,1
ardilgulez,"thank you so much again, superman",7/29/16,1
lossyrob,ha no problem!,7/29/16,1
owcm,@lossyrob On a related note to  your discussion with Kemal.  If I have an attributestore and read the metadata can I get the the  list of Spatialkeys that are in the store.,7/29/16,1
lossyrob,"No, you can get the keybounds off the metadata, but if the layer does not cover every tile in the space (has gaps) then that doesn_Ñét supply the correct info",7/29/16,1
lossyrob,"it would be something you_Ñéd have to save off at ingest time, or do an RDD operation to collect",7/29/16,1
owcm,"When I use the distance function from extent  "" md.layoutExtent.northEast.distance( md.layoutExtent.northWest )""  for example. Am I getting a value that is in cells or units of the projection?",7/29/16,1
lossyrob,map coords,7/29/16,1
owcm,"Aw,  and best way to transform them to some unit like  meters ?  ",7/29/16,1
lossyrob,depends on the CRS,7/29/16,1
lossyrob,What I_Ñéve usually done for distance is to move to a UTM zone and then count them as meters,7/29/16,1
lossyrob,a la https://github.com/geotrellis/geotrellis/blob/master/spark/src/main/scala/geotrellis/spark/tiling/ZoomedLayoutScheme.scala#L55,7/29/16,1
owcm,@lossyrob Nice thanks,7/29/16,1
ardilgulez,hey guys,8/1/16,1
ardilgulez,how do I find min and max values of an entire layer?,8/1/16,1
ardilgulez,nvm found it,8/1/16,1
ardilgulez,"val rdd = readerSet.layerReader.query[SpatialKey, Tile, TileLayerMetadata[SpatialKey]](layerId).resultval (min, max) = ContextRDD(rdd1, rdd1.metadata).minMax",8/1/16,1
ardilgulez,damn it markdown :(,8/1/16,1
ardilgulez,"``` scalaval catalog = readerSet.layerReaderval rdd1 = catalog    .query[SpatialKey, Tile, TileLayerMetadata[SpatialKey]](layerId)    .resultval (min, max) = ContextRDD(rdd1, rdd1.metadata).minMax```",8/1/16,1
lossyrob,:+1:,8/1/16,1
lossyrob,ah actually,8/1/16,1
lossyrob,you don_Ñét need that contextRDD,8/1/16,1
lossyrob,"the result is already a `RDD[(SpatialKey, Tile)] with Metadata[TileLayerMetadata[SpatialKey]]`",8/1/16,1
lossyrob,which is basically what the `ContextRDD` is there to produce (that `with Metadata` part),8/1/16,1
lossyrob,so you can change that to,8/1/16,1
lossyrob,"```scalaval catalog = readerSet.layerReaderval (min, max) = catalog    .query[SpatialKey, Tile, TileLayerMetadata[SpatialKey]](layerId)    .result    .minMax```",8/1/16,1
walleyyang,"I_Ñém new to the Haddop environment and Scala. I was able to get geodocker-cluster running. I_Ñém now trying to get the chatty-demo up, but the documentation seems to be outdated. Can someone point me in the right directions to get it up and running? ",8/1/16,1
lossyrob,"@WalleyYang we have some schedule work for the next couple of weeks to update that demo, but unfortunatly we don_Ñét have it yet. The best place to start is the branch of this PR https://github.com/geotrellis/geotrellis-chatta-demo/pull/20",8/1/16,1
lossyrob,which @pomadchin is working with to update the demo,8/1/16,1
walleyyang,"Ok, thanks.",8/1/16,1
ardilgulez,how can i create a single layer out of a whole lot of single band tiffs?,8/2/16,1
ardilgulez,"can I just go with writer.write(layerId, rdd, ZCurveKeyIndexMethod) for each rdd that I generate from each tif?",8/2/16,1
echeipesh,"@ardilgulez I assume you don_Ñét have access to them at the same time, otherwise it would be best to read them in all at once and rely on the tiler to sort out the merging. If you_Ñére writing them at different times you have to rely on the updater to fetch the intersecting tiles that have already been stored and merge them before writing.",8/2/16,1
aaron-santos,"@echeipesh  I'm interested in the updater piece. I'm having an awfully hard time ingesting large tiles using MultibandIngest. I'm subdividing them using gdal_retile but all the pieces get ingested at once and hit memory constraints. I'm wondering, can I ingest one of my smaller tiles and then fill in the rest of the of the layer's data by updating the layer iteratively using the rest of the tiles?",8/2/16,1
echeipesh,@aaron-santos You can mitigate memory problems to an extent by forcing higher partition count on input so that processing each partition doesn_Ñét blow your memory. How are you reading your inputs?,8/2/16,1
echeipesh,However at some point the input will be too large for cluster and it will fail doing this huge tiling job. You can totally use the udpater in that case. You just need to new up an updater like: https://github.com/geotrellis/geotrellis/blob/master/accumulo/src/main/scala/geotrellis/spark/io/accumulo/AccumuloLayerUpdater.scala and call the inherited `update` function: https://github.com/geotrellis/geotrellis/blob/master/spark/src/main/scala/geotrellis/spark/io/LayerUpdater.scala#L43-L47,8/2/16,1
echeipesh,"Again there is a question what backend you_Ñére using. Accumulo updater will do the smart thing and load only the tile that potentially intersect the update and merge those before writing. Hadoop updater, at the moment, will need to load the whole layer to udpate it and write it, which makes it inappropriate for large layers (there is an issue written up how to solve that for future work).",8/2/16,1
aaron-santos,@echeipesh  cool. My input is an RDD of of rasters that come out of gdal_retile,8/2/16,1
aaron-santos,"I'm kind of assuming that I need some of the metadata up front, like extent and whatnot before updating. But that should be easy to get. Does that sound right?",8/2/16,1
aaron-santos,I'm using a combination of S3 for tiles and Elasticsearch for attributestore,8/2/16,1
aaron-santos,"It's sounds doable, but it might be a bit of upfront work",8/2/16,1
echeipesh,"Yes, S3 also does incremental updates. You would get your update RDDs through the same tiling process as you do currently, so they will already be `RDD[(K,V)] with Metadata[M]`, that `M` will have extent.",8/2/16,1
echeipesh,How big is your dataset in relation to your memory. Are you calling `gdal_retile` in `RDD.pipe` ?,8/2/16,1
aaron-santos,It's about half of physical ram,8/2/16,1
aaron-santos,I'm calling gdal\_retile by shelling out with `sys.process._`. The sentinel2 basetiles come in as a set of singleband rasters so the RDD is created by transforming a seq of paths to SinglebandGeoTiffs to MultibandGeotiffs that serve as the input to SparkContext.parallelize.,8/2/16,1
echeipesh,Are you passing `numSlices` to `SparkContext.paralleize` ?,8/2/16,1
echeipesh,"Half the memory of the cluster really should be small enough to process without trickery, tweaking the partition count should get you through this without having to use the updater.",8/2/16,1
aaron-santos,yep. my RDD has about 1800 multibandtiles in it and I'm using 100 partitions. Would it be most efficient to match the partition size to the number of elements?,8/2/16,1
echeipesh,"I_Ñéve found that each partition being ~ 250MB - 512MB is about the sweet spot, whatever the works out to with your imagery.",8/2/16,1
fosskers,General Geotrellis question: how would one convert between Geometries living on an arbitrary grid to those living in some real Projection?,8/2/16,1
aaron-santos,ohh. good to know. :),8/2/16,1
echeipesh,"@fosskers That_Ñés kind of a trick question. I assume you_Ñére asking in context of vector tiles, which store coords as offset from the tile corner rather than some CRS. However those tiles overall would have to be in SOME projection system and those tile corners would have some absolte coord.  So you would need to add those pixel offests to absolute position of the tile scaled by the pixel width/height in that CRS.",8/2/16,1
echeipesh,I would guess that almost any tile you will get your hands on will be in WebMercator. If that_Ñés the case you can use `ZoomedLayoutScheme` to get the tile layout for the zoom level you_Ñére at https://github.com/geotrellis/geotrellis/blob/master/spark/src/main/scala/geotrellis/spark/tiling/ZoomedLayoutScheme.scala#L23 and use the tile x/y to get the extent of the tile.,8/2/16,1
fosskers,"From the spec:> A Vector Tile represents data based on a square extent within a projection. A Vector Tile SHOULD NOT contain information about its bounds and projection. The file format assumes that the decoder knows the bounds and projection of a Vector Tile before decoding it.> Web Mercator is the projection of reference, and the Google tile scheme is the tile extent convention of reference. Together, they provide a 1-to-1 relationship between a specific geographical area, at a specific level of detail, and a path such as https://example.com/17/65535/43602.mvt.> Vector Tiles MAY be used to represent data with any projection and tile extent scheme.",8/2/16,1
echeipesh,"Right, so in geotrellis terms we would need `LayoutDefinition` as an input.",8/2/16,1
fosskers,"VectorTiles have no zoom, interestingly",8/2/16,1
fosskers,"or pardon me, I suppose you could store different tiles of the same area in a pyramiding way and call that zoom layers",8/2/16,1
echeipesh,The way they are stored they do. Looking at https://www.mapbox.com/vector-tiles/ you see reference to `http://a.tiles.mapbox.com/v4/mapbox.mapbox-streets-v7/14/4823/6160.mvt?access_token=<your access token>` where 14 is zoom level and 4823 is column and 6160 is row.,8/2/16,1
echeipesh,"Yes, exactly as you say.",8/2/16,1
fosskers,"Alright, I'm digging into the relationship between projections, layout definitions, and extents",8/2/16,1
echeipesh,That sounds like a title of an excellent doc page I wish we had.,8/2/16,1
fosskers,Yeah we definitely need more docs,8/2/16,1
ardilgulez,I need to tile a 1.9 gb tif file,8/3/16,1
ardilgulez,I set my java memory to 4g,8/3/16,1
ardilgulez,but still I get heap space error,8/3/16,1
ardilgulez,how much java memory do I need to be able to tile a 1.9gb tif file ,8/3/16,1
ardilgulez,just set java memory to 6g and still fails :(,8/3/16,1
pomadchin,@ardilgulez i can recommend you to retile your big tiff into smaller tiffs,8/3/16,1
pomadchin,`gdal_retile.py`,8/3/16,1
ardilgulez,how can i make a layer out of multiple tiffs?,8/3/16,1
ardilgulez,restatement of question: how can i merge a tiff rdd into a layer,8/3/16,1
pomadchin,"do you mean that you have a directory with tiffs, and you want to ingest them into layer?",8/3/16,1
ardilgulez,exactly,8/3/16,1
ardilgulez,"when I read one tiff from the directory and ingest it into a layer, and then I read the next tiff",8/3/16,1
ardilgulez,I can't ingest that into the same layer,8/3/16,1
pomadchin,"to do it iterativey, you need to use LayerWriter for the first insert, with preset KeyBounds, after that to ingest tiles using LayerUpdater",8/3/16,1
pomadchin,definitely i need to preapre code demo for you ):,8/3/16,1
ardilgulez,code demo would be real good for noobs like me :(,8/3/16,1
pomadchin,+ to ingest tiles from a directory as a single layer; you may use our spark-etl subproject (prefered user oriented way),8/3/16,1
ardilgulez,"``` scalaimport geotrellis.raster.Tileimport geotrellis.spark._import geotrellis.spark.etl.Etlimport geotrellis.spark.io.index.ZCurveKeyIndexMethodimport geotrellis.spark.util.SparkUtilsimport geotrellis.vector.ProjectedExtentimport org.apache.spark.SparkConfobject GeoTrellisETL {  def main(args: Array[String]): Unit = {    implicit val sc = SparkUtils.createSparkContext(""GeoTrellis ETL"", new SparkConf(true))    try {      /* parse command line arguments */      val etl = Etl(args)      /* load source tiles using input module specified */      val sourceTiles = etl.load[ProjectedExtent, Tile]      /* perform the reprojection and mosaicing step to fit tiles to LayoutScheme specified */      val (zoom, tiled) = etl.tile(sourceTiles)      /* save and optionally pyramid the mosaiced layer */      etl.save(LayerId(etl.conf.layerName(), zoom), tiled, ZCurveKeyIndexMethod)    } finally {      sc.stop()    }  }}```",8/3/16,1
ardilgulez,you mean like this?,8/3/16,1
pomadchin,"yes, definitely; ",8/3/16,1
ardilgulez,how can I construct the Etl object in this? so that I can get all my tiffs?,8/3/16,1
pomadchin,"you can even just build spark-etl assembly, and to use https://github.com/geotrellis/geotrellis/blob/master/spark-etl/src/main/scala/geotrellis/spark/etl/SinglebandIngest.scala",8/3/16,1
pomadchin,https://github.com/geotrellis/geotrellis/blob/master/docs/spark-etl/spark-etl-intro.md,8/3/16,1
pomadchin,and examples: https://github.com/geotrellis/geotrellis/blob/master/docs/spark-etl/spark-etl-run-examples.md,8/3/16,1
pomadchin,"so try the following pipeline: * make directory of tiffs* ./sbt spark-etl/assembly* ```#!/bin/shexport JAR=""geotrellis-etl-assembly-0.10-SNAPSHOT.jar""spark-submit \--class geotrellis.spark.etl.{SinglebandIngest | MultibandIngest} \--master local[*] \--driver-memory 2G \$JAR \--input hadoop --format {geotiff | temporal-geotiff} --cache NONE -I path=""file:///Data/nlcd/tiles"" \--output s3 -O bucket=com.azavea.datahub key=catalog \--layer nlcd-tms --crs EPSG:3857 --pyramid --layoutScheme {tms | floating}```",8/3/16,1
pomadchin,it_Ñés just a cli to ingest tiles,8/3/16,1
pomadchin,"but you can so similar in the code; to read folder of tiffs you can do: ```scalaimport geotrellis.spark.io.hadoop._val rdd: RDD[(ProjectedExtent, Tile)] = sc.hadoopGeoTiffRDD(props(""path_Ñù))```",8/3/16,1
ardilgulez,"``` scalaval rdd: RDD[(ProjectedExtent, Tile)] = sc.hadoopGeoTiffRDD(props(""path_Ñù))```",8/3/16,1
ardilgulez,"so this thing takes everything in the ""path"" directory into an RDD?",8/3/16,1
pomadchin,yep,8/3/16,1
pomadchin,"you need to merge it, retile, pyramid etc",8/3/16,1
ardilgulez,how do I merge?,8/3/16,1
ardilgulez,and where is the props function,8/3/16,1
pomadchin,"```scalaval inputRdd: RDD[(ProjectedExtent, Tile)] = sc.hadoopGeoTiffRDD(_ÑÒpath"")val (_, rasterMetaData) =TileLayerMetadata.fromRdd(inputRdd, FloatingLayoutScheme(512))val tiled: RDD[(SpatialKey, MultibandTile)] =  inputRdd    .tileToLayout(rasterMetaData.cellType, rasterMetaData.layout, Bilinear)    .repartition(100)val layoutScheme = ZoomedLayoutScheme(WebMercator, tileSize = 256)val (zoom, reprojected): (Int, RDD[(SpatialKey, MultibandTile)] with Metadata[TileLayerMetadata[SpatialKey]]) =  MultibandTileLayerRDD(tiled, rasterMetaData)    .reproject(WebMercator, layoutScheme, Bilinear)val attributeStore = FileAttributeStore(outputPath)val writer = FileLayerWriter(attributeStore)Pyramid.upLevels(reprojected, layoutScheme, zoom, Bilinear) { (rdd, z) =>  val layerId = LayerId(""landsat"", z)  if(attributeStore.layerExists(layerId)) {    new FileLayerManager(attributeStore).delete(layerId)  }  writer.write(layerId, rdd, ZCurveKeyIndexMethod)}```",8/3/16,1
ardilgulez,I have this exact code for tiling tifs,8/3/16,1
ardilgulez,without the props thing,8/3/16,1
ardilgulez,it only works with path and takes a single tif,8/3/16,1
pomadchin,it takes the whole directory,8/3/16,1
ardilgulez,the whole directory?,8/3/16,1
pomadchin,yes,8/3/16,1
ardilgulez,no matter how many tifs?,8/3/16,1
pomadchin,no matter,8/3/16,1
ardilgulez,wow cool,8/3/16,1
ardilgulez,can I actually combine tiles in a spark-etl job?,8/4/16,1
pomadchin,@ardilgulez what do you mean by that?,8/4/16,1
ardilgulez,get singleband geotiffs from a certain location -> combine them into a multiband geotiff -> ingest the multiband geotiff,8/4/16,1
ardilgulez,I think I can easily do that in multiple steps,8/4/16,1
ardilgulez,actually I think I know how to do that,8/4/16,1
ardilgulez,after thinking on that a little,8/4/16,1
ardilgulez,geotrellis-landsat-tutorial is a good example for that ,8/4/16,1
pomadchin,"@ardilgulez yep, you can do it; \+ it would be usefull for you to have a look there: https://github.com/geotrellis/geotrellis/blob/master/spark-etl/src/main/scala/geotrellis/spark/etl/Etl.scala#L26-L48",8/4/16,1
pomadchin,"```scala    /* parse command line arguments */    val etl = Etl(args)    /* load source tiles using input module specified */    val sourceTiles = etl.load[I, V]    /* perform the reprojection and mosaicing step to fit tiles to LayoutScheme specified */    val (zoom, tiled) = etl.tile(sourceTiles)    /* save and optionally pyramid the mosaiced layer */    etl.save[K, V](LayerId(etl.conf.layerName(), zoom), tiled, keyIndexMethod)```",8/4/16,1
pomadchin,there are different steps; that_Ñés not a problem to cmobine them into multiband tile at any of these stages,8/4/16,1
ardilgulez,"``` scalanames.foreach((key) => {    val geoTiff = SinglebandGeoTiff(inputPath(key))    tileArray += geoTiff.tile})```",8/4/16,1
ardilgulez,this thing throws heap space error for 4 tiles that have a combined size of 8 mb,8/4/16,1
ardilgulez,along with one of those tiles being taken as a singleband geotiff prior to this thing,8/4/16,1
ardilgulez,"and I even have javaOptions += ""-Xmx6G"" in my build.sbt",8/4/16,1
pomadchin,"mm im curious; what are you trying to do by reading tiles in a such way, without spark?",8/4/16,1
ardilgulez,to combine them into a single multiband tile,8/4/16,1
pomadchin,have you tried to read by `hadoopGeoTiffRDD` ?,8/4/16,1
ardilgulez,I use it all the time in the server actor,8/4/16,1
ardilgulez,I just don't know how to read and combine with hadoopGeoTiffRDD,8/4/16,1
ardilgulez,"I remember being able to do this with 3 landsat bands, 60mb each",8/4/16,1
pomadchin,@ardilgulez you can try to do,8/4/16,1
pomadchin,"```scalaval rdd: RDD[(ProjectedExtent, Tile)] = ???val mbRdd: RDD[(ProjectedExtent, MultibandTile)] = rdd.groupByKey.map { case (k, tiles) => k -> ArrayMultibandTile(tiles)```",8/4/16,1
pomadchin,im a bit not sure about similarity of keys,8/4/16,1
ardilgulez,still the heap space,8/4/16,1
ardilgulez,"""Size is 44638, 32458""",8/4/16,1
ardilgulez,this is the result of gdalinfo,8/4/16,1
ardilgulez,might this be causing heap space errors?,8/4/16,1
lossyrob,it_Ñés too big,8/4/16,1
lossyrob,we don_Ñét have the rasters compressed in memory,8/4/16,1
lossyrob,"so it might be that size on disk compressed, but 44000 x 32000 is huge",8/4/16,1
lossyrob,you_Ñéll need to use gdal_retile.py to cut them up into smaller chunks,8/4/16,1
lossyrob,"or try to use the split method, although that might fail with out of memory anyway",8/4/16,1
lossyrob,https://github.com/lossyrob/geotrellis-ned-example/blob/master/src/main/scala/elevation/Main.scala#L86-L90,8/4/16,1
ardilgulez,I thought it was size on disk that was causing stuff,8/4/16,1
lossyrob,"what does `gdalinfo` tell you about the geotiff, specifically in the `COMPRESS` metadata section",8/4/16,1
ardilgulez,"I might as well just gdalwarp them into smaller tiles but when I run the spark job again, won't it still give me heap space error?",8/4/16,1
ardilgulez,Gdalinfo says that it's lzw,8/4/16,1
ardilgulez,"Driver: GTiff/GeoTIFFFiles: something.tifSize is 57109, 32458Coordinate System is:GEOGCS[""WGS 84"",    DATUM[""WGS_1984"",        SPHEROID[""WGS 84"",6378137,298.257223560493,            AUTHORITY[""EPSG"",""7030""]],        AUTHORITY[""EPSG"",""6326""]],    PRIMEM[""Greenwich"",0],    UNIT[""degree"",0.0174532925199433],    AUTHORITY[""EPSG"",""4326""]]Origin = (26.303836441373896,39.157680967189165)Pixel Size = (0.000035107177822,-0.000035107177822)Metadata:  AREA_OR_POINT=AreaImage Structure Metadata:  COMPRESSION=LZW  INTERLEAVE=BANDCorner Coordinates:Upper Left  (  26.3038364,  39.1576810) ( 26d18'13.81""E, 39d 9'27.65""N)Lower Left  (  26.3038364,  38.0181722) ( 26d18'13.81""E, 38d 1' 5.42""N)Upper Right (  28.3087723,  39.1576810) ( 28d18'31.58""E, 39d 9'27.65""N)Lower Right (  28.3087723,  38.0181722) ( 28d18'31.58""E, 38d 1' 5.42""N)Center      (  27.3063044,  38.5879266) ( 27d18'22.70""E, 38d35'16.54""N)Band 1 Block=57109x32 Type=Byte, ColorInterp=Gray",8/4/16,1
lossyrob,"if you want to see how big it_Ñéll be in memory, do `gdal_translate something.tif something-else.tif` - without setting compression it_Ñéll store a decompressed version",8/4/16,1
ardilgulez,1.8G,8/4/16,1
ardilgulez,from 2.5M,8/4/16,1
ardilgulez,damn these compression algorithms are effective,8/4/16,1
lossyrob,"ah ok. well, you might be able to do the split like I linked to ",8/4/16,1
lossyrob,just gotta give the executors a lot of memory,8/4/16,1
ardilgulez,"If I give executors a lot of memory (the same amount that you mentioned), won't I get away with not doing the split?",8/4/16,1
ardilgulez,or does doing the split require less memory than not doing the split,8/4/16,1
lossyrob,doing the split and repartitioning,8/4/16,1
lossyrob,means that you are chunking up the work into smaller bites,8/4/16,1
lossyrob,and then letting spark work on the smaller bite-sized amounts of the raster at a time instead of trying to feed it the whole large thing,8/4/16,1
uuuuq,"still cannot ingest temporal-geotiff data with spark-etl, got `Unable to find input module of type 'hadoop' for format `temporal-geotiff`",8/5/16,1
uuuuq,"I am using the latest version of geotrellis, so what should i do if i want to ingest the temporal geotiff data?",8/5/16,1
lossyrob,you_Ñére using 0.10.2?,8/5/16,1
ardilgulez,I want to set pixel values in a tile according to proximity,8/5/16,1
ardilgulez,"for each 1 in my tile, I want to set values of neighboring pixels of certain radius to certain values",8/5/16,1
uuuuq,@lossyrob yes i m using 0.10.2,8/5/16,1
owcm,"Question:  As I recall there is a way to create vectors from a Geotiff using ""toVector""    is there something similar for pulling vectors from a RDD of rasters in the spark world or is that upcoming functionality.",8/5/16,1
lossyrob,@uuuuq that is very strange indeed. Is there a way you can give us some code to try to duplicate?,8/5/16,1
lossyrob,"@owcm you could map the values to a .toVector call, but then what you get is a set of disjoin vectors per tile. If you want to join polygons that cross tiles, that would be the tricky part",8/5/16,1
lossyrob,"which would be upcoming functionality, we haven_Ñét worked on vectorization of raster RDDs yet",8/5/16,1
owcm,@lossyrob Okay thats what I thought.  Thanks,8/5/16,1
owcm,"Probably have asked this before, save this time, is there a simple example of taking results of toVector and writing it to GeoJson file.  Thanks in advance.",8/5/16,1
lossyrob,"```scala    import geotrellis.vector._    import geotrellis.vector.io._    // This import is important: otherwise the JsonFormat for the    // feature data type is not available (the feature data type being Int)    import spray.json.DefaultJsonProtocol._    // Starting with a list of polygon features,    // e.g. the return type of tile.toVector    val features: List[PolygonFeature[Int]] = ???    // Because we've imported geotrellis.vector.io, we get    // GeoJson methods implicitly added to vector types,    // including any Traversable[Feature[G, D]]    val geojson: String = features.toGeoJson    println(geojson)```",8/5/16,1
lossyrob,@owcm ,8/5/16,1
owcm,@lossyrob  Thank you that is much more concise then what I came up with.,8/5/16,1
uuuuq,@lossyrob i clone the code from github and assembly the spark-etl class followed this guide: https://github.com/geotrellis/geotrellis/blob/master/docs/spark-etl/spark-etl-intro.md,8/6/16,1
ardilgulez,I want to do a weighted sum of every single pixel in a tile,8/6/16,1
ardilgulez,"can I just do tile.map((col, row, pixel) => weightedSum(col, row, pixel))",8/6/16,1
ardilgulez,weightedSum method depends on the values of a neighboring tile ,8/6/16,1
ardilgulez,"the question is: assume that when I process the 0,0 of the tile, which has a value v before weighted sum applied and ws(v) after the weighted sum applied, when I process the 0,1 of the tile and the weightedSum method will use the value of 0,0",8/6/16,1
ardilgulez,"will it use v or ws(v) while calculating the value of 0,1?",8/6/16,1
crakjie,There is a way to add geotrellis/vector in dependency or should I have to build it in local?,8/6/16,1
pomadchin,@crakjie we published on maven central ;),8/6/16,1
echeipesh,"@ardilgulez That sounds a lot like a focal operations, where the result of each cell is determined by the cells around it. We already implement the mechanisms required to handle crossing tile boundaries, you can just hook into `.focalOperation` method (https://github.com/geotrellis/geotrellis/blob/master/spark/src/main/scala/geotrellis/spark/mapalgebra/focal/FocalOperation.scala#L20-L39) given you can express your operation as `(Tile, Option[GridBounds]) => Tile)`. Here `Tile` actually contains border pixels and `Option[GridBounds]` tells you which pixels correspond to original tile.",8/9/16,1
echeipesh,"You can also extend our focal `Sum` implementation to provide an instance of the `(Tile, Option[GridBounds]) => Tile)` function.",8/9/16,1
echeipesh,"And to answer your specific question, the input cells are always form the original layer, not the results of the focal operation. Specifically the `Tile` interface is immutable, so all operations create copies through transformations.",8/9/16,1
ardilgulez,how can I get chatta demo working without geodocker cluster?,8/10/16,1
ardilgulez,"./ingest.sh results with a whole bunch of ""class not found"" stuff",8/10/16,1
ardilgulez,./sbt assembly before running ./ingest.sh seemed to cool stuff off ,8/10/16,1
ardilgulez,"but now I'm getting a whole bunch of [scallop] Error: Bad arguments for option 'I': 'path=file:/path-to/geotrellis-chatta-demo/geotrellis/data/arg_wm/Wetlands.tiff' - wrong arguments format",8/10/16,1
pomadchin,@ardilgulez hi; thats smth that needs to be updated,8/10/16,1
pomadchin,there is a [pr](https://github.com/geotrellis/geotrellis-chatta-demo/pull/23) with updates,8/10/16,1
pomadchin,it depends on https://github.com/geotrellis/geotrellis/pull/1553,8/10/16,1
pomadchin,about geotrellis chatta demo i am not sure what_Ñés wrong with it ._. the only thing i can suggest you is to use that pr;,8/10/16,1
pomadchin,\+ it has updated instructions to run demo on geodocker cluster,8/10/16,1
pomadchin,though i still test scripts ,8/10/16,1
ardilgulez,"so i just say ""git fetch github.com/pomadchin/geotrellis-chatta-demo feature/json-etl-support"" on the main folder and get your pull request?",8/10/16,1
pomadchin,yep,8/10/16,1
ardilgulez,[scallop] Error: Bad arguments for option 'I': 'path=file:///home/ardil/Desktop/projeler/scala adoption/geotrellis-chatta-demo/geotrellis/data/arg_wm/PublicallyOwnedWorkingLands.tiff' - wrong arguments format,8/10/16,1
pomadchin,\+ geotrellis/geotrellis#1553 ./scripts/publish-local.sh,8/10/16,1
pomadchin,eeeeh,8/10/16,1
ardilgulez,where is that ./scripts thing?,8/10/16,1
ardilgulez,can't find,8/10/16,1
pomadchin,"1. pull pr https://github.com/geotrellis/geotrellis/pull/15532. publish it locally ([./scripts/publish-local.sh](https://github.com/geotrellis/geotrellis/blob/master/scripts/publish-local.sh))3. pull https://github.com/geotrellis/geotrellis-chatta-demo/pull/234. build",8/10/16,1
pomadchin,@ardilgulez \+ mb updated README would be interesting for you: https://github.com/pomadchin/geotrellis-chatta-demo/tree/feature/json-etl-support/README.md,8/10/16,1
ardilgulez,this thing is giving me cancer :(,8/10/16,1
pomadchin,o:,8/10/16,1
pomadchin,there are some explanations about how to run demo on geodocker cluster,8/10/16,1
pomadchin,"installation one liner (local build): ```bashcd /data/geotrellis-chatta-demo; rm -rf geotrellis/src/main/resources && \cd /data; rm -rf geotrellis; git clone https://github.com/pomadchin/geotrellis.git && cd geotrellis && git checkout feature/etl-json && \cd /data/geotrellis; ./scripts/publish-local.sh && cd ../ && \cd /data/geotrellis-chatta-demo/geotrellis; ./sbt assembly```",8/10/16,1
owcm,"A few weeks ago I reported an error occurring when using the HadoopLayerWriter.  Did not have much of stack to report.  This is randomly happening during runs using spark.  This time I have a stack dump to report.  Placed my code and the stack trace in a Gist.  Wondered if this has been seen before.  https://gist.github.com/owcm/f201461467e7c4ed81666c123b23c3a9.   I am using version ""0.10.1""",8/11/16,1
vishalanand,"@julienrf @lossyrob  Hello Julien, apologies for reaching out so late with regards to kriging prediction error that you are receiving. (Got to see a mention while browsing a gitter email digest today). ```org.apache.commons.math3.linear.NonPositiveDefiniteMatrixException: -197,970.652 is smaller than, or equal to, the minimum (0): not positive definite matrix: value -197,970.652 at index 0```Can you please send the tile you used when the error was thrown so that I can investigate in detail (maybe via mail : anand.vishal2994@gmail.com). (N.B. Some datasets' points can't be interpolated, for them we need to handle the matrix exceptions and dish out median output and/or related data, which seems to be the origin of the error you are observing). Will try to work up a solution after office hours or over the weekend",8/11/16,1
owcm,@lossyrob Replying back to your response on Jul15 comment and my latest post on the same issues.  I think its memory issues killing the executors as well.   You suggest mucking with partition sizes.  Where would suggest adjusting them.    I am reading in 3 datasets in from Avro and merging them to create a suitability raster.    I have played with partition sizes when ingesting files into the Avro system put have not made adjustments in other areas.  There is something on the hairy edge because 2 out of 3 times the same process will run all the way through.,8/11/16,1
ardilgulez,Do you guys have the source code for the weighted overlay example that's on the main page geotrellis.io ?,8/11/16,1
pomadchin,@ardilgulez chatta demo is mostly similar to that,8/11/16,1
ardilgulez,What I actually wanted is the data associated with it,8/11/16,1
ardilgulez,I made similar examples work but it would be very good for me if I could check out the data as well,8/11/16,1
pomadchin,http://demo.geotrellis.com/chatta/,8/11/16,1
ardilgulez,"I'm looking for the data files for: philly_bars, philly_grocery_stores, philly_rail_stops",8/11/16,1
ardilgulez,the tifs,8/11/16,1
ardilgulez,or whatever,8/11/16,1
ardilgulez,can I obtain them from anywhere,8/11/16,1
ardilgulez,"how can I render an RDD[SpatialKey, Tile]",8/11/16,1
pomadchin,you can `import geotrellis.spark.render._` and to use render function,8/11/16,1
pomadchin,renderPng,8/11/16,1
pomadchin,"to have and `RDD[SpatialKey, Png]`",8/11/16,1
fosskers,"`RDD[(SpatialKey, Png)]`, to be pedantic",8/15/16,1
fosskers,although the paren-less version is a nice shorthand,8/15/16,1
pomadchin,"@fosskers thx for fix, ofc tuple inside",8/15/16,1
uuuuq,"error when read tile metadata from rdd```val layoutDefinition = ZoomedLayoutScheme.layoutForZoom(maxZoom, destCRS.worldExtent, 256)val metadata = TileLayerMetadata.fromRdd(rdd, destCRS, layoutDefinition)```where `rdd: RDD[(TemporalProjectedExtent, MultibandTile)]``destCRS: CRS`got:```[error] /home/geotrellis/project/test/ingest/src/main/scala/demo/Ingest.scala:39: ambiguous implicit values:[error]  both method withProjectedExtentTemporalTilerKeyMethods in package spark of type [K](self: K)(implicit evidence$11: geotrellis.util.Component[K,geotrellis.vector.ProjectedExtent], implicit evidence$12: geotrellis.util.Component[K,geotrellis.spark.TemporalKey])geotrellis.spark.withProjectedExtentTemporalTilerKeyMethods[K][error]  and method withProjectedExtentTilerKeyMethods in package spark of type [K](self: K)(implicit evidence$13: geotrellis.util.Component[K,geotrellis.vector.ProjectedExtent])geotrellis.spark.withProjectedExtentTilerKeyMethods[K][error]  match expected type geotrellis.spark.TemporalProjectedExtent => geotrellis.spark.tiling.TilerKeyMethods[geotrellis.spark.TemporalProjectedExtent,K2][error]     val metadata = TileLayerMetadata.fromRdd(rdd, destCRS, layoutDefinition)```",8/16/16,1
pomadchin,"@uuuuq try to use fromRdd function with explicitly passed type parameters: ```scalaTileLayerMetadata.fromRdd[TemporalProjectedExtent, MultibandTile, SpaceTimeKey](rdd, destCRS, layoutDefinition)```",8/16/16,1
ardilgulez,is there a way where I can get keys and respective colors from a color map*,8/16/16,1
pomadchin,@ardilgulez what do you mean by that? o:,8/16/16,1
pomadchin,do you mean `ColorMap`?,8/16/16,1
ardilgulez,yes,8/16/16,1
ardilgulez,"when I generate a color map from a color ramp and an array of integers (histogram), it turns that thing into this:",8/16/16,1
ardilgulez,"  ``` scalaColorMap(    Map(      3.5 -> RGB(0,255,0),      7.5 -> RGB(63,255,51),      11.5 -> RGB(102,255,102),      15.5 -> RGB(178,255,102),      19.5 -> RGB(255,255,0),      23.5 -> RGB(255,255,51),      26.5 -> RGB(255,153,51),      31.5 -> RGB(255,128,0),      35.0 -> RGB(255,51,51),      40.0 -> RGB(255,0,0)    )  )```",8/16/16,1
ardilgulez,right?,8/16/16,1
ardilgulez,I want to get the keys of it,8/16/16,1
ardilgulez,and the values,8/16/16,1
pomadchin,not possible with the current api design ):,8/16/16,1
ardilgulez,Colors in a ColorRamp?,8/16/16,1
ardilgulez,or ColorMap?,8/16/16,1
pomadchin,ColorRamp converts to ColorMap using its histogram,8/16/16,1
ardilgulez,Let's say I get ahold of the histogram some other way,8/16/16,1
ardilgulez,I want the colors in the ColorRamp in the order,8/16/16,1
ardilgulez,can I get them by ColorRamp.map or some other thing?,8/16/16,1
pomadchin,"how do you get your color ramp? and how do you get / want to get color map? i mean that having color ramp, to get colorMap you need to use .toColorMap function and to pass `breaks` into it; that means that you need to get `breaks` of your tile; `(_: Tile).histogramExactInt.quantileBreaks(numBreaks)`",8/16/16,1
ardilgulez,I have my color ramp from ColorRamp utility,8/16/16,1
pomadchin,and `breaks` are the `keys` of color map,8/16/16,1
ardilgulez,"yes, I generate breaks myself ",8/16/16,1
ardilgulez,"the thing is, I want to mask the colors if the corresponding break is too low for my taste",8/16/16,1
ardilgulez,and I want that thing to work regardless of what color ramp user chooses in the first place,8/16/16,1
ardilgulez,can I just use stuff like:,8/16/16,1
ardilgulez,"```scalacolormap.map((color) => {    if(index <= colormap.length / 2)        color.alpha = 0    index += 1})",8/16/16,1
ardilgulez,"``` scalacolormap.map((color) => {    if(index <= colormap.length / 2)        color.alpha = 0    index += 1})```",8/16/16,1
pomadchin,hm,8/16/16,1
ardilgulez,does this mask the first colormap.length/2 colors,8/16/16,1
ardilgulez,or does this mask some random colors,8/16/16,1
uuuuq,"@pomadchin cool, it works. many thanks",8/16/16,1
pomadchin,yes first; but ehhh  what are you trying to do finally?,8/16/16,1
pomadchin,setting nodata values not suits your case?,8/16/16,1
ardilgulez,no it does not,8/16/16,1
ardilgulez,tif looks messed up that way,8/16/16,1
ardilgulez,I'm hoping this makes any difference,8/16/16,1
pomadchin,"there can be made a bad trick with type casting; if you have Int breaks: ```scalaval map: Map[Int, Int] = colorMap.asInstanceOf[IntColorMap].breaksToColors```",8/16/16,1
ardilgulez,"can I do the same trick with double breaks, like such:",8/16/16,1
pomadchin,"so that trick makes possible to Map over breaks; but! as it is map, its unordered; you need to do smth like```scalamap.toVector.orderWith(_._1)```",8/16/16,1
ardilgulez,"``` scalaval map: Map[Int, Int] = colorMap.asInstanceOf[IntColorMap].breaksToColors```",8/16/16,1
ardilgulez,I mean,8/16/16,1
pomadchin,"if you need your double breaks, you may use `DoubleColorMap`",8/16/16,1
pomadchin,"otherwise there could be siginficant values loss (i mean if you have double breaks, but trying to get int breaks)",8/16/16,1
ardilgulez,"``` scalaval map: Map[Double, Int] = colorMap.asInstanceOf[DoubleColorMap].breaksToColors```",8/16/16,1
pomadchin,yes,8/16/16,1
pomadchin,"be careful, its a dirty thing (type casts)",8/16/16,1
ardilgulez,it is,8/16/16,1
pomadchin,so you may have lots of side effects;,8/16/16,1
pomadchin,"i can talk to the team, probably they have some ideas to do what you need in a more _ÑÒcorrect"" way",8/16/16,1
ardilgulez,that would be real awesome ,8/16/16,1
ardilgulez,value breaksToColors is not a member of geotrellis.raster.render.DoubleColorMap,8/16/16,1
ardilgulez,am I doing something wrong with this?,8/16/16,1
pomadchin,https://github.com/geotrellis/geotrellis/blob/v0.10.2/raster/src/main/scala/geotrellis/raster/render/ColorMap.scala#L299,8/16/16,1
uuuuq,"how to read tiff files in hdfs as projectedraster?i use```  def readAsRaster: ProjectedRaster[MultibandTile]  = {    val tif = GeoTiffReader.readMultiband(path)    logger.info(s""Getting $path"")    val tile = tif.tile    val extent = tif.extent    val crs = tif.crs    ProjectedRaster(tile, extent, crs)  }```and i got ```Exception in thread ""main"" java.io.FileNotFoundException: hdfs:/host:port/tmp/GF1_WFV1_E123.3_N48.0_20160519_L1A0001592646.tiff (No such file or directory)```i do not want to read it as RDD so i did not use `hadoopTemporalGeoTiffRDD` method",8/16/16,1
pomadchin,@uuuuq i dont remember exactly but it seems to me GeoTiffReader not supports hdfs reads,8/16/16,1
pomadchin,"however it supports `bytes` reads: ```scalaGeoTiffReader.readMultiband(bytes)```",8/16/16,1
pomadchin,you can read file as a byte array from hdfs,8/16/16,1
uuuuq,"@pomadchin thx, i' ll try",8/16/16,1
uuuuq,"btw, i still got problem when ingesting temporal-geotiff files with `spark-etl` as i mentioned before, i got `Unable to find input module of type 'hadoop' for formattemporal-geotiff`i tried the 3 latest releases `0.10.2`, `0.10.1`, `0.10.0`, got the same massage.",8/16/16,1
pomadchin,hm,8/16/16,1
pomadchin,will try to track it down,8/16/16,1
uuuuq,@pomadchin so embarrased i searched but could not find how to read hdfs files as bytes array,8/16/16,1
pomadchin,"@uuuuq smth like that (not tested though)```scaladef getBytesArr(filePath: String, conf: Configuration): Array[Byte] = {  val path = new Path(filePath)  val fs = path.getFileSystem(conf)  val is = fs.open(path)  val arr = scala.io.Source.fromInputStream(is).map(_.toByte).toArray // needs to be double checked how to convert input stream to a byte array  is.close(); fs.close(); arr}```",8/16/16,1
ardilgulez,how can I build a mask tile out of layer extent and geom ,8/17/16,1
ardilgulez,and use it for localmask,8/17/16,1
pomadchin,,8/17/16,1
ardilgulez,@pomadchin what was the thing that you deleted :( ,8/17/16,1
pomadchin,@ardilgulez just have not read completely / properly your sentnce and after that forgot about it ): sry,8/17/16,1
ardilgulez,np lol,8/17/16,1
pomadchin,so you just need to mask a tile with a cetain extent?,8/17/16,1
ardilgulez,yeah,8/17/16,1
pomadchin,"```scalatile.mask(extent, geom)```",8/17/16,1
pomadchin,O:,8/17/16,1
ardilgulez,"I can't use that for RDD[SpatialKey, Tile] :(",8/17/16,1
ardilgulez,or can I,8/17/16,1
pomadchin,hm,8/17/16,1
pomadchin,let me see,8/17/16,1
ardilgulez,"there might be other ways to mask with extent and geom without creating a mask tile using them, I just couldn't get to know them ",8/17/16,1
pomadchin,"so you can use mask for ```scalaRDD[(K, V)] with Metadata[M]```",8/17/16,1
pomadchin,just using ```rdd.mask(geom)```,8/17/16,1
ardilgulez,"``` scaladef tms = pathPrefix(IntNumber / IntNumber / IntNumber) { (zoom, x, y) =>		parameters(			'layers,			'weights,			'opacity.as[Int],			'colorRamp ? ""blue-to-orange""		) { (layersParam, weightsParam, opacityParam, colorRamp) =>			val layers = layersParam.split("","")			val weights = weightsParam.split("","").map(_.toInt)			val key = SpatialKey(x, y)			val extent = attributeStore				.readMetadata[TileLayerMetadata[SpatialKey]](LayerId(layers(0), zoom))				.mapTransform(SpatialKey(x, y))				.extent				.reproject(WebMercator, LatLng)			val (extSeq, tileSeq) =				layers.zip(weights)					.map { case (l, weight) =>						getMetaData(LayerId(l, zoom)).mapTransform(key) ->							fileValueReader.reader[SpatialKey, Tile](LayerId(l, zoom)).read(key).convert(DoubleConstantNoDataCellType) * weight					}.toSeq.unzip			val tileAdd =					tileSeq.localAdd			val tileMap =					tileAdd.map(i => if(i == 0) NODATA else i)			val ramp = ColorRampMap.getOrElse(colorRamp, Ramps.BlueToOrange)			val breaks = getBreaks(layersParam, weightsParam, ramp.size, zoom)			val colorMap = ColorMap(breaks, ramp, colorMapDefaultOptions)			respondWithMediaType(MediaTypes.`image/png`) {				val result =						tileMap							.mask(extent, geom)							.renderPng(colorMap)							.bytes				complete(result)			}		}	}```",8/17/16,1
ardilgulez,,8/17/16,1
ardilgulez,"it gives me ""spray.json.DeserializationException: TileLayerMetadata expected""",8/17/16,1
ardilgulez,"when I hit the url like: http://localhost:8586/tiles/9/294/196.png?layers=l1,l2,l3,l4,l5&weights=3,3,3,3,3&opacity=128&colorRamp=blue-to-orange",8/17/16,1
ardilgulez,stack trace says it's from the line starting with getMetaData or the one below that,8/17/16,1
ardilgulez,also:,8/17/16,1
ardilgulez,"``` scaladef getMetaData(id: LayerId): TileLayerMetadata[SpatialKey] =		attributeStore.read[TileLayerMetadata[SpatialKey]](id, Fields.metadata)```",8/17/16,1
pomadchin,thx for a good catch,8/17/16,1
pomadchin,deprecated code ._. missed commit,8/17/16,1
pomadchin,"```scalaattributeStore.readMetadata[TileLayerMetadata[SpatialKey]](id)```",8/17/16,1
ardilgulez,this fixes my problems?,8/17/16,1
pomadchin,yep,8/17/16,1
pomadchin,should fix,8/17/16,1
pomadchin,"if no, just write there, i_Ñéll double check",8/17/16,1
ardilgulez,it indeed fixes,8/17/16,1
ardilgulez,"just one thing: geotrellis.spark.io.package$TileNotFoundError: Tile with key SpatialKey(1175,786) not found for layer Layer(name = ""l1"", zoom = 11)",8/17/16,1
pomadchin,seems like ok; ,8/17/16,1
pomadchin,what layers you have ingested?,8/17/16,1
ardilgulez,just a random layer that covers only central part of smyrna,8/17/16,1
ardilgulez,perfectly normal,8/17/16,1
ardilgulez,is this error thrown from the attributeStore.readMetadata method?,8/17/16,1
pomadchin,"i believe thrown by ```scalatileReader.reader[SpatialKey, Tile](layerid).read(key)```",8/17/16,1
pomadchin,means that there is no requested tile on that layer ,8/17/16,1
ardilgulez,"yeah, that's it",8/17/16,1
ardilgulez,stack trace also says that,8/17/16,1
ardilgulez,yeah perfectly normal,8/17/16,1
ardilgulez,lol,8/17/16,1
ardilgulez,my brain is out of service ,8/17/16,1
pomadchin,"try to use leaflet to look at all the tiles ingested (:that may help",8/17/16,1
ardilgulez,"tiles have no problem, map is super smooth and stuff it's just that I don't want to throw a 5xx error code when this error happens",8/17/16,1
pomadchin,oh; that_Ñés just a spray error catch,8/17/16,1
pomadchin,so the fastest woraround is to return an `Option[Byte[Array]]` type,8/17/16,1
pomadchin,return of None would mean 404 for spray,8/17/16,1
pomadchin,"you just need to handle exception, and to return optional type",8/17/16,1
ardilgulez,"OK, I would like to ask for some serious advice here",8/17/16,1
ardilgulez,I'm developing a weighted overlay kind of thing,8/17/16,1
ardilgulez,you can check it out here: http://harita.citysurf.com.tr/weightedoverlay/,8/17/16,1
ardilgulez,I would like to ask a few questions here,8/17/16,1
ardilgulez,"1- My tiles look kinda clanky. I generated them from raw point data, which is ""if there is a point of interest, pixel = 1, else pixel = nodata"", using arcmap's point density analysis",8/17/16,1
ardilgulez,rest of the process was pretty straightforward: cut the tiles and serve,8/17/16,1
ardilgulez,"the look of that thing is just... there are circles and shit, for lack of a better term",8/17/16,1
ardilgulez,"my #1 question is: what kind of an analysis should I perform on raw point tiles, so that the shapes look better than they do now?",8/17/16,1
ardilgulez,"2- Performance is in the toilet, but now I can at least get the tiles now, even if it takes 4 seconds per 256x256 tile. When I tried adding landsat ndvi, performance was even worse. I couldn't even get tiles. What can I do to increase the performance?",8/17/16,1
ardilgulez,"Right now, what I'm doing is the same thing in geotrellis-chatta-demo",8/17/16,1
ardilgulez,"I get individual singleband tiles that are composed of individual layers, zip them with weights, map them, unzip them and turn them into seq -> local add -> render png -> serve",8/17/16,1
ardilgulez,"3- Any other tips on adding landsat layer and mixing that up with non-landsat layers? I should admit, my first attempt was pretty horrible. I didn't even gdalwarp the landsat tile",8/17/16,1
rossbernet,@ardilgulez  What do you mean by clanky?,8/18/16,1
ardilgulez,It's just circles on top of circles on top of circles,8/18/16,1
ardilgulez,I would like a better look,8/18/16,1
rossbernet,"It looks like for each point you created a buffer of uniform value. Then summed all the circles that each point created. If that's the case, you can try creating individual grids from each of the parameters that take into account distance to each of the points.  ",8/18/16,1
rossbernet,[![blob](https://files.gitter.im/geotrellis/geotrellis/1nxg/thumb/blob.png)](https://files.gitter.im/geotrellis/geotrellis/1nxg/blob),8/18/16,1
rossbernet,versus,8/18/16,1
rossbernet,[![blob](https://files.gitter.im/geotrellis/geotrellis/TBTj/thumb/blob.png)](https://files.gitter.im/geotrellis/geotrellis/TBTj/blob),8/18/16,1
rossbernet,,8/18/16,1
rossbernet,,8/18/16,1
ardilgulez,This looks better for sure,8/18/16,1
echeipesh,@ardilgulez and you should be able to use newly added spark kernel density op to go from RDD of Point to RDD of Tile: https://github.com/geotrellis/geotrellis/blob/master/spark/src/test/scala/geotrellis/spark/density/RDDKernelDensitySpec.scala#L60,8/18/16,1
ardilgulez,Uh huh,8/18/16,1
pomadchin,"@ardilgulez btw, @echeipesh tried to answer your questions https://groups.google.com/forum/#!topic/geotrellis-user/eIFby171Qkk",8/18/16,1
ardilgulez,Do I get to pick and choose the radius for kernel density?,8/18/16,1
echeipesh,"Yes, that test just uses a circle as you do right now, there is also Gausian kernel. I think for inverse distance you_Ñéd have to write the kernel yourself. I can take a look at it to give you some directions if you_Ñére interested in that path.",8/18/16,1
ardilgulez,Take a look at my raw tiles? ,8/18/16,1
echeipesh,sure,8/18/16,1
ardilgulez,Let me get them for you,8/18/16,1
echeipesh,What do you want me to see on them ?,8/18/16,1
ardilgulez,"""I can take a look at it to give you some directions if you_Ñére interested in that path""",8/18/16,1
ardilgulez,"I thought ""it"" meant my raw tiles",8/18/16,1
echeipesh,"Ah, I was talking about making an inverse distance kernel to turn points to tiles",8/18/16,1
ardilgulez,Oh,8/18/16,1
echeipesh,"It sounds like you have two questions, the performance and the tile generation. So if you_Ñére able to read those points into scala in any way you can generate the tiles using GeoTrellis. You_Ñéd just need inverse distance kernel to do the rendering @rossbernet was showing.",8/18/16,1
ardilgulez,hey,8/19/16,1
ardilgulez,thank you for all the help :),8/19/16,1
ardilgulez,you guys rock,8/19/16,1
ardilgulez,i have yet another question,8/19/16,1
ardilgulez,is there a way to get a geojson feature collection of enclosing polygons ,8/19/16,1
ardilgulez,"using ``` scalareaderSet.layerReader	.query[SpatialKey, Tile, TileLayerMetadata[SpatialKey]](LayerId(layers(0), zoom))	.where()```",8/19/16,1
ardilgulez,or some other thing ,8/19/16,1
ardilgulez,and how do I do the same thing with a combination of layers,8/19/16,1
echeipesh,"@ardilgulez glad things are working out. Sure, so that will give you an RDD for tiles. You can do ```scalaval rasterRDD: RDD[(SpatialKey, Raster[Tile])] = rdd.asRasters` // each raster knows it_Ñés extentval featureRDD: RDD[PolygonFeature[Int]] = rasterRDD.map { case (key, raster) => 	raster.asFeature.mapData(  tile => tile.cols) //turning a tile into something light you might want in GeoJSON}val features: Array[PolygonFeature[Int]] = featureRDD.collect // hopefully this is now small enough to be collectedval geoJson = features.toGeoJson// ```You_Ñéll need `import geotrellis.vector.io._` and possibly `import geotrellis.vector.io.json._` to make the last part work",8/19/16,1
palmerlao,"Hi all, I'm new to dealing with geographical data and am working on an entity resolution problem where i try to resolve events with lat long coordinates to edges on a large spatial graph. Does `SpacePartitioner` do something like geohashing or bucketing that would be useful for this kind of problem?",8/19/16,1
echeipesh,"Thats pretty much exactly what it does, it will partition based on a spacefilling curve, geohash uses z-order curve. Not sure what you mean by large spatial graph, but it does what you think it does.",8/19/16,1
echeipesh,"You_Ñére trying to de-dup points essentially, right ?",8/19/16,1
palmerlao,i suppose that is one way of looking at it - the events i need to resolve onto the graph are sort of noisy observations of points on the graph,8/19/16,1
palmerlao,thanks!,8/19/16,1
echeipesh,"No problem, let me know if you have questions hackong GT, sounds like a really interesting project.",8/19/16,1
echeipesh,Out of curiosity what would be your plan for dealing with points that would span hash boundaries but belong to the same graph node ?,8/19/16,1
palmerlao,hmm,8/19/16,1
palmerlao,i have no idea!,8/19/16,1
palmerlao,do you know if there's a well-established way to deal with it in GeoTrellis?,8/19/16,1
palmerlao,it sounds like geohashes have nice algorithms to compute adjacent hash buckets,8/19/16,1
palmerlao,not sure if the same holds for z-order curves,8/19/16,1
echeipesh,geohash==z-order curves,8/19/16,1
palmerlao,oh oops,8/19/16,1
palmerlao,i meant space-filling,8/19/16,1
echeipesh,"Yes, they all share that property, so you can always go from some (x,y) grid to a bucket",8/19/16,1
echeipesh,"So given you know your (x,y) grid you can find out your neighbords.",8/19/16,1
echeipesh,"We had to do the reverse of that for calculating KernelDensity, where we mapped each point to an extent and then intersected the extent with a grid (that was the grip of tiles in that case). That assumes there is some maximum distance at which point can be considrered for membership to a node. You can check out the mechanics here: https://github.com/geotrellis/geotrellis/blob/master/spark/src/main/scala/geotrellis/spark/density/RDDKernelDensity.scala",8/19/16,1
kevinmhinson,"any examples of use of Tags passed to MultibandGeoTiff or some other raster?  I'd like to be able to name the bands, pass some color maps to each band...  it looks like it may be possible based on the GDAL ref listed on the Tags scaladocs page, but I cannot make it happen.  ",8/20/16,1
ardilgulez,"so I have 5 variables of type RDD[(SpatialKey, Tile)]",8/22/16,1
ardilgulez,how can I get a single RDD out of them that is equal to the sum of all 5 RDDs?,8/22/16,1
pomadchin,"@ardilgulez hi, sum you mean _union_?",8/22/16,1
ardilgulez,what does union do?,8/22/16,1
pomadchin,"makes union of rdds; ```RDD(elem1, elem2, _Ñ_, elemN) union RDD(zelem1, zelem2, _Ñ_, zelemM) = RDD(elem1, _Ñ_, elemN, zelem1, _Ñ_, zelemM) ```",8/22/16,1
ardilgulez,so what we're saying is that:,8/22/16,1
ardilgulez,"``` scalaRDD(1, 2, 3) union RDD(4, 5, 6) = RDD(1, 2, 3, 4, 5, 6)```",8/22/16,1
ardilgulez,but what I need is:,8/22/16,1
ardilgulez,"``` scalaRDD(1, 2, 3) operation RDD(4, 5, 6) = RDD(1+4, 2+5, 3+6)",8/22/16,1
ardilgulez,"``` scalaRDD(1, 2, 3) operation RDD(4, 5, 6) = RDD(1+4, 2+5, 3+6)```",8/22/16,1
ardilgulez,I think this kind of a thing works on 2 RDDs: (rdd1 union rdd2).reduceByKey(_ ++ _),8/22/16,1
pomadchin,that_Ñés correct; but that would work in case when you have keys,8/22/16,1
pomadchin,"in generall it_Ñés not possible to sum any random rdds, because at least you dont know their sizes",8/22/16,1
pomadchin,and don_Ñét know corresponding partitions will be exactly identical in size ,8/22/16,1
pomadchin,so you can use `zip` function for your purposes; but be sure that sizes are idential,8/22/16,1
ardilgulez,layers that are used for construction of RDDs have the exact same extent,8/22/16,1
pomadchin,"oh, than reduceByKey works",8/22/16,1
ardilgulez,,8/22/16,1
ardilgulez,"```scalaval layerSeq =    layers.zip(weights)        .map { case (l, weight) =>		readerSet		        .layerReader			.query[SpatialKey, Tile, TileLayerMetadata[SpatialKey]](LayerId(l, zoom))			.result * weight        }.toSeqval singleRdd = layerSeq.reduce(_ union _).reduceByKey((a, b) => a + b)val breaks = getBreaks(layersParam, weightsParam, 10, zoom).reverseval filteredRdd = singleRdd.mapValues((tile) => {	tile.map((pixel) => {		if(pixel < breaks(0))			NODATA		else			pixel	})})val rasterRDD: RDD[(SpatialKey, Raster[Tile])] = filteredRdd.asRasters```",8/22/16,1
ardilgulez,"because I lose the metadata in the ""val layerSeq = "" thing",8/22/16,1
ardilgulez,filteredRdd.asRasters stops working,8/22/16,1
ardilgulez,how can I get around this thing?,8/22/16,1
pomadchin,try `layers.withContext { _.zip (weights).map _Ñ_ }`,8/22/16,1
ardilgulez,layers is a string array,8/22/16,1
pomadchin,ah gothca,8/22/16,1
pomadchin,"for each layer you can get metadata manually: ```scaladef getMetaData(id: LayerId): TileLayerMetadata[SpatialKey] =    attributeStore.readMetadata[TileLayerMetadata[SpatialKey]](id)val (lmd, l) =    layers.zip(weights)        .map { case (l, weight) =>        val id = LayerId(l, zoom)       val md = getMetaData(id)        val layer = readerSet                .layerReader            .query[SpatialKey, Tile, TileLayerMetadata[SpatialKey]](LayerId(id))            .result * weight          md -> layer        }.toSeq.unzipval contextRdd = ContextRDD(l, lmd)```",8/22/16,1
pomadchin,thought it_Ñés a workaround ,8/22/16,1
pomadchin,after that using `.withContext` function you may safe your metadata ,8/22/16,1
ardilgulez,"lmd and l are seq(metadata) and seq(rdd[spatialkey, tile]) respectively",8/22/16,1
pomadchin,"oops, def; just cant completely repeat your code ._.",8/22/16,1
ardilgulez,I think I'll just go crazy over this thing,8/22/16,1
ardilgulez,can I use reduce somehow before unzipping?,8/22/16,1
pomadchin,"```scaladef getMetaData(id: LayerId): TileLayerMetadata[SpatialKey] =    attributeStore.readMetadata[TileLayerMetadata[SpatialKey]](id)// seq of rdds though not sure that is wht you wantval layerSeq =    layers.zip(weights)        .map { case (l, weight) =>        val id = LayerId(l, zoom)       val md = getMetaData(id)        val layer = readerSet                .layerReader            .query[SpatialKey, Tile, TileLayerMetadata[SpatialKey]](LayerId(id))            .result * weight          ContextRDD(layer, lmd)        }.toSeq```",8/22/16,1
ardilgulez,or this too lol,8/22/16,1
pomadchin,:D ,8/22/16,1
pomadchin,thoug seq of rdds looks like not the fastest thing ,8/22/16,1
ardilgulez,seq or any iterable of RDDs is probably way worse than how I'm thinking now,8/22/16,1
ardilgulez,"``` scalalayerSeq.reduce(_ union _).reduceByKey((a, b) => a + b)```and now this reduceByKey loses my metadata :D this thing lol",8/22/16,1
pomadchin,"```scalalayerSeq.reduce((a,b) => a.withContext { _.union(b) }).reduceByKey((a, b) => a.withContext { _.+(b) })```",8/22/16,1
ardilgulez,layerSeq.reduce(_ union _) part works fine,8/22/16,1
ardilgulez,"the reduceByKey((a, b) => a + b) doesn't ",8/22/16,1
ardilgulez,"this thing looks like ""saving private tilelayermetadata"" to me ",8/22/16,1
ardilgulez,after a whole bunch of hours into this,8/22/16,1
ardilgulez,and a doesn't have withContext in the last reduceByKey ,8/22/16,1
pomadchin,"```scalalayerSeq.reduce((a,b) => a.withContext { _.union(b) }).withContext { _.reduceByKey((a, b) => a +b ) }```",8/22/16,1
pomadchin,sry for that; just havent tried to compile that,8/22/16,1
pomadchin,however last variant should definitely work,8/22/16,1
ardilgulez,lol I'm literally making you write my code and you apologize for a code that's not working,8/22/16,1
ardilgulez,now I understand how my boss feels when my code doesn't work,8/22/16,1
ardilgulez,:D,8/22/16,1
ardilgulez,thank you for that too,8/22/16,1
ardilgulez,I need the polygons that enclose only certain points and now I'm getting everything,8/22/16,1
pomadchin,"in that code above, therare no constrains on data that you query",8/22/16,1
pomadchin,"try to do: ```scala.query[SpatialKey, Tile, TileLayerMetadata[SpatialKey]](LayerId(id)).where(<conditions>)```",8/22/16,1
ardilgulez,there is a problem about this: the <conditions> should depend on the aggregation of all layers,8/22/16,1
pomadchin,what do you mean by that?,8/22/16,1
ardilgulez,"I do this thing: val singleRdd = layerSeq.reduce((a,b) => a.withContext { _.union(b) }).withContext { _.reduceByKey((a, b) => a +b ) }",8/22/16,1
ardilgulez,"what I mean is, the <conditions> should depend on the reduced values",8/22/16,1
pomadchin,eeeee,8/22/16,1
pomadchin,you see that there is a logical problem :D,8/22/16,1
ardilgulez,yeah :D,8/22/16,1
ardilgulez,is there an actual way to pull this thing off?,8/22/16,1
pomadchin,what constrains do you need?,8/22/16,1
ardilgulez,I have a number of layers,8/22/16,1
ardilgulez,that are of same extent,8/22/16,1
ardilgulez,I do a weighted sum over them,8/22/16,1
ardilgulez,and I want the polygons enclosing weighted sum values that are greater than a threshold,8/22/16,1
ardilgulez,aka the largest quantile break,8/22/16,1
echeipesh,"@ardilgulez ah, you actually need to extract the polygons of high suitability regions, which may span multiple tiles in the result RDD?",8/22/16,1
ardilgulez,Yes,8/22/16,1
echeipesh,"That could be a little tricky, we have not done vectorization over tile sets yet. This works well for single tiles: https://github.com/geotrellis/geotrellis/blob/master/raster/src/main/scala/geotrellis/raster/vectorize/Vectorize.scala. I_Ñém not sure if it will do the trick for you but you could try downsampling the tiles a bit and stitching them before extracting the polygon. (I_Ñém thinking you might need to downsample for the stitch, which will cause a collect, not to blow the memory on the driver)",8/22/16,1
echeipesh,Or since you already ingested into a TMS pyramid you may be able to choose a zoom level that is a good comporimse of resolution vs size and stitch that?,8/22/16,1
jnh5y,"@echeipesh how's it going mate?  While we were looking at a protected access issue, Andrew Hulbert ran across this SO question: http://stackoverflow.com/questions/35841585/illegalaccessexception-can-not-access-a-member-of-class-with-modifiers-prote",8/22/16,1
jnh5y,thought you might want to see it,8/22/16,1
jnh5y,don't know how far you all have made it with proj4j,8/22/16,1
echeipesh,"@jnh5y Greetings. Thanks for linking the issue, I don_Ñét think I remember seeing that and we use proj4j laregely without issues at this point. Are you guys running into a smiliar issue ?",8/22/16,1
echeipesh,"Looking at that class, it has a _ÑÉpublic_Ñé modifier, which it probably had when we abosorbed the code-base a few years ago. I wonder if in that case an alternative version of `Registry` was being found.",8/22/16,1
jnh5y,*shrugs*  we were doing something totally different,8/22/16,1
jnh5y,we aren't blocked; we were messing with a different class / package /etc,8/22/16,1
owcm,I wanted to render the results of a RDD dataset that I stored in a FileAttributeStore using FileLayerWriter.  I used Pyramid.upLevels to build the different zoom levels.  I wanted to us a qualitative scheme and Histograms to render the data.  The endpoint I have stood up receives the the Z/X/Y value and I fetch the tile from the store using FileReader.  At this point I can use ColorMap.fromQuantileBreaks on the tile to apply my Colormap but of course the mapping and the breaks are calculated in the context of the individual tile.   The question: Is there away to consider all the values stored for the RDD when rendering the tile.   The approach I am using  is evaluating each tile in isolation not in the context of the range of values for the complete stored RDD.,8/22/16,1
echeipesh,"@owcm The thing to do is to calculate the histogram as you save your layers in `Pyramid.upLevels` and save it in the `AttributeStore`. Generally we store layer wide metadata under zoom level 0 for a layer. Also since histogram only needs to be aproximate you may calcualte it on one of the intermidiate zoom levels, not only the base level.",8/22/16,1
echeipesh,"then your tile endpoint can just query the `AttributeStore` for that histogram, cache it memory and render tiles on the fly.",8/22/16,1
owcm,"@echeipesh Thanks, I was starting to move in that direction.  Are there any examples you can point me to of writing  and reading ""layer wide metadata""  to  the attribute store?",8/22/16,1
echeipesh,"Sure, this bit doesn_Ñét save the histogram, but it is the same trick: https://github.com/geotrellis/geotrellis-landsat-emr-demo/blob/master/ingest/src/main/scala/demo/LandsatIngest.scala#L114-L124",8/22/16,1
owcm,@echeipesh Thank you much,8/22/16,1
echeipesh,and then they_Ñére read like this: https://github.com/geotrellis/geotrellis-landsat-emr-demo/blob/master/server/src/main/scala/demo/DemoServiceActor.scala#L188-L200,8/22/16,1
echeipesh,No problem :),8/22/16,1
wsf1990,"when I use `tile.reproject(extent, srcCRS, dstCRS, method) ` from 3857 reproject to 4326,the tile's data at the edge will be changed.",8/23/16,1
wsf1990,"use `tile.findMinMax` will show the reproject tile's minmax became (-32724,32719) and the src's (1711,4893)",8/23/16,1
wsf1990,why the data be cahnged? ,8/23/16,1
wsf1990,I found that caused by the method.if I use CubicConvolution the result is error. If I use Bilinear the result is right.,8/23/16,1
ardilgulez,"@echeipesh thanks for the method you suggested, results are pretty accurate, but the result is more a whole lot of small squares rather than a (number of) larger polygon(s)",8/23/16,1
ardilgulez,can you also suggest me a method to merge these small squares into a polygon?,8/23/16,1
ardilgulez,"I think those ""small squares"" are individual pixels :D ",8/23/16,1
ardilgulez,and I think I can merge them with reduce and union,8/23/16,1
ardilgulez,just don't know how exactly to do that :(,8/23/16,1
wsf1990, How can I cut a tiff by extent not mask?,8/23/16,1
ardilgulez,"``` scalareaderSet	.layerReader	.query[SpatialKey, Tile, TileLayerMetadata[SpatialKey]](LayerId(layerName, zoom))	.where(Intersects(extent))	.result```",8/23/16,1
ardilgulez,"> and I think I can merge them with reduce and unionI now know how to do that thing. ""toVector"" method does merging automatically among pixels of equal values",8/23/16,1
ardilgulez,"@wsf1990 after my code, you'll get an RDD[(SpatialKey, Tile)] and you can use geotrellis.spark.render and just perform rendering operations on the RDD",8/23/16,1
ardilgulez,if that's what you mean,8/23/16,1
wsf1990,I mean a tile or a geotiff to cut not query from rdd,8/23/16,1
ardilgulez,I think gdal_retile is your answer here,8/23/16,1
wsf1990,"yes, but how can I do it by gt?",8/23/16,1
ardilgulez,"from what I understand, your problem is:1- you have a geotiff whose extent is, let's say 20, 20, 30, 302- you want a geotiff whose extent is, let's say 22, 22, 28, 28",8/23/16,1
wsf1990,you are right,8/23/16,1
wsf1990,If i use tile.mask the extent is not change,8/23/16,1
ardilgulez,I don't think geotrellis itself supports gdal operations,8/23/16,1
ardilgulez,but good news is: you can just write a very small bash/python script and gdal does it for you,8/23/16,1
ardilgulez,very performantly too,8/23/16,1
wsf1990,But the tile is calculate by gt and cut is just a sub,8/23/16,1
pomadchin,"@wsf1990 you can read a layer, and cut it by polygon, after that save it as a tiff; the extent would be correct: ```scalaval raster = layerReader  .read[SpatialKey, Tile, TileLayerMetadata[SpatialKey]](layerId, filter)  .mask(polygon)  .stitch  .crop(polygon.envelope)```",8/23/16,1
pomadchin,"you should read all tiles from layer which intersect your polygon, mask them, stitch, and crop",8/23/16,1
wsf1990,"got it,I'll try",8/23/16,1
owcm,I wanted to blend two tile PNG results together is there any helper methods in Geotrellis?   I came across the merge method but I was  not sure if that was the way to go. ,8/23/16,1
wsf1990,When I use `tiff.merge(other)` It told me `could not find implicit value for evidence parameter of type geotrellis.spark.merge.Mergable[geotrellis.raster.io.geotiff.SinglebandGeoTiff]`,8/24/16,1
wsf1990,how can I import this implicit ?,8/24/16,1
wsf1990,or how can I merge two singlebandtiff which dont have the same extent?,8/24/16,1
pomadchin,you can_Ñét merge tiffs; but you can merge rasters,8/24/16,1
pomadchin,"so what you need to do is to convert singlebandgeotiff into raster: ```scalatiff.raster.merge( <other raster there> )```",8/24/16,1
wsf1990,I see the source code the function of merge can just merge the raster with the same or interact extent. Which I need more like stitch,8/24/16,1
pomadchin,mmm i see,8/24/16,1
pomadchin,by design merge is a sort of a mosaic function,8/24/16,1
pomadchin,"you can try to stich tiles, btw",8/24/16,1
pomadchin,that should work,8/24/16,1
wsf1990,"yes,How can I stitch tiles?",8/24/16,1
pomadchin,"mmm; our sticher bases on `key`; so you need to have already tiled layer; https://github.com/geotrellis/geotrellis/blob/master/raster/src/main/scala/geotrellis/raster/stitch/Stitcher.scala#L25",8/24/16,1
wsf1990,I just have 9 tiles which read by AccumuloValueReader,8/24/16,1
pomadchin,"ok, that suits our case O:",8/24/16,1
pomadchin,"```scalaval pairs: Seq[(SpatialKey, Tile)] = ???val cols: Int = ??? // cols in the result tile (maxKey.col - minKey.col)val rows: Int = ??? // rows in the result tile (maxKey.row - minKey.row)implicitly[Stitcher[Tile]].stitch(pairs.map { case (key, tile) => tile -> (key.col, key.row) }, cols, rows)```",8/24/16,1
pomadchin,"so if you read everything using value reader, you know keys; if you know keys, you know your key range; ",8/24/16,1
wsf1990,"ok, I'll try!thanks!",8/24/16,1
wsf1990,dose need the  (maxKey.col - minKey.col) * 256?,8/24/16,1
pomadchin,@wsf1990 looks like yes,8/24/16,1
wsf1990,"OK,Thanks!",8/24/16,1
echeipesh,@ardilgulez So you have several regions as polygons and you want to make a multipolygon? Then you can use `.unionGeometries` (ex: https://github.com/geotrellis/geotrellis/blob/998e34f0d4c5884c76e17c388cc735165ce96404/vector-test/src/test/scala/spec/geotrellis/vector/SeqMethodsSpec.scala#L74-L86),8/25/16,1
pljplj,I have a scala question:  What is wrong with the following I get the error,8/25/16,1
pljplj,"Error:(18, 3) type mismatch; found   : Unit required: org.apache.spark.rdd.RDD[(Int, Int, geotrellis.vector.Geometry)]  }  ^",8/25/16,1
pljplj,"object dumtester {  def stupid(tiles:RDD[(Int,Int,Geometry)]) : RDD[(Int,Int,Geometry)]  = {    val latest = tiles.map(f =>{      if (tiles.count() == 2){        f      }else{        f      }    })  }}",8/25/16,1
kevinmhinson,"you're not returning an RDD, you're just creating val latest and returning nothing...  either remove the ""val latest ="" or just pass latest out at the end of stupid()",8/25/16,1
pljplj,thanx -- I foolishly thought that it would automagically return latest since that was the last thing.,8/25/16,1
dvntucker,"Hi all, I've been trying to check out GeoTrellis and get it compiling in IntelliJ, but running into an odd error and was wondering if anyone had any ideas",8/26/16,1
dvntucker,https://gist.github.com/dvntucker/1ef53e5139f72d3503a62b641fd67734,8/26/16,1
dvntucker,sbt compile runs fine in the command line,8/26/16,1
pomadchin,@dvntucker probably it_Ñés intelij idea issues,8/26/16,1
pomadchin,@dvntucker + be sure that you included boilerplate code generated ,8/26/16,1
dvntucker,Yeah I have the boilerplate generated,8/26/16,1
dvntucker,Are there many IntelliJ users in general? Should I just move on to Eclipse?,8/26/16,1
pomadchin,"@dvntucker i use intelij idea, but never compile geotrellis using it",8/26/16,1
pomadchin,however more simple client apps are building easily / successfully with intelijidea and gt as a dep,8/26/16,1
pomadchin,to be honest i never use intelij idea to compile scala ._.,8/26/16,1
dvntucker,"Hmm, K",8/26/16,1
dvntucker,Thank you,8/26/16,1
ardilgulez,I have 30m resolution tiles,8/26/16,1
ardilgulez,tiffs*,8/26/16,1
ardilgulez,how do I get cut them into zoom levels > 13,8/26/16,1
pomadchin,@ardilgulez gt stores zoom lvl depding on the utm zone,8/26/16,1
pomadchin,https://github.com/geotrellis/geotrellis/blob/2402607b0c6f5a4e087bc46fe32c5f56e206947e/spark/src/main/scala/geotrellis/spark/tiling/ZoomedLayoutScheme.scala#L51-L82,8/26/16,1
pomadchin,"if you need increased resution you can resample tiles on demand: ```scalaval LayoutLevel(_, layout) = layoutScheme.levelForZoom(projection.worldExtent, zoom)val mapTransform = MapKeyTransform(meta.crs, layout.layoutCols, layout.layoutRows)val targetExtent = mapTransform(x, y)val gb @ GridBounds(nx, ny, _, _) = meta.mapTransform(targetExtent)val sourceExtent = meta.mapTransform(nx, ny)val largerTile = tileReader.reader[SpatialKey, Tile](layerId).read(SpatialKey(nx, ny))(largerTile.resample(sourceExtent, RasterExtent(targetExtent, 256, 256)), meta.extent, meta.crs)```",8/26/16,1
mbertrand,"Hi, I was wondering if there's a way to create an RDD from a MultibandTile object instead of a geotiff file.For instance, instead of:```  val inputPath = '/data/image.tif'  val inputRdd: RDD[(ProjectedExtent, MultibandTile)] = sc.hadoopMultibandGeoTiffRDD(inputPath)```I'd like to do something like this:```  val geoTiff1 = SinglebandGeoTiff(new File('/data/image1.tif')  val geoTiff2 = SinglebandGeoTiff(new File('/data/image2.tif')  val multiTile = MultibandTile(geoTiff1.tile, geoTiff2.tile)  val inputRDD: RDD[(ProjectedExtent, MultibandTile)] = sc.???(multiTile)```",8/26/16,1
ardilgulez,I think that the ??? can be replaced by hadoopMultibandGeotiffRDD or something like that and stuff will work just fine,8/26/16,1
kevinmhinson,"Is there a good raster format with a sparse data structure behind it?  I looked at the cloud filtering example where you fill in GeoTiffs with a NODATA val, but that's still using an Int value in a dense Array[Int].",8/27/16,1
pomadchin,@mbertrand are you sure that you need it that way? ._.,8/29/16,1
pomadchin,"it is possible but not convenient / fast / etc / ```scalaval crs: CRS = ???val tileLayout: TileLayout = ???val layoutScheme = FloatingLayoutScheme(tileLayout.tileCols, tileLayout.tileRows)val r1 = SinglebandGeoTiff("""").rasterval r2 = SinglebandGeoTiff("""").rasterval multiTile = MultibandTile(r1, r2)val inputRdd: RDD[(ProjectedExtent, MultibandTile)] = sc.parallelize(Seq((ProjectedExtent(r1.extent, crs), multiTile)))```",8/29/16,1
mbertrand,"thanks @pomadchin, in that case I will try to avoid doing it that way if I can.",8/29/16,1
owcm,"I have a geotiff that I am ingesting and pyramiding using FileLayerWriter using spark. The geotiffs native reference system is ""+proj=utm +zone=18 +datum=WGS84 +units=m +no_defs"" .  During the ingest process I reproject to WebMercator.   Code I use to ingest is in gist https://gist.github.com/owcm/e9d8dcf54bee271650c2f7a95c2c3af3.   I run a simple hillshade on the saved data and save the RDD hillshade result to another tiled pyramid using FileLayerWriter spark.  I use a simple gray scale map to render that is in the gist as well.   I end up getting white borders on the tiles that I render, almost like there are artifacts remaining from the reprojection process.  Is there a crop or trim process I am missing?   I am attaching the image I am seeing.",8/29/16,1
owcm,[![Screen Shot 2016-08-29 at 5.46.44 PM.png](https://files.gitter.im/geotrellis/geotrellis/mIrk/thumb/Screen-Shot-2016-08-29-at-5.46.44-PM.png)](https://files.gitter.im/geotrellis/geotrellis/mIrk/Screen-Shot-2016-08-29-at-5.46.44-PM.png),8/29/16,1
owcm,Follow on I am using 0.10.1,8/29/16,1
kevinmhinson,"i can't figure out why proj4.Transform won't give me consistent results when I transform from lat/long to web mercator...  extrapolate this out over billions of records, and i've been going mad the last few days trying to figure it out.",8/31/16,1
kevinmhinson,[![blob](https://files.gitter.im/geotrellis/geotrellis/KoP1/thumb/blob.png)](https://files.gitter.im/geotrellis/geotrellis/KoP1/blob),8/31/16,1
kevinmhinson,"it's dispersing lat/lons from inside the US all over the globe and stacking tons of values up around the web mercator equivalent of 180 lon and/or 0 lat...  I'd love to be told I'm doing something stupid, but I can't find it",8/31/16,1
lossyrob,"are you treating EPSG:4326 coordinates as (x, y), which is (lon, lat)",8/31/16,1
lossyrob,ah it looks like it from your code,8/31/16,1
lossyrob,I suspect some weird mutability issue,8/31/16,1
lossyrob,what happens if you change the vals of the object `reproj` to defs,8/31/16,1
lossyrob,"@owcm that border issue only happens with hillshade, and not aspect? It feels like there_Ñés some nodata issue going on. What_Ñés the color ramp your using, can you paste that code?",8/31/16,1
wsf1990,I have a mulitiband tiff. and I want to ingest it to accumulo.Can the tile in the accumulo also be MultiBandTile?,8/31/16,1
pomadchin,"@wsf1990 sure, type called MultibandTile",8/31/16,1
pomadchin,"Hi everybody, small announcement related to GeoDocker environment; We are not going to maintain [old](https://github.com/geotrellis/geodocker-cluster) repo anymore, the new GeoDocker repository is available [here](https://github.com/geodocker/geodocker); all questions are appreciated in our GeoTrellis [gitter channel](https://gitter.im/geotrellis/geotrellis/)",8/31/16,1
rfecher,"thanks @pomadchin, awesome work!  quick question for my own point of reference, when you did `geodocker-accumulo-geowave` were the rpms insufficient in some ways which is why you had to go through the harder work of building everything from source?  Just want to understand if there are things we should be doing to improve our packaging/distribution. thanks again!",8/31/16,1
pomadchin,"@rfecher This question can be answered deeper by @jamesmcclain but main reasons i believe were realted to bugs, and neccesity to follow geowave master branch ",8/31/16,1
pomadchin,I'll ping him to leave feedback ;),8/31/16,1
rfecher,"thanks @pomadchin - yeah, I saw there was a specific commit hash being used from snapshot...with every commit to master we build [dev rpms](http://ngageoint.github.io/geowave/packages.html) (separate from our releases repo).  anyways just curious",8/31/16,1
pomadchin,"@rfecher no worries, think the reason just in an old rpm package :D ",8/31/16,1
jamesmcclain,,8/31/16,1
jamesmcclain,,8/31/16,1
jamesmcclain,wrong channel!,8/31/16,1
pomadchin,I believe kind of a correct channel :D ,8/31/16,1
jamesmcclain,:smile:,8/31/16,1
jamesmcclain,"Hello.  I think the main reason for building form source was (as Grisha mentioned) wanting to be as up-to-date as possible, and the fact that I was unaware that these between-release RPMs existed!  If RPMs for the server-side components are generated for every hash, then I don't think there is a good reason for us to build from source.",8/31/16,1
owcm,"@lossyrob Created a Gist with my render code and the ColorRamp I am using.  https://gist.github.com/owcm/bd56380e9a4a24762bde0ab7abfa0dea    If I change the first entry in my color ramp to transparent RGBA(0xF4,0xFB,0xF6,0x00) then the borders go array but then I get transparent pixels within my hillshade product result which is indicating nodata I guess.    I use the same coloramp on the raw elevation data I ingested I get no borders.   So it seems that during my hillshade operation I am introducing nodata values.",8/31/16,1
owcm,Is there away to take two TileLayerRDDs  and combine them into a MultibandTileLayerRDD?,8/31/16,1
pomadchin,"@owcm requires a bit of client code: ```scalaval rdd1: TileLayerRDD[SpatialKey] = ???val rdd2: TileLayerRDD[SpatialKey] = ???val rdd12: RDD[(SpatialKey, MultibandTile)] with Metadata[TileLayerMetadata[SpatialKey]] =   rdd1.withContext { _.union(rdd2).groupByKey.map { case (key, seq) =>    key -> MultibandTile(seq)  } }// or using joinval rdd12j: RDD[(SpatialKey, MultibandTile)] with Metadata[TileLayerMetadata[SpatialKey]] =    rdd1.withContext { _.join(rdd2).map { case (key: SpatialKey, tup: (Tile, Tile)) =>      key -> MultibandTile(tup._1, tup._2)    } }```",8/31/16,1
kevinmhinson,"@lossyrob changing Transform from val to def seems to fix the test case, and looks promising on real data...  thanks for the help!  I never would have guessed that would make a difference.",8/31/16,1
lossyrob,"@kevinmhinson there_Ñés weird mutability issues with proj4j, also in JTS. One of the reasons scala and FP pushes immutability is for these reasons. I_Ñéve just been burned by similar issues before...",8/31/16,1
lossyrob,"It does really bug me to know what the actual problem is, perhaps we should write an issue with that? Do you have the text code of the screenshot you posted so I can make an issue (or could you make an issue on out github)?",8/31/16,1
kevinmhinson,"Yeah, either works...  I'm happy to open an issue later today.",8/31/16,1
lossyrob,"that would be greath, thank you!",8/31/16,1
kevinmhinson,no problem.  Thank you for the help!,8/31/16,1
lossyrob,"@owcm it just seems like those values are, instead of some declared NoData, to be 0 or some small number. So it_Ñés taking on the value of the lowest color ramp value instead of being transparent. It is quite strange that this would only be happening in hillshade. The clue there is that Hillshade uniquely (as compared to aspect) returns a Short typed tile, where aspect returns a Double tile. ",8/31/16,1
lossyrob,This could very well be a bug that resulted when we changed around how we handled nodata in the CellType architecture,8/31/16,1
lossyrob,Would adding a `.map { z => if(z == 0) NODATA else z }` on the tile create the desired tiles?,8/31/16,1
owcm,"@lossyrob Thanks, I am giving it a try",8/31/16,1
rfecher,"woops @jamesmcclain, didn't catch your response above earlier...yeah just to confirm rpm's should be generated for every commit hash on master and stored in our dev repo to include geoserver, commandline tools, and server-side components.  I initially just wanted to see if there was something we had overlooked that made using rpms insufficient for your needs so I'm not criticizing what you had done with building it from source, it certainly took more effort on your part to go that route.  If it becomes difficult to maintain in the future we might want to look at using the rpms instead - one intended benefit of it was one central managed way for deployment, rather than having to roll your own from source and deal with changes.",8/31/16,1
jamesmcclain,"@rfecher  Okay, that makes perfect sense.  Thanks!",8/31/16,1
owcm,@lossyrob I loose the borders but it also seems to be interpreting a lot of other cells as no data.   Now looking at this file using gdalinfo there is a no   NODATA value defined.   So i don't know if that is causing a problem as well.  I was going to to try to add a NODATA value.,8/31/16,1
lossyrob,yeah I was afraid of that. Adding a NODATA might do the trick,8/31/16,1
owcm,"@pomadchin  Thanks that worked.  Now if I wanted to write this to a Avro file using FileLayerWriter, how do I tell the writer I am writing a geotrellis.raster.ArrayMultibandTile.   It appears by default to write the data as geotrellis.raster.DoubleArrayTile.",8/31/16,1
lossyrob,"The avro codec should come in as an implicit context bound on the type parameter `V`, from the write call. What is the `write` call look like on the FileLayerWriter?",8/31/16,1
owcm,"```val writer = FileLayerWriter(attributeStore)      val layoutScheme = ZoomedLayoutScheme(WebMercator, tileSize = 256)      val ccmLayerName = ""ftstoryccmmulti""      import spray.json.DefaultJsonProtocol._      import geotrellis.raster.io._      // Pyramiding up the zoom levels, write our tiles out to the local file system.      Pyramid.upLevels( rddCombined, layoutScheme, maxZoom) { (rdd, z) =>        val layerId = LayerId(ccmLayerName, z)        // If the layer exists already, delete it out before writing        if(attributeStore.layerExists(layerId)) {          new FileLayerManager(attributeStore).delete(layerId)        }        writer.write( layerId, rdd, ZCurveKeyIndexMethod )      }",8/31/16,1
lossyrob,"if the `rdd` type is something like `RDD[(SpatialKey, MultibandTile)] with Metadata[TileLayerMetadata[SpatailKey]]`, it should do the right thing",8/31/16,1
owcm,"Yea this is how I combine the tiles ```  val rddCombined: RDD[(SpatialKey, MultibandTile)] with Metadata[TileLayerMetadata[SpatialKey]] =        finalResultRDD.withContext { _.join(tileSpeedRDD).map { case (key: SpatialKey, tup: (Tile, Tile)) =>          key -> MultibandTile(tup._1, tup._2)        } } ```",8/31/16,1
owcm,On the other end I set up my reader as:,8/31/16,1
owcm,```,8/31/16,1
owcm,"``val fileValueReader = FileValueReader(catalogPath)  val fileAS = FileAttributeStore(catalogPath)  def reader(layerId: LayerId) = fileValueReader.reader[SpatialKey, MultibandTile] (layerId)```",8/31/16,1
lossyrob,where are you getting the exception that indicates it_Ñés writing a DoubleArrayTile?,8/31/16,1
owcm,Yes,8/31/16,1
owcm,Actually on reading getting:,8/31/16,1
owcm,"Found geotrellis.raster.DoubleArrayTile, expecting geotrellis.raster.ArrayMultibandTile, missing required field bands",8/31/16,1
lossyrob,`rdd` => `rddCombined` ?,8/31/16,1
owcm,Sorry not sure I understand,8/31/16,1
lossyrob,In your write,8/31/16,1
lossyrob,you are writing `rdd`,8/31/16,1
lossyrob,seems like you would want to be writing `rddCombined`,8/31/16,1
lossyrob,in order to make it a multiband,8/31/16,1
owcm,"yes, that is correct thats what I want to be writing",8/31/16,1
owcm,@lossyrob @pomadchin Thanks for your help,8/31/16,1
wsf1990,I have ingest the multiband tiff to accumulo.and then how can I got the band count by gt? dose attributeStore can do this?,9/1/16,1
pomadchin,"@wsf1990 by default no, but you can do the following: ```scala(mt: MultibandTile).bandCount```",9/1/16,1
pomadchin,"if you want to store this information in accumulo, you can use attribute store for these purposes: ```scalaattributeStore.write(layerId, _ÑÒbandCount_Ñù, bandCount)```",9/1/16,1
wsf1990,@pomadchin Got it. Thanks!,9/1/16,1
uuuuq,"hiif i have 3 zookeeper instance 10.0.0.4-10.0.0.6how to define  `ZOOKEEPERS` in geodocker-zookeeper?many thx",9/5/16,1
pomadchin,in geodocker zookeeper?,9/5/16,1
pomadchin,mb in accumulo containers?,9/5/16,1
pomadchin,"try to define it as a list: ```10.0.0.4,10.0.0.5,10.0.0.6```",9/5/16,1
uuuuq,"@pomadchin how to start a geodocker cluster? i edit entrypoint.sh, add environment variables like `HADOOP_MASTER_ADDRESS`, `SPARK_MASTER`, then`docker-compse up -d` in each node , but it seems they are working locally, .",9/6/16,1
uuuuq,or should i run command like `docker-compose run -e HADOOP_MASTER_ADDRESS=10.0.0.4 spark-master`?,9/6/16,1
uuuuq,"okay, i figured that. thanks a lot",9/6/16,1
uuuuq,well...,9/6/16,1
uuuuq,"got `accumulo-tserver_1  | Ncat: Could not resolve hostname ""10.0.0.4,10.0.0.5,10.0.0.6"": Name or service not known. QUITTING.`",9/6/16,1
wsf1990,"Dose there some different between `reader.read[SpatialKey, Tile, TileLayerMetadata[SpatialKey]](layer, new LayerQuery[SpatialKey, TileLayerMetadata[SpatialKey]].where(Intersects(bounds))` and `reader.query[SpatialKey, Tile, TileLayerMetadata[SpatialKey]](layerId).where(Intersects(bounds)).result` and `reader.read[SpatialKey, Tile, TileLayerMetadata[SpatialKey]](layerId)`.",9/7/16,1
wsf1990,the reader is FilteringLayerReader[LayerId],9/7/16,1
pomadchin,"i can try to describe you in the following example:```scalaval rdd: RDD[(K, V)] with Metadata[M] = reader.read[K, V, M](layerId)val rdd2: BoundLayerQuery[K, M, RDD[(K, V)] with Metadata[M]] = reader.query[K, V, M](layerId).where(Intersects(bounds))val rdd3: RDD[(K, V)] with Metadata[M] = rdd2.result```",9/7/16,1
pomadchin,"so the idea that `read` reads the entire layer, `query` makes a query builder, query `result` you can get using `query.result`",9/7/16,1
pomadchin,"you used a bit different functions usages, i just showed a more carefull api :)",9/7/16,1
pomadchin,`.query` is a syntax sugar to passing `LayerQuery` into `read` function overload,9/7/16,1
wsf1990,Thanks!So `read` reads all layer data to mem? ,9/7/16,1
wsf1990,and can `Intersects(polygon)`? ,9/7/16,1
pomadchin,"@wsf1990 `read` without passing `LayerQuery` as an argument into it would read the whole layer; if you would pass `LayerQuery`, it would load into mem tiles which correspond your query conditions",9/7/16,1
wsf1990,use `query` or  `LayerQuery ` will more better? and faster?,9/7/16,1
pomadchin,"the same, `query` is ~ a syntax sugar; i think using `query` just more convinient ",9/7/16,1
wsf1990,I use `read` to read whole rdd and use `mask` to Intersects what I need before,9/7/16,1
wsf1990,today I found query and try to use it.,9/7/16,1
wsf1990,but when I use `Intersects(polygon)` it show me some error,9/7/16,1
pomadchin,o: what error?,9/7/16,1
wsf1990,"```Error:(43, 85) Unable to filter geotrellis.spark.SpatialKey by geotrellis.spark.io.Intersects.type given geotrellis.spark.TileLayerMetadata[geotrellis.spark.SpatialKey], Please provide LayerFilter[geotrellis.spark.SpatialKey, geotrellis.spark.io.Intersects.type, geotrellis.vector.Polygon, geotrellis.spark.TileLayerMetadata[geotrellis.spark.SpatialKey]]        reader.query[SpatialKey, Tile, TileLayerMetadata[SpatialKey]](layerId).where(Intersects(polygon)).result ```",9/7/16,1
pomadchin,what type has `polygon` ?,9/7/16,1
wsf1990,Polygon,9/7/16,1
wsf1990,geotrellis.vector.Polygon,9/7/16,1
wsf1990,I found there is an implicit fun `forPolygon` in Intersects,9/7/16,1
pomadchin,"ye, it should be MultiPolygon",9/7/16,1
pomadchin,"just wrap it, that should work `MultiPolygon(geom)`",9/7/16,1
wsf1990,"thanks,I will try.dose it seem to calc the polygon's extent and use Intersects(extent)?",9/7/16,1
pomadchin,"basing on polygon it computes extent, and after that keys range which would be neccesary to query tiles ",9/7/16,1
wsf1990,Got it.thanks!,9/7/16,1
lossyrob,"we should def have an `Intersects` for Polygon, suprised we missed taht",9/7/16,1
lossyrob,https://github.com/geotrellis/geotrellis/issues/1623,9/7/16,1
wsf1990,"@lossyrob Got it. if the polygon and the data have different crs, dose it need change the polygon's crs by myself?",9/7/16,1
lossyrob,you_Ñéd need to reproject the polygon,9/7/16,1
lossyrob,"`polygon.reproject(srcCRS, targetCRS)`",9/7/16,1
wsf1990,got it.thanks!,9/7/16,1
wsf1990,@lossyrob Excuse me.I dont understand how and when  about this `forX` implicit method be called.Can you explain it?,9/7/16,1
lossyrob,sorry what `forX` call do you mean?,9/7/16,1
wsf1990,means like `forPolygon` in `Intersects`,9/8/16,1
lossyrob,that type structure is pretty complicated,9/8/16,1
lossyrob,but basically the query method looks for a LayerFilter that satisfies some type constraints,9/8/16,1
lossyrob,the `forPolygon` defines an implicit layer filter for,9/8/16,1
lossyrob,"`[K, Intersects.type, MultiPolygon, M]`",9/8/16,1
lossyrob,"where K and M have the corresponding context bounds, `K: SpatialComponent: Boundable` and `M: GetComponent[?, LayoutDefinition]`",9/8/16,1
lossyrob,"so by defining a `implicit def forPolygon[K: SpatialComponent: Boundable, M: GetComponent[?, LayoutDefinition]]`",9/8/16,1
lossyrob,"that returns a `LayerFilter[K, Intersects.type, MultiPolygon, M]`",9/8/16,1
lossyrob,https://github.com/geotrellis/geotrellis/blob/master/spark/src/main/scala/geotrellis/spark/io/LayerQuery.scala#L31,9/8/16,1
lossyrob,"this `where` call can find the `implicit filter: LayerFilter[K, F, T, M]` that satisfies all the type constraints",9/8/16,1
lossyrob,So when we pass in an `Intersects(multiPolygon)`,9/8/16,1
lossyrob,to a query that has K = SpatialKey and M = TileLayerMetadata,9/8/16,1
lossyrob,"it can find a `LayerFilter[SpatialKey, LayerFilter[SpatialKey, Intersects.type, MultiPolygon, TileLayerMetadata], MultiPolygon, TileLayerMetadata]`",9/8/16,1
wsf1990,Maybe understand. Thanks!,9/8/16,1
lossyrob,no problem! that_Ñés probably the most complex implicit resolution that we have :),9/8/16,1
wsf1990,"Sure,may be hard to me.But I had studied more from your code",9/8/16,1
wsf1990,"When I create an sc by `SparkUtils.createSparkContext` to call Etl by program.It always told me `Address already in use`,the detail is",9/8/16,1
wsf1990,"```WARN AbstractLifeCycle: FAILED org.spark-project.jetty.server.Server@1de96bc7: java.net.BindException: Address already in usejava.net.BindException: Address already in use```",9/8/16,1
pomadchin,How do you start your app? ,9/8/16,1
pomadchin,"Btw, looks like ports like 8080/4040/4041 are took by some other application ",9/8/16,1
pomadchin,Try to kill them,9/8/16,1
wsf1990,I start my app by spark.I think my app had used 4040.so when create new sc.the address in use.,9/8/16,1
zakipatel,Has anyone successfully used Geotrellis on Databricks ? ,9/9/16,1
lossyrob,"I have not. I don_Ñét see a reason why you could not, though.",9/9/16,1
owcm,"I have a small Geotrellis Tile Service that reads Avro layer Records using HadoopValueReader Geotrellis infrastructure.   The service is configured to run on a AWS EC2  Instance to access HDFS data on a EMR cluster.   If I build this service and run it as a jar on my EC2 instance  that has all the spark libraries installed instance  and kick of using ""spark-submit surgetileserver-assembly-0.1.0.jar""  everything works like a charm.   I get the tiles back and and everything works nicely.     I try putting the same code into a Docker container.   It starts up and registers the HadoopValueReader and Attribute store.   I can read meta data for the layers and test that the layer exists but when reading the actual image data I keep getting exceptions ""No tile: Tile with key SpatialKey(9463,12765) not found for layer Layer(name = ""a08bbc45-b2a4-4d1a-8b47-88037e67cfd5-hs"", zoom = 15)"".  I know the tile exists in the Avro store.    I am wondering if this is an issue with ranges of HDFS ports not being opened to Docker.   Wondered if anyone else had tried a similar  setup using Docker and had issues getting the actual tile content back from the Avrostore using Docker. Thanks in advance.",9/13/16,1
pomadchin,"@owcm can you post there if possible how do you run images, what images, and so on",9/13/16,1
pomadchin,\+ how do you start your server?As a separate node or via spark-submit ?,9/13/16,1
owcm,@pomadchin Would posting the whole project help?,9/13/16,1
owcm,Sharing the Git Project,9/13/16,1
pomadchin,@owcm that may help,9/13/16,1
pomadchin,\+ strongly recommend you to start conatiners with falg `--net=host` this may help you to avoid prot forwarding problems and works faster,9/13/16,1
owcm,"I am trying to write out a MultibandTile Avro record using HadoopLayerWriter. Build my Multiband tile as follows ``` val rddCombined: RDD[(SpatialKey, MultibandTile)] with Metadata[TileLayerMetadata[SpatialKey]] =            finalResultRDD.withContext { _.join(elevRDD).map { case (key: SpatialKey, tup: (Tile, Tile)) =>              key -> MultibandTile(tup._1, tup._2)            } }```",9/14/16,1
owcm,"I get stack error  ``geotrellis.spark.io.package$LayerWriteError: Failed to write Layer(name = ""2fe2847b-5f62-42b8-a538-9b789d307144-ccmmultlayer"", zoom = 18)	at geotrellis.spark.io.hadoop.HadoopLayerWriter._write(HadoopLayerWriter.scala:46)	at geotrellis.spark.io.hadoop.HadoopLayerWriter._write(HadoopLayerWriter.scala:20)	at geotrellis.spark.io.LayerWriter$class.write(LayerWriter.scala:43)	at geotrellis.spark.io.hadoop.HadoopLayerWriter.write(HadoopLayerWriter.scala:20)	at surgesparkhandler.SurgePipelineCommonOps$$anonfun$20.apply(SurgePipelineCommonOps.scala:975)	at surgesparkhandler.SurgePipelineCommonOps$$anonfun$20.apply(SurgePipelineCommonOps.scala:966)	at geotrellis.spark.pyramid.Pyramid$.runLevel$1(Pyramid.scala:180)	at geotrellis.spark.pyramid.Pyramid$.upLevels(Pyramid.scala:188)	at geotrellis.spark.pyramid.Pyramid$.upLevels(Pyramid.scala:211)	at geotrellis.spark.pyramid.Pyramid$.upLevels(Pyramid.scala:221)	at surgesparkhandler.SurgePipelineCommonOps$.writeAvroLayerMulti(SurgePipelineCommonOps.scala:966)	at surgesparkhandler.SurgePipeLineGroupOps$.handleCCMGeneration(SurgePipeLineGroupOps.scala:213)	at surgesparkhandler.SurgePipelines$.testpoint(SurgePipelines.scala:1087)	at surgesparkhandler.SurgePipelines$.Process(SurgePipelines.scala:86)	at surgesparkhandler.RequestHandler$.run(RequestHandler.scala:61)	at surgesparkhandler.RequestHandler$.main(RequestHandler.scala:41)	at surgesparkhandler.RequestHandler.main(RequestHandler.scala)	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)	at java.lang.reflect.Method.invoke(Method.java:606)	at org.apache.spark.deploy.yarn.ApplicationMaster$$anon$2.run(ApplicationMaster.scala:525)``",9/14/16,1
lossyrob,does it have a inner exception,9/14/16,1
lossyrob,"_ÑÒcause by_Ñ_""",9/14/16,1
lossyrob,https://github.com/geotrellis/geotrellis/blob/master/spark/src/main/scala/geotrellis/spark/io/hadoop/HadoopLayerWriter.scala#L51,9/14/16,1
lossyrob,"this is where it points to in code, so we_Ñére really interested in the `initCause`",9/14/16,1
owcm,"Yea, how the heck do you get code blocks to show correctly, 3 ticks or 2",9/14/16,1
lossyrob,3,9/14/16,1
lossyrob,3 for the whole block,9/14/16,1
owcm,"```Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 13 in stage 7.0 failed 4 times, most recent failure: Lost task 13.3 in stage 7.0 (TID 582, ip-10-55-4-163.ec2.internal): java.lang.AssertionError: assertion failed: Band 1 cell type does not match, float32 != float64	at geotrellis.raster.ArrayMultibandTile.<init>(ArrayMultibandTile.scala:86)	at geotrellis.raster.ArrayMultibandTile$.apply(ArrayMultibandTile.scala:32)	at geotrellis.raster.MultibandTile$.apply(MultibandTile.scala:21)	at surgesparkhandler.SurgePipeLineGroupOps$$anonfun$3$$anonfun$apply$3.apply(SurgePipeLineGroupOps.scala:199)	at surgesparkhandler.SurgePipeLineGroupOps$$anonfun$3$$anonfun$apply$3.apply(SurgePipeLineGroupOps.scala:198)	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)	at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1555)	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1125)	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1125)	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1850)	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1850)	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)	at org.apache.spark.scheduler.Task.run(Task.scala:88)	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)	at java.lang.Thread.run(Thread.java:745)```",9/14/16,1
lossyrob,"```like this:```line1,line2,_Ñ_``````",9/14/16,1
lossyrob,hm,9/14/16,1
lossyrob,doesn_Ñét like nested back ticks,9/14/16,1
lossyrob,so there you go,9/14/16,1
lossyrob,the assertion error,9/14/16,1
owcm,right,9/14/16,1
owcm,And I check the celltypes of my RDDs and they are all owe showing float32,9/14/16,1
owcm,they all show float32,9/14/16,1
lossyrob,your RDD seems to be lying then,9/14/16,1
lossyrob,was there anywhere where you change the cell type on the tiles but don_Ñét change it on the layer metadata,9/14/16,1
lossyrob,in the ingest,9/14/16,1
owcm,"Reading the code seems to indicate that things try to match the first band, which in this case must be float64",9/14/16,1
lossyrob,because that second RDD acutally has tiles of Double,9/14/16,1
lossyrob,oh. yeah then that ordering in the exception message is kind of confusing ha,9/14/16,1
lossyrob,yeah you_Ñére totally right,9/14/16,1
lossyrob,so the first rdd is actually Double,9/14/16,1
owcm,I run this on everything I ingest sc.hadoopGeoTiffRDD(fsPath).mapValues { tile => tile.convert(FloatConstantNoDataCellType) },9/14/16,1
lossyrob,yeah,9/14/16,1
lossyrob,that_Ñés probably my fault,9/14/16,1
lossyrob,oh wait,9/14/16,1
owcm,lol,9/14/16,1
lossyrob,"actually if you_Ñére doing that from the source files, then it should work",9/14/16,1
lossyrob,how are you creating the TileLayerMetadata for the layers?,9/14/16,1
owcm,Not sure there,9/14/16,1
owcm,Looking,9/14/16,1
owcm,"I am loading the data from the Avro files I created```val lcRDD: RDD[(SpatialKey, Tile)] with Metadata[TileLayerMetadata[SpatialKey]] =            reader.query[SpatialKey, Tile, TileLayerMetadata[SpatialKey]](lcLayerToUse).result```",9/14/16,1
lossyrob,I mean on ingest,9/14/16,1
lossyrob,"you go from a RDD[ProjectedExtent, Tile] => TileLayerRDD[SpatialKey, Tile, TileLayerMetadata[SpatialKey]] at some point in the ingest process",9/14/16,1
lossyrob,and that TIleLayerMetadata holds the erronious float64 type,9/14/16,1
lossyrob,or something.,9/14/16,1
lossyrob,something where on ingest the cellType that the metadata is holding does not match the actual tile types,9/14/16,1
owcm,"I use this utlility:```def tile(rdd: RDD[(ProjectedExtent, Tile)], tileSize: Int, method: ResampleMethod, partitioner: Partitioner): (Int, TileLayerRDD[SpatialKey]) = {    val (_, md) = TileLayerMetadata.fromRdd(rdd, FloatingLayoutScheme(tileSize))    val tilerOptions = Tiler.Options(method, Some(partitioner))    val tiled = ContextRDD(rdd.tileToLayout[SpatialKey](md, tilerOptions), md)    tiled.reproject(WebMercator, ZoomedLayoutScheme(WebMercator, 256), method)  }```",9/14/16,1
owcm,Is that what you are talking about,9/14/16,1
lossyrob,yeah. so that `fromRdd` call is saying that the cellType is one thing. And at some point it turns out that_Ñés not true. Just a matter of figuring out how the cell type of the tiles could be different from the metadata created there,9/14/16,1
owcm,hmm,9/14/16,1
owcm,@lossyrob Yep one of my operations (calculations) was modifying the tiles type but not the metadata for the RDD.  Hard lesson learned,9/14/16,1
owcm,@lossyrob and thanks for your help,9/14/16,1
lossyrob,no problem,9/14/16,1
owcm,@lossyrob After doing a little more testing I notice one curious result.  I had a elevation RDD layer that was read from Avro.  Tile and RDD celltype was Float32.  I run a slope on the elevation RDD and the resulting slope RDD has a RDD celltype of float32 and Tile celltype values of float64. Should this be expected?,9/15/16,1
lossyrob,"no, that_Ñés a bug",9/15/16,1
lossyrob,"well, its a weird thing. slope does return double tiles.",9/15/16,1
owcm,I am running with 0.10.2,9/15/16,1
owcm,Well tiles are right but should RDD metadata represent the same value.,9/15/16,1
lossyrob,yes,9/15/16,1
lossyrob,https://github.com/geotrellis/geotrellis/blob/master/spark/src/main/scala/geotrellis/spark/mapalgebra/focal/FocalTileRDDMethods.scala#L28,9/15/16,1
lossyrob,"here, we are just delegating to the `focalWithCellSize` call",9/15/16,1
lossyrob,which is here https://github.com/geotrellis/geotrellis/blob/master/spark/src/main/scala/geotrellis/spark/mapalgebra/focal/FocalOperation.scala#L47,9/15/16,1
lossyrob,and delegates to this apply https://github.com/geotrellis/geotrellis/blob/master/spark/src/main/scala/geotrellis/spark/mapalgebra/focal/FocalOperation.scala#L34,9/15/16,1
lossyrob,which does a `withContext`,9/15/16,1
lossyrob,"however, the context changes",9/15/16,1
lossyrob,"so really slope should not delegate, should do the work that those two methods do and then set the metadata correctly",9/15/16,1
owcm,ok,9/15/16,1
lossyrob,"so the tiles are actaully telling the truth in your case, and you can_Ñét make a multiband out of the different cell types",9/15/16,1
lossyrob,so you_Ñéll have to convert tiles before you new up a multiband,9/15/16,1
owcm,"Well looks like code, when doing the multiband check, uses the tile celltype value.  I have everything in celltype double at the tile level so it is working for me.  Just was confusing when I was getting the error earlier and looking at the Metadata level value. ",9/15/16,1
lossyrob,could you write up an issue for that? definitely a bug,9/15/16,1
owcm,"Yea, that should be an easy one to writeup",9/15/16,1
lossyrob,thanks!,9/15/16,1
willtemperley,"Hello all,  I've just been having some fun with the Landsat tutorial,  worked out-of-the-box, great stuff.",9/15/16,1
willtemperley,I'm working a lot with OpenStreetMap and I was interested in the section on the roadmap which mentioned detecting areas to update from satellite imagery,9/15/16,1
feihugis,where did you get the landsat tutorial? Is there any materials which can be shared?,9/15/16,1
willtemperley,https://github.com/geotrellis/geotrellis-landsat-tutorial,9/15/16,1
willtemperley,It's pretty much all there,9/15/16,1
pomadchin,if it is interesting we have a wider landsat demo: https://github.com/geotrellis/geotrellis-landsat-emr-demo,9/15/16,1
willtemperley,We're running a cluster onsite so not sure that's suitable for me,9/15/16,1
feihugis,thank you guys,9/15/16,1
willtemperley,I was interested in knowing how the vector-raster work was coming on,9/15/16,1
willtemperley,https://github.com/geotrellis/geotrellis/issues/1565,9/15/16,1
pomadchin,"@willtemperley we have couple of prs related to vector tiles by @fosskers : * https://github.com/geotrellis/geotrellis/pull/1563* https://github.com/geotrellis/geotrellis/pull/1610* https://github.com/geotrellis/geotrellis/pull/1622Not sure about dates that would be merged",9/15/16,1
echeipesh,"It_Ñés moving along the Vector Tiles PR is getting very close to being merged. There is a set of work that related to that is about dealing with fragmented geometries that would get produced from reading vector tiles. Distributed vectorization has just started. We_Ñére not really chasing feature storage right now, instead we_Ñére going for using GeoWave and GeoMesa to store/query features.",9/15/16,1
willtemperley,Thanks @pomadchin  I'll have a look at that,9/15/16,1
willtemperley,"@echeipesh  interesting, thanks.  How about distributed rasterization?  I've been doing this myself (e.g. rasterizing OSM), but I'd like to integrate better with Geotrellis",9/15/16,1
echeipesh,"hasn_Ñét been scheduled yet, but I wouldn_Ñét expect that to be super tricky. Would you be interested in submitting a PR with your work ?",9/15/16,1
willtemperley,Sorry what do you mean by PR?,9/15/16,1
echeipesh,Pull Request against GeoTrellis master :),9/15/16,1
willtemperley,"Ah of course - yes I would be, but it's pure MapReduce right now, so I'd need to figure out how to target the GT data model",9/15/16,1
lossyrob,we can help with that :),9/15/16,1
willtemperley,Great :)  can you think of an example I could start hacking with?  I was thinking it's similar to image ingestion really,9/15/16,1
lossyrob,so funny you guys have the same avatar,9/15/16,1
lossyrob,"so there_Ñés the landsat tutorial, which only works local, but you said you got through that - what questions do you have about that or what things would you like to see next?",9/15/16,1
willtemperley,Yes! Its rare to find someone who knows the Hedgehog in the fog,9/15/16,1
willtemperley,Yes I hacked that one to get it running on the cluster (at least the ingest part),9/15/16,1
willtemperley,Wasn't immediately obvious how to serve the tiles via Spark though,9/15/16,1
echeipesh,You should be able to specify the writer to actually save it to HDFS (or Accumulo if you_Ñére interested in running it),9/15/16,1
willtemperley,Yep did that,9/15/16,1
willtemperley,Or do you mean the rasterization?,9/15/16,1
echeipesh,"No no, I mean what you understood",9/15/16,1
willtemperley,"Alright thanks chaps, I'll have a play around with the landsat tutorial, and see if I can get something sensible done, borobably",9/15/16,1
willtemperley,*probably with some toy geojson examples,9/15/16,1
echeipesh,Sorry had to step away for a sec. So just to be clear you_Ñére reading OSM data somehow into Geometries and you_Ñére rasterising them into GeoTrellis tiles ?,9/15/16,1
lossyrob,"You wouldn_Ñét want to server the tiles _ÑÉout of spark_Ñé though, you_Ñéd want to make transformations of data, and then pull tiles out individually and do rendering/etc on the fly",9/15/16,1
willtemperley,@echeipesh my use case is best explained here: https://github.com/willtemperley/osm-hadoop,9/15/16,1
willtemperley,"What I want to do is get a dynamically updated version of OSM into HBase, replicated with Osmosis",9/15/16,1
willtemperley,Then be able to rasterize this to compare with satellite imagery (or actually forest loss products derived from landsat),9/15/16,1
lossyrob,sounds like a pretty awesome use case,9/15/16,1
lossyrob,what parts of that did you want to use geotrellis for?,9/15/16,1
willtemperley,The raster processing engine,9/15/16,1
lossyrob,"we wrap JTS in scala, and I see you_Ñére using JTS so that_Ñés an easy win if you_Ñére doing geometry stuff in scala",9/15/16,1
lossyrob,"then for rasterization, we have functionality for that",9/15/16,1
lossyrob,if you_Ñére really only looking for per-pixel values that are covered by geometries,9/15/16,1
lossyrob,not any sort of pretty graphics stuff,9/15/16,1
willtemperley,Yes - or per pixel areas not covered by geometries in fact,9/15/16,1
willtemperley,Where there's deforestation but no OSM data,9/15/16,1
lossyrob,"its pretty generic, just iterates over cells covered by geometries",9/15/16,1
lossyrob,"given a specific RasterExtent (combination of Extent, which is a bounding box type in geotrellis, and cols/rows grid definition)",9/15/16,1
willtemperley,Yes so it's all quite doable,9/15/16,1
lossyrob,"so you could initialize a tile that is all positive values, and then rasterize the vector data over that tile to set things to NODATA that are covered",9/15/16,1
willtemperley,That's one way I guess,9/15/16,1
willtemperley,So GeoTrellis gives the rasterization and the option to do Map Algebra at scale ,9/15/16,1
lossyrob,"rasterization would have to happen per-tile if you want to do it at scale, and I don_Ñét think we have that story full fleshed out. but the components are there",9/15/16,1
lossyrob,and yes we provide ways to do map algebra on RDDs of raster data,9/15/16,1
willtemperley,Using MapReduce I've rasterized the global OSM product,9/15/16,1
willtemperley,into tiles,9/15/16,1
lossyrob,nice,9/15/16,1
willtemperley,So I just sent the pixels to a tileId and then prioritized,9/15/16,1
lossyrob,in that codebase?,9/15/16,1
lossyrob,send the pixels...,9/15/16,1
willtemperley,Actually that was per-pixel ,9/15/16,1
lossyrob,"so yu rasterize each geometry, and then those are the pixels you are sending?",9/15/16,1
willtemperley,I think I took out the tiling in the final version as it wasn't necessary at that resolution,9/15/16,1
willtemperley,Yes that's right,9/15/16,1
lossyrob,gotcha,9/15/16,1
willtemperley,It was only 1km though so at 30m res that might get tricky,9/15/16,1
lossyrob,"if you pick your CRS and tile layout prior, you could match each geom to the spatial keys it covers, rasterize against each of those, and then do a big merge at the end",9/15/16,1
lossyrob,"using our TileLayout, LayoutDefinition, MapKeyTransform, SpatialKey types",9/15/16,1
willtemperley,You mean map the tile(s) for each geom then merge?,9/15/16,1
lossyrob,well let_Ñés see,9/15/16,1
lossyrob,"if we start with `RDD[Feature[Geometry, OsmData]]`",9/15/16,1
lossyrob,or some feature type,9/15/16,1
lossyrob,"we could `rdd.flatMap` those to the spatial keys (the tile ids, tile keys) that they intersect",9/15/16,1
lossyrob,and then try to group the geometries by the tile key,9/15/16,1
willtemperley,Ah - actually intersect the geometries with the tile?,9/15/16,1
lossyrob,"```scalardd.flatMap { feature =>  val keys: Seq[SpatialKey] = getCoveredKeys(feature)  keys.map { key => (key, feature) }}.groupByKey() .map { case (key, features) =>  // Now create a tile for that key that is the rasterization of all those features}```",9/15/16,1
willtemperley,"I see, so the tile would get all the geometries that it intersects with",9/15/16,1
willtemperley,That'd be an issue though with big geometries - lots of data movement and unnecessary work,9/15/16,1
lossyrob,"yeah. and then it_Ñés a matter of creating a tile and rasterizing those geoms. and at that point you have a `RDD[(SpatialKey, Tile)]`",9/15/16,1
lossyrob,"true, but you can choose to shuffle geoms or choose to shuffle tiles, tiles are the bigger ones normally",9/15/16,1
lossyrob,you can try to do a reduceByKey instead of the group,9/15/16,1
lossyrob,but yeah that would produce intermediate Tiles,9/15/16,1
willtemperley,I honestly don't know Spark well enough to judge these things,9/15/16,1
willtemperley,I think much more in MapReduce,9/15/16,1
lossyrob,it_Ñés a similiar concern though. the shuffle and trying to avoid big data shuffling around,9/15/16,1
lossyrob,what_Ñés the smaller set of data that you would shuffle than the geoms,9/15/16,1
lossyrob,?,9/15/16,1
willtemperley,"I'd think of intermediate tiles in Spark, e.g. one for each geometry in the tile",9/15/16,1
willtemperley,Pixels don't work well in Spark because there's a lot of object overhead,9/15/16,1
willtemperley,Ok in MR because there's almost none ,9/15/16,1
lossyrob,hm. how is there no object overhead for pixels in MR?,9/15/16,1
willtemperley,I could be wrong but it was my impression that reusing writables meant little object overhead,9/15/16,1
willtemperley,whereas in spark you kind of have to create an object for every value,9/15/16,1
lossyrob,wouldn_Ñét that only be from reading from disk?,9/15/16,1
lossyrob,"I_Ñém honestly not sure how MR shuffles work, or how the serialization happens",9/15/16,1
willtemperley,"I can't explain it either, especially at 11pm after a pizza :) but personally I can't get spark to peform anywhere near MR",9/15/16,1
willtemperley,At least for ETL,9/15/16,1
lossyrob,"interesting. well knowing spark pretty well, I_Ñéd be happy to take a look at some of the spark code to see if I can see anything",9/15/16,1
willtemperley,"That'd be great, I'd love to know if I'm doing something wrong. ",9/15/16,1
Sharathmk99,Hi All,9/18/16,1
Sharathmk99,I need to calculate NDVI using Geotrellis. Please help how to,9/18/16,1
pomadchin,"@Sharathmk99 hi! gt can store _layers_ of tiles in some backend (hdfs / accumulo / s3 / cassandra) you can store single band tiles and multiband tiles, to query them and to make any algebra operation you want on tiles (on the whole layers, or on certain tiles);projects that may be useful for you, to show our api and work with it: * https://github.com/geotrellis/geotrellis-landsat-tutorial // here showed example of how we count ndvi (nir - r)/(nir + r)* https://github.com/geotrellis/geotrellis-landsat-emr-demo // here showed example of how we count ndvi, water, rgb* https://github.com/geotrellis/geotrellis-chatta-demo // weighted overlay example",9/18/16,1
Sharathmk99,@pomadchin thank you.. sure I'll look into those URLs. If any queries I'll ask here..,9/18/16,1
pomadchin,Any questions are appreciated ;),9/18/16,1
Sharathmk99,@pomadchin any system configuration suggestion for spark??,9/18/16,1
pomadchin,:o I believe Amazon m3.xlarge / m3.2xlarge would be enough for a prodlike cluster ; nodes amount >= 3 (slaves) ; but for tests I think it's enough to have one machine with >= 4gbs of ram ,9/18/16,1
dorukozturk,"Hi, in the landsat tutorial example how can I render rgb rather than NDVI? Thanks in advance !!! Best, Doruk",9/18/16,1
Sharathmk99,@pomadchin sure for testing I'll use 2core and 8gb ram machine..,9/18/16,1
pomadchin,"@Sharathmk99 so yep, cluster config is up to you and it depends on ingest / query jobs you would run :0",9/18/16,1
dorukozturk,Just saw this one. https://github.com/geotrellis/geotrellis-landsat-emr-demo/blob/43d029e452d12e51446929fbfb8c9dbbc86b3844/server/src/main/scala/demo/Render.scala#L85 Thanks anyway :D,9/18/16,1
pomadchin,@dorukozturk yep + it depends on the way you store your tiles,9/18/16,1
pomadchin,any questions / problems you would have just write there :D sry my laptop jsut turned off,9/18/16,1
dorukozturk,"Cool thanks for the quick response. Let me shoot another question then :D The tiles, are they generated on the fly? I see bunch of binary files on my system inside folders. Folders have the names of zoom levels. I am a little confused about that process. so if I do a computation how do you generate the actual png files?",9/18/16,1
pomadchin,"on the spot; so how everything happens: 1. you have you raw tiffs2. ingest them and tile into projections you want and tiling scheme you want (for example, zooming layout scheme, web mercator) 3. you can query whatever you wantthe basic Type we operate is Tile (basically it_Ñés a tiled tiff, array of bytes), so you can convert it into whatever you want",9/18/16,1
pomadchin,"and in case of line you pointed, on the fly: you query 3 tiles under x/y/z , merge them, and give the output as Png (everything on the fly)",9/18/16,1
dorukozturk,"Ok, that makes perfect sense. What about higher zoom levels? I see on default you compute level 12. What if I wanna zoom into level 17 or 18? I need to specify the maxZoomLevel then but assuming that is going to take forever. Do you have any approach on that? ",9/18/16,1
pomadchin,mm we calculate max zoom level basing on its resolution in a UTM zone (you can read more about it [there](https://caniban.files.wordpress.com/2011/04/tile-based-geospatial-information-systems.pdf) but i dont rememeber exact page) O: https://github.com/geotrellis/geotrellis/blob/48162b824df222afbd75c6495fa1e4bc00344fd9/spark/src/main/scala/geotrellis/spark/tiling/ZoomedLayoutScheme.scala#L51 ,9/18/16,1
pomadchin,you _can_ resample to a zoom level you want on demand,9/18/16,1
pomadchin,and there would be no _siginificant_ quality loss,9/18/16,1
dorukozturk,@pomadchin I really appreciate your help. I will try to wrap my head around all of those. Thanks...,9/18/16,1
pomadchin,"yep, any questions ;) and the pseudo example of resampling on demand: ```scalaval LayoutLevel(_, layout) = layoutScheme.levelForZoom(projection.worldExtent, zoom)val mapTransform = MapKeyTransform(meta.crs, layout.layoutCols, layout.layoutRows)val targetExtent = mapTransform(x, y)val gb @ GridBounds(nx, ny, _, _) = meta.mapTransform(targetExtent)val sourceExtent = meta.mapTransform(nx, ny)val largerTile = tileReader.reader[SpaceTimeKey, Tile](layerId).read(SpaceTimeKey(nx, ny, dt))largerTile.resample(sourceExtent, RasterExtent(targetExtent, 256, 256)```",9/18/16,1
dorukozturk,Fantastic. Thxx,9/18/16,1
wsf1990,"I use `reader.query[SpatialKey, Tile, TileLayerMetadata[SpatialKey]](layerId).where(Contains(point)).result` to get the tiles which contains the point,but it told me:",9/19/16,1
wsf1990,"```Unable to filter geotrellis.spark.SpatialKey by geotrellis.spark.io.Contains.type given geotrellis.spark.TileLayerMetadata[geotrellis.spark.SpatialKey], Please provide LayerFilter[geotrellis.spark.SpatialKey, geotrellis.spark.io.Contains.type, geotrellis.vector.Point, geotrellis.spark.TileLayerMetadata[geotrellis.spark.SpatialKey]]    reader.query[SpatialKey, Tile, TileLayerMetadata[SpatialKey]](layerId).where(Contains(point)).result```",9/19/16,1
wsf1990,dose it caused by `TileLayerMetadata[SpatialKey]` dont match `M: (? => MapKeyTransform)`?,9/19/16,1
pomadchin,"@wsf1990 hi! Polygon should be Multipolygon, just wrap it; right now we dont have resolution for a Polygon type ",9/19/16,1
wsf1990,yeah«_ÔBut this time I used point,9/19/16,1
wsf1990,Use Contains(point),9/19/16,1
pomadchin,oh i see,9/19/16,1
pomadchin,"._. seems yep, that caused by lack of this type class",9/19/16,1
wsf1990,How to fix this?,9/19/16,1
pomadchin,mmmm,9/19/16,1
pomadchin,create this implicit in function usage scope,9/19/16,1
pomadchin,"```scalaimplicit def tmToMapKeyTransform(tm: TileLayerMetadata[SpatialKey]): MapKeyTransform = tm.mapTransform```",9/19/16,1
pomadchin,think should help,9/19/16,1
wsf1990,"Yeah, It works! I think I must go to study the basic of scala. Thank you very much!",9/19/16,1
pomadchin,;) think we need to create this implicit in io or spark package objects,9/19/16,1
wsf1990,"Sure,that will be very convenient«_Å",9/19/16,1
dorukozturk,"Hi all, I have a simple question, Let's say I have 5 satellite images for some path row for different time steps. How do I ingest those 5 images as spatiotemporal rdds and not lose the time information so that I can do things like taking the mean across the time dimension (Like a reduce in Earth Engine)? My images are preprocessed so I will be getting the time information from the file name. Any help would be appreciated",9/19/16,1
wsf1990,I think you can use `temporal-geotiff` ingest your data,9/20/16,1
dorukozturk,"@wsf1990 Thanks for the recommendation. I found this googling: https://groups.google.com/forum/#!topic/geotrellis-user/6e7I-Ykybes , looks like I have the similar use case. I will give this a shot. ",9/20/16,1
wsf1990,@dorukozturk Welcome,9/20/16,1
schBen,"Hi all!I have an `RDD[ProjectedExtent, Tile] ` which contains my result data (processed single Landsat scene). So I want to save that RDD without further transformations as a Geotiff, on S3. Is there a shortcut?At the moment, I first create a `SinglebandGeotiff ` object and then upload it to S3, but this seems to be expensive because it all happens on the driver.",9/20/16,1
pomadchin,"hi @schBen yes, its possible, but we store tiles accroding to some layout scheme, so in fact here always would be a sort of some transofrmation form raw `RDD[ProjectedExtent, Tile]` representation",9/20/16,1
pomadchin,as we need to reproject and to index tiles,9/20/16,1
schBen,@pomadchin Okay... Is there some example code maybe? :-),9/20/16,1
mheppner,"We have an app that uses [Thunder](http://docs.thunder-project.org/) to analyze images with Spark, but it's performing pretty poorly. We have millions of images residing on an NFS share but we are evaluating using either HDFS or HBase. GeoTrellis seems like it's exactly what we need, but are there any benchmarks, particularly with storing/retrieving tiles from HDFS? What is the most common way people are storing data from GeoTrellis?",9/20/16,1
Sharathmk99,Hi I'm using Drone woth RGB camera to capture images of my field. I use opendronemap to stitch the images. I'll get geotiff image from opendronemap. Now I want to calculate VARI index. Please help to how to do. Formula for VARI is (Green - Red)/(Green + Red - Blue). Please help,9/20/16,1
Sharathmk99,@pomadchin please help,9/20/16,1
Sharathmk99,@lossyrob please help,9/20/16,1
pomadchin,@schBen sec would preapre,9/20/16,1
pomadchin,"@mheppner we dont have real benchmarks, but our main backend is Accumulo; but HBase should has similar to Accumulo perfromance results; HBase and Cassandra (potentially our fastest backend) support is still in beta (though stable, but we dont have benchmarks yet); Ingestion tiles into accumulo is a bit slower than into raw HDFS backend; thought with Accumulo we can update cells, and make faster readsBut yes, Accumulo is the most tested backend",9/20/16,1
mheppner,"@pomadchin thanks, I'll set up a small scale environment to test out GeoTrellis!",9/20/16,1
pomadchin,@mheppner perfect! any questions are appreciated ;),9/20/16,1
pomadchin,@Sharathmk99 how are you planing to store tiles? as a multiband tile layers or singelband?,9/20/16,1
Sharathmk99,@pomadchin I get normal RGB images.. you suggest me how to store..,9/20/16,1
Sharathmk99,"Sorry for basic question, I'm very new so_ªÉ_",9/20/16,1
pomadchin,"so it_Ñés up to you, if you recieve images as rgbs with geo headers in your tiff, then i think that the most convinient way for you to store tiles would be to ingest them as is, as multiband tiles; use ETL to ingest them;",9/20/16,1
mheppner,"@pomadchin I'm sure I'll have additional questions! I'm a government contractor and we're developing this for a top secret environment which won't have access to S3 (well, all of the internet). We may reach out to the core devs or Azavea (the parent company?) at some point.",9/20/16,1
pomadchin,"@mheppner yep, i believe then that Accumulo is one of the best usecases for you (per cell security and so on), or Cassandra (if you are interested in it) // any questions ;)",9/20/16,1
Sharathmk99,@pomadchin any example link for ETL,9/20/16,1
pomadchin,"@Sharathmk99 client code (useless in fact): https://github.com/geotrellis/geotrellis-chatta-demo/blob/master/geotrellis/src/main/scala/geotrellis/chatta/ChattaIngest.scalapreferred way: https://github.com/geotrellis/geotrellis/blob/master/spark-etl/src/main/scala/geotrellis/spark/etl/MultibandIngest.scala (just build spark etl assembly) spark etl explanations: https://github.com/geotrellis/geotrellis/blob/master/docs/spark-etl/spark-etl-intro.md",9/20/16,1
pomadchin,be carefull that that docs link is valid for a master branch,9/20/16,1
pomadchin,"if you are using some other gt version, use git tags",9/20/16,1
pomadchin,btw @Sharathmk99 how are you planing to query your tiles?,9/20/16,1
pomadchin,tile server?,9/20/16,1
Sharathmk99,Yes..,9/20/16,1
pomadchin,perfect,9/20/16,1
pomadchin,"for effective x/y/z queries you can use `ValueReaders` (example `AccumuloValueReader`); let say you have an instance of one, and you store tiles in a temporal layer as multiband tiles: ```scalaval reader = AccumuloValueReader(instance).reader[SpaceTimeKey, MultibandTile](LayerId(name, zoom))val tile = reader.read(SpaceTimeKey(x, y, dt)) // it contains 3 bands// result tile would have double values, we need to convert md type firsttile.convert(DoubleCellType).combineDouble(0, 1, 2) { (r, g, b) =>   (g - r) / (g + r - b) }```",9/20/16,1
Sharathmk99,@pomadchin awesome!! I'll understand the concept first.. I'm reading docs from first.. ,9/20/16,1
Sharathmk99,I need some background to understand so..,9/20/16,1
Sharathmk99,I'll read and come back if any queries,9/20/16,1
pomadchin,"@schBen so to ingest you may use our spark etl project: https://github.com/geotrellis/geotrellis/blob/master/docs/spark-etl/spark-etl-intro.md some basic explanations on how we ingest tiles; and https://github.com/geotrellis/geotrellis/blob/master/docs/spark/spark-intro.md would be usefull as well please let me know if that_Ñés not enough for you ;) \+ take a look at following projects: * https://github.com/geotrellis/geotrellis-landsat-tutorial // here showed example of how we count ndvi (nir - r)/(nir + r)* https://github.com/geotrellis/geotrellis-landsat-emr-demo // here showed example of how we count ndvi, water, rgb* https://github.com/geotrellis/geotrellis-chatta-demo // weighted overlay example",9/20/16,1
Sharathmk99,@pomadchin thanks man,9/20/16,1
wsf1990,"I'm using `Etl.ingest[TemporalProjectedExtent, SpaceTimeKey, Tile](args)` to ingest temporal tiff. And the output.json like this:",9/21/16,1
wsf1990,"```""keyIndexMethod"": {    ""type"": ""zorder""  }```",9/21/16,1
wsf1990,"but when I run this code,it show me ",9/21/16,1
wsf1990,"```Cannot serialize this key index with this KeyIndexJsonFormat: geotrellis.spark.io.index.zcurve.ZSpatialKeyIndex@64d93096```",9/21/16,1
wsf1990,why dose I got this issue?,9/21/16,1
lossyrob,the etl code looks like its using an old version of geotrellis,9/21/16,1
lossyrob,what are the versions in your sbt files for geotrellis? are they different between the etl code and the reading code?,9/21/16,1
wsf1990,1,9/21/16,1
wsf1990,"```object Etl {  val defaultModules = Array(s3.S3Module, hadoop.HadoopModule, file.FileModule, accumulo.AccumuloModule, cassandra.CassandraModule, hbase.HBaseModule)  def ingest[    I: Component[?, ProjectedExtent]: TypeTag: ? => TilerKeyMethods[I, K],    K: SpatialComponent: Boundable: TypeTag,    V <: CellGrid: TypeTag: Stitcher: (? => TileReprojectMethods[V]): (? => CropMethods[V]): (? => TileMergeMethods[V]): (? => TilePrototypeMethods[V])  ](     args: Seq[String], modules: Seq[TypedModule] = Etl.defaultModules   )(implicit sc: SparkContext) = {    implicit def classTagK = ClassTag(typeTag[K].mirror.runtimeClass(typeTag[K].tpe)).asInstanceOf[ClassTag[K]]    implicit def classTagV = ClassTag(typeTag[V].mirror.runtimeClass(typeTag[V].tpe)).asInstanceOf[ClassTag[V]]    EtlConf(args) foreach { conf =>      /* parse command line arguments */      val etl = Etl(conf, modules)      /* load source tiles using input module specified */      val sourceTiles = etl.load[I, V]      /* perform the reprojection and mosaicing step to fit tiles to LayoutScheme specified */      val (zoom, tiled) = etl.tile(sourceTiles)      /* save and optionally pyramid the mosaiced layer */      etl.save[K, V](LayerId(etl.input.name, zoom), tiled)    }  }}```",9/21/16,1
pomadchin,"@wsf1990 you are tyring to ingest temporal layer, but provide config for a spatial index",9/21/16,1
pomadchin,add temporal resolution,9/21/16,1
pomadchin,"```json""keyIndexMethod"": {  ""type"": ""zorder"",   ""temporalResolution"": 86400000}```",9/21/16,1
wsf1990,"ok,I'll try.thanks!",9/21/16,1
wsf1990,Can I use geotrellis to add a head tag like TIFFTAG_DATETIME to a tiff?,9/21/16,1
wsf1990,now I use this method:,9/21/16,1
wsf1990,"```val tiff = SinglebandGeoTiff(""1.tif"")    val headTags: Map[String, String] = tiff.tags.headTags    val map = headTags + (Tags.TIFFTAG_DATETIME -> s""2015:03:25 18:01:04"")    val newtiff = new SinglebandGeoTiff(tiff.tile, tiff.extent, tiff.crs, Tags(map, tiff.tags.bandTags), tiff.options)```dose you have some better method?",9/21/16,1
pomadchin,"nope, but you can do it in a preprocessing step, via gdal",9/21/16,1
wsf1990,got it!,9/21/16,1
wsf1990,thanks,9/21/16,1
dorukozturk,"@lossyrob I have been trying to use Geotrellis. A little challenging but super useful(Python and Javascript Background). I saw Rob has a youtube video processing NEX climate data. From what I understood rcp45 and rcp85 variables are acting like a giant image so that you can reduce or aggregate easily. https://www.youtube.com/watch?v=oVifSo_bbMw#t=10m56s Here is the youtube video and exact time where I see rcp45 and rcp85 variables.  I want to know more about the ingestion part. I know there is a conversion script from netcdf to tiff here --> https://github.com/lossyrob/nex-chunker-worker, again let's say I have 10 tiff files generated from that netcdf files. How do I ingest it with the time component and everything to accumulo? ",9/21/16,1
pomadchin,"@dorukozturk it is a very old video with an old api; you can ingest tiles using our ETL subproject https://github.com/geotrellis/geotrellis/blob/master/docs/spark-etl/spark-etl-intro.mdingest examples can be found in following projects* https://github.com/geotrellis/geotrellis-landsat-emr-demo // temporal* https://github.com/geotrellis/geotrellis-chatta-demo // spatial@wsf1990 already asked a question how to ingest tiles temporal; the fastest way is to setup `TIFFTAG_DATETIME` into your tiffs metadata using gdal, or you can change metadata information of loaded tiffs; setting up temporal tag using gdal: ```bash# run it in a folder with .tifsfor img in *.tif; do  gdal_edit.py -mo TIFFTAG_DATETIME=""2015:12:29 12:00:00"" $imgdone```",9/21/16,1
lossyrob,"@dorukozturk ditto what @pomadchin is saying; that was based on old code, but the way to do it is to do an ETL ingest using spatio-temporal ",9/21/16,1
lossyrob,and that code you mentioned should have set the date tiff tag correctly I believe,9/21/16,1
lossyrob,so it_Ñés a matter of building a simple ETL application that uses something like https://github.com/geotrellis/geotrellis/blob/master/spark-etl/src/main/scala/geotrellis/spark/etl/s3/TemporalGeoTiffS3Input.scala,9/21/16,1
lossyrob,The ETL intro is probably the best doc to work off of,9/21/16,1
lossyrob,"but that currently works with master, not 0.10.x",9/21/16,1
lossyrob,so you would have to pull a bintray version,9/21/16,1
lossyrob,e.g. a hashed snapshot version here https://bintray.com/azavea/geotrellis/geotrellis-spark-etl,9/21/16,1
lossyrob,which you can pull in by adding this resolver https://github.com/geotrellis/geotrellis-landsat-emr-demo/blob/master/build.sbt#L22,9/21/16,1
lossyrob,if you try to build that ETL app up and run into problems let us know,9/21/16,1
lossyrob,"@wsf1990 @pomadchin a right, that JSON represents a spatial Z index. Which we need to index against a spatiotemporal Z. I_Ñém a bit confused about how you were ingesting a SpaceTimeKey rdd with a spatial index though, that index should actually be rejected in the save function...",9/21/16,1
lossyrob,"@pomadchin I guess because it_Ñés all json-configured reflectance based, and i you provide a spatial index for a spatiotemporal RDD, it will throw at the point when it tries to JSON serialize the index? Should we catch that earlier with a better error message?",9/21/16,1
pomadchin,@lossyrob im looking to improve that check; yep definitely we should catch it during json parsing,9/21/16,1
lossyrob,"cool, that makes sense",9/21/16,1
wsf1990,"@lossyrob I think it's caused by loss of temporalResolution. Sure, you can add the check during output json validate.",9/21/16,1
lossyrob,"yeah sorry about misunderstanding the output.json, I thought that was the post-ingested metadata json",9/21/16,1
wsf1990,I think caused by this code,9/21/16,1
wsf1990,"```def getKeyIndexMethod[K] = (((keyIndexMethod.`type`, keyIndexMethod.temporalResolution) match {    case (""rowmajor"", None)    => RowMajorKeyIndexMethod    case (""hilbert"", None)     => HilbertKeyIndexMethod    case (""hilbert"", Some(tr)) => HilbertKeyIndexMethod(tr.toInt)    case (""zorder"", None)      => ZCurveKeyIndexMethod    case (""zorder"", Some(tr))  => ZCurveKeyIndexMethod.byMilliseconds(tr)    case _                     => throw new Exception(""unsupported keyIndexMethod definition"")  }): KeyIndexMethod[_]).asInstanceOf[KeyIndexMethod[K]]```",9/21/16,1
lossyrob,so it looks like it successfully casts it,9/21/16,1
lossyrob,or else you would get an exception on that `asInstanceOf` call,9/21/16,1
lossyrob,which seems strange,9/21/16,1
pomadchin,it casts for some reason o: but cant serialize as we have no proper json format,9/21/16,1
pomadchin,weird,9/21/16,1
wsf1990,when the json dont have tr it will get an `ZCurveKeyIndexMethod` not `ZCurveKeyIndexMethod.byMilliseconds(tr)`,9/21/16,1
pomadchin,stop i know the problem,9/21/16,1
wsf1990,"Sure,I'm sorry for trouble you so many times. I should read your source code carefully.",9/21/16,1
lossyrob,"right. And the ZCurveMethod is a `KeyIndex[SpatialKey]`, so if `K = SpaceTimeKey` then I_Ñém suprised the cast works",9/21/16,1
lossyrob,"no need to apologize, I think that _ÑÒstop_Ñù was more out of excitement of figuring it out :)",9/21/16,1
lossyrob,what_Ñés the problem grisha,9/21/16,1
pomadchin,:D :D sry we had a call / standup,9/21/16,1
pomadchin,"ahahah stop yes, it was about type inference bug, not about your questions @wsf1990 :) we appreciate your questions",9/21/16,1
pomadchin,"so the problem is in this line: ```scala): KeyIndexMethod[_]).asInstanceOf[KeyIndexMethod[K]]```firstly we upcast K to Any; after that making it a K type; think it just looses generic type completely, i think it can be casted to any key type",9/21/16,1
lossyrob,"that_Ñés weird, I would think at run time it would throw on the downcast",9/21/16,1
lossyrob,even when first being upcasted,9/21/16,1
pomadchin,"i think it_Ñés scala compiler hack; but without upcast, it wouldn_Ñét even compile",9/21/16,1
lossyrob,looks like I_Ñém wrong,9/21/16,1
lossyrob,"```scalascala> val l = List(""one"")l: List[String] = List(one)scala> val la = l.asInstanceOf[List[_]]la: List[Any] = List(one)scala> case class Foo(s: String)defined class Fooscala> val ls = la.asInstanceOf[List[Foo]]ls: List[Foo] = List(one)```",9/21/16,1
pomadchin,._. yep,9/21/16,1
pomadchin,we need to improve json validation first; think that totally should be a json step fail,9/21/16,1
dorukozturk,@lossyrob @pomadchin Thanks for the answers guys. Keep up the great work. I will try and let you know :D,9/21/16,1
jbouffard,Hello everyone. I just wanted to announce that GeoTrellis has two new features: Local Stream and Windowed Reading of GeoTiffs,9/23/16,1
jbouffard,"Now, if you're interested in just a certain area of a GeoTiff, you can now supply that regions's extent when loading in a GeoTiff file and only that one section will be loaded.",9/23/16,1
jbouffard,"If before you were unable to load in a GeoTiff locally due to its size, you now can thanks to the new streaming feature.",9/23/16,1
jbouffard,We are currently in the process of expanding this feature to read GeoTiffs off of S3; and it should be released in the coming weeks.,9/23/16,1
lossyrob,:+1: :clap: ,9/23/16,1
aaron-santos,"ooo, are there any easy ways to generate a `RDD[(T, MultibandTile)]` from a large GeoTiff such that it would pair well with Ingest/MultibandIngest?",9/23/16,1
lossyrob,we_Ñére working on that currently,9/23/16,1
lossyrob,once the windowed S3 reader is verified (currently finishing/testing),9/23/16,1
lossyrob,then we make an ingest that does windowed reads out of large geotiffs on S3,9/23/16,1
lossyrob,"and then yeah, not having to pre-tile images is the goal",9/23/16,1
lossyrob,"@aaron-santos if that feature is useful to you, you can help it out by looking at/reviewing/testing code :)",9/23/16,1
lossyrob,The PR that @jbouffard is working off is here https://github.com/geotrellis/geotrellis/pull/1617,9/23/16,1
aaron-santos,nice! I'd love to :),9/23/16,1
lossyrob,"cool! @jbouffard want to give an idea of where things are at, and maybe some good places to look/test?",9/23/16,1
jbouffard,"@lossyrob @aaron-santos Right now, PR is out of date, but I will be updating it in the next few days. I still need to do more testing and code clean up.",9/23/16,1
jbouffard,@aaron-santos Do you have any GeoTiffs on S3 that you could use this feature on once I'm done?,9/23/16,1
jbouffard,I'd appreciate your feedback on the process and if you experience any problems.,9/23/16,1
aaron-santos,"From what it sounds like, local streaming might be what I'd be interested in first? I have sentinel2 tiles that I grab from http://sentinel-s2-l1c.s3-website.eu-central-1.amazonaws.com/ but they are `jp2`s that undergo a conversion to GeoTiffs on disk prior to ingest.",9/23/16,1
lossyrob,where does the local streaming fit into that pipeline?,9/23/16,1
aaron-santos,"I might be mistaken about local streaming, I was making some assumptions just based on the name",9/23/16,1
lossyrob,yeah I think I_Ñém just confused on where it fits_Ñ_the `jp2` -> `tiff` or after that,9/23/16,1
lossyrob,?,9/23/16,1
aaron-santos,I was thinking after that,9/23/16,1
lossyrob,yeah gotcha. so there_Ñés big tiffs that end up out of that,9/23/16,1
lossyrob,and then you want to read windows out of that tiff?,9/23/16,1
aaron-santos,"cool, and as far as I know data from jp2s can be read using windowing fns",9/23/16,1
aaron-santos,yep! that's right,9/23/16,1
lossyrob,yeah. that should work in master now,9/23/16,1
lossyrob,"I think the docs explain it well enough, but you read in with the extent you want, and it_Ñéll only read that window",9/23/16,1
aaron-santos,I'm kind of making another assumption that local streaming will require less RAM at the expense of io/time and that's something I'm willing to do with these huge sentinel2 tiles,9/23/16,1
aaron-santos,nice! I'm looking forward to checking it out :) thx for the info,9/23/16,1
lossyrob,"yeah it_Ñés a cool feature, thanks to jake the snake :snake: ",9/23/16,1
pomadchin,:snake: :snake: :snake: :snake: :snake:,9/23/16,1
jbouffard,@aaron-santos Here is the doc for the Windowed Reading,9/23/16,1
jbouffard,https://github.com/jbouffard/geotrellis/blob/wgt-docs/docs/raster/WindowedGeoTiffReader.md,9/23/16,1
jbouffard,"If I'm understanding correctly, the tiles will already be cropped, thus, you should just have to do this:",9/23/16,1
jbouffard,"```val geoTiff = SinglebandGeoTiff.streaming(""path/to/your/geotiff.tif"")```",9/23/16,1
jbouffard,"Streaming will actually only read in parts of the file as you need them, so it should be lighter on RAM and time.",9/23/16,1
dorukozturk,"Hey everyone, I am trying to create a simple ETL application on local. When I understand the concept I will move to aws. I created the fat jar (spark-etl) with sbt assembly. Everything is great. As far as I understand now I need to create 3 json files. input.json, output.json and backend-profiles. I was reading both the docs and looking at the 2 examples : Landsat emr and chatta. https://github.com/geotrellis/geotrellis-chatta-demo/blob/9f58a2db2829053ccbc796e79bc9aa44832aa8b5/geotrellis/conf/backend-profiles.json My question is on this backend-profiles.json why there are both cassandra and accumulo backends defined? http://bigdata-guide.blogspot.com/2014/01/hbase-versus-cassandra-versus-accumulo.html I am still on the learning phase but as far as I understood they are pretty much doing the same thing with some variety. (Accumulo, hbase amd cassandra).  So my question is how do you decide on selecting between accumulo, hbase and cassandra and why in the chatta example 2 backends were specified?",9/25/16,1
lossyrob,"We give users an option between using a variety of backends, in case they would want to use that particular backend. As far as how to decide on one, that's a bit up to the user. In my opinion, Cassandra is easier to deploy, but we use accumulo more since other projects (geomesa, geowave) in our ecosystem use it. HDFS is a simpler choice, but on the past it has been slower than accumulo; @echeipesh has made some changes though that have made it pretty optimized. Cassandra support is newer to us; but is pretty fast and good for what we've tested for. S3 had been a great backend for us; it's significantly slower but so easy to manage.",9/25/16,1
lossyrob,"This is all you say, there's not a clear set of guidelines on what backend to use in what situation, and it's a bit of work to figure out which would be best for your situation. We would really like to figure it out with you, though. What is your use case?",9/25/16,1
dorukozturk,"I have 2 use cases. 1) We have some Landsat products (called ""Golden Tiles""), which are in hdf format. I wrote a gdal script to convert them into geotiffs. we have 2 objectives. We want to be able to show them on a Web Map and we want to be able to do arithmetic on them. Per geotiff is like 1.2 gigs with 24 bands in them and there are plenty of images. So it is classified as a ""big data"" problem. I used Earth Engine for 2-3 years and pretty good with it but naturally I am not super familiar with the underlying magic. I feel like Geotrellis will satisfy my curiosity on how distributed computations work . 2) We want to do some map algebra and tiling again on gddp data. Again our work requires tiling and we don't like to store pngs (gdal2tiles is not working for us). When I say map algebra it can be basic arithmetic. Also we might want to do some mosaicing with the Golden Tiles or create time series for an area (by taking the mean) and so on.    ",9/25/16,1
dorukozturk,The problem is all my experience is with Python and Javascript. Scala is a new world to me. Same with Spark. So it is taking some time to wrap my head around. ,9/26/16,1
lossyrob,That makes sense. ,9/26/16,1
lossyrob,"So really the choice of backends has to do with how comfortable you are with storing the amount of data you have in a specific backend, which for S3 is one thing. For accumulo is another thing - your talking terabytes of ebs or ephemeral storage (talking aws language)",9/26/16,1
lossyrob,But serving tiles from s3 is more costly than ephemeral disk on an economic instance grabbed through accumulo 's index,9/26/16,1
lossyrob,"So if you want to serve many tiles fast, and the IO cost is important to keep down, then paying for that quick indexed storage makes sense ",9/26/16,1
lossyrob,"Either way, geotrellis is set up so it is quick enough to render on the fly via a tile server ",9/26/16,1
lossyrob,"So you store raw data that you can perform you map algebra on, per tile (with many layers overlapping spatial extent, or spatial extent over time in the case of landsat)",9/26/16,1
lossyrob,"And after you perform your map algebra, with dynamic parameters usually, you paint it and then put it on a leaflet map",9/26/16,1
lossyrob,That's a usual use case anyway ,9/26/16,1
dorukozturk,Yepp,9/26/16,1
lossyrob,"If you want to do time series dynamically, you get into the world of calling spark jobs per request",9/26/16,1
lossyrob,Request,9/26/16,1
lossyrob,Which is actually not clear that spark supports very well.  But we do it.,9/26/16,1
lossyrob,"But if there's a small enough collection of tiles to analyze, it is fast.",9/26/16,1
lossyrob,"You pull back your query area and time range from you layer, and you do the analysis in an RDD. We do that dynamically, buy still working out the limitations of it.",9/26/16,1
lossyrob,As far as speed goes.  And multitenancy.,9/26/16,1
lossyrob,"Scala and spark are a hurdle, and we",9/26/16,1
dorukozturk,For now my concern is understanding GT do some basic things on local. Then launch a cluster provision it and try the stuff there.,9/26/16,1
lossyrob,Are here to help with that. ,9/26/16,1
dorukozturk,Yes you are being super helpful. It is a sunday evening and you are answering my questions :D,9/26/16,1
lossyrob,"Ok. For local, we have a File backend, which is what you should always use for the local case. Just acts like any other backend, but just reading and writing to disk",9/26/16,1
lossyrob,"You can write whole applications using local file backend, and then move it to accumulo or whatever by changing what LayerReader etc you are using ",9/26/16,1
dorukozturk,Thank you very much for the guidance.  I will let you know as there is progress.,9/26/16,1
lossyrob,Ok cool. Let us know if there's any other things we can make Clearer!,9/26/16,1
dorukozturk,Will do. Thanks,9/26/16,1
wsf1990,@lossyrob I'm sorry for trouble you again.today I changed the environment to cdh5.4 but my app can't start.The stacks is ,9/26/16,1
wsf1990,"```Uncaught error from thread [sjzx-akka.actor.default-dispatcher-3] shutting down JVM since 'akka.jvm-exit-on-fatal-error' is enabled for ActorSystem[sjzx]java.lang.AbstractMethodError: spray.can.HttpManager.akka$actor$ActorLogging$_setter_$log_$eq(Lakka/event/LoggingAdapter;)V	at akka.actor.ActorLogging$class.$init$(Actor.scala:335)	at spray.can.HttpManager.<init>(HttpManager.scala:29)	at spray.can.HttpExt$$anonfun$1.apply(Http.scala:153)	at spray.can.HttpExt$$anonfun$1.apply(Http.scala:153)	at akka.actor.TypedCreatorFunctionConsumer.produce(Props.scala:401)	at akka.actor.Props.newActor(Props.scala:339)	at akka.actor.ActorCell.newActor(ActorCell.scala:534)	at akka.actor.ActorCell.create(ActorCell.scala:560)	at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:425)	at akka.actor.ActorCell.systemInvoke(ActorCell.scala:447)	at akka.dispatch.Mailbox.processAllSystemMessages(Mailbox.scala:262)	at akka.dispatch.Mailbox.run(Mailbox.scala:218)	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:386)	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)```",9/26/16,1
wsf1990,and this:,9/26/16,1
wsf1990,"```Exception in thread ""main"" java.lang.NoSuchMethodError: com.google.common.collect.Queues.newArrayDeque()Ljava/util/ArrayDeque;	at com.github.fge.jsonschema.processors.validation.ValidationStack.<init>(ValidationStack.java:70)	at com.github.fge.jsonschema.processors.validation.InstanceValidator.<init>(InstanceValidator.java:87)	at com.github.fge.jsonschema.processors.validation.ValidationProcessor.process(ValidationProcessor.java:54)	at com.github.fge.jsonschema.processors.validation.ValidationProcessor.process(ValidationProcessor.java:34)	at com.github.fge.jsonschema.core.processing.ProcessingResult.of(ProcessingResult.java:79)	at com.github.fge.jsonschema.main.JsonSchema.doValidate(JsonSchema.java:76)	at com.github.fge.jsonschema.main.JsonSchema.validate(JsonSchema.java:109)	at geotrellis.spark.etl.config.EtlConf$.apply(EtlConf.scala:78)	at geotrellis.dataimport.LayerEtl$.ingest(LayerEtl.scala:44)	at geotrellis.dataimport.SinglebandDataIngest$delayedInit$body.apply(SinglebandDataIngest.scala:19)	at scala.Function0$class.apply$mcV$sp(Function0.scala:40)	at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:12)	at scala.App$$anonfun$main$1.apply(App.scala:71)	at scala.App$$anonfun$main$1.apply(App.scala:71)	at scala.collection.immutable.List.foreach(List.scala:318)	at scala.collection.generic.TraversableForwarder$class.foreach(TraversableForwarder.scala:32)	at scala.App$class.main(App.scala:71)	at geotrellis.dataimport.SinglebandDataIngest$.main(SinglebandDataIngest.scala:15)	at geotrellis.dataimport.SinglebandDataIngest.main(SinglebandDataIngest.scala)	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)	at java.lang.reflect.Method.invoke(Method.java:606)	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:731)	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181)	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206)	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121)	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)```",9/26/16,1
dorukozturk,"Hi, I am getting this error```sh                                                                                                                                                                                                                                                                                                                                           Exception in thread ""main"" java.lang.NoSuchMethodError: scala.reflect.api.JavaUniverse.runtimeMirror(Ljava/lang/ClassLoader;)Lscala/reflect/api/JavaUniverse$JavaMirror;	at geotrellis.spark.etl.s3.S3Module$.<init>(S3Module.scala:6)	at geotrellis.spark.etl.s3.S3Module$.<clinit>(S3Module.scala)	at geotrellis.spark.etl.Etl$.<init>(Etl.scala:26)	at geotrellis.spark.etl.Etl$.<clinit>(Etl.scala)	at geotrellis.spark.etl.MultibandIngest$.main(MultibandIngest.scala:13)	at geotrellis.spark.etl.MultibandIngest.main(MultibandIngest.scala)	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)	at java.lang.reflect.Method.invoke(Method.java:498)	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:731)	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181)	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206)	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121)	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)```with this command:```shspark-submit --class geotrellis.spark.etl.MultibandIngest --master local[*] --driver-memory 2G spark-etl/target/scala-2.11/geotrellis-spark-etl-assembly-1.0.0-SNAPSHOT.jar --backend-profiles ""file://conf/backend-profiles.json"" --input ""file://conf/input.json"" --output ""file://conf/output.json""```",9/26/16,1
wsf1990,Dose it caused by cdh?,9/26/16,1
wsf1990,@dorukozturk Dose you use cdh too?,9/26/16,1
dorukozturk,"@wsf1990 no I am not",9/26/16,1
pomadchin,@wsf1990 hi! think different java versions is the reason,9/26/16,1
pomadchin,like you built against java 8 but your cluster and your spark use java 7,9/26/16,1
pomadchin,moreover; if you are using master branch; that means spark 2 usage and scala 2.11 dependence; be sure that you run on a spark2 machine with scala 2.11; and double check java versions as well ^^,9/26/16,1
pomadchin,@dorukozturk what gt version are you using? (sry not caught that from the context),9/26/16,1
pomadchin,@dorukozturk + possible problems to what @wsf1990 has,9/26/16,1
wsf1990,I use gt 1.0 and I checked the cdh's spark version is 1.6 and the jdk maybe 1.6,9/26/16,1
pomadchin,"btw, we are planning to deprecate Java 7; how do you guyz feel about it? O:",9/26/16,1
pomadchin,@wsf1990 heh; and the jar you are planing to run; what_Ñés the version?,9/26/16,1
pomadchin,I am just not sure that master branch should work properly on Spark version < 2,9/26/16,1
wsf1990,"I try to use jdk 1.6 and jdk 1.8 build the jar,But have the same error",9/26/16,1
pomadchin,yes,9/26/16,1
pomadchin,try to build your jar with java 6,9/26/16,1
wsf1990,I checked the jdk is 1.7.0_67 and I use it build my app.But the issue go on.,9/26/16,1
wsf1990,"```Uncaught fatal error from thread [sjzx-through-akka.actor.default-dispatcher-2] shutting down ActorSystem [sjzx-through]java.lang.AbstractMethodError: spray.can.HttpManager.akka$actor$ActorLogging$_setter_$log_$eq(Lakka/event/LoggingAdapter;)V	at akka.actor.ActorLogging$class.$init$(Actor.scala:335)	at spray.can.HttpManager.<init>(HttpManager.scala:29)	at spray.can.HttpExt$$anonfun$1.apply(Http.scala:153)	at spray.can.HttpExt$$anonfun$1.apply(Http.scala:153)	at akka.actor.TypedCreatorFunctionConsumer.produce(Props.scala:401)	at akka.actor.Props.newActor(Props.scala:339)	at akka.actor.ActorCell.newActor(ActorCell.scala:534)	at akka.actor.ActorCell.create(ActorCell.scala:560)	at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:425)	at akka.actor.ActorCell.systemInvoke(ActorCell.scala:447)	at akka.dispatch.Mailbox.processAllSystemMessages(Mailbox.scala:262)	at akka.dispatch.Mailbox.run(Mailbox.scala:218)	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:386)	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)```",9/26/16,1
wsf1990,gt version is 1.0.0-2077839,9/26/16,1
pomadchin,mb you have different akka versions in a cluster classpath and in your app?,9/26/16,1
pomadchin,what server version do you use?,9/26/16,1
pomadchin,looks like it_Ñés a problem of your spray server,9/26/16,1
wsf1990,When I clone the `geotrellis-chatta-demo` to my server.and use `--class geotrellis.chatta.Main --driver-memory=2G target/scala-2.10/GeoTrellis-Tutorial-Project-assembly-0.1-SNAPSHOT.jar` run it.Just take same error,9/26/16,1
wsf1990,my server is ubuntu 14.04,9/26/16,1
pomadchin,i believe chatta demo needs to be updated ):,9/26/16,1
pomadchin,but,9/26/16,1
pomadchin,i believe it worked ,9/26/16,1
pomadchin,hm,9/26/16,1
wsf1990,how to update it? Before I test my code on hdp.It worked well.,9/26/16,1
wsf1990,"Today I change env to cdh,nothing had been changed",9/26/16,1
pomadchin,"in theory yes, it may cause problems",9/26/16,1
wsf1990,The code can't start up,9/26/16,1
pomadchin,sec i have an idea,9/26/16,1
pomadchin,https://github.com/geotrellis/geotrellis-chatta-demo/blob/master/geotrellis/build.sbt#L36,9/26/16,1
pomadchin,"replace on that: ```scalalibraryDependencies ++= (((Seq(  ""org.apache.spark""  %% ""spark-core""    % _ÑÒ1.5.2_Ñù % _ÑÒprovided"",  ""io.spray""          %% ""spray-routing"" % ""1.3.3"",  ""io.spray""          %% ""spray-can""     % ""1.3.3"",  ""org.apache.hadoop""  % ""hadoop-client"" % _ÑÒ2.7.1"" % _ÑÒprovided_Ñù) ++ geotrellis) map (_ exclude(""com.google.guava"", ""guava""))) ++ Seq(""com.google.guava"" % ""guava"" % ""16.0.1""))```",9/26/16,1
wsf1990,"Ok,I'll try",9/26/16,1
wsf1990,I'm sorry.The issue is going on,9/26/16,1
wsf1990,Dose somebody had tested the gt on cdh?,9/26/16,1
pomadchin,it_Ñés not a gt problem; more spray / akka ,9/26/16,1
pomadchin,mmm,9/26/16,1
pomadchin,trying to find out what can help you,9/26/16,1
pomadchin,hm,9/26/16,1
pomadchin,i think you can shade akka for spray,9/26/16,1
pomadchin,"```scalaassemblyShadeRules in assembly := {  val shadePackage = ""com.azavea.shaded.demo""  Seq(    ShadeRule.rename(""com.typesafe.akka.**"" -> s""$shadePackage.com.typesafe.akka.@1"")      .inLibrary(        ""io.spray""          %% ""spray-routing"" % ""1.3.3"",        ""io.spray""          %% ""spray-can""     % _ÑÒ1.3.3"",        ""org.apache.spark""  %% ""spark-core""    % _ÑÒ1.5.2""      ).inAll  )}```",9/26/16,1
pomadchin,add these lines into build.sbt,9/26/16,1
wsf1990,yes.I think it's caused by akka or cdh.thanks!I test it,9/26/16,1
pomadchin,"yep, sbt-assmebly plugin version should be > 0.14.0",9/26/16,1
pomadchin,googled that spark job server has a similar issue: https://github.com/spark-jobserver/spark-jobserver/pull/275,9/26/16,1
pomadchin,https://github.com/spark-jobserver/spark-jobserver/issues/60,9/26/16,1
wsf1990,@pomadchin You said the sbt version be > 0.14.0 but i find that the latest version is 0.13.12,9/26/16,1
pomadchin,` sbt-assmebly plugin`,9/26/16,1
wsf1990,[![C(%JC1}~U66I9RR3)I6BTLP.png](https://files.gitter.im/geotrellis/geotrellis/jtm6/thumb/C__JC1__U66I9RR3_I6BTLP.png)](https://files.gitter.im/geotrellis/geotrellis/jtm6/C__JC1__U66I9RR3_I6BTLP.png),9/26/16,1
wsf1990,I'm sorry.I dont understand what is ` sbt-assmebly plugin`?Can you explain it,9/26/16,1
pomadchin,"@wsf1990 yes ofcource, sorry was busy a bit ._.",9/26/16,1
pomadchin,"so sbt has a term _ÑÒplugin""",9/26/16,1
pomadchin,some addition: https://github.com/geotrellis/geotrellis-chatta-demo/blob/master/geotrellis/project/plugins.sbt,9/26/16,1
pomadchin,i just was not sure that chatta demo uses proper assembly plugin version,9/26/16,1
pomadchin,^^ sounds like it has and you don need changes,9/26/16,1
pomadchin,sbt assembly plugin itself: https://github.com/sbt/sbt-assembly,9/26/16,1
wsf1990,Understand.Thank you very much.I think I should googled for cdh and akka.,9/26/16,1
pomadchin,"plugin ~ a special jar that is used by sbt and can add cool features into sbt (additional functions ~ dependency tree for example), into final jars packaging ways / strategies, additional compiler extensions (like we use kind projector in our geotrellis project)",9/26/16,1
wsf1990,Got it,9/26/16,1
pomadchin,yep; feel free to ask more questions; + i_Ñém interested in what you_Ñéll get in result on cdh deps problem; what_Ñés your final soultion would be,9/26/16,1
wsf1990,Sure.if I resolved this problem.I'll write a short document,9/26/16,1
pomadchin,"@wsf1990 cool, thx!",9/26/16,1
dorukozturk,"So what is the combination of versions that we should use with Geotrellis master? Java, Scala, Spark and Sbt?",9/26/16,1
pomadchin,"right now its: Java 7+, Scala 2.11, Spark 2; sbt 13.x + ",9/26/16,1
dorukozturk,Thank you very much for the quick answer @pomadchin ,9/26/16,1
pomadchin,"Hi, everyone! Want to repeart an announcement about not going to support java 7 anymoreWe deprecated Scala 2.10 (as Spark 2 is build with Scala 2.11 by default) in our current master branch, and investigating dependencies / destination where Scala moves to, we are thinking on not to support Java 7 anymore. The question to our users, what are your thoughts about Java 7 deprecation? WIP PR: https://github.com/geotrellis/geotrellis/pull/1640",9/28/16,1
lossyrob,"@/all curious to see if anyone will be affected by Grisha_Ñés question. Do we have any users that are on Java 7, can_Ñét move off of it, so would not be able to use a GeoTrellis 1.0 that only supports java 8 and up?",9/28/16,1
jnh5y,Java 7 has been end-of-life'd since April 2015;),9/28/16,1
lossyrob,I_Ñéve run into stranger things :P,9/28/16,1
jnh5y,"eh, you can't please all the people all of the time...",9/28/16,1
lossyrob,truth,9/28/16,1
jnh5y,and all those people are on our projects' email lists...;),9/28/16,1
dorukozturk," Hey, I am getting this error while trying to ingest a local file, Unable to find input module of type 'hadoop' for format `temporal-geotiff This is my input file, any ideas?[  {    ""name"": ""landsat"",    ""format"": ""temporal-geotiff"",    ""backend"": {      ""type"": ""hadoop"",      ""path"": ""file://data/test_1.tif""    },    ""cache"": ""NONE""  }]",9/28/16,1
lossyrob,"I_Ñéll try to check the code _Ñ_ grisha woud know best, but (hopefully) he_Ñés a_Ñésnoozin by now",9/28/16,1
dorukozturk,"Cool thank youI am not in a rush :D",9/28/16,1
lossyrob,is it multiband or single band?,9/28/16,1
dorukozturk,It is multiband,9/28/16,1
dorukozturk,and I added the time tag for the geotiff using gdal,9/28/16,1
lossyrob,what_Ñés the `load` call look like?,9/28/16,1
lossyrob,"`etl.load[TemporalProjectedExtent, MultibandTile](_Ñ_)` is what it should look like",9/28/16,1
lossyrob,I suspect maybe it_Ñés a bad exception message off of the inability to find that input plugin for given types,9/28/16,1
wsf1990,@dorukozturk I think you havnt change your etl.load to what  @lossyrob said.,9/29/16,1
dorukozturk,"https://github.com/geotrellis/geotrellis/blob/master/docs/spark-etl/spark-etl-intro.md on this page there is insturctions for creating a fat jar, creating 3 json files like input, output and backend-profile and running spark-submit is that not the right thing to do ?",9/29/16,1
lossyrob,"` val sourceTiles = etl.load[I, V]`",9/29/16,1
lossyrob,in `GeoTrellisETL`,9/29/16,1
lossyrob,is wrong,9/29/16,1
lossyrob,that `I` ad `V` type params aren_Ñét defined anywhere,9/29/16,1
lossyrob,not sure how that even compiles,9/29/16,1
wsf1990,"yes,But you must change your etl.load code",9/29/16,1
lossyrob,I_Ñéll add an issue to make that more clear in the docs,9/29/16,1
lossyrob,also you have to set the `K` and the `V`,9/29/16,1
lossyrob,"to `SpaceTimeKey` and `MultibandTile`, in your case",9/29/16,1
wsf1990,"`Etl.ingest[TemporalProjectedExtent, SpaceTimeKey, MultibandTile](args)`",9/29/16,1
lossyrob,ah also that,9/29/16,1
lossyrob,good point @wsf1990 ,9/29/16,1
lossyrob,I made 2 new issues for a better error message and better docs,9/29/16,1
dorukozturk,"OK, I am a little confused. It is getting late :D, so step by step is this how it should be done:1) Fix this line https://github.com/geotrellis/geotrellis/blob/0a377df953e502cfeb1b68cc91d625aaa9c84625/spark-etl/src/main/scala/geotrellis/spark/etl/Etl.scala#L422) Create the assembly3) Create input.json, output.json and backend-profiles.json4) run the spark-submit job?Or should I create something by looking at the sample etl application code? I feel like the canned multiband example will do the job for me or is that assumption wrong? Sorry guys  probably it is me not understanding.",9/29/16,1
lossyrob,That sounds right. But yeah we should have a clear example of this case,9/29/16,1
wsf1990,Not need.you just need change the code wchich called the Etl,9/29/16,1
lossyrob,ah.  yeah you don_Ñét need to modify the geotrellis code,9/29/16,1
lossyrob,"that `I,V` there is determined by the method params",9/29/16,1
wsf1990,"change to `Etl.ingest[TemporalProjectedExtent, SpaceTimeKey, MultibandTile](args)`",9/29/16,1
lossyrob,@wsf1990 do you have an example ETL app that you could share?,9/29/16,1
wsf1990,Sure,9/29/16,1
wsf1990,"```object TemporalMultibandDataIngest extends App {  implicit val sc = SparkUtils.createSparkContext(""SJZX ETL Temporal Multiband Ingest"", new SparkConf(true))//.set(""spark.executor.memory"",""3g"")  Etl.ingest[TemporalProjectedExtent, SpaceTimeKey, MultibandTile](args)  sc.stop()}```",9/29/16,1
lossyrob,nice,9/29/16,1
wsf1990,run the spark-submit and the class is TemporalMultibandDataIngest ,9/29/16,1
lossyrob,"and that_Ñés what you would do the `sbt assembly` on, and use for your `spark-submit`",9/29/16,1
lossyrob,pointing to your config json files,9/29/16,1
dorukozturk,"Ok I ll try that. Thanks for the support @lossyrob and @wsf1990 ",9/29/16,1
lossyrob,+1 @wsf1990 many thanks,9/29/16,1
wsf1990,"sure,just what you told me, thanks for your hard work. @lossyrob ",9/29/16,1
pomadchin,@wsf1990 thx! @dorukozturk  btw we have examples of spatial (not temporal) ingest classes: https://github.com/geotrellis/geotrellis/blob/master/spark-etl/src/main/scala/geotrellis/spark/etl/MultibandIngest.scala and https://github.com/geotrellis/geotrellis/blob/master/spark-etl/src/main/scala/geotrellis/spark/etl/SinglebandIngest.scala which are included in the default etl assembly by default,9/29/16,1
wsf1990,"@pomadchin He want to temporal,And I just change this code to temporal.",9/29/16,1
wsf1990,@pomadchin I have run my app to cdh successfully,9/29/16,1
wsf1990,and I'm writing a short doc to explain this for help other people,9/29/16,1
pomadchin,@wsf1990 great! would you make a pr against gt master?,9/29/16,1
pomadchin,that would be awesome,9/29/16,1
wsf1990,"I can make a demo because I havnt rebuild gt,but do something in my app",9/29/16,1
wsf1990,But also have a small error about com.google.guava on json valid,9/29/16,1
pomadchin,guava versions?,9/29/16,1
pomadchin,think just a doc with explanations what you did to build your app correct would be enough :),9/29/16,1
wsf1990,gt use 16.1. this will cause no suchmethoderror,9/29/16,1
wsf1990,and I just step this when ingest data,9/29/16,1
pomadchin,we have guava problem with cassandra and json validator on emr as well: https://github.com/geotrellis/geotrellis-landsat-emr-demo/blob/master/ingest/build.sbt#L27-L36,9/29/16,1
pomadchin,the solution is to shade guava (:,9/29/16,1
wsf1990,how to shade this,9/29/16,1
wsf1990,got it,9/29/16,1
wsf1990,I'll try ,9/29/16,1
pomadchin,"yes, like you are just _ÑÒrenaming_Ñù links which correspond to some pattern in some certain libs",9/29/16,1
wsf1990,I know,9/29/16,1
pomadchin,":D cool, sry just explaining mb to somebody who would face similar problems during assembly builds, and would read our chat",9/29/16,1
wsf1990,thanks! It worked well.I'll write a doc to explain I how to run gt app on cdh,9/29/16,1
pomadchin,thank you @wsf1990 !,9/29/16,1
wsf1990,@pomadchin this is the short doc.,9/29/16,1
wsf1990,"When you put your code to cdh,you maybe have some issue,this is some method to resolve issues which I got.gt's version is 1.0.0-2077839,cdh's version is 5.8.first,if you use cloudera > 5.8.you must change your spray version to 1.2.3.second,you must shade guava licke this:```assemblyShadeRules in assembly := {  Seq(    ShadeRule.rename(""com.google.common.**"" -> ""my.google.common.@1"")      .inLibrary(        ""com.azavea.geotrellis"" %% ""geotrellis-cassandra"" % gtVersion,        ""com.github.fge"" % ""json-schema-validator"" % ""2.2.6""      ).inAll}```third,if you use multi-user mode you'll get the issue of permission of hdfs operation when you ingest your data into accumulo.so I changed the hdfs's umask to 0000.this maybe not very good.",9/29/16,1
wsf1990,"maybe not very clear,and the method maybe very good.",9/29/16,1
pomadchin,"@wsf1990 gotcha; we would add it into docs oor you can make pr yourself ;) thx for verions investigation",9/29/16,1
pomadchin,"I_Ñém thinking on moving to a new server - akka-http // as spray server is stopped in development, and its last release date is in march 2015",9/29/16,1
pomadchin,):,9/29/16,1
wsf1990,"sure,it's very good",9/29/16,1
wsf1990,but i'm not very clear how to make a pr,9/29/16,1
pomadchin,"hm, yes, a very interesting question",9/29/16,1
pomadchin,we have a docs folder,9/29/16,1
pomadchin,https://github.com/geotrellis/geotrellis/tree/master/docs,9/29/16,1
pomadchin,"probably we need to create a special section, with features / use cases / possible deployment problems ",9/29/16,1
wsf1990,sure,9/29/16,1
dorukozturk,"Hey, I started the zookeper server on port 2181, and my backend-profile has the parameter as  ""zookeepers"": ""localhost"", ""instance"": ""accumulo"" and I am getting an ""Instance name accumulo does not exist in zookeeper.  Run ""accumulo org.apache.accumulo.server.util.ListInstances"" to see a list."" error. Then I create a new znode named accumulo but it did not help. My command looks like this: spark-submit --class geotrellis.spark.etl.MultibandIngest --master local[*] --driver-memory 2G spark-etl/target/scala-2.11/geotrellis-spark-etl-assembly-1.0.0-SNAPSHOT.jar --backend-profiles conf/backend-profiles.json --input conf/input.json --output conf/output.jsonAnyone encountered this error?",9/30/16,1
lossyrob,what_Ñés your accumulo instance name?,9/30/16,1
lossyrob,it_Ñés generally `gis` if your using geodocker,9/30/16,1
dorukozturk,"oh I am trying to build everything myself. Maybe I should start with the geodocker, where is that located?",9/30/16,1
dorukozturk,"is it this? https://github.com/geodocker/geodocker-accumulo",9/30/16,1
lossyrob,yes,9/30/16,1
lossyrob,"if you built everything yourself, what was the accumulo instance name?",9/30/16,1
lossyrob,can you shell into accumulo?,9/30/16,1
lossyrob,`accumulo shell`,9/30/16,1
dorukozturk,"actually i can't so probably i did not properly installed accumulo, i will try the geodocker",9/30/16,1
lossyrob,yeah geodocker is a great route for messing with deployments. Encapsulates a lot of pain we went through trying to figure out how to set everything up properly :),9/30/16,1
dorukozturk,"cool, will give it a try. Thx",9/30/16,1
metasim,"Is there an ETA on the next stable release of GeoTrellis, with Spark 2.0.x?",9/30/16,1
pomadchin,"@metasim nope (i believe so, probably end of octobre - novembre, @lossyrob can say more), but you can download _not stable_ from our bintary repo: https://bintray.com/azavea/geotrellis/ (latests sha)",9/30/16,1
metasim,"Thanks much. Still in n00b mode, navigating the project.",9/30/16,1
pomadchin,@metasim no problems (: any questions can be discussed there ,9/30/16,1
metasim,Am I correct in inferring that I should stay away from the 0.9 documentation? Seems there's a big disconnect between it and the 0.10 docs?,9/30/16,1
metasim,(assuming I want to move forward with the Spark-based implementation),9/30/16,1
pomadchin,"yes, 0.9 and .10 are totally different",9/30/16,1
pomadchin,you can use _git tags_ to get your gt version (>=0.10) docs: https://github.com/geotrellis/geotrellis/tree/v0.10.2/docs,9/30/16,1
pomadchin,however i still recommend you to look through master branch docs as well ,9/30/16,1
metasim,That's helpful.,9/30/16,1
metasim,"I see in Maven Central there's a 0.10.3 release, but no tag for it.... should I be using 0.10.2 instead?",9/30/16,1
pomadchin,"\+ demos: * github.com/geotrellis/geotrellis-chatta-demo* https://github.com/geotrellis/geotrellis-landsat-emr-demo* https://github.com/geotrellis/geotrellis-landsat-tutorial",9/30/16,1
metasim,Going through the landsat demo now. Extremely helpful!,9/30/16,1
pomadchin,you can use `0.10.3` (: it_Ñés just not indexed: https://repo1.maven.org/maven2/com/azavea/geotrellis/geotrellis-accumulo_2.11/0.10.3/,9/30/16,1
owbmoore,"I have a question about the different geotrellis / scala libraries.   Why is there geotrellis-raster, vector,engine, spark  versions 2_10 & 2_11, but for the base geotrellis there is only 2_10, no 2_11",9/30/16,1
pomadchin,"@owbmoore it_Ñés an old version of geotrellis (< 0.10, akka based, not spark based), it_Ñés unsupported (not developed anymore)",9/30/16,1
owbmoore,"So, basically if using 2_11 then basically this line...  ""com.azavea.geotrellis"" %% ""geotrellis"" % ""0.9.2"",  should be removed.",9/30/16,1
pomadchin,"yes, in theory you can try to build this (0.9.2) geotrellis version locally, as you wish; but i recommend you to have a look at new geotrellis",9/30/16,1
owbmoore,"Ok, thanks.   I started with the landsat tutorial, but then tried to do my own thing with 2_11 and was getting conflicts with geotrellis / spray due to versions.",9/30/16,1
pomadchin,@owbmoore what tutorial you are talking about? https://github.com/geotrellis/geotrellis-landsat-tutorial/blob/master/build.sbt#L31,9/30/16,1
pomadchin,O:,9/30/16,1
owbmoore,"Yes, that works just fine as-is.   But if you take that and move to 2_11 it doesn't work",9/30/16,1
pomadchin,hm,9/30/16,1
pomadchin,what err it throws to you?,9/30/16,1
owbmoore,"import geotrellis.source.{ValueSource, RasterSource}",9/30/16,1
owbmoore,that doesn't exist because it is in geotrellis library.,9/30/16,1
pomadchin,https://repo1.maven.org/maven2/com/azavea/geotrellis/geotrellis-spark_2.11/,9/30/16,1
pomadchin,ill try to build it locally,9/30/16,1
owbmoore,I tried to use that build.sbt to start my own project and modified my dependencies to:,9/30/16,1
owbmoore,"libraryDependencies ++=  {  val sprayVersion = ""1.3.3""  Seq(  ""io.spray"" %% ""spray-httpx"" % sprayVersion,  ""io.spray"" %% ""spray-routing"" % sprayVersion,  ""com.azavea.geotrellis"" %% ""geotrellis"" % ""0.9.2"",  ""com.azavea.geotrellis"" %% ""geotrellis-raster"" % ""0.10.2"",  ""com.azavea.geotrellis"" %% ""geotrellis-spark"" % ""0.10.2"",  ""org.apache.spark"" %% ""spark-core"" % ""1.5.2"",  ""org.scalatest""       %%  ""scalatest""      % ""2.2.0"" % ""test"")}",9/30/16,1
pomadchin,"you cant use `""com.azavea.geotrellis"" %% ""geotrellis"" % _ÑÒ0.9.2""`",9/30/16,1
pomadchin,):,9/30/16,1
owbmoore,"Right, which is where I was stuck, because that import depended on it.",9/30/16,1
pomadchin,eeee,9/30/16,1
pomadchin,but original demo has no imports issues,9/30/16,1
owbmoore,"Right, I tried to add to it.  ",9/30/16,1
owbmoore,Had the import getrellis.source....,9/30/16,1
owbmoore,which becomes a problem.,9/30/16,1
pomadchin,w8 what are you trying to do? O:,9/30/16,1
pomadchin,:D ,9/30/16,1
pomadchin,"i believe you don_Ñét need `""com.azavea.geotrellis"" %% ""geotrellis"" % _ÑÒ0.9.2_Ñù` import",9/30/16,1
owbmoore,"import geotrellis.source.{ValueSource, RasterSource}",9/30/16,1
lossyrob,"geotrellis.source is depreicated, was in the old 0.9 version, not in 0.10",9/30/16,1
lossyrob,geotrellis source actually moved to geotrellis-engine,9/30/16,1
owbmoore,"Yep, I get its old/don't use it.  On board with that.",9/30/16,1
owbmoore,"I tried to poke around in the other jars to see if I could find it elsewhere, must have missed it if it is in engine.",9/30/16,1
lossyrob,yeah it_Ñés in engine,9/30/16,1
lossyrob,weird renames before cutting that code loose...,9/30/16,1
pomadchin,though engine is deprecated as well ):,9/30/16,1
owbmoore,eek.,9/30/16,1
owbmoore,I'm a long time java programmer trying to learn scala and geotrellis at the same time.,9/30/16,1
lossyrob,@metasim we_Ñéd like to get the next GeoTrellis version out in November; we_Ñére currently trying to push through the LocationTech incubation process so it_Ñés a little bit hard to say how sure we can be about the deadline. But that_Ñés the goal,9/30/16,1
lossyrob,"also I tagged 0.10.3 as a release, thanks for pointing that out",9/30/16,1
lossyrob,@owbmoore I would recommend starting with https://github.com/geotrellis/geotrellis-landsat-tutorial,9/30/16,1
owbmoore,"Right, that's exactly what I did.  And that worked great.",9/30/16,1
lossyrob,oh nice,9/30/16,1
owbmoore,Then I looked at some other code which required geotrellis.source,9/30/16,1
lossyrob,https://github.com/geotrellis/geotrellis-landsat-emr-demo is sort of the big next step up from that,9/30/16,1
lossyrob,which code was that?,9/30/16,1
owbmoore,"One of the other tutorials I believe, not sure now.   But source wasn't resolving so I hunted it down in geotrellis_2.10 which then became the source of all these other problems.",9/30/16,1
owbmoore,let me poke in engine as a dependency and see what that does.,9/30/16,1
lossyrob,ok. yeah if the code is use `geotrellis.source` then that does not bode well for 0.10 working well for you (APIs changed all over the place),9/30/16,1
lossyrob,sorry for the confusion/run around,9/30/16,1
owbmoore,"Well, it doesn't really matter.  This project was about me building something from scratch, chasing down dependencies and just figuring out how things would go in general.",9/30/16,1
owbmoore,If I grabbed onto a moving target and getting mixed up with versions that was just bad luck.,9/30/16,1
fosskers,I'm curious what demo code you saw that had references to 0.9,9/30/16,1
lossyrob,"yeah, we should probably write  in big bold letters _ÑÒold land mines live here! haste ye away!_Ñù :)",9/30/16,1
pomadchin,I believe this: https://github.com/geotrellis/geotrellis-spray-tutorial,9/30/16,1
fosskers,"lol `""com.azavea.geotrellis"" %% ""geotrellis"" % ""0.9.1""`",9/30/16,1
owbmoore,That looks like it.,9/30/16,1
fosskers,yeah that's ancient and should be updated.,9/30/16,1
owbmoore,spray was the thing getting cross version errors,9/30/16,1
fosskers,"Immediate options are:1. update the repo to include new GT and Spark2. close the repo to avoid confusion",9/30/16,1
lossyrob,I just tagged it with a bunch of warnings,9/30/16,1
fosskers,Yeah the README is pretty clear,9/30/16,1
owbmoore,That will probably be a big help.   I was just poking around and would have run away from it had I seen something like that.,9/30/16,1
lossyrob,Thanks for bringing it to our attention and saving future explorers from looking to deeply into that!,9/30/16,1
owbmoore,It did serve as a good troubleshooting experience!,9/30/16,1
owbmoore,Is this recommended:  https://github.com/geotrellis/geotrellis-landsat-emr-demo,9/30/16,1
fosskers,eventually. that's our most up-to-date and involved demo,9/30/16,1
pomadchin,@owbmoore pull down this pr https://github.com/geotrellis/geotrellis-landsat-emr-demo/pull/18 (as master would be harder to build) sorry for that hell ._.,9/30/16,1
fosskers,"it shows you how to ingest tiles, serve them from a custom web server, and view them in a web UI",9/30/16,1
owbmoore,"Sounds good, thanks",9/30/16,1
lossyrob,"It_Ñés recommended for sure, its just, I would say, _ÑÒgreen belt_Ñù stuff",9/30/16,1
lossyrob,But it shows off som awesome things you can do with imagery and geotrellis,9/30/16,1
lossyrob,"Serve out dynamic NDVI differences between landsat scenes over the same area, do polygonal summaries, point time series",9/30/16,1
owbmoore,I forked/cloned it.  Build fails due to non-resolve of -SNAPSHOT dependencies but I've been looking at bits of the code and getting things to work in my own project.,9/30/16,1
lossyrob,they_Ñére not supposed to be tied to snapshots,9/30/16,1
lossyrob,@pomadchin is there a PR to update that?,9/30/16,1
owbmoore,Let me double check on that.,9/30/16,1
lossyrob,you should be able to move the version to `1.0.0-a2c53de`,9/30/16,1
lossyrob,which is a snapshot from our bintray repository,9/30/16,1
lossyrob,https://bintray.com/azavea/geotrellis has the latest,9/30/16,1
lossyrob,which we push out via CI on merges to master,9/30/16,1
lossyrob,"(though we have to change this behavoir soon, bintray is not happy with us)",9/30/16,1
lossyrob,you would change it here https://github.com/geotrellis/geotrellis-landsat-emr-demo/blob/master/project/Version.scala#L2,9/30/16,1
lossyrob,"https://github.com/geotrellis/geotrellis-landsat-emr-demo/blob/master/build.sbt#L22 is where the bintray repo is set up to be pulled from, so it should just work",9/30/16,1
owbmoore,Build is successful now.   Think I just didn't give it time to get everything pulled down.,9/30/16,1
lossyrob,ah,9/30/16,1
owbmoore,I like that this demo involves accumulo/hadoop/spark.  ,9/30/16,1
lossyrob,"yup it_Ñés a pretty full setup. Which is why it can be a bit more painful, dealing with AWS EMR and all",9/30/16,1
lossyrob,"if you go through it let us know what snags you hit, we_Ñéd like to get this up and clean for people to be able to get running",9/30/16,1
owbmoore,Will do,9/30/16,1
pomadchin,https://github.com/geotrellis/geotrellis-landsat-emr-demo/issues/18,9/30/16,1
lossyrob,why are those different shas?,9/30/16,1
pomadchin,latest shas for the moment pr was created,9/30/16,1
pomadchin,probably we may merge it to make demo compilable,9/30/16,1
lossyrob,oh sorry I thought they were both geotrellis,9/30/16,1
lossyrob,merged,9/30/16,1
pomadchin,thx!,9/30/16,1
ardilgulez,hey everyone :),10/1/16,1
ardilgulez,it's me again,10/1/16,1
pomadchin,yo!,10/1/16,1
ardilgulez,I have ~120 rows of point data in pgsql,10/1/16,1
ardilgulez,I retrieve them -> run kernel density on it -> cut the result into tiles,10/1/16,1
ardilgulez,"when I cut it, I can only cut up to zoom number 7",10/1/16,1
ardilgulez,yo @pomadchin how you doing,10/1/16,1
ardilgulez,just a coincidence (or not): 7 is the maximum zoom number that my kernel density extent can fit,10/1/16,1
pomadchin,"fine, hm, I may have a look at your question a bit later / tomorrow ): sry but what's the error?",10/1/16,1
pomadchin,what's expected zoom level ?,10/1/16,1
ardilgulez,I would like to still have the result at zoom levels 13-14 ,10/1/16,1
pomadchin,have you tried to track down where is possible limitation happens? :0,10/1/16,1
ardilgulez,couldn't really,10/1/16,1
ardilgulez,do you guys have the code of the tile ingester for this: http://207.245.89.238/labs1-demo/index.html ?,10/1/16,1
ardilgulez,or the entire codebase and the data ,10/1/16,1
pomadchin,"@ardilgulez yo, reread your question; so it_Ñés raster pyramiding limitation / feature",10/3/16,1
pomadchin,we store zoom levels basing on the resolution in a UTM zone containing the point https://github.com/geotrellis/geotrellis/blob/master/spark/src/main/scala/geotrellis/spark/tiling/ZoomedLayoutScheme.scala#L51,10/3/16,1
ardilgulez,Is there a way for me to get around that thing?,10/3/16,1
pomadchin,"ye, totally; you can resample tiles on demand",10/3/16,1
pomadchin,"```scalaval LayoutLevel(_, layout) = layoutScheme.levelForZoom(projection.worldExtent, zoom)val mapTransform = MapKeyTransform(meta.crs, layout.layoutCols, layout.layoutRows)val targetExtent = mapTransform(x, y)val gb @ GridBounds(nx, ny, _, _) = meta.mapTransform(targetExtent)val sourceExtent = meta.mapTransform(nx, ny)val largerTile = tileReader.reader[SpaceTimeKey, Tile](layerId).read(SpaceTimeKey(nx, ny, dt))largerTile.resample(sourceExtent, RasterExtent(targetExtent, 256, 256)```",10/3/16,1
pomadchin,or you want to persist higher zoom levels?,10/3/16,1
ardilgulez,I would like persistence,10/3/16,1
ardilgulez,And what is dt in that code,10/3/16,1
echeipesh,"@ardilgulez You_Ñére calling `rdd. kernelDensity(_Ñ_)` on RDD of point features, right ?",10/3/16,1
pomadchin,"How do you ingest data? You see, there, in schema definition: https://github.com/geotrellis/geotrellis/blob/master/spark/src/main/scala/geotrellis/spark/tiling/ZoomedLayoutScheme.scala#L43 we have a threshold, in fact that defines max zoom level in your case, as you tiles just need to fit condition: https://github.com/geotrellis/geotrellis/blob/master/spark/src/main/scala/geotrellis/spark/tiling/ZoomedLayoutScheme.scala#L75no other ways to define max zoom level right now ): ",10/3/16,1
pomadchin,@ardilgulez `dt: org.joda.DateTime`,10/3/16,1
echeipesh,"(sorry, jumping into middle of this so not up to date on the context :)",10/3/16,1
pomadchin,@echeipesh glad to see you there finally :D,10/3/16,1
echeipesh,> @echeipesh glad to see you there *finally* :D,10/3/16,1
pomadchin,:D :D :D ,10/3/16,1
ardilgulez,"I don't use rdd.kernelDensity, I just use KernelDensity.apply",10/3/16,1
echeipesh,This: https://github.com/geotrellis/geotrellis/blob/master/spark/src/main/scala/geotrellis/spark/density/RDDKernelDensity.scala#L72 ?,10/3/16,1
ardilgulez,"nah, this: https://github.com/geotrellis/geotrellis/blob/master/raster/src/main/scala/geotrellis/raster/density/KernelDensity.scala ",10/3/16,1
echeipesh,"Ok, I guess you must be doing that always with all your points, so you don_Ñét have to worry about kernel density zones around the tile edges.",10/3/16,1
echeipesh,The zoom level that is going to be ingested at is basically implied by the `rasterExtent` parameter of that function. You if double the cells/rows for the same extent is aproximately going to put it one zoom level down.,10/3/16,1
echeipesh,"Since you want to go from level 7 to level 13, that would make each tile really big, you_Ñéd probably want to make the tiles smaller (Extent wise) and just cut more of them.",10/3/16,1
echeipesh,"Also, it sounds like you_Ñére saving these guys are as RDD through a LayerWriter? If so why are you trying to avoid the `RDDKernelDensity` ? It should make your life much easier here.",10/3/16,1
pomadchin,mb changing threshold is a workaround in this situation? ,10/3/16,1
echeipesh,"Well, threshold would only put it down at most one level if I understand that code correctly.",10/3/16,1
ardilgulez,"The point with threshold is, I can use it to put my stuff down up to 5 levels and my stuff can still run",10/3/16,1
ardilgulez,but down 5 levels means I still get stuck at zoom level 10,10/3/16,1
ardilgulez,which is still not really good,10/3/16,1
pomadchin,"btw, @ardilgulez @echeipesh points that you can use `rdd.kernelDensity` function",10/3/16,1
ardilgulez,does it allow me to get to higher zoom levels?,10/3/16,1
ardilgulez,"if yes, I might try that",10/3/16,1
echeipesh,@ardilgulez It should look something like this:,10/3/16,1
echeipesh,"```scalaval pointRdd: RDD[PointFeature[Int]] = ???val scheme = ZoomedLayoutScheme(WebMercator, 256)val (_, layout) = scheme.levelForZoom(13)val kernel: Kernel = ???val tileRdd: RDD[(SpatialKey, Tile)] = pointRdd.kernelDensity(kernel, layout, WebMercator)```",10/3/16,1
echeipesh,"As a warning, every time you go down a level you essentially need 4x as many bytes as the previous level, so you will see the storage used shoot way up. If there isn_Ñét that much detail in the data that you would want the resolution of zoom level 13, you would probably be better doing dynamic resampling on the tile server, or even just letting Leaflet handle (which it does if you tell it what the max zoom level of the TMS endpoint is). But you_Ñéll be able to play around with it yourself of course.",10/3/16,1
vsimko,"I noticed that when deleting the layers, what remains are empty dirs.Is this expected ?",10/4/16,1
pomadchin,"it is for S3, HDFS, and File?",10/4/16,1
vsimko,regular filesystem,10/4/16,1
pomadchin,"eh, think yes",10/4/16,1
pomadchin,"as we have `Layer` deleters, but in theory we can check after layer deletion is parent fodler(s) empty or not and to remove it",10/4/16,1
vsimko,I think it should be encapsulated inside the FileLayerDeleter because otherwise the user would have the tendency to fiddle with the catalog,10/4/16,1
pomadchin,"@vsimko hm, probably it makes sense; though this `File` backend is designed more for tests rather than production api :D ",10/4/16,1
pomadchin,@vsimko https://github.com/geotrellis/geotrellis/pull/1647/commits/8af7477d0e0241305010e981abd4e95ad1926a07#diff-50babd1e4272845612de8ba11d9ff9d5R37,10/4/16,1
metasim,"I'd like to mask a raster with a GeoJSON-defined shape. I have the `geotrellis.vector.Polygon` from the `JsonFeatureCollection`, and a `MultibandTile` from a `MultibandGeoTiff`. Could someone point me in the general direction toward masking the `MultibandTile` with the `Polygon`?",10/4/16,1
pomadchin,@metasim we have a `(tile: Raster[Tile]).mask(polygon)` function,10/4/16,1
pomadchin,would it solve your issue?,10/4/16,1
pomadchin,"yes, it works with `Raster[MultibandTile]` as well",10/4/16,1
metasim,Indeed... sorry I missed it.,10/4/16,1
metasim,@metasim looking at empty coffee cup,10/4/16,1
pomadchin,"`Raster[T]` is a Tile (MultibandTile) with extent, so I believe we have an overload ~ `(tile: Tile).mask(tileextent, polygon)` but I am not sure :D",10/4/16,1
metasim,"There appears to be a `MultibandTile.mask(Extent, Geometry)` method.",10/4/16,1
pomadchin,"yep, so it would work for you (:",10/4/16,1
metasim,thanks much,10/4/16,1
owbmoore,So I've gone through all the steps for running the landsat-emr-demo locally.  Pointed my browser at localhost:3000/ and have a map.   Pressed load but I don't see anything happen.,10/4/16,1
owbmoore,What should I see?,10/4/16,1
pomadchin,you ran through ingest step?,10/4/16,1
pomadchin,`make ingest-local`?,10/4/16,1
owbmoore,YEs,10/4/16,1
pomadchin,and after that you started local tile server,10/4/16,1
owbmoore,right,10/4/16,1
owbmoore,did the curl command to see the catalog,10/4/16,1
pomadchin,"so after pressing load, metadata should load; and tile visible on a map should be shown as well",10/4/16,1
owbmoore,"Ok, I just see a ""blank"" map.",10/4/16,1
pomadchin,O:,10/4/16,1
pomadchin,what_Ñés js error ?,10/4/16,1
owbmoore,"let me trace thru the steps again real quick.   I think ingest fails if you run it a second time, says already exists",10/4/16,1
pomadchin,ah,10/4/16,1
owbmoore,I don't see any errors,10/4/16,1
pomadchin,"yes, remove local catalog folder",10/4/16,1
owbmoore,when I run make local-tile-server that command does not exit correct.,10/4/16,1
owbmoore,"[INFO] [10/04/2016 11:13:29.631] [demo-system-akka.actor.default-dispatcher-2] [akka://demo-system/deadLetters] Message [akka.io.Tcp$Bound] from Actor[akka://demo-system/user/IO-HTTP/listener-0#-1107998334] to Actor[akka://demo-system/deadLetters] was not delivered. [1] dead letters encountered. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.",10/4/16,1
owbmoore,"That is the last line from the make tile server, and it's sitting there",10/4/16,1
pomadchin,that_Ñés not looks like an err ,10/4/16,1
pomadchin,more like a common warn,10/4/16,1
owbmoore,"Right,  looks good to me",10/4/16,1
pomadchin,and what_Ñés the ingest result?,10/4/16,1
pomadchin,is it correct?,10/4/16,1
pomadchin,how many files in the local catalog folder?,10/4/16,1
owbmoore,sec...,10/4/16,1
owbmoore," curl localhost:8899/catalog{  ""layers"": [{    ""name"": ""landsat"",    ""extent"": [[-43.84811, -21.283660000000005], [-41.65311, -19.178710000000017]],    ""times"": [""2015-06-05T00:00:00-0400""],    ""isLandsat"": true  }]",10/4/16,1
pomadchin,hm looks good,10/4/16,1
pomadchin,and what happens after you press load on `localhost:3000` page?,10/4/16,1
owbmoore,Nothing,10/4/16,1
pomadchin,can you open js dev console in your browser?,10/4/16,1
owbmoore,"tbh I'm sorta new a Mac,  just a regular linux guy.  So Safari,  dunno about console.",10/4/16,1
pomadchin,oh,10/4/16,1
pomadchin,chrome should work ,10/4/16,1
pomadchin,":D",10/4/16,1
owbmoore,I'd be much more at home there.  Let me install chrome and try it.,10/4/16,1
pomadchin,;) ,10/4/16,1
echeipesh,"@owbmoore can you try hitting `localhost:8899/ping` and `localhost:8899/catalog` ?\",10/4/16,1
owbmoore,ping gives me pong,10/4/16,1
pomadchin,@echeipesh think works as `/catalog` works ,10/4/16,1
echeipesh,"cool, so it_Ñés just javascript problems `npm install` didn_Ñét complain? ",10/4/16,1
owbmoore,"aye, catalog give same as from curl.",10/4/16,1
owbmoore,npm install only warned about connection count limit,10/4/16,1
echeipesh,"oy, `ctrl-alt-delete` of js word remove `viewer/node_modules` and do `npm install` in there ?",10/4/16,1
owbmoore,running...,10/4/16,1
echeipesh,"wait, what did you mean by _ÑÒblank_Ñù map, basically a basemap but no images ?",10/4/16,1
owbmoore,yes,10/4/16,1
owbmoore,"npm installnpm WARN react-leaflet-draw@0.7.1 requires a peer of react-leaflet@^0.12.1 but none was installed.npm WARN geotrellis-viewer@0.0.2 No repository field.npm WARN geotrellis-viewer@0.0.2 No license field.",10/4/16,1
owbmoore,Can zoom/pan.  All good,10/4/16,1
echeipesh,And hitting the `Load` button in upper right generates no JS errors ?,10/4/16,1
owbmoore,Nothing pops up.  I'm using Safari which I'm not familiar with.  Does it show JS errors?,10/4/16,1
owbmoore,Loading chrome now.,10/4/16,1
echeipesh,"You_Ñéd see the js errors in dev console, if you git `alt-cmd-i` and clicked on Console tab. ",10/4/16,1
pomadchin,@echeipesh not works in safari by default ):,10/4/16,1
owbmoore,trying to make sense of errors so I can send something useful,10/4/16,1
owbmoore,Uncaught TypeError: Cannot read property 'on' of undefined react-leaflet-draw.js 139,10/4/16,1
echeipesh,"hmm, sounds suspitiously like we should have pinned a version we didn_Ñét. Getting a demo going. I should be able to verify that with clean pull.",10/4/16,1
owbmoore,"Uncaught TypeError: Cannot read property 'on' of undefined[HMR] connected action @ 11:39:38.258 LOAD_CATALOG_REQEST prev state Object action Object next state ObjectFETCH CATALOG http://localhost:8899/catalog action @ 11:39:38.270 LOAD_CATALOG_SUCCESS prev state Object action Object error TypeError: Cannot read property 'on' of undefined(_Ñ_) next state ObjectUncaught (in promise) TypeError: Cannot read property 'on' of undefined(_Ñ_)",10/4/16,1
echeipesh,Noticed this in the npm scroll `_ÓÒ_ÓÑ_ÓÎ UNMET PEER DEPENDENCY react-leaflet@0.11.7`,10/4/16,1
owbmoore,npm WARN react-leaflet-draw@0.7.1 requires a peer of react-leaflet@^0.12.1 but none was installed.,10/4/16,1
echeipesh,"editing `viewer/package.json` to set     `""react-leaflet-draw"": ""0.7.0_Ñù` and `    ""react-leaflet"": _ÑÒ0.11.6_Ñù` makes it happy ",10/4/16,1
echeipesh,basically removing the `^`,10/4/16,1
echeipesh,"makes npm happy, haven_Ñét finished the ingest to verify vs a catalog yet",10/4/16,1
owbmoore,"I made same changes and running make now, downloading the images.",10/4/16,1
owbmoore,the tar.bz must be pretty big,10/4/16,1
echeipesh,Yeah each scene is about 800MB tarred up.,10/4/16,1
echeipesh,"Sorry about that, hope you don_Ñét judge us too harshly by our slapdash JS code :)",10/4/16,1
owbmoore,"haha, no problem",10/4/16,1
owbmoore,"hey, it works",10/4/16,1
echeipesh,:fireworks: ,10/4/16,1
owbmoore,"Quick suggestion for the docs.  Might want to mention prerequisites.  npm, sbt, tree, scala.   Other than stopping to install stuff it went pretty smooth.",10/4/16,1
owbmoore,node.js,10/4/16,1
echeipesh,"That_Ñés a good point, noted it.",10/4/16,1
owbmoore,"Thanks for the quick assist, appreciate it.",10/4/16,1
echeipesh,"You_Ñére welcome, and thanks for pushing through :)",10/4/16,1
pomadchin,@echeipesh should we do changes in lc emr demo // npm deps versions? O: O:,10/4/16,1
lossyrob,he made a PR and merged for those changes,10/4/16,1
pomadchin,"ah cool, just didn_Ñét notice that",10/4/16,1
vsimko,I have a bunch of geotiffs without SRS. How can I replace the following GDAL command using geotrellis? `gdal_translate -a_srs EPSG:25832 srcfile targetfile`,10/5/16,1
pomadchin,"@vsimko it looks more like a preprocessing step; to read it as a valid geotiff o: but mb it is possible to use `geotrellis.raster.io.geotiff.reader.GeoTiffReader` though i think it may throw exception as crs not set; and we have an outdated geotrellis-gdal project: https://github.com/geotrellis/geotrellis-gdal and it is possible to do what you want with it 100%",10/5/16,1
vsimko,"Reading a geotiff without SRS caused an exception. Therefore, we are currently using `gdal_translate` as a preprocessing step. But I'd like to avoid using gdal if possible.",10/5/16,1
lossyrob,What_Ñés the exception?,10/5/16,1
lossyrob,"it shouldn_Ñét throw, I believe it should just default to LatLng",10/5/16,1
lossyrob,do you have an example geotiff that we can test/debug with?,10/5/16,1
lossyrob,"you should be able to read the geotiff, igore the CRS that it is set with (or not set with) and use your own known one (since CRSs are implicitly used, you have to supply it manually most of the time)",10/5/16,1
vsimko,Should I create an issue including the geotiff (or link to the file)?,10/5/16,1
lossyrob,"yes that would be great, thanks!",10/5/16,1
pomadchin,and a sample of code you are reading these tiles,10/5/16,1
metasim,"When writing out `MultibandGeoTiff` files, what's the appropriate means of specifying the `ColorInterp` field? My first band gets assigned `ColorInterp=Gray`, and subsequent bands are `ColorInterp=Undefined`. I tried setting it in the `(t: MultibandGeoTiff).tags.bandTags` map, but they don't end up getting put in the apropriate field (added to the generic metadata section).",10/7/16,1
lossyrob,We actually don_Ñét have a way to set the colorinterp,10/7/16,1
lossyrob,that would be a good feature; not clear on how difficult that would be,10/7/16,1
lossyrob,probably could read something out of the bandTags and convert it to the appropriate TIFF tags,10/7/16,1
metasim,"Thanks. From what I'm reading, even GDAL has limits in this area.",10/7/16,1
metasim,"@lossyrob I have a prototype hack of `GeoTiffWriter` that appears to do what I want it to (e.g. I can get a 3 band GeoTiff to be interpreted as RGB):https://gist.github.com/metasim/b6ae998430a6c8148567d4d487984489",10/7/16,1
metasim,"To properly integrate into GeoTrellis, it looks like `GeoTiffImageData` would need to be updated to contain the Photometric Interpretation code.",10/7/16,1
lossyrob,Nice. that sounds like the right approach to me.,10/7/16,1
lossyrob,would you be interested in making that contribution?,10/7/16,1
metasim,"If I were to do that (for a PR), should `PhotometricInterpretationCodes` be changed into something more along the lines of `BandType`?",10/7/16,1
metasim,"IOW, should it be changed into something more like an enum?",10/7/16,1
lossyrob,"hm, let me look into that",10/7/16,1
metasim,I used `Int`s because `TagCodes` was that way.,10/7/16,1
lossyrob,"I think we mainly keep it to Ints, BandType is a special case because of it_Ñés use in covernsion to CellType",10/7/16,1
lossyrob,@lossyrob pulls out the TIFF spec for the first time in a while ...,10/7/16,1
metasim,"So, to be clear, I'm thinking that I'd modify [this line](https://github.com/geotrellis/geotrellis/blob/24eaa04d049bbf49283cb608f98964cf9f8a3794/raster/src/main/scala/geotrellis/raster/io/geotiff/writer/TiffTagFieldValue.scala#L75) to reference a field in [`GeoTiffImageData`](https://github.com/geotrellis/geotrellis/blob/24eaa04d049bbf49283cb608f98964cf9f8a3794/raster/src/main/scala/geotrellis/raster/io/geotiff/GeoTiffImageData.scala)",10/7/16,1
metasim,"FWIW, [this](http://www.awaresystems.be/imaging/tiff/tifftags/photometricinterpretation.html) is the reference I used for the code values.",10/7/16,1
lossyrob,understood_Ñ_I_Ñém wondering if placing it on the top level GeoTiffImageData makes sense,10/7/16,1
lossyrob,or if it would fit better somewhere else,10/7/16,1
metasim,"To some degree, it seems to fit in with `decompressor` and `sampleFormat`.",10/7/16,1
lossyrob,"because I think we might run into this case again later, where there_Ñés some TIFF tag we aren_Ñét explicitly setting correctly now and would want to based on some information we can set on the GeoTiff in the future",10/7/16,1
lossyrob,"and if we place everything there, it may get to be a pretty long case class",10/7/16,1
metasim,"In `TiffTagFieldValue`, `PhotometricInterpTag` is the only one set to a constant value.",10/7/16,1
metasim,"OK, `PlanarConfigurationTag` is as well.",10/7/16,1
lossyrob,there might be some we aren_Ñét setting as well,10/7/16,1
metasim,The rest of the values come from `GeoTiffImageData`,10/7/16,1
metasim,"If not there, then the only other option seems to be `GeoTiffData` or `Tags`.",10/7/16,1
metasim,And currently `Tags` is just a couple of maps.,10/7/16,1
lossyrob,"yeah. and Tags I think is it_Ñés own thing, those custom string tags",10/7/16,1
lossyrob,just rummaging through the code to make sure I_Ñém clear on everything,10/7/16,1
lossyrob,if it_Ñés in `GeoTiffImageData` then the implementations end up on `GeoTiffTile` and `GeoTiffMultibandTile`,10/7/16,1
lossyrob,seems like it_Ñés more of a `GeoTiffData` concern then a `Tile` level concern,10/7/16,1
lossyrob,we did something dealing with a similar situation here https://github.com/geotrellis/geotrellis/blob/2e871993bbb76f407da760cbf59a7e805ce7c375/raster/src/main/scala/geotrellis/raster/io/geotiff/GeoTiff.scala#L21,10/7/16,1
lossyrob,"although AREA_OR_POINT doesn_Ñét come from a TIFF tag, comes from the custom tags",10/7/16,1
metasim,"Whereever it goes, it'll need to flow through to `TiffTagFieldValue.collect(GeoTiffData)`",10/7/16,1
lossyrob,"I think it makes sense on GeoTiffData. Which would work for what you need, though if you have reasons it would rather be on GeoTiffImageData I_Ñém not totally set",10/7/16,1
metasim,"I think `GeoTiffData` works too, but to be honest, I don't know the distinction enough to have an informed opinion.",10/7/16,1
lossyrob,Let_Ñés think about it from a setting side,10/7/16,1
lossyrob,do you have code in your workaround that sets the color interpretation for the GeoTiff? what does that look like?,10/7/16,1
metasim,Line 14 in the gist above is all that's necessary.,10/7/16,1
lossyrob,gotcha. I_Ñém thinking from the client side where you have a tile you_Ñéd like to write to a specific color model,10/7/16,1
metasim,"When I run `gdalinfo` on the resulting file, after using interpretation code 2, you see this:```Band 1 Block=7971x1 Type=Byte, ColorInterp=RedBand 2 Block=7971x1 Type=Byte, ColorInterp=GreenBand 3 Block=7971x1 Type=Byte, ColorInterp=Blue```",10/7/16,1
lossyrob,what_Ñés the PhotometricInterpretationCodes that produces that?,10/7/16,1
metasim,(Edit: copied wrong one),10/7/16,1
metasim,`2`,10/7/16,1
metasim,which is `RGB`,10/7/16,1
lossyrob,so I_Ñém thinking,10/7/16,1
lossyrob,"```scalaval tile: Tile = ???val crs: CRS = ???// Write this out to geotiff with RGBval gt = GeoTiff(tile, crs, colorSpace = ColorSpace.RGB)```",10/7/16,1
lossyrob,taking _ÑÒcolor space_Ñù from description in http://www.awaresystems.be/imaging/tiff/tifftags/photometricinterpretation.html,10/7/16,1
lossyrob,"because `PhotometricInterpretation` is a mouthful (or keyboardfull), but not sure that_Ñés the best name",10/7/16,1
metasim,sure...,10/7/16,1
lossyrob,"and that would place it on the GeoTiffData, and then it could be written in the TiffTagFieldValue code",10/7/16,1
metasim,I have to step out to a meeting. I'll check back shortly. Feel free to PM me if you want to take this off the main thread.,10/7/16,1
lossyrob,"ok cool. main thread is good, this is what the room_Ñés for :) Thanks for talking through it",10/7/16,1
metasim,"If we get the design settled out, I'll do a PR this weekend.",10/7/16,1
lossyrob,sounds good to me,10/7/16,1
metasim,@lossyrob Should `GeoTiffOptions` be a consideration?,10/8/16,1
lossyrob,"good call, though the options are not on the GeoTiffData trait",10/8/16,1
lossyrob,so you wouldn_Ñét have access to them in the TiffTagFieldValue code,10/8/16,1
lossyrob,though I_Ñém not sure why we couldn_Ñét expose it,10/8/16,1
metasim,I'm finding that by adding it to `GeoTiffData` then it cascades through constructors and factory methods in a messy way,10/8/16,1
metasim,"IOW, `GeoTiffData`to too broadly cast.",10/8/16,1
lossyrob,"Makes sense. I would say, if putting on GeoTiffOptions and exposing the options through the GeoTiffData is a lot less of an impact on the codebase, then that might be the better choice",10/8/16,1
metasim,`pixelSampleType` seems like a special case since it's really a convenience method over the `AREA_OR_POINT` tag.,10/8/16,1
metasim,"OK, I'll poke at that route for a bit.",10/8/16,1
metasim,Something I'm confused about: There's already a place for the value in `geotrellis.raster.io.geotiff.tags.BasicTags.photometricInterp`. Are all of these `*Tags` files for the reading side only? Is reading/writing asymmetric?,10/8/16,1
lossyrob,it_Ñés asymmetric,10/8/16,1
lossyrob,admittedly not at all ideal,10/8/16,1
lossyrob,"the reading was written before the writing, and the writing writes a lot less than the reader reads",10/8/16,1
metasim,"Well, the format does have a VERY BIG specification :p",10/8/16,1
lossyrob,"ha, that_Ñés the truth",10/8/16,1
metasim,@lossyrob Let me know what you think about #1667.,10/8/16,1
metasim,(I sent in my GeoTrellis Inidividual CLA last night).,10/8/16,1
lossyrob,awesome. I_Ñéll take a look today. you signed the eclipse CLA as well?,10/8/16,1
lossyrob,"checked with the eclipse sight, looks like you did",10/8/16,1
lossyrob,I_Ñéll merge after travis gives thumbs up. good stuff.,10/8/16,1
lossyrob,merged. thanks for the contribution @metasim!,10/8/16,1
lossyrob,is available in bintray `1.0.0-969fa72`,10/8/16,1
metasim,Thanks!,10/9/16,1
metasim,"Is there any overlap between some subset of the current GeoTrellis functionality and the [OGC Catalog Service Specification](http://www.opengeospatial.org/standards/cat)? I ask becasuse a) I need something along those lines, and b) I read some things in the 0.9 documentation on a ""catalog service"".",10/9/16,1
lossyrob,"No the catalog from 0.9 is a separate thing, perhaps some ideas overlap but it did not implement any specific",10/9/16,1
lossyrob,We've been interested in implementing some lightweight ogc spec services but have never gotten around to it (haven't had the direct need to yet),10/9/16,1
lossyrob,"The current ""layers"" concept with spark (the attributestore) probably could serve out catalog spec info, would be a matter of writing the akka-http routes",10/9/16,1
lossyrob,"We've also talked about implementing services like wms, wps, wcs etc",10/9/16,1
lossyrob,"Not sure if the best way forward for that is to implement a set of geotools interfaces and then serve through geoserver, or to write a 'geotrellis-ogc' subproject or something that would be akka-http routes that would be it's own server implementation. @dwins, who had worked closely with the geoserver project, had the opinion that the latter strategy would be best last time we talked about it",10/9/16,1
metasim,"Thanks @lossyrob . I know the GeoMesa team has WFS support (and either WCS or WMS, not sure which) by implementing the DataSource APIs to GeoServer, which may be less onerous than implementing the full spec from scratch. That said, GeoServer doesn't seem to have catalog support.",10/10/16,1
metasim,"I'd be interested in knowing if there's a technical strategy long term for webservices: either plug in to some existing framework, base it on something higher-level than akka-http, or minimize external dependencies and grow what's needed.",10/10/16,1
metasim,"Edit: there's a user extension to add CSW to GeoServer:http://docs.geoserver.org/2.7.2/user/extensions/csw/features.html#catalog-stores",10/10/16,1
metasim,"Might be fairly straightforward to add the GeoTrellis ""attributestore"".",10/10/16,1
metasim,Could you point me to the classes that handle the I/O for that?,10/10/16,1
pomadchin,"@metasim attribute store keeps layers information, for each backend we have own implementation of it (Accumulo example): https://github.com/geotrellis/geotrellis/blob/master/accumulo/src/main/scala/geotrellis/spark/io/accumulo/AccumuloAttributeStore.scala ",10/10/16,1
pomadchin,._. but I am not sure how it would help you,10/10/16,1
pomadchin,"but would be glad to help if you throw there more context, and \+ @lossyrob would be avaible in a couple of hours",10/10/16,1
lossyrob,"@metasim there_Ñés not a clear strategy for web services; we want to delineate GeoTrellis to some bounds, as not to have it be some uber umbrella project, so I_Ñém not sure having a long term strategy to web services is something that falls into the purvew of the project. Possibly though, I don_Ñét want to shut off the idea. We_Ñére aimed at being a Scala library for geospatial, and to geospatially enable Spark. What I_Ñéve done with web services is write akka http routes; but anyone else could use it however they like. If we were to build up a server with a set of capabilities, that had generic use, I_Ñém not even sure that would be _ÑÒgeotrellis_Ñù, or a separate project built w/ GeoTrellis. If we were to make a DataStore in order to plug into GeoServer, then perhaps that would be part of the library. Although I wouldn_Ñét want to say _ÑÒyou use GeoTrellis in webservices by using GeoServer_Ñù, because that would perhaps impose some arbitrary restriction on how you could use the library. So that_Ñés a long way to saying, we don_Ñét have a clear long term strategy for web services, I_Ñém not sure we need one, which I think falls into the _ÑÒminimize external dependencies and grow what_Ñés needed_Ñù camp although I_Ñéd be happy to see projects built up to plug into existing frameworks like GeoServer and the like.",10/10/16,1
lossyrob,"As Grisha pointed out, all the *AttributeStore classes are the ones that handle the layer-level metadata for each of the layers, can list available layers, etc",10/10/16,1
metasim,"@lossyrob This is extremely helpful; I can extrapolate some thinking from this, and clarifies the place GeoTrellis is trying to fill, at least currently. I see great value in having deliniated boundaries.",10/10/16,1
metasim,"And since Scala web frameworks change almost as fast as JavaScript frameworks, you don't want to get locked into any one camp if you don't have to! :-)",10/10/16,1
lossyrob,that is true. glad you see value in that too :),10/10/16,1
metasim,"More of a n00b question now: I have two GeoTiffs in different CRSs but of overlapping areas. I want to cut/crop out the same region of ground from both, and then perform local algebra operations against each other. Due to numerical error, the cropped tiles are of slightly different sizes, so when I try to `combine` them I get a `GeoAttrsError: Cannot combine rasters with different dimensions...`. To put it another way, I'd like to overlay the finest grid I can get away with over both scenes, sample values from each across the grid, and perform math on those samples. Is there a recommended approach to do this?",10/10/16,1
lossyrob,"You could reproject them (or one of them) so that the CRS_Ñés match, resample them to the same grid, mask w/ the region, then perform the combine",10/10/16,1
lossyrob,"local ops need the raster to be the same dimensions, the same exact layout",10/10/16,1
lossyrob,so there_Ñés some transformation needed beforehand to get them looking that way,10/10/16,1
metasim,Do I do the resampling with `GridBounds`?,10/10/16,1
metasim,I'm already getting the CRS's to match.,10/10/16,1
lossyrob,"p.s. if you just need a small crop of the raster it might be good to reproject the crop area to the crs and crop in the native CRS before reprojecting, I think that would mean a performance gain",10/10/16,1
lossyrob,what is the thing your cropping by? polygon or extent?,10/10/16,1
metasim,extent,10/10/16,1
metasim,Actually doesn't matter too much,10/10/16,1
lossyrob,I would construct a `RasterExtent`,10/10/16,1
lossyrob,you have to base the cell size on what you want_Ñ_perhaps matching the more resolute raster,10/10/16,1
metasim,awesome,10/10/16,1
metasim,that's the tip I needed,10/10/16,1
lossyrob,"cool. and then resample will take the raster extent and then you have what you need",10/10/16,1
metasim,Wow..... one line of code and it works!,10/10/16,1
metasim,@metasim such a GeoTrellis fan boi,10/10/16,1
lossyrob,:D,10/10/16,1
metasim,"```      val c2 = s2.raster        .reproject(s2.crs, s1.crs)        .resample(s1.rasterExtent)        .crop(s1Region.envelope)```",10/10/16,1
lossyrob,:+1: ,10/10/16,1
metasim,Thanks (again) for the help!,10/10/16,1
lossyrob,no problem at all,10/10/16,1
mendoza7,"Hey, I'm interested in knowing more on how GeoTrellis works with metadata, is there any documentation on that?",10/12/16,1
metasim,What kind of metadata?,10/12/16,1
metasim,GeoTiff?,10/12/16,1
mendoza7,Specifically with RDDs,10/12/16,1
metasim,I know there's this: `TileLayerMetadata.fromRdd`,10/12/16,1
mendoza7,"Oh yes, I have looked at that, how do you manage to write that metadata to hadoop or other backends? Or it depends more on hdfs api than GeoTrellis itself. I'm just having trouble visualizing how you associate that metadata throughout the process(ingest,operations,etc). ",10/12/16,1
pomadchin,"@mendoza7 yo; every backend has an `AttributeStore` class (exmpl. for accumulo: `AccumuloAttributeStore`); in fact there are two tables / dirs: one contains tiles (index, bytearray), the other one contains _per layer _ metadata",10/12/16,1
pomadchin,you can have a look into any our `LayerWriter` to see it,10/12/16,1
mendoza7,"oh ok I see, thanks! ",10/12/16,1
pomadchin,"you can notice, write function signature accepts `rdd: RDD[(K, V)] with Metadata[M]` this rdd is just a pair of a common `RDD[(K, V)]` and metadata `M`",10/12/16,1
lossyrob,"Another thing to note is, you can use the attributestore as an arbitrary key Value store to write anything Json serializable to a key of the LayerId and an attribute name ",10/12/16,1
lossyrob,"For instance in some ingest processes we pyramid up a layer through zoom levels (each zoom level has its own LayerId), we will place information, such as an array of timestamps of imagery, in the zoom level 0 LayerId in the attributestore store, so we can use that later to feed UI elements",10/12/16,1
wsf1990,dose the version of 1.0.0-fb57dc4 is the lastet version? and what's new features it contained compare to 1.0.0-2077839?Thanks!,10/12/16,1
mendoza7,"@lossyrob Thanks, great example. I'm looking into this now.",10/12/16,1
pomadchin,@wsf1990 `1.0.0-40a2f7a` is the latest,10/12/16,1
pomadchin,recommend you to update your dep o: as there were some stability and functionality improvements,10/12/16,1
pomadchin,lots of prs were merged since #1553,10/12/16,1
pomadchin," @wsf1990 oh; in short, rather major changes: * goewave subproject* geomesa subproject * scala 2.10 not supported* java 8 not supported* no joda time in deps* spark 2 support",10/12/16,1
jbouffard,"Hello, everyone. A new feature has been in development these past few weeks to allow for stream and windowed reading of GeoTiffs from S3. While there is still work to be done, the progress made so far allows for successful readings off S3.",10/12/16,1
jbouffard,"If you're interested and have the time, we'd greatly appreciate it if you could try this S3StreamReader out for yourself and let us know any feedback you might have on it.",10/12/16,1
jbouffard,The PR that contains this feature can be found here: https://github.com/geotrellis/geotrellis/pull/1617,10/12/16,1
jbouffard,In addition. a short overview/guide has also been written for this feature: https://gist.github.com/jbouffard/95a071167ded7e7a8c90956743f97393,10/12/16,1
lossyrob,:+1:,10/12/16,1
metasim,"Is there any direct suppport in GeoTrellis, beyond the map algebra stuff, for performing atomospheric correction on imagery?",10/13/16,1
pomadchin,"@metasim you need to do it your own, using basis operations we have ): but i believe in future suh fnunction may be included in our api",10/13/16,1
pomadchin,"I_Ñém curious, how do you want to make correction? ^^_Ñé (if it_Ñés not a secret ofc)",10/13/16,1
joshmarcus,hello!  i have a question: is there a current release that includes kernel density ops?  i don't see that in 0.10 ,10/13/16,1
metasim,"@pomadchin The ""how"" is what we're trying to figure out.. still reviewing the Landst 8 ""LaSRC"" product manual (this is a new field for me). I'm assuming there's a standard process for converting sensor ""DN"" values to physical units, given various scaling and quality coefficients provided with the source data product. As I learn that process, I want to see how it interesects with GT capabilities and where I'll need to implement it myself.",10/13/16,1
metasim,"As with other roadmap aspects, interested in knowing how what I'm doing might interesect with long term plans for the platform.",10/13/16,1
joshmarcus,also: if there isn't a current release with kernel density is there a snapshot?,10/13/16,1
metasim,"PS: I'm not familiar with what a ""suh function"" is.",10/13/16,1
pomadchin,"@joshmarcus yes, you can find an actual version using sha versions from our bintray repo",10/13/16,1
pomadchin,"@metasim i have an interesting code that may help you (probably, or may not) I tried to do it as well: https://github.com/pomadchin/geotrellis-landsat-emr-demo/commit/dec030366907128abd3f4ff137f29fc3c1fa4dd7",10/13/16,1
pomadchin,"but yes, we are interested in color correction stuff, so any your ideas are appreciated ^^'",10/13/16,1
metasim,Awesome! Feel lucky to be coming in on this field with so many people having paved the way already.,10/13/16,1
joshmarcus,@pomadchin do you know if there is a basic raster KDE implementation in 0.10?  i'm just wondering if i missed it since it's in 0.9 and master,10/13/16,1
fosskers,@joshmarcus `0.10.3` has everything in master up until two weeks ago or so,10/13/16,1
pomadchin,@joshmarcus i believe no; let me double check it,10/13/16,1
pomadchin,"no, looks like nothing ):",10/13/16,1
pomadchin,OH,10/13/16,1
pomadchin,@joshmarcus https://github.com/geotrellis/geotrellis/blob/v0.10.3/raster-test/src/test/scala/geotrellis/raster/VectorToRasterSpec.scala#L114,10/13/16,1
metasim,"@pomadchin The demo depends on GeoTrellis `1.0.0-SNAPSHOT`. I changed it to `1.0.0-40a2f7a`, the latest available in BinTray, but I get compilation errors around not finding `com.github.nscala_time.time.Imports`. Would you expect that I'd need to build and locally install the latest GeoTrellis SNAPSHOT for it to work? Or should I try to fix the build to work with the BinTray version?",10/13/16,1
pomadchin,@metasim what demo are you talking about?,10/13/16,1
pomadchin,chatta demo?,10/13/16,1
metasim,landsat-emr-demo,10/13/16,1
metasim,Also trying to find a non-snapshot version of `scala-landsat-util`.,10/13/16,1
pomadchin,try to pull down updated lc emr demo,10/13/16,1
pomadchin,https://github.com/geotrellis/geotrellis-landsat-emr-demo/blob/master/project/Version.scala,10/13/16,1
pomadchin,"we fixed this thing, i believe a week ago",10/13/16,1
pomadchin,https://github.com/geotrellis/geotrellis-landsat-emr-demo/blob/master/ingest/build.sbt#L10,10/13/16,1
pomadchin,oh even two weeks,10/13/16,1
metasim,"Ah, I pulled it from your repo instead of `geotrellis`",10/13/16,1
metasim,(your fork),10/13/16,1
pomadchin,:D,10/13/16,1
pomadchin,"yes, we dont depend on joda time anymore",10/13/16,1
metasim,"For a ""demo"", the code is pretty substantial.",10/13/16,1
metasim,Any thoughts of spinning parts of it off into sub-projects?,10/13/16,1
metasim,(sub-repos),10/13/16,1
pomadchin,sub repos mm dunno,10/13/16,1
pomadchin,in fact it_Ñés not a hard app; with ingest part and server part,10/13/16,1
pomadchin,it demonstrates a small part of gt capabilities o:,10/13/16,1
metasim,fair.,10/13/16,1
pomadchin,"but yes, probably it makes sense to make aditional demos, more simple ._.",10/13/16,1
metasim,The server is a great example of  combining GT ingested imagery with on-the-fly analysis.,10/13/16,1
metasim,I didn't pay much attention to it because I thought it was all about EMR integration (only noting the name).,10/13/16,1
pomadchin,"ah, emr there means, that we have emr deployment scripts :D nothing more",10/13/16,1
metasim,"Anyway, just my way of saying ""nice demo!"" :)",10/13/16,1
pomadchin,:D thx for your comments,10/13/16,1
fosskers,https://github.com/geotrellis/geotrellis/issues/1659#issuecomment-253588554 :S,10/13/16,1
lossyrob,"@joshmarcus it's still there, we have a spark version too. Should be in a 'density' package",10/13/16,1
lossyrob,https://github.com/geotrellis/geotrellis/blob/master/raster/src/main/scala/geotrellis/raster/density/Implicits.scala,10/13/16,1
lossyrob,@metasim I agree with modulerizing the landsat EMR tutorial...matter of having the time and a good approach. And documenting it...,10/13/16,1
metasim,I'm trying to modify the [`GeoTrellisETL` example](https://github.com/geotrellis/geotrellis/blob/master/docs/spark-etl/spark-etl-intro.md#sample-etl-application) to use `TemporalProjectedExtent` and `SpaceTimeKey`. Changing either one of them results in compilation errors. ,10/14/16,1
pomadchin,what errors?,10/14/16,1
pomadchin,can you throw them here?),10/14/16,1
pomadchin,"```import geotrellis.raster.Tileimport geotrellis.spark._import geotrellis.spark.etl.config.EtlConfimport geotrellis.spark.util.SparkUtilsimport org.apache.spark.SparkConfobject GeoTrellisETL {  type I = TemporalProjectedExtent // or TemporalProjectedExtent for temporal ingest  type K = SpaceTimeKey // or SpaceTimeKey for temporal ingest  type V = Tile // or MultibandTile to ingest multiband tile  def main(args: Array[String]): Unit = {    implicit val sc = SparkUtils.createSparkContext(""GeoTrellis ETL"", new SparkConf(true))    try {      EtlConf(args) foreach { conf =>        /* parse command line arguments */        val etl = Etl(conf)        /* load source tiles using input module specified */        val sourceTiles = etl.load[I, V]        /* perform the reprojection and mosaicing step to fit tiles to LayoutScheme specified */        val (zoom, tiled) = etl.tile[I, V, K](sourceTiles)        /* save and optionally pyramid the mosaiced layer */        etl.save[K, V](LayerId(etl.input.name, zoom), tiled)      }    } finally {      sc.stop()    }  }}```",10/14/16,1
pomadchin,"the most important thing is explicit `I, V, K` types here: `etl.tile[I, V, K](sourceTiles)`; as temporal key usage is more difficut (i mean implicits reoslution)",10/14/16,1
metasim,"Ah, yes, I didn't have the explicit type parameters to `tile`.",10/16/16,1
metasim,Thanks!,10/16/16,1
owcm,"I am trying to read in geojson data into my application.  Primarily  line and multiline data.  Using Geotrellis 10.2.    Code I have been using  looks as follows:```                val lines = scala.io.Source.fromFile(lcName).getLines.mkString                var geoJsonType = ""LINES""                // Read in Road Features                val jsonFeatureMapping = lines.parseGeoJson[JsonFeatureCollection]                var roadLines = jsonFeatureMapping.getAllLines()                var roadMultiLines = jsonFeatureMapping.getAllMultiLines()                if (roadLines.size != 0) {                  // lets try individual lines                  println( ""roadLines"")                  theExtent = roadLines.envelope                  geoJsonType = ""LINES""                }                else if (roadMultiLines.size != 0) {                  println( ""roadMultiLines"")                  theExtent = roadMultiLines.envelope                  geoJsonType = ""MULTILINES""                } else {                  geoJsonType = ""NONE""                  println( ""No Feature Data"")                }```Feature counts are coming back with 0 values.   The geojson seems to be correctly format I verified with ogr2ogr .  I have used this code to read in other geojson Linestring feature files with out a problem.Example of geojson is as follows:```{""type"": ""FeatureCollection"",""crs"": { ""type"": ""name"", ""properties"": { ""name"": ""urn:ogc:def:crs:EPSG::32638"" } },                                                                                ""features"": [{ ""type"": ""Feature"", ""properties"": { ""GFID"": ""66AF8BBB-0710-4064-B266-991BAB8AC6C8"", ""EXS"": 28, ""LOC"": 8, ""LTN"": 1, ""MED"": 2, ""NAM"": null, ""NFI"": null, ""NFN"": null, ""RST"": 1, ""RTN"": ""UNK"", ""RTT"": 15, ""TUC"": 6, ""USG"": 0, ""WD1"": 25, ""WTC"": 1, ""ORIG_SOURC"": ""2001\/01\/01"", ""ORIG_SOU_1"": ""R15536"", ""ORIGINATIN"": ""Imagery (NTM)"", ""UPDATE_SOU"": ""Digital Globe"", ""UPDATE_S_1"": ""2006\/04\/01"", ""UPDATE_S_2"": ""01APR2006_p1bs_005563509020_01_p001_gp"", ""FCODE"": ""AP030"", ""FCODE_DESC"": ""Road"", ""SHAPE_Leng"": 0.001173 }, ""geometry"": { ""type"": ""LineString"", ""coordinates"": [ [ 332237.636310937232338, 4022332.556734973099083, 3.832781434623091 ], [ 332265.159629545174539, 4022269.875848380383104, 3.837893504002183 ], [ 332285.695189275196753, 4022217.318341301754117, 3.827133549163638 ] ] } },{ ""type"": ""Feature"", ""properties"": { ""GFID"": ""F2E3B9A0-4077-44AE-8CB6-D16F46F40C4A"", ""EXS"": 28, ""LOC"": 8, ""LTN"": 1, ""MED"": 2, ""NAM"": null, ""NFI"": null, ""NFN"": null, ""RST"": 1, ""RTN"": ""UNK"", ""RTT"": 15, ""TUC"": 6, ""USG"": 0, ""WD1"": 25, ""WTC"": 1, ""ORIG_SOURC"": ""2001\/01\/01"", ""ORIG_SOU_1"": ""R15536"", ""ORIGINATIN"": ""Imagery (NTM)"", ""UPDATE_SOU"": ""Digital Globe"", ""UPDATE_S_1"": ""2006\/04\/01"", ""UPDATE_S_2"": ""01APR2006_p1bs_005563509020_01_p001_gp"", ""FCODE"": ""AP030"", ""FCODE_DESC"": ""Road"", ""SHAPE_Leng"": 0.001146 }, ""geometry"": { ""type"": ""LineString"", ""coordinates"": [ [ 332198.673346261901315, 4022314.443548480048776, 3.834919462956784 ], [ 332222.627342973602936, 4022252.94134766003117, 3.839097781186059 ], [ 332244.979449089965783, 4022201.45840354077518, 3.837893504002183 ] ] } },```Is the parseGeoJson[JsonFeatureCollection] approach still the best approach under 10.2 to parse geojson.Thanks in advance Chris",10/17/16,1
lossyrob,@owcm can you put the geojson and parsing code into a Gist so I can run it and debug?,10/17/16,1
owcm,"@lossyrob Sure Gist is @ https://gist.github.com/owcm/ac9421ed390c73ec0b5b6847649f5304I think the issue might be that this geojson has a z value in addition to x,y coordinates.  My other files did not have the z value.  Just a hunch.   Trying to lose the z value to validate my hunch",10/17/16,1
owcm,@lossyrob ogrinfo indicates file contains Geometry: 3D Line String. So that might be the issue.,10/17/16,1
lossyrob,"that_Ñés a good clue, and if it_Ñés the case then that_Ñés a bug with our parser logic",10/17/16,1
metasim,"Given a `CellType`, is there a way of programmatically getting it's corresponding `NoDataCellType`?",10/17/16,1
metasim,"IOW, I want to enable `NoData` in a tile, but don't want to change the underlying storage mechanism. So I want to pass to `convert` the variant of what's already being used but with `NoData` set.",10/17/16,1
lossyrob,hm. I_Ñém not sure we have that. but we should,10/17/16,1
lossyrob,I think I_Ñéve been meaning to add that,10/17/16,1
lossyrob,`cellType.withConstantNoData`,10/17/16,1
lossyrob,or `cellType.withNoData(..)`,10/17/16,1
metasim,Exactly,10/17/16,1
metasim,If it already has `NoData` it would be a noop.,10/17/16,1
lossyrob,"Or if it had constant nodata, and you changed to userdefined, it would do that",10/17/16,1
metasim,Right.,10/17/16,1
lossyrob,or userdefined to some other userdefined value,10/17/16,1
metasim,Do all `convert` operations reallocate storage?,10/17/16,1
lossyrob,Yeah we def don_Ñét have that,10/17/16,1
lossyrob,yes they will,10/17/16,1
metasim,Or is it smart enough to know if it's only a `NoData` change?,10/17/16,1
lossyrob,nope not that smart. yet_Ñ_.,10/17/16,1
metasim,'k,10/17/16,1
metasim,Sounds like a feature request then.,10/17/16,1
lossyrob,yup,10/17/16,1
metasim,Want me to file something?,10/17/16,1
lossyrob,yeah filing an Issue would be good,10/17/16,1
lossyrob,"what happens when you have a raster that has for instance NODATA (Int.MinValue) in the raster, and you convert it to another type of nodata?",10/17/16,1
metasim,It would make sense for that to be a full copy conversion.,10/17/16,1
lossyrob,"convert might be different then the thing which doesn_Ñét edit the raster at all, which would be to change the interpretation of the values",10/17/16,1
lossyrob,"or maybe the interpretation is just for things that don_Ñét currently have a NoData value, and you_Ñére setting it for the first time",10/17/16,1
metasim,Here's my forinstance: Landsat images have an implicit `NO_DATA` value of zero. I want to enable it.,10/17/16,1
lossyrob,otherwise it does copy convert the previous ND values to the new ones,10/17/16,1
metasim,"Yes, that's my case.",10/17/16,1
lossyrob,yeah. I_Ñéve bumped up against that before,10/17/16,1
lossyrob,"well, things to think through for whoever codes up that feature :)",10/17/16,1
metasim,:-D,10/17/16,1
metasim,A full 10 band Landsat 8 image is over 1GB. Would be nice to enable `NoData` without a copy.,10/17/16,1
lossyrob,definitely,10/17/16,1
lossyrob,@owcm it does seem like the z coord is the problem,10/17/16,1
owcm,"Okay, something that should be reported as a bug then.  Were you expecting it to be something supported under the current implementation.",10/17/16,1
lossyrob,"yeah, I would expect to be able to at least read it without the Z values",10/17/16,1
lossyrob,"I think we hit against this at some point, and I thought we had gotten around it.. @moradology do you remember hitting up against this issue?",10/17/16,1
owcm,"Yea, that would be acceptable to me.",10/17/16,1
lossyrob,"@owcm I believe it would just take handling another case here, where it was `Seq(JsNumber(x), JsNumber(y), JsNumber(z))` (and ignoring the z) here: https://github.com/geotrellis/geotrellis/blob/master/vector/src/main/scala/geotrellis/vector/io/json/GeometryFormats.scala#L24-L32",10/17/16,1
lossyrob,If you felt like playing around with a solution to that issue,10/17/16,1
owcm,@lossyrob so create an issue and assign it to me.   I submit a fix when I think have one?,10/17/16,1
lossyrob,"For sure, you can make a WIP PR when your have even the start down if you want some feeback early on, or just let me know when there_Ñés a PR with tests passing and I_Ñéll review",10/17/16,1
owcm,Ok,10/17/16,1
initjsk,"Hi - Is there any best practice for reading in geotiffs from S3/hdfs? Tried to follow the emr-landsat demo to read them into an RDD but I get OutOfMemoryException when handling more than two images at once (~200M each). Have tried increasing driver and executor memory, but nothing changes. Any ideas as to what I am getting wrong?```	// Split        val retiled = source.mapPartitions({ x =>            for {                (extent, multibandTile) <- x                raster = Raster[MultibandTile](multibandTile, extent.extent)                layoutCols = math.ceil(raster.cols.toDouble / 256).toInt                layoutRows = math.ceil(raster.rows.toDouble / 256).toInt                chunk <- raster.split(TileLayout(layoutCols, layoutRows, 256, 256), Split.Options(cropped = false, extend = false))            }            yield {                (TemporalProjectedExtent(chunk.extent, extent.crs, extent.instant), chunk.tile)            }        }, preservesPartitioning = true)            .repartition(8*16)                // Create context RDD        val (_, layerMetadata) = TileLayerMetadata.fromRdd[TemporalProjectedExtent, MultibandTile, SpaceTimeKey](retiled, FloatingLayoutScheme(256))        val tiled = retiled.tileToLayout(layerMetadata)        val contextRdd = ContextRDD(tiled, layerMetadata)                // Calculate NDVI        val ndvi = contextRdd            .map(x => NDVI(x._2))```",10/17/16,1
lossyrob,"Is ~200M the size compressed? because that might not reflect the true size in memory. How are you reading the source RDD? What is the machien setup, is this on a local machine? What are the executor/driver memory settings?",10/17/16,1
initjsk,"@lossyrob Uncompressed 10,000 x 10,000 x 16bit. I am reading source RDD directly from HDFS with newAPIHadoopRDD. Right now I am running on a local machine.  3 executors with 3gb memory and a driver with memory of 4gb.",10/17/16,1
lossyrob,"in local mode, the executor memory doesn_Ñét have an effect, it all uses driver mem",10/17/16,1
lossyrob,"I ask about the source RDD, because you might want to place them each into their own partition (which would happen before the repartition at the split code)",10/17/16,1
lossyrob,"not sure where the out of memory is occuring though, before or after the split repartition",10/17/16,1
metasim,"Does GeoTrellis support setting a color palette/lookup table when writing GeoTiffs? I'm not seeing it, but just wanted to confirm.",10/17/16,1
lossyrob,"we have worked with them for reading I believe, but not for writing",10/17/16,1
metasim,Basically want to be able to do what you can with `geotrellis.raster.render.png.Settings`,10/17/16,1
metasim,OK,10/17/16,1
lossyrob,"I can_Ñét remember if we actually implemented it in the reader, or it_Ñés something we needed in the reader but then the use case got turned around and we didn_Ñét end up implementing it",10/17/16,1
initjsk,"Thanks! I was running deploy-mode cluster on local machine with yarn installed, so assumed executor mem would be counted in. Changed to local and upped driver-mem and I see that OOM threshold definitely increased. How exactly would you place each of them in  their own partition?",10/17/16,1
lossyrob,just repartition so that there_Ñés one partition per file,10/17/16,1
lossyrob,"if you_Ñére loading in via some client read, you can do that before the client read so that it_Ñés a path-per-partition",10/17/16,1
wsf1990,,10/18/16,1
wsf1990,"When I changed the gt version to `1.0.0-40a2f7a` and start my app,It shown this error:",10/18/16,1
wsf1990,"```Caused by: java.lang.NoSuchMethodError: scala.collection.mutable.ArrayOps.$plus$colon(Ljava/lang/Object;Lscala/reflect/ClassTag;)Ljava/lang/Object;	at geotrellis.kryo.AvroRegistrator$.apply(AvroRegistrator.scala:10)```",10/18/16,1
wsf1990,this 'AvroRegistrator' class is which in chatta-demo ,10/18/16,1
wsf1990,"```def apply(conf: SparkConf) =    conf.registerKryoClasses(      new Field(""a"", Schema.create(Type.NULL), null, null).order.getClass +: classOf[org.apache.avro.Schema].getDeclaredClasses    )```",10/18/16,1
wsf1990,the more stacks is this:,10/18/16,1
wsf1990,"```[ERROR] [10/18/2016 17:45:14.992] [sjzx-through-akka.actor.default-dispatcher-5] [akka://sjzx-through/user/through-service] nullakka.actor.ActorInitializationException: exception during creation	at akka.actor.ActorInitializationException$.apply(Actor.scala:166)	at akka.actor.ActorCell.create(ActorCell.scala:596)	at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:456)	at akka.actor.ActorCell.systemInvoke(ActorCell.scala:478)	at akka.dispatch.Mailbox.processAllSystemMessages(Mailbox.scala:263)	at akka.dispatch.Mailbox.run(Mailbox.scala:219)	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:397)	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)Caused by: java.lang.reflect.InvocationTargetException	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)	at java.lang.reflect.Constructor.newInstance(Constructor.java:422)	at akka.util.Reflect$.instantiate(Reflect.scala:66)	at akka.actor.ArgsReflectConstructor.produce(Props.scala:352)	at akka.actor.Props.newActor(Props.scala:252)	at akka.actor.ActorCell.newActor(ActorCell.scala:552)	at akka.actor.ActorCell.create(ActorCell.scala:578)	... 9 moreCaused by: java.lang.NoSuchMethodError: scala.collection.mutable.ArrayOps.$plus$colon(Ljava/lang/Object;Lscala/reflect/ClassTag;)Ljava/lang/Object;	at geotrellis.kryo.AvroRegistrator$.apply(AvroRegistrator.scala:10)	... 18 moreUncaught error from thread [sjzx-through-akka.actor.default-dispatcher-5] shutting down JVM since 'akka.jvm-exit-on-fatal-error' is enabled for ActorSystem[sjzx-through]java.lang.NoSuchMethodError: scala.Predef$.ArrowAssoc(Ljava/lang/Object;)Ljava/lang/Object;	at spray.http.parser.HttpParser$$anonfun$3.apply(HttpParser.scala:57)	at spray.http.parser.HttpParser$$anonfun$3.apply(HttpParser.scala:56)	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108)	at scala.collection.TraversableLike$class.map(TraversableLike.scala:244)	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:108)	at spray.http.parser.HttpParser$.<init>(HttpParser.scala:58)	at spray.http.parser.HttpParser$.<clinit>(HttpParser.scala)	at spray.can.server.ServerSettings$.fromSubConfig(ServerSettings.scala:108)	at spray.can.server.ServerSettings$.fromSubConfig(ServerSettings.scala:60)[ERROR] [10/18/2016 17:45:15.062] [sjzx-through-akka.actor.default-dispatcher-5] [ActorSystem(sjzx-through)] Uncaught error from thread [sjzx-through-akka.actor.default-dispatcher-5] shutting down JVM since 'akka.jvm-exit-on-fatal-error' is enabledjava.lang.NoSuchMethodError: scala.Predef$.ArrowAssoc(Ljava/lang/Object;)Ljava/lang/Object;	at spray.http.parser.HttpParser$$anonfun$3.apply(HttpParser.scala:57)	at spray.http.parser.HttpParser$$anonfun$3.apply(HttpParser.scala:56)	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108)	at scala.collection.TraversableLike$class.map(TraversableLike.scala:244```",10/18/16,1
wsf1990,Dose it caused by the version of spark.which I used is 1.6.0,10/18/16,1
pomadchin,`1.0.0-40a2f7a` supports spark 2 only,10/18/16,1
wsf1990,"sure,gt dosen't support the spark < 2.0?",10/18/16,1
pomadchin,yep,10/18/16,1
pomadchin,"gt 1.0.0 - x can be built only against scala 2.11, java 8(+?) and spark 2+",10/18/16,1
fosskers,@wsf1990 GeoTrellis `0.10.3` is our last Spark 1.6 release,10/18/16,1
Sharathmk99,"Hi. I need to give threshold for NDVI values like, if NDVI values is >0.5 then red else green..",10/18/16,1
Sharathmk99,Please help @pomadchin ,10/18/16,1
moradology,"hey @Sharathmk99 that should be no problem. assuming you've already calculated your NDVI, you should be able to render the tiles with those NDVI values after importing the `render` package from `geotrellis.raster`",10/18/16,1
moradology,I'll track you down a quick example,10/18/16,1
moradology,https://github.com/geotrellis/geotrellis/blob/master/raster-test/src/test/scala/geotrellis/raster/render/jpg/RenderJpgTests.scala#L95,10/18/16,1
moradology,you can see the breakpoints (the keys/indices) and the colors to use (the values) of the `ColorMap` being assigned and immediately used thereafter,10/18/16,1
moradology,"if anything i've said is less than clear, feel free to tell me :)",10/18/16,1
wsf1990,@fosskers Thanks!,10/19/16,1
Sharathmk99,@moradology thank you,10/19/16,1
Sharathmk99,I'll try,10/19/16,1
wsf1990,"When I run app based on gt `1.0.0-40a2f7a` on spark 2.0.0,it also token this error on start: ",10/19/16,1
wsf1990,"```Exception in thread ""main"" java.lang.VerifyError: Inconsistent stackmap frames at branch target 152Exception Details:  Location:    akka/dispatch/Mailbox.processAllSystemMessages()V @152: getstatic  Reason:    Type top (current frame, locals[9]) is not assignable to 'akka/dispatch/sysmsg/SystemMessage' (stack map, locals[9])  Current Frame:    bci: @131    flags: { }    locals: { 'akka/dispatch/Mailbox', 'java/lang/InterruptedException', 'akka/dispatch/sysmsg/SystemMessage', top, 'akka/dispatch/Mailbox', 'java/lang/Throwable', 'java/lang/Throwable' }    stack: { integer }  Stackmap Frame:    bci: @152    flags: { }    locals: { 'akka/dispatch/Mailbox', 'java/lang/InterruptedException', 'akka/dispatch/sysmsg/SystemMessage', top, 'akka/dispatch/Mailbox', 'java/lang/Throwable', 'java/lang/Throwable', top, top, 'akka/dispatch/sysmsg/SystemMessage' }    stack: { }  Bytecode:    0x0000000: 014c 2ab2 0132 b601 35b6 0139 4db2 013e    0x0000010: 2cb6 0142 9900 522a b600 c69a 004b 2c4e    0x0000020: b201 3e2c b601 454d 2db9 0148 0100 2ab6    0x0000030: 0052 2db6 014b b801 0999 000e bb00 e759    0x0000040: 1301 4db7 010f 4cb2 013e 2cb6 0150 99ff    0x0000050: bf2a b600 c69a ffb8 2ab2 0132 b601 35b6    0x0000060: 0139 4da7 ffaa 2ab6 0052 b600 56b6 0154    0x0000070: b601 5a3a 04a7 0091 3a05 1905 3a06 1906    0x0000080: c100 e799 0015 1906 c000 e73a 0719 074c    0x0000090: b200 f63a 08a7 0071 b201 5f19 06b6 0163    0x00000a0: 3a0a 190a b601 6899 0006 1905 bf19 0ab6    0x00000b0: 016c c000 df3a 0b2a b600 52b6 0170 b601    0x00000c0: 76bb 000f 5919 0b2a b600 52b6 017a b601    0x00000d0: 80b6 0186 2ab6 018a bb01 8c59 b701 8e13    0x00000e0: 0190 b601 9419 09b6 0194 1301 96b6 0194    0x00000f0: 190b b601 99b6 0194 b601 9ab7 019d b601    0x0000100: a3b2 00f6 3a08 b201 3e2c b601 4299 0026    0x0000110: 2c3a 09b2 013e 2cb6 0145 4d19 09b9 0148    0x0000120: 0100 1904 2ab6 0052 b601 7a19 09b6 01a7    0x0000130: a7ff d62b c600 09b8 0109 572b bfb1       Exception Handler Table:    bci [290, 307] => handler: 120  Stackmap Table:    append_frame(@13,Object[#231],Object[#177])    append_frame(@71,Object[#177])    chop_frame(@102,1)    full_frame(@120,{Object[#2],Object[#231],Object[#177],Top,Object[#2],Object[#177]},{Object[#223]})    full_frame(@152,{Object[#2],Object[#231],Object[#177],Top,Object[#2],Object[#223],Object[#223],Top,Top,Object[#177]},{})    append_frame(@173,Object[#357])    full_frame(@262,{Object[#2],Object[#231],Object[#177],Top,Object[#2]},{})    same_frame(@307)    same_frame(@317)```",10/19/16,1
wsf1990,what caused this?,10/19/16,1
pomadchin,@wsf1990 what Java version are you using?,10/19/16,1
fosskers,what a strange error,10/19/16,1
pomadchin,@fosskers i believe some versions mismatch ,10/19/16,1
lossyrob,haven_Ñét seen that one before,10/19/16,1
lossyrob,I would recommend doing a heavy clean of the binaries and rebuild,10/19/16,1
wsf1990,@pomadchin  @fosskers  @lossyrob I have resolved this.It's caused by akka version.when you use cdh and spark2.0.0-beta you must change the akka version to 2.3.14(<2.4).,10/20/16,1
wsf1990,jdk is 1.8,10/20/16,1
wsf1990,thanks!,10/20/16,1
lossyrob,can I ask why your using spark2.0.0-beta and not the official release? They are actually on 2.0.1 now,10/20/16,1
wsf1990,I used cloudera to deploy hadoop and spark cluster,10/20/16,1
pomadchin,@wsf1990 hi! GeoTrellis not depends on akka (we use it in vector tests only),10/20/16,1
pomadchin,i believe it caused by spray / smth like that; so it_Ñés more a client code,10/20/16,1
wsf1990,"yes,but my app used it to start service like what chatta-demo do.",10/20/16,1
pomadchin,ha; gotcha,10/20/16,1
jnh5y,@echeipesh I just happened to stumble past the Eclipse portal; you should vote on your new GeoTrellis committers;),10/21/16,1
lossyrob,@jnh5y i was waiting to remind him because I have a couple more I_Ñém trying to get into the pipeline,10/21/16,1
lossyrob,they need to be approved for eclipse accounts before I can nominate them,10/21/16,1
lossyrob,and then figure it_Ñés easier with just 3 committers to get the nomination process complete,10/21/16,1
lossyrob,but I guess the cat_Ñés out of the bag now ;),10/21/16,1
lossyrob,also @echeipesh do you have a gmail filter that gets rid of those emails? there were a bunch of them during the nominating process ,10/21/16,1
mteldridge,"Does geotrellis support a urn-based projection spec?  I have a GeoJson file that is specifying the CRS as ""urn:ogc:def:crs:OGC:1.3:CRS84"".  Calling CRS.fromName with this string is failing: ""Unable to access CRS file: /geotrellis/proj4/nad/ogc"".  Perhaps I'm doing something wrong...",10/21/16,1
lossyrob,It handles a couple of authories but mainly EPSG,10/21/16,1
lossyrob,is that just EPSG:4326?,10/21/16,1
lossyrob,"I guess the answer is, no I don_Ñét believe we handle urn based projection specs",10/21/16,1
mteldridge,"Thanks for the quick response.  Yes, this is just 4326.  I was hoping generally not to have to interpret the CRS as specified, just let geotrellis handle it.  The javadocs for geotrellis.vector.io.json.NamedCRS had the following note:  ""OGC CRS URNs such as urn:ogc:def:crs:OGC:1.3:CRS84 shall be preferred over legacy identifiers such as EPSG:4326"", so I was hopeful.  :)",10/21/16,1
lossyrob,haha really,10/21/16,1
lossyrob,github is down,10/21/16,1
lossyrob,(for our office),10/21/16,1
lossyrob,but I_Ñém going to look in the code for that,10/21/16,1
lossyrob,and do a git blame :),10/21/16,1
lossyrob,@moradology I think that was you_Ñ_do you remember putting that docstring in? and why?,10/21/16,1
jnh5y,@lossyrob oops;) sorry about that; just trying to 'help';),10/21/16,1
Sharathmk99,"I have a Geotiff image of 3 bands, now I want to separate all 3 bands and save as new Geotiff image. How to do this. Please help.",10/22/16,1
Sharathmk99,"I'm getting below error,",10/22/16,1
Sharathmk99,Compression type JPEG of not supported by this reader,10/22/16,1
Sharathmk99,Please help,10/22/16,1
laursen86,Someone who have experience with working with the OGC gml file format in GeoTrellis? I am going to use it to removed the features in a set which intersect with a cloud mask defined in the gml file.,10/23/16,1
lossyrob,"hey @laursen86, we don_Ñét have a GML reader in GeoTrellis. If GeoTools has one, you could read SimpleFeatures out of there, and convert it to GeoTrellis types via some simple conversion methods",10/24/16,1
lossyrob,"@Sharathmk99 we don_Ñét currently handle jpeg compression. so If that_Ñés the compression, that_Ñés why you are getting that error",10/24/16,1
laursen86,"Thanks for the answer @lossyrob, I will do that then :-)",10/24/16,1
echeipesh,"@mteldridge Yeah, that doc string was from GeoJSON spec and those couple of classes try to deal with `crs` fields in GeoJSON geometries. I was literally yeterday mulling over how to overall that part of the API and how much to support the OGC URNs for CRS.",10/24/16,1
wsf1990,"Hi, I'm very strange why my tile has a white edge when i use `tile.mask(extent, poly)`",10/25/16,1
wsf1990,[![tile.jpg](https://files.gitter.im/geotrellis/geotrellis/MWp8/thumb/tile.jpg)](https://files.gitter.im/geotrellis/geotrellis/MWp8/tile.jpg),10/25/16,1
wsf1990,just like this,10/25/16,1
wsf1990,if dont use mask the result is ok,10/25/16,1
wsf1990,the poly is a big region which conclude all these tiles,10/25/16,1
wsf1990,"I've resloved this ,I used TileLayerMetadata.extent not TileLayerMetadata.mapTransform(key) to get the extent.thanks!",10/25/16,1
lossyrob,":+1: didn_Ñét have any ideas, glad you figured it out :)",10/25/16,1
pvillard31,"Hi, I'm completely new to GeoTrellis and I'm trying to play with it from a Spark Zeppelin notebook. Is there a requirement regarding the java version? I get an UnsupportedClassVersionError with proj4j when using java 7.",10/25/16,1
lossyrob,"If you_Ñére using snapshot versions (the 1.0 series), then java 8 is required. 0.10 still supports java 7",10/25/16,1
pomadchin,"+> Java 8, Scala 2.11 and Spark 2+",10/25/16,1
MichiB7_twitter,"Hi, I'm new to GeoTrellis and I'm trying out the 'geotrellis-landsat-tutorial'. Everythink works fine until step 'Ingest the multiband geotiff into a GeoTrellis catalog'. When I'm running the class IngestImage I get following error:16/10/25 16:38:31 INFO SparkContext: Created broadcast 0 from newAPIHadoopRDD at HadoopSparkContextMethods.scala:8316/10/25 16:38:31 INFO SparkUI: Stopped Spark web UI at http://10.20.8.246:404016/10/25 16:38:31 INFO DAGScheduler: Stopping DAGScheduler16/10/25 16:38:31 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!16/10/25 16:38:31 INFO MemoryStore: MemoryStore cleared16/10/25 16:38:31 INFO BlockManager: BlockManager stopped16/10/25 16:38:31 INFO BlockManagerMaster: BlockManagerMaster stopped16/10/25 16:38:31 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!16/10/25 16:38:31 INFO SparkContext: Successfully stopped SparkContextException in thread ""main"" org.apache.hadoop.mapreduce.lib.input.InvalidInputException: Input path does not exist: file:/D:/Users/Java/git/geotrellis-landsat-tutorial/.gitignoreInput path does not exist: file:/D:/Users/Java/git/geotrellis-landsat-tutorial/.idea/.nameInput path does not exist: file:/D:/Users/Java/git/geotrellis-landsat-tutorial/target/.history	at org.apache.hadoop.mapreduce.lib.input.FileInputFormat.listStatus(FileInputFormat.java:285)	at org.apache.hadoop.mapreduce.lib.input.FileInputFormat.getSplits(FileInputFormat.java:340)	at org.apache.spark.rdd.NewHadoopRDD.getPartitions(NewHadoopRDD.scala:115)	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:239)	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:237)	at scala.Option.getOrElse(Option.scala:120)	at org.apache.spark.rdd.RDD.partitions(RDD.scala:237)	at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35)	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:239)	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:237)	at scala.Option.getOrElse(Option.scala:120)	at org.apache.spark.rdd.RDD.partitions(RDD.scala:237)	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1944)	at org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1007)	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:147)	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:108)	at org.apache.spark.rdd.RDD.withScope(RDD.scala:310)	at org.apache.spark.rdd.RDD.reduce(RDD.scala:989)	at geotrellis.spark.TileLayerMetadata$.collectMetadataWithCRS(TileLayerMetadata.scala:127)	at geotrellis.spark.TileLayerMetadata$.fromRdd(TileLayerMetadata.scala:174)	at tutorial.IngestImage$.run(IngestImage.scala:67)	at tutorial.IngestImage$.main(IngestImage.scala:43)	at tutorial.IngestImage.main(IngestImage.scala)	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)	at java.lang.reflect.Method.invoke(Method.java:498)	at com.intellij.rt.execution.application.AppMain.main(AppMain.java:144)16/10/25 16:38:31 INFO ShutdownHookManager: Shutdown hook calledDoes anybody has an idea whats wrong?Thanks for help!",10/25/16,1
pomadchin,@MichiB7_twitter hm it says you have incorrect paths,10/25/16,1
pomadchin,`file:/D:/Users/Java/git/geotrellis-landsat-tutorial/.idea/.name`,10/25/16,1
MichiB7_twitter,"When I change the input path, I get another error. So it seems to me, that the r-nir.tif is found. ",10/25/16,1
MichiB7_twitter,I think I got a solution. I'm working on a Windows system. When I use for input path 'file:///.....' (with 3 slashes) instead of 'file://....' then it works. ,10/25/16,1
pomadchin,ah,10/25/16,1
pomadchin,"yep, it_Ñés safer to provide paths with fs idenitfier ",10/25/16,1
pomadchin,cool that you figured it out :) ,10/25/16,1
pomadchin,"btw, you can run GeoDocker environment on Windows",10/25/16,1
pomadchin,it_Ñés tested and really works :D ,10/25/16,1
pvillard31,"On my side: Spark 1.6.2, Java 7, Scala 2.10, and getting java.lang.UnsupportedClassVersionError: org/osgeo/proj4j/CRSFactory : Unsupported major.minor version 52.0",10/25/16,1
MichiB7_twitter,Are there some tutorials for it? I don't really know Docker so far...,10/25/16,1
pvillard31,and using package 0.10.3,10/25/16,1
lossyrob,@pvillard31 was your assembly compiled with java 8,10/25/16,1
lossyrob,e.g. are you compiling with java 8 and running with java 7,10/25/16,1
pvillard31,@lossyrob directly calling maven dependencies from my notebook,10/25/16,1
pomadchin,"@MichiB7_twitter * https://github.com/geodocker/geodocker * https://github.com/azavea/geotrellis-geomesa-template-project",10/25/16,1
pomadchin,all environment would be launched just by writing `docker-compose up` ;),10/25/16,1
lossyrob,ah. then I think I mispoke; 0.10 also needs java 8,10/25/16,1
lossyrob,"(when I published 0.10.3, I had compiled the jars with java 8)",10/25/16,1
MichiB7_twitter,@pomadchin Thank you very much for your support :-D,10/25/16,1
pvillard31,"ok will upgrade java, just wanted a confirmation before taking that path, otherwise I'll go with a 0.10.2 version, just running some basic tests so far ;) thanks!",10/25/16,1
lossyrob,no problem! sorry for the confusion!,10/25/16,1
metasim,"I'm using `ArrayMultibandTile.foreachDouble(f: Array[Double] => Unit)` to linearize and transpose the pixel values across multiple bands into a two-dimensional table. For each row/column value, `foreachDouble` allocates a new `Array[Double]`, fills it up, and passes it to the given function. This array allocation and data copying is currently my performance bottleneck (I understand why it's necessary given the function prototype). If performance is of interest, is there a better way of iterating over values across multiple tiles?",10/25/16,1
lossyrob,"if you have a set of specific bands that don_Ñét outnumber what we have functions for, you can use those specific bands",10/25/16,1
lossyrob,which means you have to bake in the band number into the type system ,10/25/16,1
lossyrob,"it_Ñés a bit funky since we have some macro generation going on, I believe",10/25/16,1
lossyrob,"I might be thinking about _ÑÒcombine""",10/25/16,1
lossyrob,"I forget what the justification was for not reusing one array was there, if I was the one that coded that",10/25/16,1
lossyrob,but that would give a performance boost,10/25/16,1
lossyrob,"ah no that was someone else. That_Ñés my bad for code review. There should be only one array allocation, not per cell",10/25/16,1
lossyrob,"pretty simple fix, do you want to make it, test that it helps your peformance, and PR it?",10/25/16,1
pvillard31,"i've created a fork of the repo and set up a dev environment to run some tests and improve my knowledge by going through the code but I've compilation errors (even on branch 0.10). Example: class MultibandTile, the MacroCombinableMultibandTile reference is unknown. Is it normal? Am I missing something here?",10/26/16,1
pomadchin,@pvillard31 how do you compile it?,10/26/16,1
pomadchin,/ trying to compile,10/26/16,1
pomadchin,if you are trying to compile using some ide / etc/ be sure that `sourceManaged` (`/target/scala-{2.10|2.11}/src_managed`) dir is recognized by your ide as a source directory for all projects,10/26/16,1
pvillard31,"@pomadchin i'm working on ij ide, according to my module settings all src_managed directories are considered as source folders for each project",10/26/16,1
pomadchin,so the problem that mb idea not recognizes the way we generate boilerplate macros,10/26/16,1
pomadchin,"try to compile using common sbt, from terminal",10/26/16,1
pvillard31,ok,10/26/16,1
pomadchin,"btw, for me idea highlights with red most of lines inside build.sbt files for subprojects",10/26/16,1
pvillard31,same,10/26/16,1
pomadchin,how do you compile code?,10/26/16,1
pvillard31,"I let idea do all the job, i'm currently trying to compile it with sbt",10/26/16,1
pvillard31,strange thing is I don't have src_managed directories in target folders,10/26/16,1
pomadchin,have a look into `scripts/buildall.sh` file;,10/26/16,1
pomadchin,mb you are doing smth wrong O:,10/26/16,1
pomadchin,"and before the build process, cleanup all target dirs of all projects, as idea could make smth weird",10/26/16,1
pvillard31,"sbt compile was ok, I'm running the buildall.sh",10/26/16,1
pvillard31,mb I need some a specific plugin I don't have?,10/26/16,1
pomadchin,not sure; I use idea only as a code editor O: ,10/26/16,1
pomadchin,but don_Ñét remember any plugins,10/26/16,1
pomadchin,want to say that _tests_ never even tried to compile using idea; so this macro problem is not the biggest one :D (definitely idea fail),10/26/16,1
pvillard31,"some tests failed but all are related to postgres so I believe this is expected. I do now have src_managed folders generated... it solved a lot of compilations issues, but it remains two :/ and it is kind of cryptic to me. Example:`Error:scalac: Error: assertion failed: List(object package$RGBA, object package$RGBA)java.lang.AssertionError: assertion failed: List(object package$RGBA, object package$RGBA)	at scala.reflect.internal.Symbols$Symbol.suchThat(Symbols.scala:1678)...`",10/26/16,1
pomadchin,in what subproject?,10/26/16,1
pvillard31,"Error:scalac:      while compiling: .../geotrellis/raster/target/scala-2.10/src_managed/main/geotrellis/raster/SegmentCombiner.scala",10/26/16,1
pomadchin,what branch / tag is it?,10/26/16,1
pvillard31,currently trying on branch 0.10,10/26/16,1
pomadchin,and what os?,10/26/16,1
pvillard31,mac os x,10/26/16,1
pomadchin,well,10/26/16,1
pomadchin,._.,10/26/16,1
pomadchin,and sbt version? do you use local ./sbt ?,10/26/16,1
pomadchin,i know it should not affect somehow but,10/26/16,1
pvillard31,"what is killing me is that `sbt compile`is OK, it is failing when I try to run a unit test into idea",10/26/16,1
pvillard31,sbt 0.13 btw,10/26/16,1
pomadchin,ah,10/26/16,1
pomadchin,yes,10/26/16,1
pomadchin,unit tests are not working with idea as well,10/26/16,1
pomadchin,i mentioned it somewhere above,10/26/16,1
pomadchin,if you_Ñéll find a way to compile / run tests using intelij idea that would be super useful for some of our users ^^',10/26/16,1
pvillard31,oh! I now understand what you meant before ^^,10/26/16,1
pvillard31,how do you test you ut then? using sbt? :),10/26/16,1
pomadchin,"yes, definitely",10/26/16,1
pomadchin,just by writing _ÑÒtest_Ñù in terminal,10/26/16,1
pvillard31,"ok thanks a lot! if I find something I'll let you know, but I'll go this way for now :)",10/26/16,1
lossyrob,"also if you want to run just a single test, you can do `test-only package.name.NameOfSpec`",10/26/16,1
lossyrob,or `test-only *NameOfSpec`,10/26/16,1
pvillard31,"@lossyrob good to know, thanks",10/26/16,1
pomadchin,"to select subproject:```/> ./sbt /> project accumulo/> test```",10/26/16,1
lossyrob,"also if you are in the sbt console (by just starting up `./sbt` without a command), and you want to run the tests whenver you save a file, it_Ñés `~test-only *NameOfSpec`",10/26/16,1
lossyrob,"notice that for `raster` and `vector`, the tests are in separete subprojects, `raster-test` and `vector-tests` (we should get around to moving them to one subproject eventually)",10/26/16,1
pvillard31,"ok, well noted",10/26/16,1
pvillard31,"just realized the downside is you can't put breakpoint in the code, correct?",10/26/16,1
pomadchin,"1. breakpoints in scala (in general, at least in idea) are not extremely useful 2. guess we cant :D (as we cant even compile using idea)",10/26/16,1
lossyrob,@pvillard31 that is true.,10/26/16,1
metasim,"@lossyrob Thanks for the response on the array allocation issue. I'll take a look at the implementation, but one concern I'd have about reusing the array is around mutability semantics: If your lambda receives an array, is it yours to do whatever you want with, or might it be changed from underneath you? If a `Seq` is passed in, you're guaranteeing immutabiliy, wity an array, it's less clear what the promises are (and allocating an array per cell as it does now is the safest approach).",10/26/16,1
lossyrob,"The Array is mutable, but also you_Ñére getting it from right before the lambda call. And the lambda could edit the array, but it gets reset on the next call. so if you_Ñére looking for immutability gauantees, that won_Ñét give it to you (besides practically). But this is a good example of pureness vs performance where I would side with the latter",10/26/16,1
lossyrob,"Is there a way to internally use a once-allocated array, but pass a wrapped version that is immutable to the user, without any performance cost?",10/27/16,1
ehochmuth,It looks like the 0.10 branch has some compatibility issues with Spark 2.0. Whats the general timeline for cutting a new release from master with Spark 2.0 support?,10/27/16,1
pomadchin,"@ehochmuth geotrellis 0.10 not supports spark 2+; but yes, geotrellis 1.0.0+  supports only spark 2+ not sure about a certain date, but smth around one moth definitely",10/27/16,1
pomadchin,we need to close our milestone: https://github.com/geotrellis/geotrellis/milestone/9,10/27/16,1
initjsk,Hi. I am combining multiple uint16raw tiles into one single float32raw tile. Is this possible without first converting the uint16raw tiles to float32raw?,10/27/16,1
pomadchin,"Hi @initjsk! not possible with our curent API, but it would be avaible after release: https://github.com/geotrellis/geotrellis/issues/1410",10/27/16,1
schBen,Hi! What is the best way to read in public Sentinel2 data from Amazon (http://sentinel-pds.s3-website.eu-central-1.amazonaws.com/) into RDDs? The source format is JPEG2000.,10/27/16,1
pomadchin,"@schBen convert into tiff first; usually you can do smth like```gdal_translate -of GTiff -co COMPRESS=JPEG file.jp2 file.tif```",10/27/16,1
pomadchin,but we don_Ñét support jpeg compression yet,10/27/16,1
pomadchin,so just a common preingest convertion is the only way right now ):,10/27/16,1
pomadchin,So it_Ñés in case you are talking about reading Sentinel2 data using GeoTrellis :D ,10/27/16,1
schBen,@pomadchin Okay thank you. I saw that there is also a GeoTrellis-GDAL project. Can I use that somehow for convenience?,10/27/16,1
pomadchin,"definitely you can; it_Ñés in a separate repo right now, and it_Ñés a bit outdated ):",10/27/16,1
pomadchin,"so you can fix deps, and it_Ñés ready to use 100%; it_Ñés just a wrapper on top of gdal java dep, and it uses gdal jni bindings",10/27/16,1
pomadchin,any questions related to geotrellis gdal usage you can throw there as well ;),10/27/16,1
schBen,okay I will check that out...,10/27/16,1
schBen,thank you,10/27/16,1
pomadchin,no problems; be sure that you have installed gdal with jni bindings on all nodes ._. if you are going to use it with spark ofc,10/27/16,1
ehochmuth,@pomadchin thanks for the update,10/27/16,1
metasim,"I'm getting the following warning and am trying to trace in my code where I might be causing it:```WARN  geotrellis.raster.ShortUserDefinedNoDataArrayTile - Conversion from int16ud-9999 to float32 may lead to data loss.```However, when I look in `geotrellis.raster.ShortUserDefinedNoDataArrayTile` I don't see any logging operations  to place a breakpoint on. Where should I be looking for logging calls?",10/27/16,1
lossyrob,"@initjsk I believe if you convert the one, and then combine with that always as the left operator  (the calling source), you shouldn't need to convert all",10/27/16,1
owcm,"I have written code with Geotrellis to rasterize vector line and multiline data from geojson format .   I am working with vector polygon data now, reading in from geojson.   Trying to rasterize the polygons.   I wondered if anyone knows of a gist or sample project that had a good example of reading in polygons from  geojson.  Thanks in advance.",10/27/16,1
moradology,@metasim I think the spot you're looking for is just a bit up the chain of inheritance: https://github.com/geotrellis/geotrellis/blob/ecde956ed8f3a5312265655545a6bb2aab00c9bb/raster/src/main/scala/geotrellis/raster/ArrayTile.scala#L45,10/27/16,1
metasim,Super! Many thanks!,10/27/16,1
metasim,Very helpful.,10/27/16,1
moradology,you're not making an explicit call to `convert` i assume?,10/27/16,1
metasim,I /thought/ I was.... and want to be.,10/27/16,1
metasim,Looks like I am doing an explict call.... `ArrayMultibandTile.convert`,10/27/16,1
metasim,I convert cell type before computing ratio ops on multiple bands.,10/27/16,1
metasim,I'm going from a UInt16 to Float32.... should I do it some other way besides the `convert` method?,10/27/16,1
wsf1990,@owcm I think you can use this sample:,10/28/16,1
wsf1990,"```import geotrellis.vector.io.json.Implicits._geoJson.parseGeoJson[Polygon]```",10/28/16,1
owcm,@wsf1990 thank you',10/28/16,1
owcm,Another question I have a RDD of type TileLayerRDD[SpatialKey].   I want to rip through the tiles in the RDD and write them out as individual GeoTiffs.   Of course I need an extent for the individual tile to create the Geotiff.   Is there a  transformation routine to turn the SpatialKey for the current tile into an extent?,10/28/16,1
pomadchin,"hi! @owcm ```scalaval rdd: TileLayerRDD[SpatialKey] = ???val rastersRdd: RDD[SpatialKey, Raster[Tile]] = rdd.asRasters()```",10/28/16,1
pomadchin,and after that you can map over these pairs and to save `Raster[Tile]`s as GeoTiffs,10/28/16,1
pomadchin,`Raster` is a pair of `Tile` and `Extent` :) so we have a function to save `Raster[_ <: Tile]` types,10/28/16,1
pomadchin,"@owcm oh sry, looked through the api, ```scalaval raster: Raster[Tile] = ???val crs: CRS = ???GeoTiff(raster, crs).write(path)```",10/28/16,1
owcm,"@pomadchin so would  the following be a valid approach```val (_, elevRDDLatLng) = packResultRDDSwap.reproject(_ÑŒ      LatLng,_ÑŒ      FloatingLayoutScheme(512),_ÑŒ      Options(_ÑŒ        method = Bilinear,_ÑŒ        errorThreshold = 0.125_ÑŒ      ))_ÑŒ_ÑŒ_ÑŒ    var geotiffCnt = 1_ÑŒ    var asRasters = elevRDDLatLng.asRasters()_ÑŒ_ÑŒ    _ÑŒ    asRasters.foreach( f => {_ÑŒ_ÑŒ      var gttiff = GeoTiff( f._2, LatLng ).write(""test"" + geotiffCnt + "".tif"")_ÑŒ_ÑŒ      geotiffCnt = geotiffCnt + 1_ÑŒ      _ÑŒ    }) ```",10/28/16,1
owcm,"@pomadchin ```  val (_, elevRDDLatLng) = packResultRDDSwap.reproject(      LatLng,      FloatingLayoutScheme(1024),      Options(        method = Bilinear,        errorThreshold = 0.125      ))    var geotiffCnt = 1    var asRasters = elevRDDLatLng.asRasters()    asRasters.foreach( f => {      GeoTiff( f._2, LatLng ).write(""test"" + geotiffCnt + "".tif"")      geotiffCnt = geotiffCnt + 1    })```Cleaner version.   I tried this approach and I get gaps in the geotiffs written out.  Missing tiles.",10/28/16,1
pomadchin,@owcm let me think a bit,10/28/16,1
pomadchin,"@owcm ```scalaimport geotrellis.spark.render._  val (_, elevRDDLatLng) = packResultRDDSwap.reproject(      LatLng,      FloatingLayoutScheme(1024),      Options(        method = Bilinear,        errorThreshold = 0.125      ))    var geotiffCnt = 1    var rddGeotiffs: RDD[(SpatialKey, SinglebandGeoTiff)] = elevRDDLatLng.renderGeoTiff()rddGeotiffs.foreach( geotiff => {      geotiff.write(""test"" + geotiffCnt + "".tif"")      geotiffCnt = geotiffCnt + 1    })```",10/28/16,1
pomadchin,"but yes, your code is absolutely correct as well",10/28/16,1
pomadchin,(:,10/28/16,1
pomadchin,this variant is a bit shorter,10/28/16,1
lossyrob,you can_Ñét count in a var over an RDD,10/28/16,1
lossyrob,so that count won_Ñét work. what you could do is use a spark Accumulator,10/28/16,1
lossyrob,http://www.edureka.co/blog/spark-accumulators-explained,10/28/16,1
pomadchin,oh :0 haven't noticed var usage outside of rdd,10/28/16,1
owcm,@lossyrob Hence why I might be missing tiles,10/28/16,1
owcm,@lossyrob because they are over writing each other,10/28/16,1
lossyrob,ah. yes.,10/28/16,1
owcm,"Has anybody worked with trying to write out a Color Geotiff, RGBA  bands?",10/28/16,1
initjsk,"I am using the polygonalSummary on a tile that intersects my polygon partially, but I am randomly getting ArrayIndexOutOfBounds from PolygonRasterizer.foreachCellByPolygon. Any idea as to why this would happen?```java.lang.ArrayIndexOutOfBoundsException: 3        at geotrellis.raster.rasterize.polygon.PolygonRasterizer$.foreachCellByPolygon(PolygonRasterizer.scala:351)        at geotrellis.raster.rasterize.Rasterizer$.foreachCellByGeometry(Rasterizer.scala:116)        at geotrellis.raster.rasterize.GeometryRasterizeMethods$class.foreach(GeometryRasterizeMethods.scala:39)        at geotrellis.raster.package$withGeometryRasterizeMethods.foreach(package.scala:43)        at geotrellis.raster.summary.polygonal.MeanSummary$.handlePartialTile(MeanSummary.scala:40)        at geotrellis.raster.summary.polygonal.MeanSummary$.handlePartialTile(MeanSummary.scala:33)        at geotrellis.raster.summary.polygonal.PolygonalSummaryMethods$class.polygonalSummary(PolygonalSummaryMethods.scala:26)        at geotrellis.raster.package$withTileMethods.polygonalSummary(package.scala:52)```",10/30/16,1
lossyrob,no_Ñ_that seems like it_Ñés a bug. Could you give geojson of the polygon and the target raster extent?,10/30/16,1
metasim,"@lossyrob I hope to find time this afternoon to look at #1756. Things are a bit chaotic at work, so 50/50.",10/31/16,1
lossyrob,:+1: ,10/31/16,1
lossyrob,"it_Ñés an odd one, but can_Ñét imagine it_Ñés too deep of a bug",10/31/16,1
metasim,"Yeh, I think it's somewhere in `TiffTagFieldValue`, around 139.",10/31/16,1
metasim,There's some kind of serialization going on that I don't quite understand.,10/31/16,1
metasim,(Converting the tags to xml?!?),10/31/16,1
lossyrob,yeah_Ñ_that_Ñés how GDAL does it.,10/31/16,1
lossyrob,"those tags are custom to GDAL, but the only way that per band tags are supported AFAIK",10/31/16,1
metasim,Does it call out to GDAL?,10/31/16,1
lossyrob,no,10/31/16,1
lossyrob,it_Ñés probably the fact that it does PrettyPrint,10/31/16,1
lossyrob,just copies what GDAL does,10/31/16,1
metasim,"Ah.... so it's not part of the official GeoTiff spec, but some GDAL extension?",10/31/16,1
lossyrob,yeah exactly,10/31/16,1
metasim,ooof,10/31/16,1
metasim,I guess arguably GDAL /is/ the standard now....,10/31/16,1
lossyrob,"yeah_Ñ_I saw geotiffs out in the wild that I needed to read those bands for, and that_Ñés how they were encoded...",10/31/16,1
lossyrob,@pomadchin is this incorrect? shouldn_Ñét this be `_Ñùtype_Ñù: _ÑÒfile_Ñù` and `_Ñùpath_Ñù : _ÑÒ/some/local/path_Ñù`?,10/31/16,1
lossyrob,https://github.com/geotrellis/geotrellis/blob/master/docs/spark-etl/spark-etl-run-examples.md#backend-json-examples-local-fs,10/31/16,1
pomadchin,@lossyrob in this context should be as you said,10/31/16,1
lossyrob,"hm I_Ñém getting `Exception in thread ""main"" spray.json.DeserializationException: Not supported backend profile type file.`",10/31/16,1
pomadchin,sec let me have a look,10/31/16,1
pomadchin,AH,10/31/16,1
pomadchin,sure,10/31/16,1
pomadchin,"yes, it_Ñés INPUT",10/31/16,1
pomadchin,and we have a generic input reader,10/31/16,1
pomadchin,it is hadoop reader in fact,10/31/16,1
lossyrob,well,10/31/16,1
lossyrob,it_Ñés output backend,10/31/16,1
lossyrob,https://github.com/geotrellis/geotrellis/blob/891319f7de9122993ae6585c6298d334cc1c34bf/spark-etl/src/main/scala/geotrellis/spark/etl/config/json/ConfigFormats.scala#L121-L125,10/31/16,1
lossyrob,and File is missing,10/31/16,1
pomadchin,it_Ñés a bug,10/31/16,1
lossyrob,yerp,10/31/16,1
lossyrob,I can mess with it,10/31/16,1
lossyrob,I_Ñém trying to do a file based ingest now,10/31/16,1
pomadchin,Im wondering; how it worked with lc emr demo,10/31/16,1
lossyrob,the landsat emr demo?,10/31/16,1
pomadchin,yes,10/31/16,1
lossyrob,I don_Ñét think it would,10/31/16,1
lossyrob,because it couldn_Ñét be on EMR,10/31/16,1
pomadchin,"oh, i mean this demo has `local-ingest` and `local-tile-server` instructions",10/31/16,1
lossyrob,oh,10/31/16,1
lossyrob,probably used the HDFS backend with file:///,10/31/16,1
pomadchin,https://github.com/geotrellis/geotrellis-landsat-emr-demo/blob/master/conf/output-local.json,10/31/16,1
lossyrob,it_Ñés missing here https://github.com/geotrellis/geotrellis-landsat-emr-demo/blob/master/conf/backend-profiles.json,10/31/16,1
lossyrob,doesn_Ñét it need to be there in order to be a valid backend?,10/31/16,1
pomadchin,nope,10/31/16,1
pomadchin,it_Ñés an optional field,10/31/16,1
lossyrob,that_Ñés a bit confusing,10/31/16,1
pomadchin,ok,10/31/16,1
pomadchin,figured out,10/31/16,1
pomadchin,https://github.com/geotrellis/geotrellis-landsat-emr-demo/blob/master/ingest/src/main/scala/demo/etl/landsat/package.scala#L39,10/31/16,1
pomadchin,"in lc emr demo a custom logic, a bit overrided",10/31/16,1
pomadchin,that_Ñés why it works for lc emr demo,10/31/16,1
pomadchin,hm,10/31/16,1
lossyrob,so for landsat it says file but it_Ñés actually hadoop?,10/31/16,1
pomadchin,"._.I_Ñéll fix this weird stuff right now ",10/31/16,1
pomadchin,oh Rob,10/31/16,1
pomadchin,can you throw me full config?,10/31/16,1
lossyrob,for the backend?,10/31/16,1
pomadchin,yep,10/31/16,1
pomadchin,config you are trying to use / to write,10/31/16,1
lossyrob,"```{    ""backend-profiles"": [      {        ""name"": ""fs"",        ""type"": ""file"",        ""path"": _ÑÒ/data/catalog""      }    ]}```",10/31/16,1
pomadchin,OK; ,10/31/16,1
pomadchin,so you need to write it into output.json,10/31/16,1
pomadchin,"```json{  ""backend"": {    ""type"": ""file"",    ""path"": ""/data/catalog""  } ...```",10/31/16,1
pomadchin,and del this object in the backend-profiles.json,10/31/16,1
pomadchin,so backend profiles used for specifying connection credentials / options / etc,10/31/16,1
lossyrob,ok. but we shoudl be able to support a profile though,10/31/16,1
lossyrob,and not have the app tell me that _ÑÒfile_Ñù isn_Ñét a valid backend,10/31/16,1
pomadchin,it is not a valid profile type,10/31/16,1
pomadchin,`Not supported backend profile type file`,10/31/16,1
lossyrob,so I can_Ñét define a backend in the profile and use it for an output? I thought that was the regular path,10/31/16,1
lossyrob,maybe this just needs clarification in the docs,10/31/16,1
lossyrob,because to me it felt like the steps were 1. setup input 2. setup backends 3. setup output,10/31/16,1
lossyrob,so _ÑÒbackend profiles_Ñù sounds like it_Ñés what defines the backends,10/31/16,1
pomadchin,"arrr; backend-profile: connection description; it_Ñés a set of credentials in fact (for file and for hadoop not defined at all)input.json: a set of inputs, with input description (like backend, paths to insert from, etc)output.json: only one output, with output settings, output backend, output path / table, etc",10/31/16,1
pomadchin,"think yes, it definitely requires more clarification in docs O:",10/31/16,1
lossyrob,does support HDFS https://github.com/geotrellis/geotrellis/blob/891319f7de9122993ae6585c6298d334cc1c34bf/spark-etl/src/main/scala/geotrellis/spark/etl/config/json/ConfigFormats.scala#L121,10/31/16,1
pomadchin,currenlty no: https://github.com/geotrellis/geotrellis/blob/2e871993bbb76f407da760cbf59a7e805ce7c375/spark-etl/src/main/scala/geotrellis/spark/etl/config/BackendProfiles.scala#L14,10/31/16,1
pomadchin,it can have a name,10/31/16,1
pomadchin,it can be definitely removed,10/31/16,1
pomadchin,or can be completed with special settings (hadoop specific),10/31/16,1
lossyrob,yeah_Ñ_something has to change. it_Ñés just confusing,10/31/16,1
pomadchin,mb a better name for credentials?,10/31/16,1
lossyrob,well it_Ñés only called credentials in one place,10/31/16,1
lossyrob,in a docs where the line above it is backend-profiles,10/31/16,1
lossyrob,backend-settings?,10/31/16,1
pomadchin,backend-configs?,10/31/16,1
lossyrob,"I kindof see it as a valid way to define a backend though. Like if I called my thing _ÑÒfs_Ñù, and defined where the catalog was, I want to in my output to just say `_ÑÒbackend_Ñù : _ÑÒfs_Ñù`",10/31/16,1
pomadchin,mmm but it would require lot_Ñés of copy paste,10/31/16,1
pomadchin,like you need to define separate json objects for input and for output,10/31/16,1
lossyrob,why input?,10/31/16,1
lossyrob,ok so a backend also defines input credentials,10/31/16,1
pomadchin,"right now backend defined:```json""backend"": {    ""type"": ""accumulo"",    _ÑÒpath"": ""catalog"",    ""profile"": ""someprofile""  }```",10/31/16,1
lossyrob,so is there a `backend-profile` just to keep credentials out of checked in json?,10/31/16,1
pomadchin,what do you mean?,10/31/16,1
lossyrob,why not inline the backends,10/31/16,1
lossyrob,well that and multiple inputs,10/31/16,1
lossyrob,I guess also backends being input and output definitions is confusing,10/31/16,1
pomadchin,"ah, because it is useful to ingest into the same accumulo but into different tables",10/31/16,1
lossyrob,hm,10/31/16,1
pomadchin,a sort of abstraction of _ÑÒconfigurations_Ñù and input / outputs,10/31/16,1
pomadchin,like ok we can use these things as input and to use credentials with this name ,10/31/16,1
lossyrob,"it almost feels like _ÑÒbackend_Ñù is something that you should be able to specify full, or specify a name which fills out potentially all of the things, or the backend definition only fills out part (e.g. credentials) and you have to supply the rest",10/31/16,1
lossyrob,but it_Ñés also weird to me that input and output _ÑÒbackends_Ñù get treated the same. I usually think of supported backends as only the things we write to,10/31/16,1
lossyrob,"so backend-profiles is really only used in input for S3 credentials, yeah?",10/31/16,1
pomadchin,yes,10/31/16,1
pomadchin,): ,10/31/16,1
lossyrob,interesting,10/31/16,1
lossyrob,"just trying to figure out how I was thinking about it, what it actually is, and if there_Ñés some improvements in the layout or if it_Ñés just I have to reconfigure how I was thinking about it",10/31/16,1
lossyrob,gonna put it down for a little bit though,10/31/16,1
pomadchin,yep; but it_Ñés cool that you have an alternative vision O: as I probably was focused on my understanding ,10/31/16,1
pomadchin,and it_Ñés very confusing that it_Ñés not obvious how to use it or mb we need more complete docs,10/31/16,1
lossyrob,"maybe I was just confused by my own thoughts on it, getting someone else_Ñés take would help",10/31/16,1
metasim,@lossyrob Do you remember if there's anything magical about the 80 column width?: https://github.com/geotrellis/geotrellis/blob/master/raster/src/main/scala/geotrellis/raster/io/geotiff/writer/TiffTagFieldValue.scala#L139,10/31/16,1
lossyrob,I think that_Ñés how it came out of GDAL? tbh not sure. ,10/31/16,1
metasim,OK. I'll try and see if the tests break ;),10/31/16,1
lossyrob,":+1: ",10/31/16,1
avalcepina,"Hi everyone, I'm working on a project where we are using Breeze matrices to represent images and apply various filters/transformations to them. At one point I have to convert a set of this matrices into geoTiffs adding some fixed geographic metadata. Is there any example of how to do that?",11/1/16,1
avalcepina,"I think I'm, getting somewhere. I'm generating a file with some of the metadata I need (at the moment I'm just copying it from another file). However the result of this process (a SingleBAndGeoTiff like the example file I'm trying to reproduce) is not readable and I think this is due to the fact that in the original I have a color table (I can see it with gdal) but I have nothing like that for the file I'm creating, in fact the colorInterp is set to Grey while it was Palette in the original. Anyway to change that?",11/1/16,1
avalcepina,this is what I have up till now:,11/1/16,1
avalcepina,"```       //All zeros matrix for now      val matrix : CSCMatrix[Double] = CSCMatrix.zeros(800, 800)        val tile = DoubleArrayTile.empty(matrix.cols, matrix.rows)      matrix.pairs.iterator.foreach { case ((row, col), value) =>        tile.setDouble(col, row, value)      }      //These come from the original file      val extent = raster.extent      val tags = image.tags        val resultFile = new File(""result.tif"")      val tiff: SinglebandGeoTiff = new SinglebandGeoTiff(tile, extent, crs, tags, GeoTiffOptions.DEFAULT)      tiff.write(resultFile.getAbsolutePath)``` ",11/1/16,1
lossyrob,"will try to answer more later, but for now would this change help? https://github.com/geotrellis/geotrellis/pull/1667",11/1/16,1
avalcepina,"it might help, I will have to look into it a bit more though. Thanks for the reply!",11/1/16,1
lossyrob,"when you say _ÑÒnot readable_Ñù, where is it not readable from?",11/1/16,1
lossyrob,"that should write a valid geotiff, with the values given",11/1/16,1
lossyrob,"you_Ñére writing double values out, so it_Ñés unclear how they would be representative of colors if you want a photometric interpretation...",11/1/16,1
lossyrob,"we currently have a way to read color maps in, but don_Ñét write them out",11/1/16,1
lossyrob,"@pomadchin @echeipesh this feels hacky, but also in the context of what we have is really useful: https://github.com/geotrellis/geotrellis/pull/1764",11/1/16,1
lossyrob,thoughts?,11/1/16,1
echeipesh,Mostly I am concerned about layerWriter being exposed there,11/1/16,1
echeipesh,"What about: ` type PostSaveHook[K, V, M] = (AttributeStore,  LayerId, RDD[(K, V)] with Metadata[M]) => RDD[(K, V)] with Metadata[M]` ?",11/1/16,1
lossyrob,well,11/1/16,1
lossyrob,i figured there might be a situation where someone would want to do something to the input and save it off,11/1/16,1
lossyrob,"for instance, if we wanted to save an ndvi layer on incoming imagery",11/1/16,1
lossyrob,they could just write it off the processing of the raw layer,11/1/16,1
echeipesh,Fair _Ñ_ then about about:,11/1/16,1
echeipesh,"```scaladef DEFAULT[K, V, M] = {      (_: AttributeStore, w: Writer[LayerId, RDD[(K, V)] with Metadata[M]], id: LayerId, rdd: RDD[(K, V)] with Metadata[M]) => w.write(id, rdd) }```",11/1/16,1
echeipesh,^ Basically have the default just write the layer. So if somebody overwrites the save hook they have to write the layer as well as whatever else they choose to do.,11/1/16,1
lossyrob,not really a post save hook then,11/1/16,1
lossyrob,but I see waht your saying,11/1/16,1
lossyrob,is that just to prevent people from thinking they have to write the layer themselves,11/1/16,1
echeipesh,"right, it_Ñéd be more of a `WriterHook`",11/1/16,1
echeipesh,"Basically, its to prevent magic action when it seems like there is an explicit action.",11/1/16,1
lossyrob,yeah i buy that,11/1/16,1
echeipesh,"That also makes it more flexible, if there is some final transfomation that has to be made, it could be made here.",11/1/16,1
lossyrob,`SaveAction` as the type alias?,11/1/16,1
echeipesh,`FinalAction` ?,11/1/16,1
lossyrob,hm. I like SaveAction more,11/1/16,1
lossyrob,"because the method is called `save`",11/1/16,1
echeipesh,"Oh yeah, that makes total sense",11/1/16,1
lossyrob,alright. will ping when updated,11/1/16,1
lossyrob,makes `SpatialRenderOutput` seem even more hacky,11/1/16,1
echeipesh,"Well, if it makes a hack seem more hacky that just means its a good change ;)",11/1/16,1
lossyrob,lol. truth.,11/1/16,1
echeipesh,"@fosskers Feel like I_Ñém just littering that issue, what do you mean about target cell types being set ?",11/1/16,1
echeipesh,this issue: https://github.com/geotrellis/geotrellis/pull/1760#issuecomment-257629977,11/1/16,1
lossyrob,ah. well it looks like you are just delegating to map,11/1/16,1
lossyrob,so yeah that could totally be an apply,11/1/16,1
lossyrob,thought what happens if it needs double values?,11/1/16,1
lossyrob,"I guess there would be overloads with `BreakMap[Double, _]` for that?",11/1/16,1
echeipesh,Hmm either _ÑÒuser beware_Ñù or we can encode `  def dualMap(f: Int => Int)(g: Double => Double): Tile` as a type,11/1/16,1
echeipesh,I don_Ñét think you can rely on overloads to act sane here,11/1/16,1
lossyrob,"right. well they will act sane if they remain as overloads of `map` (dropping the apply thing)",11/1/16,1
echeipesh,What do you mean? `Tile.map` will always give `Int`s?,11/1/16,1
echeipesh,"or do you mean overload on `BreakMap` like `def mapWith(bm: BreakMap[Int, Int]): Tile` ?",11/1/16,1
lossyrob,"```scaladef map(m: BreakMap[Int, Int], targetCellType: CellType): Tile = ???def map(m: BreakMap[Int, Double], targetCellType: CellType): Tile = ???def map(m: BreakMap[Double, Int], targetCellType: CellType): Tile = ???def map(m: BreakMap[Double, Double], targetCellType: CellType): Tile = ???```",11/1/16,1
echeipesh,That_Ñéll probably work,11/1/16,1
lossyrob,updated https://github.com/geotrellis/geotrellis/pull/1764,11/1/16,1
echeipesh,"heh, see what you mean about `SpatialRenderOutput`",11/1/16,1
echeipesh,really `SpatialRenderOutput` is actually just an instance of `SaveAction` that doesnt use the writer,11/1/16,1
echeipesh,@lossyrob do you think that would be more or less hacky if there was a `RenderLayerWriter` that did the rendering on `LayerWriter._write` ?,11/1/16,1
echeipesh,All the Avro type classes would be pointless,11/1/16,1
lossyrob,yeah,11/1/16,1
lossyrob,it_Ñés another writer hierarchy,11/1/16,1
lossyrob,LayerWriter is really an AvroLayerWriter,11/1/16,1
echeipesh,Seems like it,11/1/16,1
lossyrob,was talking with someone about how it would actually be cool to serialize multiban tiles such that you could do geotiff-like byte requests to pull individual bands,11/1/16,1
lossyrob,"so you could have 11 bands saved in the layer, but only grab 2 if you wanted to do e.g. NDVI",11/1/16,1
echeipesh,So basically there might be some milage in having a broader writer/reader hiarchy,11/1/16,1
lossyrob,:+1: ,11/1/16,1
echeipesh,"Which means that `AvroRecordCodec` at least is out, other things we can probably leave. Lets say that conversation is 1.1 or something",11/1/16,1
echeipesh,"So on unrelated topic: `tile1.combine(tile2, IntCellType) { (v1, v2) => v1 + v2 }` does not smell nice to me but it seems the only _ÑÒcheap_Ñù option to determine the target cell type.",11/1/16,1
echeipesh,"The nicest alternative seems to be building off:```scalaclass LazyTile(base: => Tile) extends CellGrid    with IterableTile    with MappableTile[Tile]    with LazyLogging{  lazy val tile: Tile = base_Ñ_}```",11/1/16,1
lossyrob,I think that combine seems fine,11/1/16,1
lossyrob,doesn_Ñét smell that bad to me,11/1/16,1
lossyrob,"other quick change of topic: I_Ñém changing AttributeStore.layerExists to read specifically the metadata attribute to check for existance. This always bothered me_Ñ_if you wrote say the histogram of zoom 6 in zoom level 0 for a stack of layers, then when it went to go write zoom 0 it would throw saying the layer already existed",11/1/16,1
echeipesh,That seems fine. I think it might have done that at one point,11/1/16,1
lossyrob,@pomadchin what_Ñés going on with this for hbase (OS X),11/2/16,1
lossyrob,"```bash/proj/gt/geotrellis scripts/hbaseTestDB.sh latest: Pulling from daunnc/geodocker-hbase-standaloneDigest: sha256:4b056deafc366ccf449ba956bd5d1a64cd8b5aeb6ffcacaf3727c4020c8776c5Status: Image is up to date for daunnc/geodocker-hbase-standalone:latestRequires additional setup in case of Windows / OS X, known limitations: https://docs.docker.com/docker-for-mac/networking/Starting HBase containerContainer has ID a1aeec250578f04f369b3b684cad4635118c4c4cf8d8636f380fdf1245d94c5eUpdating /etc/hosts to make hbase-docker point to 172.17.0.2 (localhost)sed: -e expression #1, char 1: unknown command: `.'```",11/2/16,1
pomadchin,"@lossyrob hbase has 'host security', it requires hbase to be in a machine local network (this thing actually can be fixed by adding records in a hosts for), that makes remote hbase usage a real inconvenient hell, I can check how hbase container works ",11/2/16,1
pomadchin,"*a sort of 'host security', similar to what spark has for workers",11/2/16,1
pomadchin,in a hosts for = in a hosts file**,11/2/16,1
lossyrob,hm. not sure how that translates into a fix so I can run the unit tests_Ñ_do I have to modify the containers /etc/host?,11/2/16,1
pomadchin,eh; yes,11/2/16,1
pomadchin,so hbase is quite sensitive to hosts setings (of client machine and of remote machines),11/2/16,1
pomadchin,"the problem using docker beta in that we need to connect to hbase through VM; host _ÑÓ> VM _ÑÓ> Docker in theory we need to forward all ports into VM and docker for mac logic forwards them into host (I am tolktin about exposing ports)But it causes collisions in hostnames, and yes we have access to zookeeper, but zookeeper manager names; and these names not correspond to ip neccesary addresses",11/2/16,1
pomadchin,the simple workaround is to start container with `_ÑÓnet=host` flag and to ooperate with VM the way we connect to common remote hbase,11/2/16,1
pomadchin,but it not works with docker for mac and docker for windows,11/2/16,1
pomadchin,spent lot_Ñés of hours today trying to run hbase on mac in docker container O:,11/2/16,1
pomadchin,"mb smb has idea how to launch container, because i feel like it is possible",11/2/16,1
pomadchin,and this `.sh` script we have to lanch hbase works only on linux machines ):,11/2/16,1
ameetkini,"hey guys, does geotrellis work with spark 2.0.x? ",11/3/16,1
lossyrob,the 1.0 series does,11/3/16,1
lossyrob,on bintray,11/3/16,1
lossyrob,we should be releasing offically 1.0 soon (before end of month target) under locationtech,11/3/16,1
lossyrob,but for now you can use the snapshots on bintray,11/3/16,1
ameetkini,"awesome, I'll check out the 1.0 version",11/3/16,1
ameetkini,looking at 1.0.0-f510e5a - any of those work?,11/3/16,1
pomadchin,"yep, f510e5a definitely works with spark 2",11/3/16,1
ameetkini,Is there no geotrellis-engine in 1.0? Maven's not finding it and bintray doesn't seem to host it,11/3/16,1
lossyrob,nope. that_Ñés gone,11/3/16,1
lossyrob,we moved away from the Akka stuff,11/3/16,1
lossyrob,"it_Ñés just geotrellis-raster, geotrellis-spark and co",11/3/16,1
ameetkini,oh wow (feeling jittery),11/3/16,1
ameetkini,I'm probably very outdated - are there docs I should be reading up?,11/3/16,1
lossyrob,"hm, yeah we are currently working on rebooting our docs",11/3/16,1
pomadchin,._. though probably even https://github.com/geotrellis/geotrellis/tree/master/docs can be useful ,11/3/16,1
ameetkini,"I can imagine, 1.0! Good job guys",11/3/16,1
lossyrob,and we are going for http://geotrellis.readthedocs.io/en/latest/,11/3/16,1
ameetkini,thanks @grisha and @lossyrob ,11/3/16,1
ameetkini,I'll look at those two links,11/3/16,1
lossyrob,but that needs all these changes: https://github.com/geotrellis/geotrellis/pull/1744,11/3/16,1
lossyrob,we_Ñéll have the readthedocs up to date next week,11/3/16,1
pomadchin,:D the workaround before publishing new docs is to write questions there :D ,11/3/16,1
avalcepina,"Hi, I'm following up on this post on stackoverflow http://stackoverflow.com/questions/40287702/scala-how-to-convert-a-breeze-matrix-to-a-geotiff-image/40405655#40405655 . I tried doing what was suggested but I ended up with a black imageI start with a Double matrix and I create the 3 tiles as IntArrayTile by applying thresholds to the initial matrix in order to identify the colours.The tif gets created, I see all the metadata and I can open it but i's just black. I checked the 3 arrays before creation and they do contain non-black values",11/4/16,1
avalcepina,I'm using version 1.0.0-cd1ca27,11/4/16,1
pomadchin,@avalcepina hi!,11/4/16,1
avalcepina,@pomadchin hi there,11/4/16,1
pomadchin,"hm, i think that should be just 3 grayscaled, without any offsets / conversions/ etc bands, but with tag `TIFFTAG_PHOTOMETRIC=rgb`",11/4/16,1
pomadchin,or o: what_Ñés your pipeline?,11/4/16,1
pomadchin,can you throw a sample of code (: ?,11/4/16,1
avalcepina,yes sorry I got caught up in the standup,11/4/16,1
avalcepina,"this is an actor replying to a message carrying the matrix. Just to give you a bit of context:```case CreateGeoTiff(contName, repId, piMatrix) =>      log.info(""Creating geoTiff for container {} id {}"", contName, repId)      val crs = CRS.fromName(""EPSG:4326"")      val extent = Extent(0.0000000, 48.8924513, 10.8540845, 55.9736074 )      val red: Tile = getTile(piMatrix.data, 0)      val green: Tile = getTile(piMatrix.data, 1)      val blue: Tile = getTile(piMatrix.data, 2)      val mbtile = MultibandTile(red, green, blue)      val opts = GeoTiffOptions(        Striped,        NoCompression,        ColorSpace.RGB      )      val headTags = Map(""AREA_OR_POINT"" -> ""AREA"")      val bandTags = List(Map(""GIF_BACKGRUND"" -> ""0"", ""TIFFTAG_PHOTOMETRIC"" -> ""rgb""))      val tags = Tags(headTags,bandTags)      val mbgeotiff = new MultibandGeoTiff(        mbtile,        extent,        crs,        tags,        opts      )      mbgeotiff.write(contName + ""_"" + repId + "".tif"")def getColor(piValue: Double) = piValue match {    case color if color >= THRESHOLD_1 && color < THRESHOLD_2 => PALETTE_0    case color if color >= THRESHOLD_2 && color < THRESHOLD_3 => PALETTE_1    case color if color >= THRESHOLD_3 && color < THRESHOLD_4 => PALETTE_2    case color if color >= THRESHOLD_4 && color < THRESHOLD_5 => PALETTE_3    case color if color >= THRESHOLD_5 && color < THRESHOLD_6 => PALETTE_4    case color if color >= THRESHOLD_6 && color < THRESHOLD_7 => PALETTE_5    case color if color >= THRESHOLD_7 && color < THRESHOLD_8 => PALETTE_6  }  def getTile(matrix : CSCMatrix[Double], index : Int) : Tile = {    val data: Array[Int] = matrix.toDense.data.map(x => this.getColor(x)(index))    val result : Tile = IntArrayTile(data, matrix.cols, matrix.rows)    (0 to matrix.rows).foreach(x => (0 to matrix.cols).foreach(y => result) )    result  }```",11/4/16,1
pomadchin,is it a typo? `(0 to matrix.rows).foreach(x => (0 to matrix.cols).foreach(y => result) )` ,11/4/16,1
avalcepina,you mean that x and y should be called the other way around? ,11/4/16,1
pomadchin,"i mean that you are not setting anything",11/4/16,1
avalcepina,"mmm you are right, let me check a second ",11/4/16,1
pomadchin,"you need smth like:```scalafor {  r <- matrix.rows,  c <- matrix.cols} result.set(c, r, matrix(c)(r))```",11/4/16,1
avalcepina,"ah yes it's a leftover, but it should not matter anyhow, the line above should take care of creating the tile",11/4/16,1
pomadchin,oh i see,11/4/16,1
avalcepina,"yeah I tried removing it, nothing changes",11/4/16,1
pomadchin,"can you throw me ```gdalinfo -mm resultfile.tif``` ?",11/4/16,1
avalcepina,one sec,11/4/16,1
avalcepina,"```Driver: GTiff/GeoTIFFFiles: RAD_NL25_PCP_CM_201610191955.h5_1.tifSize is 700, 765Coordinate System is:GEOGCS[""WGS 84"",    DATUM[""WGS_1984"",        SPHEROID[""WGS 84"",6378137,298.257223563,            AUTHORITY[""EPSG"",""7030""]],        AUTHORITY[""EPSG"",""6326""]],    PRIMEM[""Greenwich"",0],    UNIT[""degree"",0.0174532925199433],    AUTHORITY[""EPSG"",""4326""]]Origin = (0.000000000000000,55.973607399999999)Pixel Size = (0.015505835000000,-0.009256413202614)Metadata:  AREA_OR_POINT=AreaImage Structure Metadata:  INTERLEAVE=PIXELCorner Coordinates:Upper Left  (   0.0000000,  55.9736074) (  0d 0' 0.01""E, 55d58'24.99""N)Lower Left  (   0.0000000,  48.8924513) (  0d 0' 0.01""E, 48d53'32.82""N)Upper Right (  10.8540845,  55.9736074) ( 10d51'14.70""E, 55d58'24.99""N)Lower Right (  10.8540845,  48.8924513) ( 10d51'14.70""E, 48d53'32.82""N)Center      (   5.4270423,  52.4330293) (  5d25'37.35""E, 52d25'58.91""N)Band 1 Block=700x2 Type=Int32, ColorInterp=Red    Computed Min/Max=0.000,254.000  NoData Value=-2147483648  Metadata:    GIF_BACKGRUND=0    TIFFTAG_PHOTOMETRIC=rgbBand 2 Block=700x2 Type=Int32, ColorInterp=Green    Computed Min/Max=0.000,251.000  NoData Value=-2147483648Band 3 Block=700x2 Type=Int32, ColorInterp=Blue    Computed Min/Max=0.000,251.000  NoData Value=-2147483648```",11/4/16,1
pomadchin,interesting o: looks like everything is fine,11/4/16,1
avalcepina,yeah,11/4/16,1
pomadchin,have you tried to look at it using qgis?,11/4/16,1
pomadchin,smth wrong with histogram?,11/4/16,1
avalcepina,"no, I can try",11/4/16,1
pomadchin,"kk, very interesting",11/4/16,1
pomadchin,"`def getColor(piValue: Double)` is not complete as well? (no second arg, index)",11/4/16,1
avalcepina,"yeah I'm just getting the array and then using the index on it in getTile, these are just temporary method",11/4/16,1
pomadchin,"btw, can you try to save all bands as single band tiles, to check if singleband tiles are black too? ",11/4/16,1
pomadchin,"if they are black, that means that smth is worng with data; if not _ÑÓ probably smth wrong with multiband geotiff interpretation",11/4/16,1
avalcepina,"ok I'll try that too, thanks",11/4/16,1
pomadchin,you are welcome :),11/4/16,1
avalcepina,"they are black so it's the data, that is strange however I'm using the same matrix to generate a gif before this step and the process of thresholding is very similar, I'll look into it",11/4/16,1
pomadchin,"yep, i still recommend you to look at it with qgis, and to see at histogram (mb it_Ñés too wide, that can explain why everything is too dark)",11/4/16,1
avalcepina,ok,11/4/16,1
avalcepina,[![Screen Shot 2016-11-04 at 09.47.00.png](https://files.gitter.im/geotrellis/geotrellis/H7rw/thumb/Screen-Shot-2016-11-04-at-09.47.00.png)](https://files.gitter.im/geotrellis/geotrellis/H7rw/Screen-Shot-2016-11-04-at-09.47.00.png),11/4/16,1
avalcepina,this is what I get from qgis,11/4/16,1
pomadchin,heh,11/4/16,1
avalcepina,"I can also see the image, although colors are off compared to what I would expect",11/4/16,1
pomadchin,yes smth wrong with the data,11/4/16,1
pomadchin,O:,11/4/16,1
avalcepina,"Ok, thanks, I'll try to understand what might be causing it",11/4/16,1
pomadchin,"\+ you can throw there / in gist / somewhere else your input data, and we can help you to hack on that ",11/4/16,1
avalcepina,ok I'll try to retrieve the starting matrix,11/4/16,1
avalcepina,[matrix.csv](https://files.gitter.im/geotrellis/geotrellis/US7c/matrix.csv),11/4/16,1
avalcepina,"this is the starting matrix, it's full of zeros but there's actual data if you look for it",11/4/16,1
avalcepina,"so, I added .convert(ByteCellType) to the MultibandTile and it seems it worked. I'm still a bt cautious, but it looks okay for now",11/4/16,1
pomadchin,O:,11/4/16,1
pomadchin,AH,11/4/16,1
pomadchin,"yes, you needed int8 geotiff",11/4/16,1
pomadchin,it_Ñés a byte type,11/4/16,1
avalcepina,great :D sometimes trying random things helps :D,11/4/16,1
pomadchin,:D :D ,11/4/16,1
avalcepina,thank you very much for your support!,11/4/16,1
pomadchin,":D eh, ye, ByteCell type was one of the most important things :D ",11/4/16,1
kevinmhinson,"any ideas on best way to build rasters of Strings?  is the best approach to extend MutableArrayTile and DataType to create StringArrayTile and StringCells...  or is there maybe some approach out there already to do this that I'm not seeing?  Is the reason there's not a StringArrayTile already because StringCells would have to hard code number of bits, or are there other complications I'm not considering?",11/4/16,1
lossyrob,all of our tile types are backed by Arrays of primitive types,11/4/16,1
lossyrob,"this has served the needs of the project so far, and is performant",11/4/16,1
lossyrob,arrays of objects like strings would be pretty slow,11/4/16,1
lossyrob,"and not sure how they would actually implement the tile interface, since it deals with numeric types that can be viewed as Ints or Doubles",11/4/16,1
lossyrob,for instance how would you implement `def map(f: Int = Int): Tile`,11/4/16,1
lossyrob,what are you trying to do with strings and rasters?,11/4/16,1
kevinmhinson,"Thanks for the feedback @lossyrob .  I'm trying to build rasters that show ""best server"" in a network.  I can convert names to some Long value as a code, but it'd be better if I could embed actual names.",11/4/16,1
lossyrob,"Interesting. You could make a _ÑÒtile_Ñù type that didn_Ñét fit into the heirarchy, but was backed by a array of strings and could be accessed by col, row",11/4/16,1
lossyrob,and then use our `TileFeature` type to compose that with a primitive type tile if you needed our operations,11/4/16,1
lossyrob,what sort of computations were you looking to do on it?,11/4/16,1
kevinmhinson,"I hate showing how remedial I am here  :worried:   at the outset just display...  next step would focal operations like mode and some measure of entropy...  starting use case would be something easy that just gets data into some GIS format...  I have a test process that looks like...  Array[Array[String]] -> Array[IntArrayTile] -> ArrayMultibandTile -> MiltibandGeoTiff.toByteArray -> SaveToS3...  but I have 250k possible servers, so Strings would be better than numeric codes.",11/4/16,1
kevinmhinson,"I know tiffs won't take Strings either, so maybe the whole GIS world is telling me something.",11/4/16,1
lossyrob,"ah interesting. sorry had to step away, but that seems like a pretty cool use case. Unfortunately you_Ñéd have to use numeric codes_Ñ_Int_Ñés would handle all the server possibilities. If you had a `Map[Int, String]`, you could pretty easily translate the codes",11/4/16,1
metasim,"In `MultibandGeoTiff` there's an implicit conversion to `MultibandTile`. It's caused me a bit of (minor) confusion, because it makes it look like methods such as `convert` exist on `MultiBandGeoTiff`, but with information loosing sematics (it drops the band tags and options). I certainly don't know the full history, but a naive analysis makes me think it's a bad implicit to have automatically pulled into scope along with `MultibandGeoTiff`. Thoughts? ",11/7/16,1
pomadchin,"@metasim not sure that it is bad, each function signature has In and Out types, usually it helps to understand what_Ñés going on.",11/7/16,1
metasim,"In the case I'm thinking of, if I say `(t: MultibandGeoTiff).convert(FloatCellType)` I'd expect, without looking at the type signature, to get a `MultibandGeoTiff`.",11/7/16,1
pomadchin,"the same _ÑÒproblem_Ñù we have with `RDD[(K, V)] with Metadata[M]`: on some operations we loose type information (for example after applying `map`, `with Metadata[M]` is loosed), for this purposes we have a specaial `.withContext` method, to keep rdd metadata",11/7/16,1
metasim,"The implicit makes it look like `convert` exists on `MultibandGeoTiff`, when it really doesn't. It's more of a `convertAndThrowAwayMetadata`.",11/7/16,1
metasim,"I know this is nit-picking, but I've run into it a couple of times now.",11/7/16,1
metasim,I'd rather get conversion semantics from a type class or through explicit implementation on the type.,11/7/16,1
metasim,"Wanted to bring it up to see if filing an issue would be entertained, or it's fixed funtionality.",11/7/16,1
pomadchin,@metasim https://github.com/geotrellis/geotrellis/blob/master/raster/src/main/scala/geotrellis/raster/io/geotiff/GeoTiffMultibandTile.scala#L281-L300,11/7/16,1
pomadchin,o:,11/7/16,1
pomadchin,it_Ñés a `MultibandGeoTiff => MultibandGeoTiff` function ,11/7/16,1
metasim,Hmmmm. then I wonder why the implicit is being invoked first...,11/7/16,1
pomadchin,"mb you have a faling code example, to understand the problem?",11/7/16,1
metasim,Lemme put something together,11/7/16,1
pomadchin,"actually i believe that we don_Ñét have `implicit conversions` (ofc we have but, not too much), we usually use `implicit classes`",11/7/16,1
metasim,https://github.com/geotrellis/geotrellis/blob/master/raster/src/main/scala/geotrellis/raster/io/geotiff/MultibandGeoTiff.scala#L121,11/7/16,1
pomadchin,`(t: MultibandGeoTiff).convert(FloatCellType)` I can_Ñét see reasons why should this implicit be applied there,11/7/16,1
metasim,"Because, actually, the `convert` you referenced isn't `MultibandGeoTiff => MultibandGeoTiff`, it's `GeoTiffMultibandTile => GeoTiffMultibandTile `",11/7/16,1
pomadchin,Oh,11/7/16,1
metasim,`(t: MultibandGeoTiff).convert(...)` could simply be `(t: MultibandGeoTiff).copy(tile = t.convert(cellType))`,11/7/16,1
pomadchin,"yes, could be; we can add this func",11/7/16,1
pomadchin,._.,11/7/16,1
pomadchin,"it would be good for types consistency i believe, thx for your report ._.",11/7/16,1
pomadchin,"@metasim can you create an issue? // sry for lots of questions :D (busy with another issue), thx a lot for your help",11/7/16,1
metasim,Will do! Happy for the dialog.,11/7/16,1
lossyrob,perhaps taking that implicit out is the fix,11/7/16,1
lossyrob,"we have a `mapTile`, so you could simply do `geoTiff.mapTile(_.convert(_Ñ_))`",11/7/16,1
metasim,"If I had to choose one over the other, my personal preference would be to remove the implicit.",11/7/16,1
lossyrob,mine too,11/7/16,1
lossyrob,I_Ñéll comment on the issue and check it out,11/7/16,1
lossyrob,I put up a PR that removes the implicits,11/7/16,1
metasim,"I'm working on updating `GeoTiffReader` and `GeoTiffWriter` to provide round-trip handling of ""Paletted"" photometric interpretation. There's already some support for reading a GeoTiff color table in `TiffTagsReader.setColorMap`. I think I might have found a bug in it, but want to confirm.In the geotrellis test file `colormap.tif`, `gdalinfo` shows index 12 having value `12: 209,221,249,255`, but in the debugger I'm seeing a different value:",11/11/16,1
metasim,[![Screen Shot 2016-11-11 at 2.57.50 PM.png](https://files.gitter.im/geotrellis/geotrellis/fjoK/thumb/Screen-Shot-2016-11-11-at-2.57.50-PM.png)](https://files.gitter.im/geotrellis/geotrellis/fjoK/Screen-Shot-2016-11-11-at-2.57.50-PM.png),11/11/16,1
metasim,"I'm not yet familiar with the mechanism by which TIFF files pack bits, so it's not clear from inspection what might be going on, but wanted a sanity check to see if negative short values is exepcted or not. Best I can tell, this code (the results of`setColorMap`) isn't yet used anywhere.",11/11/16,1
metasim,Also find it interesting that it's reading and storing RGB values and not RGBA values.,11/11/16,1
metasim,"(not clear if the RGBA reporting from `gdalinfo` is just defaulting to a 255 alpha value, or if it's actually in the file)",11/11/16,1
metasim,[This](https://github.com/geotrellis/geotrellis/blob/master/raster/src/main/scala/geotrellis/raster/io/geotiff/reader/TiffTagsReader.scala#L418) is the relevant code.,11/11/16,1
lossyrob,would have to look at the Tiff spec to see if those should be RGBA,11/11/16,1
lossyrob,"if they are, then we_Ñére reading the wrong amount of data in, which would account for the bad values",11/11/16,1
metasim,I think the bug is in the call to `.toShort`,11/11/16,1
metasim,Or the assumption that the `shorts: Array[Int]` is actually containing shorts.,11/11/16,1
metasim,"Anyway, I'm digging.",11/11/16,1
lossyrob,"ok, cool. looks like a bug to me. thanks for looking into it",11/11/16,1
metasim,:+1: ,11/11/16,1
lossyrob,"> PhotometricInterpretation = 3 (Palette Color).> ColorMap> Tag = 320 (140.H)> Type = SHORT> N = 3 * (2**BitsPerSample)> This field defines a Red-Green-Blue color map (often called a lookup table) for palette color images. In a palette-color image, a pixel value is used to index into an RGB-lookup table. For example, a palette-color pixel having a value of 0 would be displayed according to the 0th Red, Green, Blue triplet.> In a TIFF ColorMap, all the Red values come first, followed by the Green values, then the Blue values. In the ColorMap, black is represented by 0,0,0 and white is represented by 65535, 65535, 65535.",11/11/16,1
lossyrob,from the spec,11/11/16,1
lossyrob,"it_Ñés true that if they are unsigned short values, the `.toShort` will ruin them",11/11/16,1
metasim,Thanks (was just looking at that in the Adobe spec). That must mean `gdalinfo` is just conveying an alpha value that doesn't exist.,11/11/16,1
metasim,"It looks like if I replace `.toShort` with a call to `def shift(c: Int) = ((c >> 8) & 0xff).toShort` it gets the right result. However, I don't know why the shift would be necessary... perhaps there's an off-by-one error in the code that constructs the `shorts` data array?",11/11/16,1
lossyrob,"perhaps, but I would think that would affect more things and be caught. but potentially",11/11/16,1
lossyrob,is it not evaluating the endian-ness correctly?,11/11/16,1
metasim,that's a possibility. I'm going to check to see  if the first 8 bits are always 0xff.... that would point to an alpha value actually being there.,11/11/16,1
metasim,"The spec doesn't speak to the short packing being ""RRRRGGGGBBBB"" etc, but that's how it appears to be layed out.",11/11/16,1
lossyrob,"well I think it reads it like that, but then holds it as Ints because we can_Ñét support unsigned shorts",11/11/16,1
lossyrob,oh but your saying,11/11/16,1
lossyrob,well hm,11/11/16,1
lossyrob,"that they are actually encoded as RGBA, like RRRAGGGABBBA or ARRRAGGGABBB?",11/11/16,1
metasim,righgt. maybe gdal is corretly reading an alpha.,11/11/16,1
lossyrob,hm.,11/11/16,1
lossyrob,A dive into the gdal code would show that. which is fun code to swim through :),11/11/16,1
metasim,that doesn't make sense,11/11/16,1
metasim,might do that.,11/11/16,1
lossyrob,"if GDAL is doing that, I wonder how it knows how to do that instead of what the spec says",11/11/16,1
metasim,Oh wierd.... it turns out that `short(i) & 0xff` and `(short(i) >> 8) & 0xff` return the same value! It's like the color value is in there twice.,11/11/16,1
metasim,I'll work on it this weekend.,11/11/16,1
lossyrob,"well to get th short out, you would need `0xFFFF`",11/11/16,1
lossyrob,might be coincidence for the color to have the same on both bytes. Is it a grayscale color map?,11/11/16,1
lossyrob,"also this should be of interest to you, comes from a discussion we had: https://github.com/geotrellis/geotrellis/pull/1797",11/11/16,1
lossyrob,"not only from that, been a problem for a long while, but you reminded us that it needed to get done before 1.0",11/11/16,1
metasim,Definitely interested in #1797.... every apropos to what I've been doing recently (having to be careful with converting to `FloatXXXType` before doing any math).,11/12/16,1
metasim,"Regarding the color map, the test file is `geotrellis/raster-test/data/geotiff-test-files/colormap.tif` which is pretty standard multi-color palette. It seems suspicious that, when treated as an RGBA value, both byte 1 and byte 2 have the same value. For instance, in the test file, index 12, red value, is integer `53713`, where the following is true:```scala> (     |   53713 >> 24 & 0xFF,     |   53713 >> 16 & 0xFF,     |   53713 >> 8 & 0xFF,     |   53713 & 0xFF)res1: (Int, Int, Int, Int) = (0,0,209,209)```",11/12/16,1
metasim,"Changing the mask to `0xFFFF` (as is necessary), results in this:```scala> (     |   53713 >> 24 & 0xFFFF,     |   53713 >> 16 & 0xFFFF,     |   53713 >> 8 & 0xFFFF,     |   53713 & 0xFFFF     | )res2: (Int, Int, Int, Int) = (0,0,209,53713)```",11/12/16,1
metasim,"And as shown above, the `gdalinfo` value is `209`.",11/12/16,1
lossyrob,"I wish I could tell you how colormap.tif was created, but I_Ñém not sure",11/12/16,1
lossyrob,its possible that it_Ñés not correctly written,11/12/16,1
metasim,"I found a tool called `tiffinfo` and it reports index 12 as being `12: 53713, 56797, 63993`, which matches what I'm seeing in the code, pointing to the file as an issue (?). So I'm going to try to create a new indexed tiff file and see what I get.",11/12/16,1
metasim,"Aha, I think I know the problem!",11/12/16,1
metasim,It's not a bug really.... it's that TIFF color maps have more resolution than an RGBA packed Int!,11/12/16,1
lossyrob,"ah, because they are actually 6 bytes wide for RGB?",11/12/16,1
metasim,"I think this is what's happening.... check me. When you discretize a 24 bit RGB packed int into three 16 bit uint values ranging from 0 to 65535, the steps look like you're repeating the same byte twice.",11/12/16,1
metasim,Yes!,11/12/16,1
lossyrob,that makes sense,11/12/16,1
lossyrob,how do we deal with that?,11/12/16,1
metasim,I was planning on using the `ColorMap` classes to store the read values.,11/12/16,1
metasim,"Technically, it seems that `gdalinfo` is assuming 1 byte color channel values..... throwing things off.",11/12/16,1
metasim,I'm in the midding of digging through the gdal code but haven't quite found the place where `gdalinfo` might read the 16 bit color channel and report it.,11/12/16,1
metasim,"So, in short, it all depends on how much resolution we care about in allowing a geotrellis user to express via color maps.",11/12/16,1
lossyrob,"interesting. so another _ÑÒTIFF says something, but in practice GDAL does it a slightly different way_Ñù moment",11/12/16,1
metasim,So this isn't an anomaly then? :/,11/12/16,1
lossyrob,"hm, I_Ñém not sure. how can we support 24 bit RGB outside of just the color map? we always assume RGBA packed ints",11/12/16,1
lossyrob,but perhaps someone would want to write them out that way,11/12/16,1
metasim,"I really wanted to use the existing `ColorMap` structures....I had implemented this:```class IndexedColorMap(indexedColors: Seq[Int]) extends IntColorMap(  indexedColors.zipWithIndex.map(p __ê p._2 -> p._1).toMap)```",11/12/16,1
metasim,But now I'm thinking one might need something like a `LongColorMap`?,11/12/16,1
metasim,(to pack three 2 byte color channels into a single type),11/12/16,1
lossyrob,that make sense with the depth,11/12/16,1
metasim,Might end up with something that breaks compatibility with everything else.,11/12/16,1
lossyrob,yeah. we don_Ñét handle Longs at all,11/12/16,1
lossyrob,"was always thinking of a Tile with a Long type, but that breaks our Int/Double interface",11/12/16,1
lossyrob,maybe something to think about for 2.0,11/12/16,1
metasim,"For the purpose that indexed colors serve, it seems silly that they use a short per channel.",11/12/16,1
lossyrob,is there something that isn_Ñét in the ColorMap heirarchy that would solve this problem?,11/12/16,1
lossyrob,right. we could also go the gdal route and assume bytes,11/12/16,1
lossyrob,if that is in fact what GDAL does,11/12/16,1
metasim,"The `Tile`s aren't the issue.... just the `ColorMap`, but I'd argue we just flatten into single byte channels.",11/12/16,1
metasim,I'll try to confirm the GDAL approach.,11/12/16,1
lossyrob,:+1:,11/12/16,1
metasim,"One doesn't technically have to use the `ColorMap` classes... just stick with `(Short, Short, Short)`, but cross compatibility with the PNG/JPG writers would be nice.",11/12/16,1
metasim,Thanks for the late night discussion; much appreciated!,11/12/16,1
lossyrob,for sure. thanks for looking into it!,11/12/16,1
metasim,"[This](https://github.com/OSGeo/gdal/blob/de847cd13c2f9cc612b405454fb516a2717569d2/gdal/frmts/gtiff/gt_overview.cpp#L653) seems relevant, if you're interested.",11/12/16,1
metasim,"In what might be called ""C code at it's finest"", I think I've got the confirmation in [this](https://github.com/OSGeo/gdal/blob/2ec1879b1b3e2122f9b326789d7fa67da2e5dc90/gdal/frmts/gtiff/libtiff/tif_getimage.c#L2430) (along with the previous link), that GDAL is contracting/expanding color map channel values between 8 & 16 bits at I/O.",11/12/16,1
metasim,"It's actually storing them as 16 bit uint, but does bit shifting/multiplication to translate between a 0-255 range and 0-65535 range. No idea why they don't just use a `uint8`.",11/12/16,1
metasim,So I see no reason we can't do the same in geotrellis ;-),11/12/16,1
lossyrob,That sounds like a good plan!,11/12/16,1
zaikin,Hello,11/14/16,1
pomadchin,Hi,11/14/16,1
zaikin,May be somebody help me? https://github.com/geotrellis/geotrellis/issues/1801,11/14/16,1
pomadchin,"Yep, I answered your issue O: ",11/14/16,1
zaikin,@pomadchin thx :+1: ,11/14/16,1
metasim,[![Screen Shot 2016-11-14 at 9.40.39 AM.png](https://files.gitter.im/geotrellis/geotrellis/QLYp/thumb/Screen-Shot-2016-11-14-at-9.40.39-AM.png)](https://files.gitter.im/geotrellis/geotrellis/QLYp/Screen-Shot-2016-11-14-at-9.40.39-AM.png),11/14/16,1
metasim,A color mapped GeoTiff :-),11/14/16,1
fosskers,nice,11/14/16,1
fosskers,what technique did you end up using to colour that?,11/14/16,1
pomadchin,@metasim w0ah looks great btw :D ,11/14/16,1
lossyrob,@metasim did you see the travis fail? Looks like a compiler error in raster-test,11/14/16,1
lossyrob,@metasim I believe this simply needs to change `e` => `testExtent` https://github.com/geotrellis/geotrellis/pull/1802/files#diff-1dda6348e93143e7ff8d297f51e74285R119,11/14/16,1
metasim,@lossyrob Sorry about that.,11/14/16,1
metasim,Sloppy merging on my part.,11/14/16,1
lossyrob,no worries!,11/14/16,1
metasim,"I'm interested in an agressive code review, should you guys have the inclination. I want to make sure it fits with your style and idioms.",11/14/16,1
lossyrob,"First pass looking at it, it_Ñés looking pretty good. I_Ñéll get a chance to look closer later on today, and I_Ñéll try to be agressive :)",11/14/16,1
metasim,:+1: ,11/14/16,1
metasim,@fosskers [Here's the test code](https://github.com/s22s/geotrellis/blob/feature/tiff-color-table/raster-test/src/test/scala/geotrellis/raster/io/geotiff/writer/GeoTiffWriterSpec.scala#L241) that created that image.,11/14/16,1
metasim,"@lossyrob One thing I'm concerned about in the design, and don't really know how to handle, is that index color models really only work for `ByteXXXCellType` (arguably `UShortXXXCellType` is valid, but you end up with huge color tables). There's nothing in there that enforces that restriction.",11/14/16,1
metasim,"So you could write out a paletted GeoTiff that's technically not valid. Wasn't sure if an exception should be thrown, or a log message, or something else.",11/14/16,1
lossyrob,interesting,11/14/16,1
lossyrob,I think an exception is better than allowing invalid tiffs being created,11/14/16,1
lossyrob,"@metasim just reviewed; it_Ñés just nitpicks. I don_Ñét know what your schedule is like, but I was planning on getting an RC out tomorrow - if we can get this into 1.0, that would be great",11/14/16,1
schBen,"Hi! I am playing around with the geotrellis-landsat-emr-demo.https://github.com/geotrellis/geotrellis-landsat-emr-demo I see there is a request /catalog that gives me an overview of layers. For a layer it is shown what times and extent are available. Is this somehow grouped, so can I know which extent is available for which time?",11/15/16,1
metasim,@lossyrob Working on it now.,11/15/16,1
pomadchin,"@Ben so you can modify manually this info, to get necessary data from attribute store, and yes, attribute store has layer extent information available ",11/15/16,1
pomadchin,"sry afk for 30 mins, will throw you code examples if it's necessary",11/15/16,1
pomadchin,"@Ben oops, re read your msg, what do you mean by grouping? Mb you can describe your case :D ",11/15/16,1
lossyrob,"@metasim thanks, replied to comments",11/15/16,1
metasim,"@lossyrob On an incombatible sample format, what exception type would you recommend? `IllegalArgumentException`?",11/15/16,1
metasim,`IllegalFormatException`?,11/15/16,1
lossyrob,there is this one https://github.com/geotrellis/geotrellis/blob/91ddc027d6d555e8c72670ac60e634c9d1c1f4ce/raster/src/main/scala/geotrellis/raster/io/geotiff/reader/GeoTiffReader.scala#L33,11/15/16,1
lossyrob,we could push it out to make it not just for reading,11/15/16,1
metasim,"Seems wierd to say ""malformed"" before it's fully formed....",11/15/16,1
lossyrob,TRUE,11/15/16,1
lossyrob,does it make sense to create one specifically for this type of case?,11/15/16,1
metasim,Could do. Personal preference I think. I tend to have a bias toward just reusing something already out there.,11/15/16,1
lossyrob,"IllegalFormatException sounds good, but the docs specifically talk about strings",11/15/16,1
metasim,But I don't have a technical argument for leaning that way.,11/15/16,1
metasim,Lemme dig around.,11/15/16,1
metasim,How about `GeoTiffWriterLimitationException`,11/15/16,1
lossyrob,"I tend to as well. but I think in this case, we have on for the reader which has been helpful to be explicit about, and I think it_Ñés a pretty specific type of error for writing as well, so might be into a custom exception in this case",11/15/16,1
metasim,It's in `CoordinateSystemParser`.... could be moved out.,11/15/16,1
lossyrob,hmm_Ñ_we had that as an indication of a limitation of our writer,11/15/16,1
lossyrob,"like, we couldn_Ñét handle that case",11/15/16,1
metasim,"Well, that's kinda what this is....",11/15/16,1
lossyrob,"This isn_Ñét a limitation, it just isn_Ñét in spec",11/15/16,1
metasim,fair,11/15/16,1
lossyrob,"To argue for Malformed a bit, I_Ñéd say that if we do this on GeoTiff creation, while the GeoTiff hasn_Ñét been written, it has been _ÑÒformed_Ñù (or you are trying to form it)",11/15/16,1
metasim,"I'm cool with that, or we create a new `IncompatibleGeoTiffParametersException`",11/15/16,1
metasim,(or whatever),11/15/16,1
lossyrob,"I like that, that latter should be fine",11/15/16,1
metasim,'k,11/15/16,1
metasim,"Should I nest it in `object GeoTiffWriter`, or do you prefer package level?",11/15/16,1
lossyrob,I would put it package level,11/15/16,1
lossyrob,Thanks for the quck turnaround @metasim !,11/15/16,1
metasim,My pleasure to contribute! I appreciate you accepting the PR.,11/15/16,1
lossyrob,Just released 1.0.0-RC1. Our org has changed to `_Ñùorg.locationtech.geotrellis_Ñù`. Example of how you can reference it is found in https://github.com/geotrellis/geotrellis-libya-weighted-overlay/commit/2cc8bc30a7f7e1be30c52f036841645f741db1f7,11/16/16,1
pomadchin,:tada: :tada: :tada: :tada: :tada: :tada: ,11/16/16,1
metasim,:clap: :clap: :clap: :clap: :clap: :clap: ,11/16/16,1
fosskers,nice,11/16/16,1
david-2012,"Hi, @lossyrob . I am new to geotrellis. And I am trying to use geotrellis and spark to build ",11/18/16,1
david-2012,"a server which can provide a tms service.  Currently it worked if I used geotrellis-spark etl object to build a pyramid of tilelayerRDD via etl.load, etl.tile  functions from the tiff files. Is there any approach for building a pyramid from the tilelayerRDD(maybe this tilelayerRDD is transformed from other operations, whatever..), without saving this tilelayerRDD in the file system and reload it again. Appreciated very much!",11/18/16,1
lossyrob,"Hi @david-2012 you can. ETL is just some convienience wrapping around some functionality, so you could unfurl that functionality and use the inner parts manually (e.g. https://github.com/geotrellis/geotrellis/blob/master/spark/src/main/scala/geotrellis/spark/pyramid/Pyramid.scala)",11/18/16,1
lossyrob,or you could use the saveAction param on Etl.save to avoid saving,11/18/16,1
lossyrob,"I think I would need to know what you_Ñéd like to do with the pyramid (which in effect is a `Map[Int, TileLayerRDD[K]]`) without saving it to be able to suggest further",11/18/16,1
lossyrob,GeoDocker question @pomadchin if your around - got clues about why a container that is based on geodocker-spark would not put any console output when running a spark-submit? It_Ñés almost as if the logger properties aren_Ñét being recognized from the parent geodocker-spark container in the child container,11/18/16,1
lossyrob,also should we have a geodocker gitter?,11/18/16,1
tkunicki,"haha, was just wondering if I should ask my GeoDocker Qs here",11/18/16,1
lossyrob,"lol. yeah this is the place so far, but I think it would be good to have a separate channel",11/18/16,1
lossyrob,"I_Ñéll make one now, but feel free to ask any q_Ñés here",11/18/16,1
lossyrob,"gitter is not being awesome about letting me make a channel, erroring without a message, so Ill have to figure out what_Ñés going on with that",11/18/16,1
lossyrob,@tkunicki https://gitter.im/geodocker/geodocker,11/18/16,1
david-2012,"@lossyrob , thanks for your kindly replies. What I'd like to do is to display  directly the result of some arithmetic operation on the TileLayerRDD objects,which represents the geotiff data via the leaflet directly using pyramid mode, without saving the result on accumulo or HDFS.Currently, there are two issues for me:1. The TileLayerRDD holds an extend for the whole raster in its metadata, however, before building the pyramid, I need get the projectExtend for each tile to call its function tileToLayout. Do you have any suggestions on how to obtain each tile projectExtend from the whole raster extend.2. Using the etl.load, we can get the suitable SpatialKey index range for building pyramid with specific zoom Index. Without etl.load, how can I transform the TileLayerRDD SpatialKey index  range for this purpose.Thanks agian for your answers.",11/21/16,1
pomadchin,hi @david-2012 ,11/21/16,1
david-2012,yes,11/21/16,1
david-2012,@pomadchin  yes ?,11/21/16,1
pomadchin,"Ok, so the steps would be the following: 1. read everything as (ProjectedExtent, Tile) pairs2. retile them into (SpatialKey, Tile)3. build the pyramid",11/21/16,1
pomadchin,sec I_Ñéll prepare you a codesample,11/21/16,1
david-2012,"@pomadchin , thanks a lot!",11/21/16,1
pomadchin,@david-2012 there is an code sample:  https://gist.github.com/pomadchin/f8fe77f24e84a862fe92c4d6e4147c96,11/21/16,1
pomadchin,another working example can be found in our Landsat EMR demo: https://github.com/geotrellis/geotrellis-landsat-emr-demo/blob/master/ingest/src/main/scala/demo/LandsatIngest.scala#L23-L59,11/21/16,1
pomadchin,"And the most unpopular variant is to use Ingest object: https://github.com/locationtech/geotrellis/blob/master/spark/src/main/scala/geotrellis/spark/ingest/Ingest.scalaUsage example: ```scalaval source: RDD[ProjectedExtent, Tile] = ???val destCrs: CRS = ???val layoutScheme: LayoutScheme = ???Ingest[ProjectedExtent, SpatialKey](source, destCrs, layoutScheme) { (rdd, zoom) =>  // your logic here, persist or smth else}```",11/21/16,1
pomadchin,O: any questions about these codelines are appreciated :D ,11/21/16,1
david-2012,"thanks, very appreciated. That's all I want.  By the way,  I transform the TileLayerRDD to MapHadoopRDD by map the SpatialKey to ProjectedExtent for each tile so that the pyramid can be built from the tileLayerRDD directly.   And it finally worked.",11/21/16,1
pomadchin,glad it worked for you :D,11/21/16,1
pomadchin,there is a single function to convert your rdd to tiled rdd: `tileToLayout`,11/21/16,1
david-2012, :D ,11/21/16,1
pomadchin,:D :D just was not sure where you are in your investigation process :D ,11/21/16,1
david-2012,:smile:  this problem has been resolved!,11/21/16,1
david-2012,Very appreciated!,11/21/16,1
lossyrob,@pomadchin are you at the point where you_Ñéve loaded up an RDD of point cloud data with the input formats (even locally)?,11/21/16,1
pomadchin,"yo! yep, not very well checked but i can load everything as `path -> PackePoints` rdd pairs",11/21/16,1
pomadchin,"there are {max,min}{x,y,z} data in files metadata, that means that would be not too hard to create a ProjectedExtent key",11/21/16,1
lossyrob,that is awesome,11/21/16,1
pomadchin,though I_Ñém still curious about z component; what to do with it?,11/21/16,1
lossyrob,z component of extent?,11/21/16,1
pomadchin,"eh, as it_Ñés a 3d data; so they have 3d bounds",11/21/16,1
pomadchin,I_Ñéll load into gist sec,11/21/16,1
lossyrob,"I think if we_Ñére going to calculate bounds, we might as well have a Bounds(xmin, ymin, zmin, xmax, ymax, zmax)",11/21/16,1
pomadchin,https://gist.github.com/pomadchin/521ee7217ce3d8d9cbca96e9702657d6,11/21/16,1
pomadchin,kk; ,11/21/16,1
pomadchin,"btw, mb this thing would be interesting for you and for everybody here: https://github.com/pomadchin/pdal-jni ",11/21/16,1
pomadchin,^^' pdal-jni usage sbt template ,11/21/16,1
pomadchin,@lossyrob if you have any comments that would be cool if you_Ñéll throw them to me; as there can be lots of interesting design / math / etc things,11/21/16,1
lossyrob,"well if we can get Las files in any form into an RDD, then we are cooking with gas",11/21/16,1
lossyrob,The next thing would be to break them out by spatialkeys,11/21/16,1
lossyrob,over some specific tile layout,11/21/16,1
lossyrob,I wonder if there would be some streaming way to create the DEM as we are deciding what spatial keys the points belong to,11/21/16,1
lossyrob,@jamesmcclain know what I mean?,11/21/16,1
lossyrob,is there a streaming tinning algorithm that we could apply here?,11/21/16,1
lossyrob,I guess because we_Ñére using the JTS triangulation we_Ñére stuck with it_Ñés method of doing it,11/21/16,1
jamesmcclain,I am not sure what you mean,11/21/16,1
jamesmcclain,"It depends on what you mean by ""as we are deciding what spatial keys""",11/21/16,1
lossyrob,"so, we have basically an `RDD[(Extent, PiontCloud)]`",11/21/16,1
lossyrob,"in order to turn those into `RDD[(SpatialKey, PointCloud)]`, we have to locate each point in a SpatialKey",11/21/16,1
jamesmcclain,I guess what I am getting at is: we cannot do a triangulation over the entire set of points,11/21/16,1
lossyrob,"Can we do parital triangulations, and then merge them",11/21/16,1
jamesmcclain,So it is necessary to have some rough idea of where the keys are with respect to groupings of points,11/21/16,1
lossyrob,"so do a (SpatialKey, Mesh)] on each element, and then merge those elements in a reduceByKey",11/21/16,1
jamesmcclain,So that triangulations can be done on subsets of the points,11/21/16,1
jamesmcclain,"It is technically possible, to merge triangulations, but it is not a good idea",11/21/16,1
lossyrob,why not?,11/21/16,1
jamesmcclain,Because it introduces the complexity of fixing up the triangulations at the seams between areas,11/21/16,1
lossyrob,"ok. that was the question, if there was a nice _ÑÉstreaming_Ñé way to do it, where you could add points dynamically",11/21/16,1
jamesmcclain,"That fixup process is entirely possible, but it is no less complicated (from the standpoint of the person writing the code) than doing an entire triangulation",11/21/16,1
lossyrob,what about the standpoint of CPU time,11/21/16,1
jamesmcclain,Well the problem is that it is difficult to get right,11/21/16,1
jamesmcclain,The same robustness issues that we ran into before with the Delaunay triangulator would show up again,11/21/16,1
jamesmcclain,"That is, unless the overall triangulation does not have to be Delaunay (but that would cause problems with the interpolation algorithm)",11/21/16,1
lossyrob,dang. alright,11/21/16,1
lossyrob,might be a future improvement/optimization,11/21/16,1
jamesmcclain,"I mean there are things you can do, but the kinds of things that I can think of right now would be fairly ad hoc",11/21/16,1
lossyrob,"rather then iterating through the points once to place them, and then iterating through them again to triangulate ",11/21/16,1
jamesmcclain,"For example, you can do Delauney over a collection of points, then to merge adjacent collections of points, you throw away part of the triangulation that you already have and use the existing triangulator over some overlapping portion",11/21/16,1
jamesmcclain,But that does not seem to be preferable to just triangulating the entire thing once you have it,11/21/16,1
lossyrob,makes sense,11/21/16,1
jamesmcclain,"I just did a little bit of googling, and there definitely are divide and conquer Delaunay algorithms.  If we could find robust predicates for Java/Scala they would probably be usable",11/21/16,1
jamesmcclain,"Actually, I missed part of what you wrote before.  When you talk about not wanting to iterate through the points twice, there is the possibility of using an incremental algorithm like bowyer-watson.  The problem with that is 1) the robustness of predicates and 2) this algorithim (like all Delaunay triangulation algorithms) requires more than linear time, so it might not be exactly what you are looking for",11/21/16,1
lossyrob,perhaps. it was a thought_Ñ_maybe something to chew on a bit more down the road,11/21/16,1
echeipesh,"A little late the conversation, but likely there is no way to generate the DEM as we_Ñére tiling. The clever approach in tin2dem  DEM streaming algorithms relies on knowing when they have seen the final point for an area, which presupposes iterating over ther points at least once (in reality 3 passes are required).",11/22/16,1
echeipesh,I would guess that pratically just  tiling out the point cloud to a tile layout is within itself very useful because it would enable bunch of parallel processing by removing _ÑÒhave I seen all the data_Ñù question.,11/22/16,1
echeipesh,PSA: Spark 2.0 no longer depends on akka-actor. They have re-implemented their RPC using netty. So any applications that have depended on `spark-core` to bring in the akka dependency will have to do it explicitly as of RC1.,11/22/16,1
willtemperley,"Hi, I've got a quick question about proj4j.  I've been looking into the history of it all and I'm wondering what the current plans are?",11/22/16,1
willtemperley,https://www.locationtech.org/proposals/proj4j,11/22/16,1
willtemperley,The repo seems to have been abandoned however ,11/22/16,1
willtemperley,https://github.com/dwins/proj4j/,11/22/16,1
willtemperley,So is the plan still to move to locationtech?,11/22/16,1
lossyrob,"That_Ñés still the current plan. However it_Ñés been held up by a couple of things: dwins moved on to another job, GeoTrellis is still trying to get itself through the LocatioTech move process, and so no one has been able to move it forward",11/22/16,1
lossyrob,"Also, there was immediate discussions on a rename of it from the Eclipse side, which I_Ñém opposed to",11/22/16,1
lossyrob,"so that issue still needs to be worked out, and then the whole LocationTech incubation process needs to be completed.",11/22/16,1
lossyrob,"So I_Ñéd say the effort is stalled, but not abandoned ",11/22/16,1
willtemperley,"Right, I see, thanks.  Where's the discussion re. the name change?",11/22/16,1
lossyrob,let me see if I can find it. Hopefully was on a public list,11/22/16,1
lossyrob,"Actually looking back through my emails, it looks like the legal team approved the name for use",11/22/16,1
willtemperley,"OK, but the group would change from org.osgo to locationtech?",11/22/16,1
willtemperley,osgeo,11/22/16,1
lossyrob,No it would be a project that was in both,11/22/16,1
lossyrob,was the hope,11/22/16,1
lossyrob,A joint OSGeo/LocationTech project would be great IMO,11/22/16,1
lossyrob,that_Ñés why I opposed the name change,11/22/16,1
willtemperley,Right so basically nothing would change except the improvements made by GeoTrellis?,11/22/16,1
lossyrob,"it just wasn_Ñét getting any support as it was, so the hope was putting it in locationtech would at least give us a platform to publish releases",11/22/16,1
lossyrob,yeah,11/22/16,1
lossyrob,well and the repo would be in the location tech github,11/22/16,1
lossyrob,and it would go through the locationtech IP process,11/22/16,1
willtemperley,Right,11/22/16,1
lossyrob,and releases published on LT servers,11/22/16,1
lossyrob,https://bugs.eclipse.org/bugs/show_bug.cgi?id=495425,11/22/16,1
lossyrob,https://locationtech.org/mhonarc/lists/location-iwg/msg01525.html,11/22/16,1
willtemperley,Ah thanks,11/22/16,1
willtemperley,"Given it's really a Java lib, it would probably make sense to remove the SBT requirement",11/22/16,1
lossyrob,"SBT can build java projects fine, but if someone wanted to replace it with a maven build I wouldn_Ñét stop them ;)",11/22/16,1
willtemperley,Well it'd be the first thing I would do ;),11/22/16,1
lossyrob,"also if someone felt like trying to push it forward a bit from the current stalled status, I_Ñéd be happy to help guide that. Just don_Ñét have the time to focus on it explicitly",11/22/16,1
lossyrob,but yeah. that would be a huge contribution the the JVM FOSS4G community if we got an actual release out of Proj4j ,11/22/16,1
willtemperley,I'd be happy to put some energy into it,11/22/16,1
lossyrob,great,11/22/16,1
willtemperley,I don't know much about the locationtech process ,11/22/16,1
lossyrob,"I have to figure out what the next steps are, and figure out what access you_Ñéd need to be granted",11/22/16,1
lossyrob,I could help with that,11/22/16,1
willtemperley,That would be great if you could. ,11/22/16,1
lossyrob,for sure. I just DM_Ñéd you about setting up an eclipse account,11/22/16,1
lossyrob,"I_Ñéll have to think about what the next step would be, so give me a bit to work that out and I_Ñéll let you know",11/22/16,1
willtemperley,Thanks! ,11/22/16,1
lossyrob,are you on the locationtech-iwg mailing list?,11/22/16,1
willtemperley,No,11/22/16,1
lossyrob,https://locationtech.org/mailman/listinfo/location-iwg,11/22/16,1
lossyrob,"I_Ñéll put out an email requesting what next steps are, so other people can correct me if I_Ñém wrong",11/22/16,1
willtemperley,"Great, I've signed up to the list",11/22/16,1
lossyrob,cool,11/22/16,1
lossyrob,so we need to put together an initial contribution,11/22/16,1
lossyrob,which is the proj4j codebase that is separated out from GeoTrellis,11/22/16,1
lossyrob,I didn_Ñét see how complete dwins_Ñés repo is of that,11/22/16,1
willtemperley,There is only one or maybe two commits,11/22/16,1
lossyrob,"getting to the point where it_Ñés a good intitial contrib, which could include the maven switch, would be the first step",11/22/16,1
lossyrob,"we then submit that in what_Ñés called a _ÑÒCQ_Ñù, which then the legal team will mull over and check for any IP issues",11/22/16,1
willtemperley,"Right, I see. ",11/22/16,1
lossyrob,"we have to also catalog any of it_Ñés dependencies and submit them as separate CQs. I_Ñém unclear if there are any that are needed, it_Ñés pretty bare bones",11/22/16,1
willtemperley,yes two or three basic deps IIRC,11/22/16,1
lossyrob,"also I submitted a CQ for proj4j for GeoTrellis, which got approved, so I think the process will be pretty light",11/22/16,1
lossyrob,"If you want to fork dwins_Ñés repo and get it into a good place, where it would be ready for that initial contribution CQ, that would be a good first step",11/22/16,1
willtemperley,"I can do that, would the code in the current GeoTrellis release be much changed?",11/22/16,1
willtemperley,It looks like dwins' repo was extracted on the 18th of May so probably not too far behind,11/22/16,1
lossyrob,I don_Ñét think we changed much in the java code,11/22/16,1
willtemperley,There seems to have been a lot of work done to tidy up license headers etc however,11/22/16,1
lossyrob,"yeah. that can be ignored, since it will require it_Ñés own headers",11/22/16,1
lossyrob,"before that, the last commit was March",11/22/16,1
lossyrob,on main at least,11/22/16,1
lossyrob,also to consider is that we wrote all the tests in scala,11/22/16,1
lossyrob,looks like _ÑÒtest_Ñù folder is also last updated in March,11/22/16,1
lossyrob,so dwins_Ñés version should be the correct version,11/22/16,1
willtemperley,I'll make a start from there then.,11/22/16,1
lossyrob,but yeah. that Scala tests might put a wrench into the maven transfer,11/22/16,1
lossyrob,we added a lot of tests from the OSGeo fork,11/22/16,1
lossyrob,but did so in scala (and ported the old tests to scala),11/22/16,1
willtemperley,"If I was able to port the tests reasonably easily to Java, would you be fine with making it a pure Java lib?",11/22/16,1
willtemperley,It just doesn't feel like it needs scala,11/22/16,1
willtemperley,Or I don't,11/22/16,1
lossyrob,Yeah I would be fine with that,11/22/16,1
willtemperley,Great.  I think there would be more chance of community involvement if it was pure Java,11/22/16,1
lossyrob,that makes sense,11/22/16,1
lossyrob,thanks for helping out with this!,11/22/16,1
willtemperley,Thanks for putting me on the right track :),11/22/16,1
lossyrob,"no problem. feel free to keep chatting about it here (until we have a proj4j gitter, which will be great)",11/22/16,1
david-2012,"Hi, @pomadchin I use the following function to build up a pyramid from TileLayerRDD. And it worked for some TileLayerRDD. For example, if rsterRDD is a tilelayerRDD read from Accumulo, the following function could build a correct pyramid list.  rster = reader.read[SpatialKey, Tile,      TileLayerMetadata[SpatialKey]](LayerId(layerId, 0)).convert(DoubleConstantNoDataCellType)However, if the rsterRDD  came from the a few stages of transformation and actions in the Spark, most of tiles in the pyramid list built by the following function are invalid, i.e. ND. such as rster = new ContextRDD(result, metaData)And I compared both types of TileLayerRDD, 1. the partition length of the former is 1, and the later length is 10, depended on my spark conf setting.   Besides, I repartitioned the later and changed its partition length to 1, it didn't help.2. the SpatialKey index order of both RDD is different before tileToLayout operation.For example, the former SpatialKey index order was :SpatialKey(0,0)SpatialKey(1,0)SpatialKey(0,1)SpatialKey(1,1)SpatialKey(2,0)SpatialKey(3,0)SpatialKey(2,1)SpatialKey(3,1)SpatialKey(0,2)SpatialKey(1,2)SpatialKey(0,3)SpatialKey(1,3)SpatialKey(2,2)SpatialKey(3,2)SpatialKey(2,3)SpatialKey(3,3)SpatialKey(4,0)SpatialKey(5,0)SpatialKey(4,1)SpatialKey(5,1)SpatialKey(4,2)SpatialKey(5,2)SpatialKey(4,3)SpatialKey(5,3)SpatialKey(0,4)SpatialKey(1,4)SpatialKey(2,4)SpatialKey(3,4)SpatialKey(4,4)SpatialKey(5,4)the later SpatialKey index order is:SpatialKey(3,1)SpatialKey(2,4)SpatialKey(3,2)SpatialKey(5,3)SpatialKey(4,0)SpatialKey(3,0)SpatialKey(0,4)SpatialKey(4,4)SpatialKey(0,0)SpatialKey(1,1)SpatialKey(2,2)SpatialKey(2,0)SpatialKey(0,1)SpatialKey(5,0)SpatialKey(3,4)SpatialKey(4,3)SpatialKey(1,4)SpatialKey(1,0)SpatialKey(2,3)SpatialKey(1,2)SpatialKey(2,1)SpatialKey(5,1)SpatialKey(4,1)SpatialKey(3,3)SpatialKey(1,3)SpatialKey(0,2)SpatialKey(5,4)SpatialKey(0,3)SpatialKey(5,2)SpatialKey(4,2)Why the former SpatialKey indexes is ordered in this way? I have no ideas.I was stucked here. Do you have any ideas or clue for the reason why the following function didn't work for the rsterRDD after transformation and actions in the spark. Thanks for your comments.def buildPyramidFromTileLayeRDD(rsterRDD : TileLayerRDD[SpatialKey], maxZoom : Int): Map[Int, TileLayerRDD[SpatialKey]] = {   val  bc_resultMetadata = sc.broadcast(rsterRDD.metadata)  val value_resultMetadata = bc_resultMetadata.value  println(s""the value_resultMetadata.layout.tileLayout   val tileExtents = new TileExtents(value_resultMetadata.extent, value_resultMetadata.layout.tileLayout)  val sourceTiles = rsterRDD.map {    case (k, tile) => {      val extent = tileExtents(k.col, k.row)      println(s""k (${k.col}, ${k.row}) extent ${extent.xmin} ${extent.xmax} ${extent.ymin} ${extent.ymax}"")      val projectedExtent = new ProjectedExtent(extent, value_resultMetadata.crs)      (projectedExtent, tile)    }  }    val method = NearestNeighbor  val layoutScheme = ZoomedLayoutScheme(rsterRDD.metadata.crs, 256)  val (_, tileLayerMetadata) = TileLayerMetadata.fromRdd(sourceTiles, rsterRDD.metadata.crs, layoutScheme, maxZoom)  val tiledRdd = sourceTiles.tileToLayout(tileLayerMetadata, method).cache()  val contextRdd = ContextRDD(tiledRdd, tileLayerMetadata)  val (_, tileLayerRdd) = contextRdd.reproject(rsterRDD.metadata.crs, layoutScheme)  tileLayerRdd.persist  var zoomIndex = maxZoom + 1  var rsterRdd = tileLayerRdd  val sourceLayout = rsterRdd.metadata.layout  pyramid(zoomIndex) = rsterRdd    def buildPyramid(zoom: Int, rdd: TileLayerRDD[SpatialKey]): List[(Int, TileLayerRDD[SpatialKey])] = {    if (zoom >= 1) {      rdd.persist()      val layoutScheme = ZoomedLayoutScheme(rdd.metadata.crs, 256)      val pyramidLevel@(nextZoom, nextRdd) = Pyramid.up(rdd, layoutScheme, zoom)      pyramidLevel :: buildPyramid(nextZoom, nextRdd)    } else {      List((zoom, rdd))    }  }  val pyramid = buildPyramid(zoomIndex, rsterRdd).toMap  pyramid}",11/24/16,1
echeipesh,"@david-2012 Neither the number of partitions nor the ordering of the keys would matter to the output of the pyramid. Matching up keys is taken care of by spark when working with `RDD[(K,V)]`. At a glance your function looks a lot like `Pyramid.levelStream` so you could double check if that surves your needs (https://github.com/locationtech/geotrellis/blob/master/spark/src/main/scala/geotrellis/spark/pyramid/Pyramid.scala#L131-L148). Not sure what could cause the ND tiles. The pixels of  tiles being merged as you go up the pyramid do not overlap, so its hard to blame anything about the merge function. Silly question but Is it possible to verify that whatever operations you_Ñére doing to the `rasterRDD` before prymid actually produces tiles with valid pixel values ? You can always pull a few out and use `Tile.ascriiDraw` on them or use `SinglebandGeoTiff` to write them out as a file.",11/24/16,1
lossyrob,"Also be wary of the CellType of your tiles. Some versions of geotrellis have bugs around merging tiles with certain cell types. A good way to avoid this is calling `convert` on the source tile RDD before tiling to something that has a constant nodata value, e.g. `convert(DoubleConstantNoDataCellType)`",11/24/16,1
david-2012,"@lossyrob  @echeipesh , thanks for your comments and suggestions. The pyramid can be built correctly for the later TileLayerRDD after the following change and no more ND tiles.-  val sourceTiles = rsterRDD.map {+  val sourceTiles = rsterRDD.convert(DoubleConstantNoDataCellType).map {Thanks very much for your response and kindly help. :smile: ",11/25/16,1
lossyrob,__GeoTrellis 1.0.0-RC2 is released__. It contains a few minor bug fixes around ETL. Please upgrade and help test!,11/27/16,1
andypetrella,@lossyrob nice buddy! Would you have a nice example with relevant data (landsat?) in your bag that I can use easily as a demo using our product (which includes the spark-notebook) ? :-D,11/27/16,1
pomadchin,"@andypetrella i believe we need to update our demos O: dunno how it would work (a lightwight api example): https://github.com/geotrellis/geotrellis-landsat-tutorial ",11/27/16,1
pomadchin,*not sure how it relevant to our current api btw,11/27/16,1
pomadchin,"think that_Ñés what you need (the link above) for the notebook demo;P.S. a more heavy example: https://github.com/geotrellis/geotrellis-landsat-emr-demo (with the ingest process)",11/27/16,1
andypetrella,"thanks @pomadchin ! Indeed, the first one looks easier to start with, but the second is really nice! Both will require sometime for me to wrap my head around, they are elaborated which is what I was expecting for a relevant demo !!! Thanks",11/27/16,1
lossyrob,"@andypetrella thanks for reaching out! Hope all is well on your side. The demos that Grisha pointed out are why I would also recommend. We have a couple more coming down the pipeline, but not yet ready - I can ping you when they are",11/27/16,1
andypetrella,"@lossyrob hey mate, that is great thanks, please poke me back when you are! Btw, things are going very well at Kensu (new name of the company) -- we are now 11, and the product is rocking customers' data science teams :smile: ",11/27/16,1
andypetrella,We will probably have some Geo project on spark where we probably have some interesting data which I'll like to cross with  some bands ;),11/27/16,1
lossyrob,"awesome, sounds like GeoTrellis might be able to help out with that sort of work",11/27/16,1
lossyrob,congrats on the growth!,11/27/16,1
lossyrob,"Also we have plans on Jupyter integration via Apache Toree. Would love to hear your thoughts on Toree and the Jupyter notebook trend, and am wondering if we can leverage the work there to also integrate with Spark Notebook/your product",11/27/16,1
andypetrella,Ahaha thanks sure we can discuss that. The spark notebook offers a really simple integration (I did some tests redoing your tes cases in a notebook),11/27/16,1
andypetrella,"Since it's jvm based you don't have to fight with python, py4j, packages and such things like pyspark :)",11/27/16,1
andypetrella,We can have a call early in 2017 for instance,11/27/16,1
lossyrob,that would be great.,11/27/16,1
lossyrob,"well until then, definitely let us know if we can help along with an example, would love to see a GeoTrellis example in spark notebook!",11/27/16,1
schBen,"Hi! I have an RDD[(ProjectedExtent, Tile)] that I want to save as a GeoTiff. However, CoordinateSystemParser gives me an exception: ""This GeoTiff writer does not currently support the projection +proj=laea ..."". (It is EPSG:3035). These projections seem to be defined in class CoordinateSystemParser in some way. What do I have to do to make this projection work, too? ",11/28/16,1
pomadchin,@schBen looks like we don_Ñét support it yet: https://github.com/locationtech/geotrellis/blob/a00c35b928e96083188d91734252d66574e3b4a7/raster/src/main/scala/geotrellis/raster/io/geotiff/writer/CoordinateSystemParser.scala#L159-L170,11/28/16,1
pomadchin,think @lossyrob can leave more comments about it,11/28/16,1
lossyrob,"This is an unfortunate hole in GeoTrellis that we need to patch up. Because we took the GeoTiff writing code from GDAL_Ñés proj4 CRS writing, and that is incomplete (they use WKT parsing to write out the GeoTiffs), we don_Ñét have this funcitonality",11/28/16,1
lossyrob,"to make the projection work, we_Ñéd need to add that projection type",11/28/16,1
schBen,Okay. So I'd have to add the respective properties lists to the `CoordinateSystemParser` ?,11/28/16,1
lossyrob,yes,11/28/16,1
lossyrob,"but it relooking at the code, it seems like if the CRS has a valid EPSG code, it should just write the code",11/28/16,1
lossyrob,https://github.com/locationtech/geotrellis/blob/master/raster/src/main/scala/geotrellis/raster/io/geotiff/writer/CoordinateSystemParser.scala#L114,11/28/16,1
lossyrob,and only if it doesn_Ñét does it fall to this line https://github.com/locationtech/geotrellis/blob/master/raster/src/main/scala/geotrellis/raster/io/geotiff/writer/CoordinateSystemParser.scala#L130,11/28/16,1
lossyrob,which gets you here https://github.com/locationtech/geotrellis/blob/master/raster/src/main/scala/geotrellis/raster/io/geotiff/writer/CoordinateSystemParser.scala#L164,11/28/16,1
lossyrob,how is that CRS being created?,11/28/16,1
schBen,we read it from a geotiff that we read in from a file to the RDD. then we want to map this to the result RDD and save it ,11/28/16,1
schBen,so it comes from the geotiff file,11/28/16,1
lossyrob,"how do you know the EPSG code, is that coming from the CRS that_Ñés being read in or is that intrinsic knowledge you have?",11/28/16,1
lossyrob,"if the CRS is being parsed in, but the EPSG code is unknown, you could get into this situation",11/28/16,1
lossyrob,there_Ñés a way to set the CRS into the GeoTiff if you know what it is but the one being read in is misbehaving,11/28/16,1
lossyrob,"if you stitch the RDD, you get a Raster[Tile] or Raster[MultibandTile]",11/28/16,1
lossyrob,then you can wrap that into a GeoTiff and save it,11/28/16,1
lossyrob,with an explicit CRS,11/28/16,1
hjaekel,"@lossyrob exactly that was the situation for @schBen 's question: the source geotiff had a CRS definition for EPSG:3035, but without the AUTHORITY[""EPSG"",""3035""] in the WKT, that means UserDefinedProjectionType for GeoTrellis. When we add the correct WKT, we get into the other branch and we can write the geotrff",11/28/16,1
lossyrob,"@hjaekel nice, that is good to know!",11/28/16,1
lossyrob,thanks for speaking up :),11/28/16,1
hjaekel,some programs write crappy CRS definitions :(,11/28/16,1
schBen,thanks @lossyrob ,11/28/16,1
lossyrob,we really need to fix our CRS writing to be able to handle arbitrary proj4 strings,11/28/16,1
lossyrob,"I_Ñéll have to write an issue up for that when I have the time, I_Ñéll make a note",11/28/16,1
vsimko,Hi! How to cite geotrellis in papers ?,11/28/16,1
lossyrob,Interesting question. How are other open source projects cited in papers?,11/28/16,1
lossyrob,"The trademark of GeoTrellis belongs to the Eclipse Foundation, while the code is mostly copyright Azavea, Inc and contributors",11/28/16,1
lossyrob,not sure if that_Ñés useful information,11/28/16,1
lossyrob,"perhaps something like `Eclipse Foundation, Azavea and contributors (2016). GeoTrellis, Apache 2.0 License. https://github.com/locationtech/geotrellis` ?",11/28/16,1
vsimko,"For example, R packages usually provide some kind of CITATION file, e.g. https://github.com/taiyun/corrplot/blob/master/CITATION",11/28/16,1
vsimko,"I think, it is always a good thing to explicitly define how the software be cited in papers.",11/28/16,1
lossyrob,that is a good point. we_Ñéll have to work that out and update the docs,11/28/16,1
vsimko,"thanks, untill then I'll use the following:```@misc{github:Geotrellis,  Author = {{Eclipse Foundation} and Azavea and contributors},  Howpublished = {https://github.com/locationtech/geotrellis},  Title = {{GeoTrellis, Apache 2.0 License}},  Year = {2016}}```",11/28/16,1
jfbourgon,Would be currious to see what would be the simplest way to mosaic an RRD of RasterTile such as,11/30/16,1
jfbourgon,``` org.apache.spark.rdd.RDD[geotrellis.raster.Raster[geotrellis.raster.Tile]] ``` ,11/30/16,1
lossyrob,"you would convert that RDD into a `RDD[(ProjectedExtent, Tile)]`, and then use `tileToLayout` to tile the raster. Tiles that overlap would be merged on a first-come-first-serve basis. Doing a more nuanced mosaic is a bit more work",11/30/16,1
lossyrob,very rough sketch,11/30/16,1
lossyrob,"```scalaval rdd: RDD[Raster[Tile]] = ???val crs: CRS = ???val keyed: RDD[(ProjectedExtent, Tile)] =  rdd.map { raster =>    (ProjectedExtent(raster.extent, crs), raster.tile)  }// Will make 256 by 256 tilesval layoutScheme =  FloatingLayoutScheme(256, 256)val metadata =  keyed.collectMetadata[Spatialkey](layoutScheme)ContextRDD(keyed.tileToLayout[SpatialKey](metadata), metadata)```",11/30/16,1
lossyrob,"let me know if that leads you in the right direction, and other questions",11/30/16,1
jfbourgon,"Thanks. Here is the code snippet that worked for me:```scalaval keyed: RDD[(ProjectedExtent, Tile)] =  filtered.map { raster =>    (ProjectedExtent(raster.extent, crs), raster.tile)  }// Will make 256 by 256 tilesval layoutScheme = FloatingLayoutScheme(256,256)val (_: Int, metadata: TileLayerMetadata[SpatialKey]) = rdd.collectMetadata[SpatialKey](crs, layoutScheme)val tilerOptions =  Tiler.Options(resampleMethod = Bilinear,  partitioner = new HashPartitioner(rdd.partitions.length))val mosaic = ContextRDD(keyed.tileToLayout[SpatialKey](metadata, tilerOptions), metadata).stitch```However is was wondering if a mosaic function would exist on RasterTile that would be aligned with the specs defined here: [MosaicSpec](https://github.com/locationtech/geotrellis/blob/master/raster-test/src/test/scala/geotrellis/raster/mosaic/MosaicSpec.scala)",11/30/16,1
echeipesh,"@jfbourgon I don_Ñét think there is such a thing right now, could be made if its clear what it should do? The main uncertainty is that `RDD` is a collection and there is no guarantee that all the rasters on the left hand side RDD share the same resolution and if they overlap the behavior is ambigious. What do you think ?",11/30/16,1
jfbourgon,"In my use case, all tiles are in the same CRS and all share the same resolution plus they already all fit into the same grid system but they may have some overlap between tiles. Ideally  we could be able to pass on a function on the way we would like to resolve overlap (such as min or max value). A resample method could also be provided if pixels are not fitting into the same grid system.",12/1/16,1
lossyrob,@jfbourgon we_Ñéve done something for HOT OpenAerialMap in the past that does something of what you want. I would recommend looking through the code and trying to see if the ideas help. https://github.com/hotosm/oam-server-tiler/tree/master/mosaic/src/main/scala/org/hotosm/oam,12/1/16,1
lossyrob,"it_Ñés based on 0.10, which shouldn_Ñét be too different from 1.0.0-RC2",12/1/16,1
jeffreyhanson,"Hi,I'm trying to access the `noDataValue` in a `CellType` object.I'm sure I'm missing something obvious. Here's what my code looks like:```import geotrellis.raster._import geotrellis.proj4.CRSimport geotrellis.raster.io.geotiff._import geotrellis.raster.io.geotiff.reader.GeoTiffReaderval r:SinglebandGeoTiff = GeoTiffReader.readSingleband(""data.tif"")val missing = r.cellType.noDataValue```I am using version 1.0.0-RC2, and can confirm `r.cellType` is of class `DoubleUserDefinedNoDataCellType`.Cheers,Jeff",12/5/16,1
jeffreyhanson,"Sorry, forgot to include error message: `error: value noDataValue is not a member of geotrellis.raster.CellType`.",12/5/16,1
pomadchin,@jeffreyhanson hi! `CellType` does not contain `noDataValue`,12/5/16,1
pomadchin,"```scalatype CellType = DataType with NoDataHandling```",12/5/16,1
pomadchin,"i think you need to pttern match on your cellType, to prove that it has a `UserDefinedNoData` type",12/5/16,1
pomadchin,"@jeffreyhanson ```scalaval tiff = GeoTiffReader.readSingleband(""data.tif"")tiff.cellType match {   case d: DoubleUserDefinedNoDataCellType => d.noDataValue  case _ => throw Exception}```",12/5/16,1
jeffreyhanson,"@pomadchin  Thanks! I had to slightly modify that it because it threw an error, but still no dice.```  type CellType = DataType with NoDataHandling  val tiff:SinglebandGeoTiff = GeoTiffReader.readSingleband(""data.tif"")  tiff.cellType match {    case d : DoubleUserDefinedNoDataCellType => d.noDataValue    case _ : Exception => ""raster does not have a noData value""  }    val v = tiff.cellType.noDataValue```  throws:```error: value noDataValue is not a member of geotrellis.raster.CellType         val v = tiff.cellType.noDataValue```I'm really new to scala so apologies if it's something obvious.Ultimately, I'm trying to turn a `SinglebandGeoTiff`into an array. If I use `toArrayDouble()` than the array contains the values used to represent missing values (eg.` -9999`) instead of  missing data values (eg. `Double.NaN`) . So I figured the way to do this would be to extract the `noDataValue` from the object and replace the values",12/5/16,1
pomadchin,"```scalatype CellType = DataType with NoDataHandlingval tiff:SinglebandGeoTiff = GeoTiffReader.readSingleband(""data.tif"")val v = tiff.cellType match {  case d : DoubleUserDefinedNoDataCellType => d.noDataValue  case _ : Exception => println(""raster does not have a noData value_Ñù); 0} ```",12/5/16,1
pomadchin,"hm, can you try: ```scalaval nd = r.cellType match {  case DoubleUserDefinedNoDataCellType(noDataValue) => noDataValue  case _ => throw new Exception(_ÑÒoops smth went wrong_Ñù)}```",12/5/16,1
jeffreyhanson,@pomadchin Success! Awesome - thank you very much!  ,12/5/16,1
jeffreyhanson,@pomadchin that works too - also unlike the previous snippet it doesn't throw a warning,12/5/16,1
jeffreyhanson,"Hi again,I'm trying to run a zonal mean analysis. Looking at the documentation, I can use `zonalStatisticsDouble`.```val tiff = GeoTiffReader.readSingleband(""data.tif"").projectedRasterval zones = GeoTiffReader.readSingleband(""zones.tif"").projectedRasterval stats = tiff.tile.zonalStatisticsDouble(zones)```However, this also calculates a lot of other statistics too. To reduce computational burden, I was wondering if it's possible to just calculate the means and not all the other statistics?   Thanks",12/6/16,1
lossyrob,"@jeffreyhanson if you look at the implementation of zonalStatistics, it_Ñés a pretty straighforward set of sugar for some iteration over the raster cells. You could go down a level and basically do whatever you_Ñéd like_Ñ_I_Ñém going to post links to code as pointers",12/6/16,1
lossyrob,https://github.com/locationtech/geotrellis/blob/master/raster/src/main/scala/geotrellis/raster/mapalgebra/zonal/ZonalMethods.scala#L34-L35,12/6/16,1
lossyrob,calls,12/6/16,1
lossyrob,https://github.com/locationtech/geotrellis/blob/master/raster/src/main/scala/geotrellis/raster/mapalgebra/zonal/DoubleZonalHistogram.scala#L32,12/6/16,1
lossyrob,which iterates over the tile to build up the histogram,12/6/16,1
lossyrob,"honestly the iteration over the tile is the heaviest cost, so doing a mean calc vs building up a histogram *shouldn_Ñét* have too much of an affect. But I appreciate fellows shavers of milliseconds :)",12/6/16,1
lossyrob,"I would set your method up as a function that takes the two tiles, verifies that the cols/rows are the same, and then performs the zonal mean by using `cfor` over the cols and rows like DoubleZonalHistogram is doing. It won_Ñét be pretty, but it will be performant.",12/6/16,1
lossyrob,__RC3 is released!__,12/6/16,1
lossyrob,"also, whe thought you wouldn_Ñét be able to publish on maven central before. this is not the case. So RC3 is published to maven central (probably not propagated yet), and so will other releases",12/6/16,1
lossyrob,"There is one more fix (actually optimization) we want to put into 1.0, which should be wrapped up soon. I imagine this is very close to the final version",12/6/16,1
metasim,:clap: ,12/6/16,1
lossyrob,"If you haven_Ñét checked out our new docs, they are quite new and improved (thanks to @fosskers  and the rest of the team) http://geotrellis.readthedocs.io/en/latest/",12/6/16,1
jeffreyhanson,"Ok, thanks for the advice @lossyrob ",12/6/16,1
jeffreyhanson,congrats on RC3!,12/6/16,1
pomadchin,"thx :tada: GeoTrellis RC3 :tada: :tada: :tada: :tada: The release version would be soon ;)",12/7/16,1
jeffreyhanson,"I think I found a bug concerning the calculation of median and standard deviation values using `.statisticsDouble`. It doesn't seem to calculate the correct values. Here's a reproducible example using 1.0.0-RC3:```  import geotrellis.raster._  import geotrellis.raster.summary._    val a = Array[Double](Double.NaN, 1.0, 3.0, 3.0, 4.0, 12.0)  val r = DoubleArrayTile(a, 3, 2)  val stats = r.statisticsDouble.get    println(""geotrellis median"")  println(stats.median)  println(""correct median"")  println(3.0)    println(""geotrellis sd"")  println(stats.stddev)  println(""correct sd"")  println(4.27785)```I thought I'd double check that it's a bug before posting an issue, in case I'm misunderstanding something. ",12/7/16,1
jeffreyhanson,On my system `stats.median` returns `2.499999999999999` and `stats.stddev` returns `3.8262252939417984`.,12/7/16,1
lossyrob,interesting. that does seem like a bug,12/7/16,1
pomadchin,hm,12/7/16,1
pomadchin,"Integer statistics would give```scala val stats = r.statistics.get``````mean: 4.6median: 3mode: 3stdev: 3.8262252939417984```",12/7/16,1
pomadchin,looks like it_Ñés a population standard deviation,12/7/16,1
marfago,Hi,12/7/16,1
marfago,@pomadchin I have updated https://github.com/geotrellis/geotrellis-chatta-demo/issues/26,12/7/16,1
pomadchin,@marfago Yo! Thx for your report ^^_Ñé we_Ñéll a look a bit later.,12/7/16,1
marfago,thx!,12/7/16,1
marfago,"Hi. Is there a fast way to change  the point size on a raster? Basically I've managed to replace the raster images in chatta demo, but they have a sparse density so it is not easy to find points on the map. I would like to increase their size and make them more evident.",12/9/16,1
lossyrob,how do you mean point size on the raster? are you rasterizing point data? you mean to increase the cell size?,12/9/16,1
lossyrob,"one option is to run a focal operation over it, for instance a focalMean on a 7x7 neighborhood to spread the values a bit",12/9/16,1
marfago,I have some rasters with  binary maps,12/9/16,1
marfago,but they are quite sparse,12/9/16,1
marfago,so the single sparse points are not very visible on the map,12/9/16,1
marfago,I  am trying to add a kernel,12/9/16,1
marfago,in order to have something like a filter ,12/9/16,1
lossyrob,"that make sense, that should help",12/9/16,1
marfago,yes,12/9/16,1
marfago,but Im not able (unfortunately I am good with java but not with scala),12/9/16,1
marfago,so I'm trying to add it in ModelSpark.scala,12/9/16,1
marfago,like this:,12/9/16,1
marfago,"  def weightedOverlay(layers: Iterable[String], weights: Iterable[Int], zoom: Int, rasterExtent: RasterExtent)                     (reader: FilteringLayerReader[LayerId]): RDD[(SpatialKey, Tile)] = {    val layerIds = layers.map(LayerId(_, zoom))    val maskId = LayerId(""mask"", zoom)    val bounds = rasterExtent.gridBoundsFor(rasterExtent.extent)    val mask = reader.query[SpatialKey, Tile, TileLayerMetadata[SpatialKey]](maskId).where(Intersects(bounds)).result     val weighted =      layerIds.zip(weights)      .map { case (layer, weight) =>        val wRaster = reader.read[SpatialKey, Tile, TileLayerMetadata[SpatialKey]](layer, new LayerQuery[SpatialKey, TileLayerMetadata[SpatialKey]].where(Intersects(bounds))) * weight        val kern: Kernel = Kernel.gaussian(9, 1.5, 25)        wRaster.convolve(kern)      }      .toSeq      .localAdd    weighted.localMask(mask, NODATA, NODATA)  }",12/9/16,1
marfago,but it does not compile,12/9/16,1
lossyrob,"hm, its a bit complicated because you want to do a focal operaiton on the visual tiles",12/9/16,1
lossyrob,but that thing from the reader isn_Ñét really a raster,12/9/16,1
marfago,ok.,12/9/16,1
lossyrob,"it_Ñés a layer, like a Spark RDD",12/9/16,1
marfago,yes. ,12/9/16,1
marfago,it is a RDD of Tile's I guess,12/9/16,1
marfago,correct?,12/9/16,1
lossyrob,"`RDD[(SpatialKey, Tile)] with Metadata[TileLayerMetadta[SpatialKey]]`, to be exact",12/9/16,1
lossyrob,"it_Ñés good to tag types so that they compiler knows what you are trying to do, and it will give better error messages",12/9/16,1
lossyrob,but that Model serves the `sum` route,12/9/16,1
lossyrob,https://github.com/geotrellis/geotrellis-chatta-demo/blob/master/geotrellis/src/main/scala/geotrellis/chatta/ChattaServiceActor.scala#L238,12/9/16,1
lossyrob,"so you_Ñéd be doing a convolution on the sum operation, but nothing visually",12/9/16,1
lossyrob,https://github.com/geotrellis/geotrellis-chatta-demo/blob/master/geotrellis/src/main/scala/geotrellis/chatta/ChattaServiceActor.scala#L238,12/9/16,1
marfago,ok. So my guess is wrong :),12/9/16,1
marfago,I supposed that ModelSpark was used for both,12/9/16,1
marfago,visual overlay calculation,12/9/16,1
lossyrob,it_Ñés a fair assumption,12/9/16,1
marfago,and aggregations,12/9/16,1
lossyrob,I would actually probably refactor it so that it was in the same place,12/9/16,1
lossyrob,"The thing you get out of `tileReader.reader[SpatialKey, Tile](LayerId(l, zoom)).read(key)` is a Tile",12/9/16,1
lossyrob,you can perform a convolution on that,12/9/16,1
marfago,how can I get tileReader in ModelSpark?,12/9/16,1
lossyrob,"https://geotrellis.github.io/scaladocs/latest/index.html#geotrellis.raster.mapalgebra.focal.FocalMethods@convolve(kernel:geotrellis.raster.mapalgebra.focal.Kernel,bounds:Option[geotrellis.raster.GridBounds],target:geotrellis.raster.mapalgebra.focal.TargetCell):geotrellis.raster.Tile",12/9/16,1
lossyrob,you could pass it in from the route,12/9/16,1
marfago,yes I see,12/9/16,1
jeffreyhanson,"Hi, I've been working on an [R package](https://github.com/jeffreyhanson/geotrellis) to interface geotrellis with the [R programming language](https://www.r-project.org/).At the moment, the benchmaking suggests that geotrellis is fairly comparable with the popular R [raster package](https://cran.r-project.org/web/packages/raster/index.html).I was wondering if anyone would mind having a quick look to at the [Scala commands](https://github.com/jeffreyhanson/geotrellis/blob/master/R/zzz.R) and could recommend any alternative and higher performing implementations?",12/12/16,1
pomadchin,"@jeffreyhanson interesting, cool; have you been thinking on porting Spark parts on R? (http://spark.apache.org/docs/latest/sparkr.html)",12/12/16,1
fosskers,@jeffreyhanson how well does R's raster package scale?,12/12/16,1
jeffreyhanson,"@pomadchin I hadn't thought of that - that's a good idea. I haven't had a chance to play around with the spark parts yet. Do you expect greater performance for geoprocessing (eg. resampling) a single raster if it's accessed using the spark stuff?@fosskers I wouldn't use it for processing moderately big data sets (eg. global at 1km resolution). It has some capacity for using parallel processing, though it isn't available for all functions.The raster package is fairly good at scaling to a point. It's generally my go to for programming with raster data. If the users tries loading a raster that R can't fit into memory, then it stores the rasters on disk and can read the memory in chunks. But generally, R uses a lot of memory to store even simple objects, so some limitations may simply be due to the fact that some of it is written in R (some parts are written in C and called through R for speed). ",12/13/16,1
jeffreyhanson,I haven't had a chance to run the benchmarking using rasters with more than 1e+6 cells yet. I suspect once past 1e+8 the R raster package might fall behind geotrellis. ,12/13/16,1
pomadchin,@jeffreyhanson the _ÑÒfeature_Ñù of GeoTrellis that it processes tbs of data on a cluster; it makes sense ot load more than one raster into spark memory and to do whatever you want,12/13/16,1
metasim,I think [this](https://github.com/locationtech/geotrellis/blob/master/proj4/src/main/scala/geotrellis/proj4/CRS.scala#L95) should say `NAD83`. Is it worth a ticket?,12/15/16,1
metasim,(or PR?),12/15/16,1
lossyrob,"ah yeah, good call",12/15/16,1
lossyrob,a PR would be a quick merge,12/15/16,1
lossyrob,fyi you have to sign off on your commits now that we_Ñére on the locationtech github org,12/15/16,1
lossyrob,`git commit -s`,12/15/16,1
metasim,:thumbsup: ,12/15/16,1
schBen,"I am playing around with the geotrellis-emr-demo. I managed to run it locally on linux (ingest, server and client). Should it be possible to run only the client on windows as well? until now I couldn't make it work...",12/16/16,1
pomadchin,Should be possible; what's the error?,12/16/16,1
schBen,"@pomadchin Good to know, thank you! Leme2014 will come back to you with some error messages :-)",12/16/16,1
Leme2014,[![npm_failed.png](https://files.gitter.im/geotrellis/geotrellis/kOJb/thumb/npm_failed.png)](https://files.gitter.im/geotrellis/geotrellis/kOJb/npm_failed.png),12/16/16,1
Leme2014,"Hi @pomadchin, i have also a problem if i install and start demo with npm on Windows. I get the following error. ",12/16/16,1
pomadchin,@Leme2014 would have a look in as couple of minutes,12/16/16,1
pomadchin,weird looks like incompatable deps(?),12/16/16,1
pomadchin,i can try it on windows too,12/16/16,1
Leme2014,@pomadchin Thank's,12/16/16,1
owcm,"If I have a  large tile  what is the best way to make a RDD[(SpatialKey, Tile)] from the original tile. Thanks in advance.",12/19/16,1
lossyrob,"When you say Tile, do you mean GoeTiff or a GeoTrellis Tile?",12/19/16,1
owcm,"Geotrellis Tile.   I am stitching together a RDD, performing a costdistance operation, which returns a tile.  So I want to create a RDD, pyramid  it and write out in Avro format.  So I was trying to remember the best approach to take a single Tile and break it up into an RDD.",12/19/16,1
metasim,"Is it possible to access a GeoTrellis RDD from pyspark? (I'm not a Python dev, so I'm ignorant of the pyspark world)",12/19/16,1
lossyrob,"@metasim we are working on that right now! We will be able to probably in 2 months or so, maybe sooner",12/19/16,1
lossyrob,@owcm gotcha. You can run `tilToLayout` on a `sc.parallelize` call,12/19/16,1
lossyrob,if you know the extent and projection,12/19/16,1
lossyrob,and the original layout. which you should have from the layer metadata,12/19/16,1
pomadchin,@Leme2014 looks like nodejs on windows is not the most trival thing; ,12/20/16,1
pomadchin,you can try to launch it in a docker container,12/20/16,1
metasim,"@lossyrob Any of that work public? I'm curious how the interaction between the Python `SparkContext` and `py4j` works (and maybe I can help test, etc.)",12/20/16,1
lossyrob,"@metasim for sure, https://github.com/geotrellis/geopyspark",12/20/16,1
metasim,:+1: ,12/20/16,1
lossyrob,"It's not much at the moment but we figured out how to translate between python and scala RDD elements via Aero, working on that currently ",12/20/16,1
lossyrob,Avro *,12/20/16,1
metasim,"I figured out how to instantiate GeoTrellis classes in the driver (via `sc._gateway.jvm`), but not in the worker, so every bit helps.",12/20/16,1
metasim,Not being able to access the `SparkContext` to gain access to the gateway is where I get hung up. But further along than I was an hour ago.,12/20/16,1
lossyrob,"@jbouffard might have some insigh there, he_Ñés been kicking around with the python binding research for a bit",12/20/16,1
jbouffard,@metasim Are you running multiple jvms? As the GeoTrellis class should be instantiated in both the driver and the workers if you imported it via the `SparkContext`.,12/20/16,1
metasim,At the moment I'm just poking around in the `pyspark` shell.,12/20/16,1
metasim,"I'm able to get `sc._gateway.jvm.geotrellis.raster.io.geotiff.reader.GeoTiffReader` to load in the driver, but can't access it in a `map`.",12/20/16,1
metasim,(won't serialize).,12/20/16,1
metasim,So I'm looking into something [like this](http://blog.cloudera.com/blog/2015/09/how-to-prepare-your-apache-hadoop-cluster-for-pyspark-jobs/) to try to get the geotrellis code to load in the worker.,12/20/16,1
metasim,"IOW: ```def readImage(data):    #jvm = sc._gateway.jvm    jvm = py4j.java_gateway.JavaGateway().jvm    reader = jvm.geotrellis.raster.io.geotiff.reader.GeoTiffReader    return reader.readMultiband(data)image = sc.binaryFiles(""../../../../data/landsat/LC80220322016169LGN00_B4.TIF"")stuff = image.values().map(lambda rawdata: readImage(rawdata)).collect()```But don't let me distract too much.... I can keep poking around.",12/20/16,1
jbouffard,"Ah, okay. Yeah, mapping is an issue when you try to do it over values that are not native to Python. You'll need to create a Python RDD (which is different from ScalaRDD), which contains analogous values to the ones in your original RDD.",12/20/16,1
jbouffard,We were able to accomplish this in this code: https://github.com/jbouffard/geopyspark/blob/master/pygeotrellis/test.py#L84,12/20/16,1
metasim,Ahhh.,12/20/16,1
lossyrob,"our approach is going to be, load up the RDD on the scala side with GeoTiffs or GeoTrellis layers, and then use Avro to encode them into python RDDs as PythonRDD[(SpatialKey, Numpy array)]",12/20/16,1
jbouffard,"If you'd like to know how to fully utilize the Spark's jvm, I'd suggested looking through PySpark's source code.",12/20/16,1
metasim,Gotcha... I still have a lot to learn about when a JVM RDD needs to be encoded as a `PythonRDD` vs. the `java_gateway` handling it.,12/20/16,1
metasim,Cool.,12/20/16,1
metasim,Good tip.,12/20/16,1
metasim,"Is `Box` in `sc._gateway.jvm.Box` part of py4j, or something  else imported into global package scope?",12/20/16,1
jbouffard,"`Box` is imported. It was a Scala object that we created and brought into python. Unfortunately, that and one or two other things are not in that repo.",12/20/16,1
jbouffard,"If there's anything that's not clear because of the missing parts let me know, and I'll try my best to explain what's happening.",12/20/16,1
metasim,Thanks,12/20/16,1
jbouffard,No problem.,12/20/16,1
jbouffard,"@metasim Right now, a PR is up that will make the whole GeoPySpark project self contained and runnable. If you're interested in seeing it before it's merged, I can send you a link to the branch that the PR is based on.",12/20/16,1
rfecher,"@jamesmcclain @lossyrob I updated the geotrellis-geowave code to use 0.9.3 now that its released, and updated/tested the NED demo that you were using - I got the pretty picture in geoserver layer preview I was expecting couldn't recreate issues you were seeing",12/20/16,1
metasim,@jbouffard That would be super!,12/20/16,1
jbouffard,@metasim Here you are! https://github.com/jbouffard/geopyspark/tree/working,12/20/16,1
rfecher,"the only hypothesis I can really come up with is some version incompatibilities were being referenced and intermingled - i did notice some build.sbt files pointing to geowave 0.9.2-SNAPSHOT and some were pointing to 0.9.3-SNAPSHOT, not sure if that was causing some weirdness or really what would have been the problem.  But it worked",12/20/16,1
metasim,@jbouffard Thanks much!,12/20/16,1
jbouffard,"@metasim You're welcome! Let me know if something doesn't work, as I've only tested it on my computer.",12/20/16,1
lossyrob,@rfecher awesome!,12/20/16,1
lossyrob,want to kick it over in a PR?,12/20/16,1
rfecher,"yeah, I failed IP!",12/20/16,1
lossyrob,lol. Do you have a CLA?,12/20/16,1
rfecher,"oh man, back to IPZilla I guess",12/20/16,1
rfecher,i should,12/20/16,1
lossyrob,also you have to sign off on your commits if you_Ñére not a committer yet,12/20/16,1
lossyrob,which is a pretty annoying step,12/20/16,1
rfecher,ahh,12/20/16,1
rfecher,thats missing,12/20/16,1
metasim,"@jbouffard I get this:```ImportError: No module named spatial_key```I tried adding:```--py-files geopyspark/dependencies/python_spatial_key.zip```to the `spark-submit` call in the `Makefile`, but that didn't fix it.",12/20/16,1
metasim,I'm not adept enough with Python search path rules to know where it might be going wrong.,12/20/16,1
jbouffard,@metasim Hmmm that's odd. It should have worked. I'll look into it now.,12/20/16,1
jbouffard,Where in the project are you running the Makefile from?,12/20/16,1
jbouffard,"Also, did you complete the assembly of the scala project?",12/20/16,1
jbouffard,I'm not sure if that was really obvious.,12/20/16,1
rfecher,so apparently I didn't have an eclipse CLA,12/20/16,1
rfecher,"I just signed the CLA after the ip validation ran and rejected me, is there an ""easy"" way to re-run that?",12/20/16,1
rfecher,"also, I did submit changes to @jamesmcclain demo harness: https://github.com/jamesmcclain/GeoWaveIngest/pull/31",12/20/16,1
rfecher,"on his master branch he removed a bunch of the ""peek"" and ""poke"" projects that seemed to have version inconsistencies on the latest that I had from months ago...so again not sure what was causing his issues, but I ran it end-to-end just fine  a little bit ago",12/20/16,1
metasim,@jbouffard Was in the `geopyspark` dir. Ran `make && make run`,12/20/16,1
metasim,Assembly building worked fine.,12/20/16,1
jbouffard,@metasim could you just try running `make run` ?,12/20/16,1
metasim,"```make runspark-submit \		--master ""local[*]"" \		--jars geopyspark/geopyspark-backend/geotrellis/target/scala-2.10/geotrellis-backend-assembly-0.0.1.jar \		geopyspark/test.pyTraceback (most recent call last):  File ""/Users/sfitch/Coding/OSS/geopyspark/geopyspark/test.py"", line 50, in <module>    from spatial_key import SpatialKeyImportError: No module named spatial_key```",12/20/16,1
jbouffard,"@metasim I'll have to look into this more. It works for me, so I'm not too sure what the issue could be at the moment. I'll let you know when I come across the issue.",12/20/16,1
metasim,No worries... I'll look into it. You've really done me a big favor.,12/20/16,1
metasim,I'll let you know if I can figure it out.,12/20/16,1
jbouffard,"Thanks, but I'll still look into it as well. The solution will probably come about faster if we both work on it.",12/20/16,1
jbouffard,"@metasim In the `test.py` file, could you add this, `print '\n'.join(sys.path)` to the line right before `SpatialKey` is imported?",12/20/16,1
lossyrob,is spatial_key something that is `setup.py` installed?,12/20/16,1
lossyrob,I found that I had to install the package on my machine in order to get it to work with pyspark,12/20/16,1
jbouffard,"It is, but I put it in a zip, so it should work.",12/20/16,1
jbouffard,I actually uninstalled it and it still worked.,12/20/16,1
jbouffard,But there's a chance it could still be somewhere on my computer.,12/20/16,1
jbouffard,"@metasim @lossyrob I just updated the branch by moving the geopyspark-backend to the main geopyspark folder, so that might resolve the issue that has arisen.",12/20/16,1
jbouffard,Actually just made another quick update to the branch,12/20/16,1
lossyrob,ok. I want to transfer the project to locationtech-labs org I just created,12/20/16,1
lossyrob,"that shouldn_Ñét mess with you too much because of redirects, but FYI about going to move now",12/20/16,1
jbouffard,Alright. Thanks for letting me know.,12/20/16,1
metasim,"@jbouffard I `git pull`ed to the latest on `origin/working`. Here's what I get after adding the `print` bit:```sfitch$ make runspark-submit \		--master ""local[*]"" \		--jars geopyspark-backend/geotrellis/target/scala-2.10/geotrellis-backend-assembly-0.0.1.jar \		geopyspark/test.py/Users/sfitch/Coding/OSS/geopyspark/geopyspark/usr/local/Cellar/apache-spark/2.0.1/libexec/python/lib/pyspark.zip/usr/local/Cellar/apache-spark/2.0.1/libexec/python/lib/py4j-0.10.3-src.zip/usr/local/Cellar/apache-spark/2.0.1/libexec/jars/spark-core_2.11-2.0.1.jar/Users/sfitch/anaconda/lib/python27.zip/Users/sfitch/anaconda/lib/python2.7/Users/sfitch/anaconda/lib/python2.7/plat-darwin/Users/sfitch/anaconda/lib/python2.7/plat-mac/Users/sfitch/anaconda/lib/python2.7/plat-mac/lib-scriptpackages/Users/sfitch/anaconda/lib/python2.7/lib-tk/Users/sfitch/anaconda/lib/python2.7/lib-old/Users/sfitch/anaconda/lib/python2.7/lib-dynload/Users/sfitch/anaconda/lib/python2.7/site-packages/Users/sfitch/anaconda/lib/python2.7/site-packages/Sphinx-1.4.6-py2.7.egg/Users/sfitch/anaconda/lib/python2.7/site-packages/aeosa/Users/sfitch/anaconda/lib/python2.7/site-packages/setuptools-27.2.0-py2.7.eggdependencies/python_spatial_key.zipTraceback (most recent call last):  File ""/Users/sfitch/Coding/OSS/geopyspark/geopyspark/test.py"", line 51, in <module>    from spatial_key import SpatialKeyImportError: No module named spatial_keymake: *** [run] Error 1```",12/21/16,1
metasim,"I'd install `python_spatial_key.zip` in my distro, but I don't know how to do that when `setup.py` isn't a full script (used to running `python setup.py install`). So I can try that.",12/21/16,1
metasim,"I note that the contents of the `.zip` file looks like this:```sfitch$ unzip -l geopyspark/dependencies/python_spatial_key.zip Archive:  geopyspark/dependencies/python_spatial_key.zip  Length     Date   Time    Name --------    ----   ----    ----        0  12-16-16 15:52   SpatialKey/        0  12-16-16 15:52   SpatialKey/build/        0  12-16-16 15:52   SpatialKey/build/lib.linux-x86_64-2.7/        0  12-16-16 15:35   SpatialKey/build/lib.linux-x86_64-2.7/__init__.py      189  12-16-16 15:38   SpatialKey/build/lib.linux-x86_64-2.7/spatial_key.py      600  12-16-16 15:52   SpatialKey/spatial_key.pyc        0  12-16-16 15:35   SpatialKey/__init__.py      132  12-16-16 15:52   SpatialKey/setup.py      189  12-16-16 15:38   SpatialKey/spatial_key.py --------                   -------     1110                   9 files```",12/21/16,1
metasim,"Does the fact that it sits in a `SpatialKey` directory inside the `.zip` file matter (again, I'm weak with Python search path resolution rules)? IOW,  should it be `from SpatialKey.spatial_key import SpatialKey` in the code?",12/21/16,1
Leme2014,"Hi @pomadchin, thank's. We have fixed the problem. The err was we have  some false configuration in node config. :-) And the next point, i would like to start server on windows.  Is this possible?  I don't know much about linux, how can i run the make cli on cygwin? Thank you!  ",12/21/16,1
jbouffard,"@metasim I'm pretty sure it shouldn't matter that the directory itself is inside the zip, but could you please try `SpatialKey.spatial_key import SpatialKey` and tell me the result?",12/21/16,1
pomadchin,"@Leme2014 wow cool; you can install sbt for windows, and run sbt from the command line from the project directory",12/21/16,1
jbouffard,"@metasim If that doesn't work, the way you'd install the `SpatialKey` module is to run this code in the `SpatialKey` root directory: `python setup.py install --user`",12/21/16,1
metasim,"@jbouffard Sorry I didn't try that earlier. That said, same problem",12/21/16,1
metasim,I'll try that.,12/21/16,1
Leme2014,"@pomadchin, thank you, i will try",12/21/16,1
jbouffard,"That's fine. Okay, that should definitely work then.",12/21/16,1
jbouffard,"Or rather, it should at least find that module.",12/21/16,1
pomadchin,@Leme2014 feel free to write here any questions ^^ :D but it should work on windows; + we have a geodocker cluster https://github.com/geodocker/geodocker and it works on windows 100% (i tested it a couple of months ago using docker for windows),12/21/16,1
jbouffard,"If you have `pip` installed, you could run `pip list` to make sure that it installed correctly.",12/21/16,1
metasim,Yep that. works. So there's something about finding that `.zip` file that's not working out of the box.,12/21/16,1
Leme2014,@pomadchin . Thank you :-),12/21/16,1
metasim,Are you sure you don't need to use the `--py-files` flag to `spark-submit`?,12/21/16,1
metasim,(Although I tried it and it didn't work),12/21/16,1
pomadchin,"@Leme2014 we have an additional project https://github.com/azavea/geotrellis-geomesa-template-project/ and you can find in description how to launch geodocker on windows + you can even launch intelij idea in docker ^^ dunno why you may need it but it works :D \+ possible issues: https://github.com/azavea/geotrellis-geomesa-template-project/#possible-issues",12/21/16,1
jbouffard,"@metasim Alright. Thanks for testing it and letting me know. I think for right now I'll just include a step that involves installing `SpatialKey`. You shouldn't have to, as we're just dealing with one python file for right now.",12/21/16,1
metasim,"@jbouffard That said, I'm good with this.",12/21/16,1
metasim,Agreed.,12/21/16,1
metasim,"This python thing is a rabbit hole that goes much deeper than I imaginged. Lookthing through the pyspark source code, it looks like amplab/databricks bascially has parallel implementations of all the APIs, hand written. Quite an effort.",12/21/16,1
jbouffard,"@metasim I'm glad we were able to get it working. I'll try to make announcements when major milestones have been reached for the python bindings. Yeah, there's a lot that goes into PySpark and you realize how much needs to be done in order to the most basic of things.",12/21/16,1
metasim,"Having to create a serialization schema for every custom type just seems insane. In our case, all we need is NumPy array(s) of the raster in an RDD, and wanting to keep the heavyweight work in Scala/GeoTrellis, but even that... figuring out how to keep most of the RDD mechanics in JVM land is not something that is well documented (as you know).",12/21/16,1
metasim,Been surprised at how few examples are out there on doing something like this without a lot of boilerplate generation.,12/21/16,1
metasim,`</rant>` :-),12/21/16,1
jbouffard,"Luckily, we can produce schemas for our types on-the-fly, which we can then bring over to the python side; so that does save us some trouble. The real issue comes with serializing and deserializing. Yeah, there is almost nothing on this, and the only thing I we have to go on is the Spark source code. I thin it's kinda fun, though, breaking into whole new territory :smile: ",12/21/16,1
jbouffard,"@metasim I was able to send a `RDD[Extent]` from scala to python, modify the `Extent`s in python, send back the new `Extent`s to scala, and from the new data create a `RDD[RasterExtent]` .",12/22/16,1
jbouffard,Here's a link to that branch if you'd like to look at it: https://github.com/jbouffard/geopyspark/tree/avro,12/22/16,1
metasim,Super!!,12/22/16,1
metasim,I've not made any more progress on my end.... pulled into  meetings most of the week.,12/22/16,1
metasim,I'll check it out.,12/22/16,1
metasim,"@jbouffard Worked ""out of the box"" :)",12/22/16,1
jbouffard,@metasim That's alright! Glad it worked the way it was supposed to! :smile: ,12/22/16,1
ardilgulez,Is there a way in Geotrellis to import GRIB or NetCDF data?,12/22/16,1
metasim,"@ardilgulez FWIW, we're wanting the ability to import HDF-EOS files (kinda lik NetCDF). For the time being we have a GDAL script to get us to GeoTiff, and then into GeoTrellis. If we have the time, we'd love to write an HDF-EOF reader for GeoTrellis, but it's not on the critcal path ATM.",12/22/16,1
metasim, PS: @ardilgulez I'm not on the GeoTrellis team--just a recent user of it--so I don't know what the plans are for other formats beyond GeoTiff.,12/23/16,1
ardilgulez,"@lossyrob did something with NetCDF and Geotrellis, I'm guessing that it works too, so I thought there would be something more to work with",12/23/16,1
pomadchin,hi @ardilgulez we have an unclosed issue: https://github.com/locationtech/geotrellis/issues/1402 related to netcdf,12/23/16,1
pomadchin,we _had_ netcdf support before; it can be found here: https://github.com/geotrellis/geotrellis-gdal (not up to date project though),12/23/16,1
pomadchin,"the problem of this project that it depends on gdal, that means you need to install gdal on all the nodes used to ingest your netcdf files, so the  easier solution would be to convert netcdf files into geotiffs using gdal (as a pre ingest step), and to ingest using geotrellis 1.0.0",12/23/16,1
ardilgulez,"I did that exact thing, used gdal_translate to convert a netcdf to tif",12/23/16,1
ardilgulez,"and I extracted a png out of it, all of it, and the png is 70x70",12/23/16,1
ardilgulez,the tif file covers an 8 degree by 8 degree area,12/23/16,1
ardilgulez,according to gdalinfo,12/23/16,1
ardilgulez,"by the way, did the ingestion process change when going into geotrellis 1.0?",12/23/16,1
pomadchin,"it is the same; but some refactors were done, ETL usage changed (it is json based in 1.0.0)",12/23/16,1
pomadchin,and some api changes ^^',12/23/16,1
ardilgulez,that might be what's biting me,12/23/16,1
ardilgulez,"``` scalaPyramid.upLevels(reprojected, layoutScheme, zoom, Bilinear) { (rdd, z) =>      val layerId = LayerId(""landsat"", z)      // If the layer exists already, delete it out before writing      if(attributeStore.layerExists(layerId)) {        new FileLayerManager(attributeStore).delete(layerId)      }      writer.write(layerId, rdd, ZCurveKeyIndexMethod)}```",12/23/16,1
pomadchin,think that would be quite the same,12/23/16,1
ardilgulez,I get some spray json blah blah error for the writer.write line,12/23/16,1
pomadchin,ah,12/23/16,1
ardilgulez,"the API looks the same, but I'm getting that error regardless",12/23/16,1
pomadchin,"be sure, that you have following imports```scalaimport geotrellis.spark._import geotrellis.spark.io._import geotrellis.spark.hadoop._import geotrellis.spark.file._```",12/23/16,1
pomadchin,"@ardilgulez noticed that you create new `FileLayerManager` on each pyramid step; you can move this function out of this function scope, that would be faster",12/23/16,1
pomadchin,and you can create just an instance of LayerDeleter ,12/23/16,1
ardilgulez,I am refactoring the landsat tutorial stuff,12/23/16,1
ardilgulez,"I did ""val flm = new FileLayerManager(attributeStore)""",12/23/16,1
ardilgulez,and flm.delete(layerId),12/23/16,1
ardilgulez,now I'm getting java.lang.NoClassDefFoundError: org/apache/spark/SparkContext,12/23/16,1
ardilgulez,nvm I think I know what it is,12/23/16,1
ardilgulez,I should install spark,12/23/16,1
ardilgulez,nvm it didn't work again,12/23/16,1
pomadchin,@ardilgulez have you included spark as a dep?,12/23/16,1
pomadchin,how do you launch your job? looks like without spark-submit,12/23/16,1
pomadchin,"to launch using `./sbt run` you need to include spark 2.0.2 dep and don_Ñét mark is as _ÑÒprovided""",12/23/16,1
ardilgulez,I cloned geotrellis sbt template and worked on it,12/23/16,1
ardilgulez,"It had spark dependency as ""provided""",12/23/16,1
ardilgulez,"deleting that ""provided"" solved",12/23/16,1
pomadchin,cool! @ardilgulez as well im updating chatta demo to use geotrellis collections api; probably that may be interesting for you,12/23/16,1
david-2012,"Hi, @pomadchin  @lossyrob Recently when we use geotrellis to import some big-size tiff file(In our case, global land cover data with 250m resolution, 400M with LZW compression, about 12G after uncompression) into accumulo, it threw some exceptions.When loading the uncompressed 12GB global land cover tiff data, it threw the following error:16/12/26 15:34:13 ERROR Executor: Exception in task 0.0 in stage 0.0 (TID 0)java.lang.RuntimeException: Cannot read path file:/data/raster/Param/LandCover/global/2014_2014_0.00208_1y_Raster_2014/Raster_LandCover_global_0.00208_1y_2014_2014.tif because it's too big...when loading the LZW-compressed 400M tiff data, it threw the following error:(run-main-0) org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0, localhost): java.lang.NegativeArraySizeExceptionDo you have any solutions or suggestions to handle with this big-size tiff file? Thanks in advance!",12/26/16,1
pomadchin,@david-2012 can you attach tiff and gdalinfo?,12/26/16,1
pomadchin,interesting err with lzw compression,12/26/16,1
pomadchin,@david-2012 \+ https://github.com/locationtech/geotrellis/issues/1627 ,12/26/16,1
pomadchin,"and yes we support big tiffs (> 4 GB); https://github.com/locationtech/geotrellis/pull/1753 ._.what err do you have? do you have enough ram / free space? ",12/26/16,1
lossyrob,Also what version geotrellis? The second error sounds suspiciously like something that was fixed in a later 0.10.x and should be fixed in 1.0.0,12/26/16,1
david-2012,"The current geotrellis version we applied is from master branch in October, 2016. And yes, we have enough RAM and space, the JVM heap size is specified to 35GB for Spark driver and each excutor.The tiff file is too big. I append the gdalinfo details as below.1. This is the Compressed tiff file info:-rwxr-xr-x 1 root root        83 8_Ò_  12 15:15 Raster_2014_THU.tfw-rwxr-xr-x 1 root root 400044317 8_Ò_  12 16:03 Raster_2014_THU.tif-rwxr-xr-x 1 root root 146951926 8_Ò_  13 21:48 Raster_2014_THU.tif.ovr-rwxr-xr-x 1 root root         5 8_Ò_  13 16:53 Raster_2014_THU.tif.vat.cpg-rwxr-xr-x 1 root root       490 8_Ò_  13 16:53 Raster_2014_THU.tif.vat.dbf-rwxr-xr-x 1 root root      2048 8_Ò_  29 22:12 Raster_2014_THU.tif.vat.Value.atx[root@cu10 2014]# gdalinfo Raster_2014_THU.tifDriver: GTiff/GeoTIFFFiles: Raster_2014_THU.tif       Raster_2014_THU.tif.ovrSize is 172800, 72000Coordinate System is:GEOGCS[""WGS 84"",    DATUM[""WGS_1984"",        SPHEROID[""WGS 84"",6378137,298.257223563,            AUTHORITY[""EPSG"",""7030""]],        AUTHORITY[""EPSG"",""6326""]],    PRIMEM[""Greenwich"",0],    UNIT[""degree"",0.0174532925199433],    AUTHORITY[""EPSG"",""4326""]]Origin = (-179.999988509145879,89.999995281091472)Pixel Size = (0.002083333333333,-0.002083333333333)Metadata:  AREA_OR_POINT=Area  DataType=GenericImage Structure Metadata:  COMPRESSION=LZW  INTERLEAVE=BANDCorner Coordinates:Upper Left  (-179.9999885,  89.9999953) (179d59'59.96""W, 89d59'59.98""N)Lower Left  (-179.9999885, -60.0000047) (179d59'59.96""W, 60d 0' 0.02""S)Upper Right ( 180.0000115,  89.9999953) (180d 0' 0.04""E, 89d59'59.98""N)Lower Right ( 180.0000115, -60.0000047) (180d 0' 0.04""E, 60d 0' 0.02""S)Center      (   0.0000115,  14.9999953) (  0d 0' 0.04""E, 14d59'59.98""N)Band 1 Block=128x128 Type=Byte, ColorInterp=Gray  NoData Value=0  Overviews: 86400x36000, 43200x18000, 21600x9000, 10800x4500, 5400x2250, 2700x1125, 1350x563, 675x282, 338x141  Metadata:    SourceBandIndex=02. This is the uncompressed tiff's file info:-rw-r--r-- 1 root root 12442752666 12_Ò_ 28 2016 Raster_LandCover_global_0.00208_1y_2014_2014.tif[root@cu10 2014_2014_0.00208_1y_Raster_2014]# gdalinfo Raster_LandCover_global.tifDriver: GTiff/GeoTIFFFiles: Raster_LandCover_global.tifSize is 172800, 72000Coordinate System is:GEOGCS[""WGS 84"",    DATUM[""WGS_1984"",        SPHEROID[""WGS 84"",6378137,298.257223563,            AUTHORITY[""EPSG"",""7030""]],        AUTHORITY[""EPSG"",""6326""]],    PRIMEM[""Greenwich"",0],    UNIT[""degree"",0.0174532925199433],    AUTHORITY[""EPSG"",""4326""]]Origin = (-179.999988509145879,89.999995281091472)Pixel Size = (0.002083333333333,-0.002083333333333)Metadata:  AREA_OR_POINT=Area  DataType=GenericImage Structure Metadata:  INTERLEAVE=BANDCorner Coordinates:Upper Left  (-179.9999885,  89.9999953) (179d59'59.96""W, 89d59'59.98""N)Lower Left  (-179.9999885, -60.0000047) (179d59'59.96""W, 60d 0' 0.02""S)Upper Right ( 180.0000115,  89.9999953) (180d 0' 0.04""E, 89d59'59.98""N)Lower Right ( 180.0000115, -60.0000047) (180d 0' 0.04""E, 60d 0' 0.02""S)Center      (   0.0000115,  14.9999953) (  0d 0' 0.04""E, 14d59'59.98""N)Band 1 Block=172800x1 Type=Byte, ColorInterp=Gray  NoData Value=0  Metadata:    SourceBandIndex=0============================================================================For the tiff file, the spatial resotluion is 250meter, compressed tiff file size is about 500M, and uncompressed size is about 12G. Notice that the raster size is (172800, 72000), the number of 172800*72000 already exceed max int so that we got following exception when ingest the compressed tiff (500M):16/12/27 09:44:08 ERROR Executor: Exception in task 0.0 in stage 0.0 (TID 0)java.lang.NegativeArraySizeException	at scala.reflect.ManifestFactory$$anon$6.newArray(Manifest.scala:93)	at scala.reflect.ManifestFactory$$anon$6.newArray(Manifest.scala:91)	at scala.Array$.ofDim(Array.scala:218)	at geotrellis.raster.io.geotiff.UByteGeoTiffTile.mutable(UByteGeoTiffTile.scala:24)	at geotrellis.raster.io.geotiff.GeoTiffTile.toArrayTile(GeoTiffTile.scala:480)	at geotrellis.",12/27/16,1
pomadchin,@david-2012 you can use \``` <content> \``` tags to wrap your code / text to mak it more readable ,12/27/16,1
pomadchin,"think you hav a buggy version, try to use the release 1.0.0",12/27/16,1
david-2012,"ok, let me try to make the message more clear",12/27/16,1
pomadchin,@david-2012 still recommend you to try 1.0.0 ^^_Ñé :D ,12/27/16,1
david-2012,"yes,  I believe current master branch has been upgraded to 1.0.0 ",12/27/16,1
david-2012,"> The current geotrellis version we applied is from master branch in October, 2016. > > And yes, we have enough RAM and space, the JVM heap size is specified to 35GB for Spark driver and each excutor.> > The tiff file is too big. I append the gdalinfo details as below.# 1. This is the Compressed tiff file info:> -rwxr-xr-x 1 root root        83 8_Ò_  12 15:15 Raster_2014_THU.tfw> -rwxr-xr-x 1 root root 400044317 8_Ò_  12 16:03 Raster_2014_THU.tif> -rwxr-xr-x 1 root root 146951926 8_Ò_  13 21:48 Raster_2014_THU.tif.ovr> -rwxr-xr-x 1 root root         5 8_Ò_  13 16:53 Raster_2014_THU.tif.vat.cpg> -rwxr-xr-x 1 root root       490 8_Ò_  13 16:53 Raster_2014_THU.tif.vat.dbf> -rwxr-xr-x 1 root root      2048 8_Ò_  29 22:12 Raster_2014_THU.tif.vat.Value.atx> > [root@cu10 2014]# gdalinfo Raster_2014_THU.tif> Driver: GTiff/GeoTIFF> Files: Raster_2014_THU.tif>        Raster_2014_THU.tif.ovr> Size is 172800, 72000> Coordinate System is:> GEOGCS[""WGS 84"",>     DATUM[""WGS_1984"",>         SPHEROID[""WGS 84"",6378137,298.257223563,>             AUTHORITY[""EPSG"",""7030""]],>         AUTHORITY[""EPSG"",""6326""]],>     PRIMEM[""Greenwich"",0],>     UNIT[""degree"",0.0174532925199433],>     AUTHORITY[""EPSG"",""4326""]]> Origin = (-179.999988509145879,89.999995281091472)> Pixel Size = (0.002083333333333,-0.002083333333333)> Metadata:>   AREA_OR_POINT=Area>   DataType=Generic> Image Structure Metadata:>   COMPRESSION=LZW>   INTERLEAVE=BAND> Corner Coordinates:> Upper Left  (-179.9999885,  89.9999953) (179d59'59.96""W, 89d59'59.98""N)> Lower Left  (-179.9999885, -60.0000047) (179d59'59.96""W, 60d 0' 0.02""S)> Upper Right ( 180.0000115,  89.9999953) (180d 0' 0.04""E, 89d59'59.98""N)> Lower Right ( 180.0000115, -60.0000047) (180d 0' 0.04""E, 60d 0' 0.02""S)> Center      (   0.0000115,  14.9999953) (  0d 0' 0.04""E, 14d59'59.98""N)> Band 1 Block=128x128 Type=Byte, ColorInterp=Gray>   NoData Value=0>   Overviews: 86400x36000, 43200x18000, 21600x9000, 10800x4500, 5400x2250, 2700x1125, 1350x563, 675x282, 338x141>   Metadata:>     SourceBandIndex=0# 2. This is the uncompressed tiff's file info:-rw-r--r-- 1 root root 12442752666 12_Ò_ 28 2016 Raster_LandCover_global_0.00208_1y_2014_2014.tif<gdalinfo detail>> [root@cu10 2014_2014_0.00208_1y_Raster_2014]# gdalinfo Raster_LandCover_global.tif> Driver: GTiff/GeoTIFF> Files: Raster_LandCover_global.tif> Size is 172800, 72000> Coordinate System is:> GEOGCS[""WGS 84"",>     DATUM[""WGS_1984"",>         SPHEROID[""WGS 84"",6378137,298.257223563,>             AUTHORITY[""EPSG"",""7030""]],>         AUTHORITY[""EPSG"",""6326""]],>     PRIMEM[""Greenwich"",0],>     UNIT[""degree"",0.0174532925199433],>     AUTHORITY[""EPSG"",""4326""]]> Origin = (-179.999988509145879,89.999995281091472)> Pixel Size = (0.002083333333333,-0.002083333333333)> Metadata:>   AREA_OR_POINT=Area>   DataType=Generic> Image Structure Metadata:>   INTERLEAVE=BAND> Corner Coordinates:> Upper Left  (-179.9999885,  89.9999953) (179d59'59.96""W, 89d59'59.98""N)> Lower Left  (-179.9999885, -60.0000047) (179d59'59.96""W, 60d 0' 0.02""S)> Upper Right ( 180.0000115,  89.9999953) (180d 0' 0.04""E, 89d59'59.98""N)> Lower Right ( 180.0000115, -60.0000047) (180d 0' 0.04""E, 60d 0' 0.02""S)> Center      (   0.0000115,  14.9999953) (  0d 0' 0.04""E, 14d59'59.98""N)> Band 1 Block=172800x1 Type=Byte, ColorInterp=Gray>   NoData Value=0>   Metadata:>       SourceBandIndex=0 ============================================================================</gdalinfo detail>> For the tiff file, the spatial resotluion is 250meter, compressed tiff file size is about 500M, and uncompressed size is about 12G. > Notice that the raster size is (172800, 72000), the number of 172800*72000 already exceed max int so that we got following exception when ingest the compressed tiff (500M):> 16/12/27 09:44:08 ERROR Executor: Exception in task 0.0 in stage 0.0 (TID 0)> java.lang.NegativeArraySizeException> 	at 'scala.reflect.ManifestFactory$$anon$6.newArray(Manifest.scala:93)'> 	at 'scala.reflect.ManifestFactory$$anon$6.newArray(Manifest.scala:91)'> 	at scal",12/27/16,1
pomadchin,"hm but you said that you used October, 2016 branch ah",12/27/16,1
pomadchin,anyway,12/27/16,1
pomadchin,what exact tiff throws an error?,12/27/16,1
david-2012,"thanks, I will try the latest branch. haha",12/27/16,1
david-2012,"Both cases of loading the compressed and uncompressed global land cover geotiff  data into Accumulo would throw some errors, the description has appended again as above.",12/27/16,1
pomadchin,cool cool,12/27/16,1
pomadchin,can you upload uncompressed tiff somewhere? or at least compressed,12/27/16,1
pomadchin,any one tiff; just to catch this error,12/27/16,1
david-2012,I will put the compressed tiff data(400M+«__ in some shared place so that you can access later.,12/27/16,1
pomadchin,"ok, thanks!",12/27/16,1
david-2012,"thanks, too! :smile: ",12/27/16,1
lossyrob,That is a very large geotiff. You'll have to use the window read functionality to ingest it,12/27/16,1
lossyrob,Are you using ETL to ingest? There is a `maxTileSize` option in the 1.0 version ,12/27/16,1
lossyrob,"I would recommend also filtering  NoData tiles out before saving, since I suspect there will be a lot of NoData tiles that could be discarded",12/27/16,1
david-2012,"yes, I use ETL to ingest. I will have a look for this this option.",12/27/16,1
david-2012,"Thanks for your suggestions, @lossyrob ",12/27/16,1
lossyrob,http://geotrellis.readthedocs.io/en/latest/guide/etl/#input-json,12/27/16,1
lossyrob,The maxTileSize option is described there,12/27/16,1
prahim,"Hi I am trying to mock ```collectionLayerReader.read[SpatialKey, Tile, TileLayerMetadata[SpatialKey]](layerId)```and I am returning ```val tiles: Seq[(SpatialKey, Tile)] with Metadata[TileLayerMetadata[SpatialKey]] =    ContextCollection(tilesSeq, tileLayerMetadata)``` where tilesSeq is Seq[(SpatialKey, Tile)] and tileLayerMetadata is TileLayerMetadata[SpatialKey]but it is not being returned when called upon -- is something wrong in the set up?",12/28/16,1
pomadchin,"@prahim hi! Sry, can_Ñét get your idea; so you are trying to mock file collection layer reader, and? o:",12/28/16,1
pomadchin,"that_Ñés a good way: ```scalaval tiles: Seq[(SpatialKey, Tile)] with Metadata[TileLayerMetadata[SpatialKey]] = ContextCollection(tilesSeq, tileLayerMetadata)```it should work",12/28/16,1
prahim,"@pomadchin yes -- okay, for some reason it doesn't return it when read is called on the mocked collectionLayerReader : mock[CollectionLayerReader[LayerId]] ",12/28/16,1
prahim,"```when(collectionLayerReader.read[SpatialKey, Tile, TileLayerMetadata[SpatialKey]](layerId))        .thenReturn(tiles)```",12/28/16,1
prahim,"I was thinking it had something to do with the SpatialKey, Tile, TileLayerMetadata[SpatialKey] params in the read[] when I stub it because I cannot mix them with read(any[LayerId])",12/28/16,1
pomadchin,do you use scala mock?,12/28/16,1
pomadchin,"though never tried to wrap geotrellis objects with mocks :O ",12/28/16,1
pomadchin,did you take into account all implicits neccesary to call this function?,12/28/16,1
prahim,I am using mockito,12/28/16,1
prahim,I did import import geotrellis.spark.io._ -- I am not getting implicit errors just when it hits the line that it is supposed to be stubbed nothing returns,12/28/16,1
prahim,there are no examples on mocking this function?,12/28/16,1
pomadchin,"@prahim i investigated this issue a bit; the problem caused by implicits; you can try to mock: ```scalaclass ATI { def f[K: AvroRecordCodec]: String = _ÑÒatimplicit_Ñù } class AT { def f: String = _ÑÒatimplicit_Ñù } ```",12/29/16,1
hjaekel,"Happy new year to eveyone! I am trying to start the viewer of the geotrellis-landsat-emr-demo on Windows with npm 3.10.10. When I open the page in Chrome or Firefox, I get the following JavaScript error in the console:",1/4/17,1
hjaekel,ëâëâëâ,1/4/17,1
hjaekel,"```transition.js:59 Uncaught ReferenceError: jQuery is not defined    at Object.<anonymous> (transition.js:59)    at __webpack_require__ (bootstrap 41d7ae1_Ñ_:555)    at fn (bootstrap 41d7ae1_Ñ_:86)    at Object.<anonymous> (bootstrap.config.js:1)    at __webpack_require__ (bootstrap 41d7ae1_Ñ_:555)    at fn (bootstrap 41d7ae1_Ñ_:86)    at Object.<anonymous> (index.js:2)    at __webpack_require__ (bootstrap 41d7ae1_Ñ_:555)    at fn (bootstrap 41d7ae1_Ñ_:86)    at Object.<anonymous> (App.js:8)```",1/4/17,1
pomadchin,hi @hjaekel! npm is a weird thing especially on windows ._.,1/4/17,1
pomadchin,you can try to use docker; if that would help you it means that just windows version has a problem,1/4/17,1
hjaekel,"hmmm, I will try on my Ubuntu machine",1/4/17,1
hjaekel,@pomadchin the demo works on Ubuntu with exactly the same versions (node 6.9.3 and npm 3.10.10). I will try to find out where the differences are,1/4/17,1
hjaekel,"@pomadchin The main difference is that in the Windows version JQuery is included later in bundle.js than in the Ubuntu version. The following code seems to force webpack to include JQuery earlier: ```",1/4/17,1
hjaekel,"```new webpack.ProvidePlugin({            'window.jQuery': 'jquery',            'window.$': 'jquery',        })```",1/4/17,1
pomadchin,@hjaekel great; thanks! O:,1/4/17,1
pomadchin,:D mb you can / want to make a pr against emr landsat demo master branch?,1/4/17,1
hjaekel,I am not a JavaScript expert and I just found this snippet on StackOverflow,1/4/17,1
pomadchin,"anyway, thanks for investigation!",1/4/17,1
hjaekel,maybe you can check if it breaks other things,1/4/17,1
pomadchin,"I_Ñéll create issue, to include it into further work",1/4/17,1
hjaekel,thank you,1/4/17,1
pomadchin,@hjaekel https://github.com/geotrellis/geotrellis-landsat-emr-demo/issues/23 done! thanks!,1/4/17,1
kthompson,is there a way to convert a geometry from one CRS to another?,1/5/17,1
pomadchin,"@kthompson sure, use `reproject` method; what type of geometry do you have?",1/5/17,1
kthompson,points and multipolygons,1/5/17,1
kthompson,centroids actually,1/5/17,1
pomadchin,"mm, i meant is it a GeoTrellis type or its a geotools type? or smth else?",1/5/17,1
kthompson,they are just in a database so I imagine I will need to convert them to geotrellis types first and then reproject them,1/5/17,1
pomadchin,"```import geotrellis.proj4.CRSimport geotrellis.vector._val g: Geometry = ???val src: CRS = ???val dest: CRS = ???g.reproject(src, dest)```",1/5/17,1
kthompson,awesome thanks :),1/5/17,1
pomadchin,@kthompson you are welcome :D ,1/5/17,1
lossyrob,you could use the `geotrellis-slick` library to pull them out of a PostGIS database if that_Ñés what you_Ñére working with,1/5/17,1
lossyrob,just realizing we don_Ñét have docs for the slick lib :/,1/5/17,1
pomadchin,w0ah O:,1/5/17,1
fosskers,"@lossyrob do you mean on RTD, or in `docs/`?",1/6/17,1
lossyrob,both AFAIK,1/6/17,1
tkunicki,@lossyrob can you point me to the repo for the shared spark python work?,1/6/17,1
lossyrob,https://github.com/locationtech-labs/geopyspark,1/6/17,1
mehtaharsh1995,"I am now working with geotrellis , I want to ingest geotif file using Etl.ingest method how could i do that?",1/9/17,1
pomadchin,yo @mehtaharsh1995 ! We have etl docs: http://geotrellis.readthedocs.io/en/latest/tutorials/etl-tutorial/ ,1/9/17,1
pomadchin,"In short, as you noticed ETL has mthods load, tile, save; ingest is a composition of these three methods;",1/9/17,1
mehtaharsh1995,"So, could i use ingest directly instead of load, tile and save?",1/9/17,1
pomadchin,"Yes you can use just the ingest function (see the docs how to use etl) but let me describe you a bit what ingest function does (rather rough description): * using `load[I, V]` function you get `RDD[(I, V)]`, where I is a `ProjectedExtent` (tuple of extent and crs) or `TemporalProjectedExtent` (tuple of extent src and time); `V` it_Ñés `Tile`or `MultibandTile` which is in fact an array with some metadata of values; * using tile function you cut your loaded `(I, V)` pairs into Spark memory into `(K, V_Ñé)` pairs, where `K` would be a key according to your destination layout scheme* save eveyrhting with or without building pyramid into desirable backend (file, hadoop, s3, accumulo, cassandra, hbase)",1/9/17,1
owcm,"I am reading in a simple DEM Geotiff file using the  GeoTiffReader.readSingleband() method. I have used the method before.  When the file loads I get the following stack dump:```java.lang.StringIndexOutOfBoundsException: String index out of range: 29	at java.lang.String.substring(String.java:1963)	at geotrellis.raster.io.geotiff.tags.GeoKeyReader$.readAsciis$1(GeoKeyReader.scala:212)	at geotrellis.raster.io.geotiff.tags.GeoKeyReader$.readGeoKeyEntry$1(GeoKeyReader.scala:39)	at geotrellis.raster.io.geotiff.tags.GeoKeyReader$.read(GeoKeyReader.scala:248)	at geotrellis.raster.io.geotiff.reader.TiffTagsReader$ByteBufferTagReaderWrapper$.readGeoKeyDirectoryTag$extension(TiffTagsReader.scala:177)	at geotrellis.raster.io.geotiff.reader.TiffTagsReader$.readTag(TiffTagsReader.scala:83)	at geotrellis.raster.io.geotiff.reader.TiffTagsReader$.read(TiffTagsReader.scala:69)	at geotrellis.raster.io.geotiff.reader.GeoTiffReader$.readGeoTiffInfo(GeoTiffReader.scala:224)	at geotrellis.raster.io.geotiff.reader.GeoTiffReader$.readSingleband(GeoTiffReader.scala:67)	at geotrellis.raster.io.geotiff.reader.GeoTiffReader$.readSingleband(GeoTiffReader.scala:55)	at geotrellis.raster.io.geotiff.reader.GeoTiffReader$.readSingleband(GeoTiffReader.scala:49)	at .<init>(<console>:15)	at .<clinit>(<console>)	at .<init>(<console>:7)	at .<clinit>(<console>)	at $print(<console>)	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)	at java.lang.reflect.Method.invoke(Method.java:497)	at scala.tools.nsc.interpreter.IMain$ReadEvalPrint.call(IMain.scala:734)	at scala.tools.nsc.interpreter.IMain$Request.loadAndRun(IMain.scala:983)	at scala.tools.nsc.interpreter.IMain.loadAndRunReq$1(IMain.scala:573)	at scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:604)	at scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:568)	at scala.tools.nsc.interpreter.ILoop.reallyInterpret$1(ILoop.scala:760)	at scala.tools.nsc.interpreter.ILoop.interpretStartingWith(ILoop.scala:805)	at scala.tools.nsc.interpreter.ILoop.command(ILoop.scala:717)	at scala.tools.nsc.interpreter.ILoop.processLine$1(ILoop.scala:581)	at scala.tools.nsc.interpreter.ILoop.innerLoop$1(ILoop.scala:588)	at scala.tools.nsc.interpreter.ILoop.loop(ILoop.scala:591)	at scala.tools.nsc.interpreter.ILoop$$anonfun$process$1.apply$mcZ$sp(ILoop.scala:882)	at scala.tools.nsc.interpreter.ILoop$$anonfun$process$1.apply(ILoop.scala:837)	at scala.tools.nsc.interpreter.ILoop$$anonfun$process$1.apply(ILoop.scala:837)	at scala.tools.nsc.util.ScalaClassLoader$.savingContextLoader(ScalaClassLoader.scala:135)	at scala.tools.nsc.interpreter.ILoop.process(ILoop.scala:837)	at scala.tools.nsc.interpreter.ILoop.main(ILoop.scala:904)	at xsbt.ConsoleInterface.run(ConsoleInterface.scala:62)	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)	at java.lang.reflect.Method.invoke(Method.java:497)	at sbt.compiler.AnalyzingCompiler.call(AnalyzingCompiler.scala:101)	at sbt.compiler.AnalyzingCompiler.console(AnalyzingCompiler.scala:76)	at sbt.Console.sbt$Console$$console0$1(Console.scala:22)	at sbt.Console$$anonfun$apply$2$$anonfun$apply$1.apply$mcV$sp(Console.scala:23)	at sbt.Console$$anonfun$apply$2$$anonfun$apply$1.apply(Console.scala:23)	at sbt.Console$$anonfun$apply$2$$anonfun$apply$1.apply(Console.scala:23)	at sbt.Logger$$anon$4.apply(Logger.scala:85)	at sbt.TrapExit$App.run(TrapExit.scala:248)	at java.lang.Thread.run(Thread.java:745)``` ",1/9/17,1
owcm,"The gdalinfo metadata for the file is as follows:```Driver: GTiff/GeoTIFFFiles: corbin_fr_test1.tifSize is 695, 577Coordinate System is:PROJCS[""WGS 84 / UTM zone 18N"",    GEOGCS[""WGS 84"",        DATUM[""WGS_1984"",            SPHEROID[""WGS 84"",6378137,298.257223563,                AUTHORITY[""EPSG"",""7030""]],            AUTHORITY[""EPSG"",""6326""]],        PRIMEM[""Greenwich"",0],        UNIT[""degree"",0.0174532925199433],        AUTHORITY[""EPSG"",""4326""]],    PROJECTION[""Transverse_Mercator""],    PARAMETER[""latitude_of_origin"",0],    PARAMETER[""central_meridian"",-75],    PARAMETER[""scale_factor"",0.9996],    PARAMETER[""false_easting"",500000],    PARAMETER[""false_northing"",0],    UNIT[""metre"",1,        AUTHORITY[""EPSG"",""9001""]],    AUTHORITY[""EPSG"",""32618""]]Origin = (291768.825000000011642,4231284.815000000409782)Pixel Size = (1.001191642651283,-1.000414930555659)Metadata:  AREA_OR_POINT=Area  TIFFTAG_ARTIST=Created by OpenWhere  TIFFTAG_SOFTWARE=Surge-las2dem by OpenWhereImage Structure Metadata:  INTERLEAVE=BANDCorner Coordinates:Upper Left  (  291768.825, 4231284.815) ( 77d22'41.57""W, 38d12'19.64""N)Lower Left  (  291768.825, 4230707.576) ( 77d22'40.96""W, 38d12' 0.93""N)Upper Right (  292464.653, 4231284.815) ( 77d22'12.98""W, 38d12'20.22""N)Lower Right (  292464.653, 4230707.576) ( 77d22'12.38""W, 38d12' 1.50""N)Center      (  292116.739, 4230996.195) ( 77d22'26.97""W, 38d12'10.57""N)Band 1 Block=695x20 Type=Float32, ColorInterp=Gray  NoData Value=-9999```",1/9/17,1
owcm,My guess is there is a compression problem with my file.  Wondered if you might have any thoughts.,1/9/17,1
lossyrob,What version of GeoTrellis?,1/9/17,1
lossyrob,"looks like it_Ñés choking on the tags - that_Ñés a `las2dem` generated geotiff, so either they are writing something wrong or writing something we didn_Ñét account for in our reader",1/9/17,1
owcm,@lossyrob Version 0.10.2,1/9/17,1
owcm,@lossyrob If I reproject the file using gdalwarp then the file is read fine.,1/9/17,1
lossyrob," @owcm interesting, can you write up a github issue and link the file?",1/9/17,1
lossyrob,Sounds like a potential bug,1/9/17,1
owcm,@lossyrob Will do Rob.   We are using a home grown las2dem tool. I have a suspicion that the Ascii field or key might be malformed.,1/9/17,1
lossyrob,"ah right. But, if GDAL handles it, we should be able to handle it without barfing, so I_Ñéd say it_Ñés a GeoTrellis bug either way. Solving it would also help debug the issue with the homegrown tool as well. I could point you in the direction of where that_Ñés happening/what_Ñés happening in the GT code if you want to take a stab :)",1/9/17,1
owcm,"@lossyrob Makes sense, I have been looking at GeoKeyReader in geotrellis.raster.io.geotiff.tags readAsciis method, is something else you wanted to pointë_me towards",1/9/17,1
lossyrob,are the strings null terminated? I think our ascii reader might make an assumption about that,1/9/17,1
lossyrob,(it_Ñés in the tiff spec),1/9/17,1
lossyrob,"I think my approach would be to set up a failing unit test, then start at that `readAsciis` area and move down the stack if necessary",1/9/17,1
owcm,ok,1/9/17,1
mehtaharsh1995,How could I write input.json and output.json to run application on Windows(Not using hadoop)  using Scala IDE.,1/10/17,1
mehtaharsh1995,Means what would I write in backend type in input.json instead of hadoop to run on local file system. ,1/10/17,1
mehtaharsh1995,[![picture.PNG](https://files.gitter.im/geotrellis/geotrellis/HJsC/thumb/picture.png)](https://files.gitter.im/geotrellis/geotrellis/HJsC/picture.PNG),1/10/17,1
mehtaharsh1995,"not enough arguments for method ingest: (args: Seq[String], keyIndexMethod: geotrellis.spark.io.index.KeyIndexMethod[geotrellis.spark.SpatialKey], modules: Seq[geotrellis.spark.etl.TypedModule])(implicit evidence$1: geotrellis.util.Component[geotrellis.vector.ProjectedExtent,geotrellis.vector.ProjectedExtent], implicit evidence$2: reflect.runtime.universe.TypeTag[geotrellis.vector.ProjectedExtent], implicit evidence$3: geotrellis.vector.ProjectedExtent __ê geotrellis.spark.tiling.TilerKeyMethods[geotrellis.vector.ProjectedExtent,geotrellis.spark.SpatialKey], implicit evidence$4: geotrellis.spark.SpatialComponent[geotrellis.spark.SpatialKey], implicit evidence$5: geotrellis.spark.Boundable[geotrellis.spark.SpatialKey], implicit evidence$6: reflect.runtime.universe.TypeTag[geotrellis.spark.SpatialKey], implicit evidence$7: reflect.runtime.universe.TypeTag[geotrellis.raster.Tile], implicit evidence$8: geotrellis.raster.stitch.Stitcher[geotrellis.raster.Tile], implicit evidence$9: geotrellis.raster.Tile __ê geotrellis.raster.reproject.TileReprojectMethods[geotrellis.raster.Tile], implicit evidence$10: geotrellis.raster.Tile __ê geotrellis.raster.crop.CropMethods[geotrellis.raster.Tile], implicit evidence$11: geotrellis.raster.Tile __ê geotrellis.raster.merge.TileMergeMethods[geotrellis.raster.Tile], implicit evidence$12: geotrellis.raster.Tile __ê geotrellis.raster.prototype.TilePrototypeMethods[geotrellis.raster.Tile], implicit sc: org.apache.spark.SparkContext)Unit. Unspecified value parameter keyIndexMethod.",1/10/17,1
pomadchin,@mehtaharsh1995 what_Ñés your build.sbt?,1/10/17,1
pomadchin,how do you build project?,1/10/17,1
mehtaharsh1995,I built without build.sbt,1/10/17,1
mehtaharsh1995,is it possible?,1/10/17,1
mehtaharsh1995,I downloaded all jars of geotrellis and spark and add them into my project. ,1/10/17,1
pomadchin,not sure it_Ñés an old style way of building projects,1/10/17,1
pomadchin,potentially you missed lots of deps,1/10/17,1
pomadchin,try to build using maven or sbt,1/10/17,1
pomadchin,"checkout our demos: * chatta demo (updated): https://github.com/pomadchin/geotrellis-chatta-demo/tree/feature/chattademo-update* https://github.com/azavea/geotrellis-geomesa-template-project* https://github.com/geotrellis/geotrellis-landsat-emr-demo* https://github.com/geotrellis/geotrellis-landsat-tutorial",1/10/17,1
pomadchin,we have a geotrellis sbt template: https://github.com/geotrellis/geotrellis-sbt-template,1/10/17,1
pomadchin,any troubles you_Ñéll have with sbt write down here,1/10/17,1
pomadchin,that would be much easier to build project using some build tool as there can be lots of underlying transitive dependencies ,1/10/17,1
MichiB7_twitter,"I have a questeion regarding logging in GeoTrellis. We are running an application in an EMR cluster that uses GeoTrellis. We are using the class ArrayMultibandTile and while running the application we are getting following NPE:`17/01/09 17:02:59 ERROR Executor: Exception in task 9.0 in stage 113.0 (TID 909)java.lang.NullPointerException	at org.slf4j.impl.Log4jLoggerAdapter.isWarnEnabled(Log4jLoggerAdapter.java:391)	at geotrellis.raster.ArrayTile$class.convert(ArrayTile.scala:46)	at geotrellis.raster.UShortArrayTile.convert(UShortArrayTile.scala:24)	at geotrellis.raster.UShortArrayTile.convert(UShortArrayTile.scala:24)	at geotrellis.raster.ArrayMultibandTile.convert(ArrayMultibandTile.scala:130)	at de.gaf.ric.algo.index.Ndvi$.apply(Ndvi.scala:27)	at de.gaf.ric.apps.NdviIsodataWorkflow$$anonfun$runSparkProcess$1$$anonfun$1.apply(NdviIsodataWorkflow.scala:63)	at de.gaf.ric.apps.NdviIsodataWorkflow$$anonfun$runSparkProcess$1$$anonfun$1.apply(NdviIsodataWorkflow.scala:63)	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$mapValues$1$$anonfun$apply$43$$anonfun$apply$44.apply(PairRDDFunctions.scala:759)	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$mapValues$1$$anonfun$apply$43$$anonfun$apply$44.apply(PairRDDFunctions.scala:759)	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)	at org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:192)	at org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:63)	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:79)	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:47)	at org.apache.spark.scheduler.Task.run(Task.scala:86)	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)	at java.lang.Thread.run(Thread.java:745)`GeoTrellis is using scala-logging which includes the Log4jLoggerAdapter. In this class the variable 'logger' is declared as transient. Thererfore, after serialization/deserialization, we are getting the NPE.Is there a way to avoid the exception?",1/10/17,1
pomadchin,"@MichiB7_twitter yo, what gt version do you use?",1/10/17,1
pomadchin,Npe caused by loggers mismatch,1/10/17,1
MichiB7_twitter,We are using the version 1.0.0,1/10/17,1
pomadchin,Interesting,1/10/17,1
pomadchin,on what operation it happens?,1/10/17,1
MichiB7_twitter,"We have a class, that calculates a NDVI index. The operation is:   MultibandTile.convert(FloatConstantNoDataCellType)",1/10/17,1
pomadchin,@MichiB7_twitter can you show me your class?,1/10/17,1
pomadchin,and your deps,1/10/17,1
pomadchin,i think that probably loggers versions mismatch may take place,1/10/17,1
pomadchin,new gt uses serializable logger,1/10/17,1
pomadchin,so that_Ñés why it_Ñés weird to see such err; + we are runing gt on emr too and we have no such problems ,1/10/17,1
pomadchin,"pls can you double check everything? (deps, code) you can even throw your build.sbt here; i_Ñéll try to help",1/10/17,1
MichiB7_twitter,"Hi Grisha,here is our NDVI class:```type SpectralIndex = MultibandTile => Tileobject Ndvi extends SpectralIndex {  /**    * Returns a Tile with NDVI values given that in the input multiband tile, the red band has the    * index 0 and the infrared band has the index 1.    *    * If you have to select the bands manually, use withBands.    *    * @param multibandTile Input multiband tile of red band (index 0) and infrared band (index 1).    * @return Tile with NDVI values.    */  override def apply(multibandTile: MultibandTile): Tile =    multibandTile      .convert(FloatConstantNoDataCellType)      .combineDouble(0, 1) { ndviFunction }...// NDVI base function.  private def ndviFunction(red: Double, infrared: Double): Double =    if(isData(red) && isData(infrared)) { (infrared - red) / (infrared + red) }    else Float.NaN}```",1/10/17,1
MichiB7_twitter,Sorry for the bad fromat!,1/10/17,1
MichiB7_twitter,"We are using Maven. Our dependencies are as following: de.gaf.ric:imageprocessing-lib:jar:0.11.0-SNAPSHOT +- de.gaf.ric:geolattice-lib:jar:0.11.0-SNAPSHOT:compile +- org.apache.spark:spark-mllib_2.11:jar:2.0.2:compile |  +- org.apache.spark:spark-core_2.11:jar:2.0.2:compile |  |  +- org.apache.avro:avro-mapred:jar:hadoop2:1.7.7:compile |  |  |  +- org.apache.avro:avro-ipc:jar:1.7.7:compile |  |  |  \- org.apache.avro:avro-ipc:jar:tests:1.7.7:compile |  |  +- com.twitter:chill_2.11:jar:0.8.0:compile |  |  |  \- com.esotericsoftware:kryo-shaded:jar:3.0.3:compile |  |  |     +- com.esotericsoftware:minlog:jar:1.3.0:compile |  |  |     \- org.objenesis:objenesis:jar:2.1:compile |  |  +- com.twitter:chill-java:jar:0.8.0:compile |  |  +- org.apache.xbean:xbean-asm5-shaded:jar:4.4:compile |  |  +- org.apache.hadoop:hadoop-client:jar:2.2.0:compile |  |  |  +- org.apache.hadoop:hadoop-common:jar:2.2.0:compile |  |  |  |  +- commons-cli:commons-cli:jar:1.2:compile |  |  |  |  +- org.apache.commons:commons-math:jar:2.1:compile |  |  |  |  +- xmlenc:xmlenc:jar:0.52:compile |  |  |  |  +- commons-lang:commons-lang:jar:2.5:compile |  |  |  |  +- commons-configuration:commons-configuration:jar:1.6:compile |  |  |  |  |  +- commons-digester:commons-digester:jar:1.8:compile |  |  |  |  |  \- commons-beanutils:commons-beanutils-core:jar:1.8.0:compile |  |  |  |  \- org.apache.hadoop:hadoop-auth:jar:2.2.0:compile |  |  |  +- org.apache.hadoop:hadoop-hdfs:jar:2.2.0:compile |  |  |  |  \- org.mortbay.jetty:jetty-util:jar:6.1.26:compile |  |  |  +- org.apache.hadoop:hadoop-mapreduce-client-app:jar:2.2.0:compile |  |  |  |  +- org.apache.hadoop:hadoop-mapreduce-client-common:jar:2.2.0:compile |  |  |  |  |  +- org.apache.hadoop:hadoop-yarn-client:jar:2.2.0:compile |  |  |  |  |  |  \- com.google.inject:guice:jar:3.0:compile |  |  |  |  |  |     +- javax.inject:javax.inject:jar:1:compile |  |  |  |  |  |     \- aopalliance:aopalliance:jar:1.0:compile |  |  |  |  |  \- org.apache.hadoop:hadoop-yarn-server-common:jar:2.2.0:compile |  |  |  |  \- org.apache.hadoop:hadoop-mapreduce-client-shuffle:jar:2.2.0:compile |  |  |  +- org.apache.hadoop:hadoop-yarn-api:jar:2.2.0:compile |  |  |  +- org.apache.hadoop:hadoop-mapreduce-client-core:jar:2.2.0:compile |  |  |  |  \- org.apache.hadoop:hadoop-yarn-common:jar:2.2.0:compile |  |  |  +- org.apache.hadoop:hadoop-mapreduce-client-jobclient:jar:2.2.0:compile |  |  |  \- org.apache.hadoop:hadoop-annotations:jar:2.2.0:compile |  |  +- org.apache.spark:spark-launcher_2.11:jar:2.0.2:compile |  |  +- org.apache.spark:spark-network-common_2.11:jar:2.0.2:compile |  |  |  +- org.fusesource.leveldbjni:leveldbjni-all:jar:1.8:compile |  |  |  \- com.fasterxml.jackson.core:jackson-annotations:jar:2.6.5:compile |  |  +- org.apache.spark:spark-network-shuffle_2.11:jar:2.0.2:compile |  |  +- org.apache.spark:spark-unsafe_2.11:jar:2.0.2:compile |  |  +- net.java.dev.jets3t:jets3t:jar:0.7.1:compile |  |  |  +- commons-codec:commons-codec:jar:1.3:compile |  |  |  \- commons-httpclient:commons-httpclient:jar:3.1:compile |  |  +- org.apache.curator:curator-recipes:jar:2.4.0:compile |  |  |  +- org.apache.curator:curator-framework:jar:2.4.0:compile |  |  |  |  \- org.apache.curator:curator-client:jar:2.4.0:compile |  |  |  \- org.apache.zookeeper:zookeeper:jar:3.4.5:compile |  |  +- javax.servlet:javax.servlet-api:jar:3.1.0:compile |  |  +- org.apache.commons:commons-lang3:jar:3.3.2:compile |  |  +- com.google.code.findbugs:jsr305:jar:1.3.9:compile |  |  +- org.slf4j:slf4j-api:jar:1.7.16:compile |  |  +- org.slf4j:jul-to-slf4j:jar:1.7.16:compile |  |  +- org.slf4j:jcl-over-slf4j:jar:1.7.16:compile |  |  +- log4j:log4j:jar:1.2.17:compile |  |  +- org.slf4j:slf4j-log4j12:jar:1.7.16:compile |  |  +- com.ning:compress-lzf:jar:1.0.3:compile |  |  +- org.xerial.snappy:snappy-java:jar:1.1.2.6:compile |  |  +- net.jpountz.lz4:lz4:jar:1.3.0:compile |  |  +- org.roaringbitmap:RoaringBitmap:jar:0.5.11:compile |  |  +- commons-net:commons-net:jar:2.2:compile |  |  +- org.js",1/10/17,1
MichiB7_twitter,@pomadchin ** Thank you for your help**,1/10/17,1
pomadchin,@MichiB7_twitter cool; can you give me your maven configuration? to reproduce error,1/10/17,1
pomadchin,it_Ñés not necessary though but would be useful,1/10/17,1
MichiB7_twitter,"That is a bit complicated because its depends on a parent and other .poms. We declare the versions in the parent pom with dependencyManagement. We don't set a version for Scala-Logging in the parent or in the project pom. So the version comes from GeoTrelles as you can see in the dependeny tree. ",1/10/17,1
MichiB7_twitter,"@pomadchin Sorry, the dependency tree was incomplete. Here ist the important part:```[INFO] +- org.locationtech.geotrellis:geotrellis-spark_2.11:jar:1.0.0:compile[INFO] |  +- org.scala-lang:scala-library:jar:2.11.8:provided[INFO] |  +- org.locationtech.geotrellis:geotrellis-util_2.11:jar:1.0.0:compile[INFO] |  +- org.locationtech.geotrellis:geotrellis-vectortile_2.11:jar:1.0.0:compile[INFO] |  |  +- org.locationtech.geotrellis:geotrellis-vector_2.11:jar:1.0.0:compile[INFO] |  |  |  \- org.locationtech.geotrellis:geotrellis-proj4_2.11:jar:1.0.0:compile[INFO] |  |  \- com.trueaccord.scalapb:scalapb-runtime_2.11:jar:0.5.32:compile[INFO] |  |     +- com.trueaccord.lenses:lenses_2.11:jar:0.4.5:compile[INFO] |  |     +- com.lihaoyi:fastparse_2.11:jar:0.3.4:compile[INFO] |  |     |  \- com.lihaoyi:fastparse-utils_2.11:jar:0.3.4:compile[INFO] |  |     \- com.google.protobuf:protobuf-java:jar:3.0.0-beta-3:compile[INFO] |  +- org.locationtech.geotrellis:geotrellis-raster_2.11:jar:1.0.0:compile[INFO] |  |  +- org.locationtech.geotrellis:geotrellis-macros_2.11:jar:1.0.0:compile[INFO] |  |  +- com.vividsolutions:jts-core:jar:1.14.0:compile[INFO] |  |  \- com.opencsv:opencsv:jar:3.8:compile[INFO] |  |     \- commons-beanutils:commons-beanutils:jar:1.9.2:compile[INFO] |  |        \- commons-collections:commons-collections:jar:3.2.1:compile[INFO] |  +- com.google.uzaygezen:uzaygezen-core:jar:0.2:compile[INFO] |  |  \- com.google.guava:guava:jar:14.0-rc1:compile**[INFO] |  +- com.typesafe.scala-logging:scala-logging_2.11:jar:3.5.0:compile**[INFO] |  +- org.apache.avro:avro:jar:1.8.1:compile```",1/10/17,1
pomadchin,@MichiB7_twitter what EMR version do you use? the latest?,1/10/17,1
MichiB7_twitter,We used the EMR version 5.2.0,1/10/17,1
lossyrob,I do think this should read different https://github.com/locationtech/geotrellis/blame/_old/master/raster/src/main/scala/geotrellis/raster/ArrayTile.scala#L45,1/10/17,1
lossyrob,because there_Ñés no data loss going from a non-floating point to a floating point,1/10/17,1
lossyrob,eh actually if you go from Int to Float...,1/10/17,1
lossyrob,but that_Ñés true between most datatypes,1/10/17,1
lossyrob,"so ArrayTile extends LazyLogging, which has a `logger`, which in this case in null. because it_Ñés transient. however we haven_Ñét run into this problem..interesting",1/10/17,1
lossyrob,how is the tile being serialized?,1/10/17,1
lossyrob,hmm we might not notice this because we use Kryo and don_Ñét ever close over a Tile,1/10/17,1
pomadchin,we can run a small job in theory and to make this operation ,1/10/17,1
lossyrob,Is the Tile that_Ñés causing the NPE being serialized via a closure (so definitely using java serialization) or via a shuffle (so could use Kryo or Java ser),1/10/17,1
lossyrob,true. and then we_Ñéd have to come up with a fix_Ñ_lazy logging might not be the best choice for our serializable types if this is indeed what_Ñés happening,1/10/17,1
echeipesh,Actually looks like its `@volatile` `@transient` would have made it survive serialization,1/10/17,1
echeipesh,https://github.com/typesafehub/scala-logging/blob/3a821e304d8fb3993ce2c5fb69387d336c65a08a/src/main/scala/com/typesafe/scalalogging/Logging.scala#L27,1/10/17,1
pomadchin,@echeipesh where is this logger marked as transient?,1/10/17,1
lossyrob,https://github.com/typesafehub/scala-logging/blob/v3.5.0/src/main/scala/com/typesafe/scalalogging/Logging.scala,1/10/17,1
lossyrob,the released version doesn_Ñét have @volatile,1/10/17,1
pomadchin,ah,1/10/17,1
lossyrob,ah so it_Ñés not marked transient,1/10/17,1
lossyrob,so we need to wrap it into something with transient,1/10/17,1
pomadchin,it_Ñés serializable ,1/10/17,1
pomadchin,https://github.com/typesafehub/scala-logging/pull/52,1/10/17,1
lossyrob,it_Ñés the lazy val though,1/10/17,1
lossyrob,is what eugene says,1/10/17,1
echeipesh,"It_Ñés not. I just know that I have made `@transient lazy val logger` which makes it survive serialization. Basically it gets serialized as un-initinitlized lazy val, which causes it to be re-inited.",1/10/17,1
pomadchin,we had a discussion here btw: https://github.com/locationtech/geotrellis/issues/1408,1/10/17,1
MichiB7_twitter,Our exception is refering to the class ``,1/10/17,1
lossyrob,this is what seems to be going on https://github.com/typesafehub/scala-logging/pull/52 and they say it_Ñés fixed,1/10/17,1
pomadchin,++,1/10/17,1
MichiB7_twitter,Our exception is refering to the class `org.slf4j.impl.Log4jLoggerAdapter` where the logger is marked transient,1/10/17,1
lossyrob,is there any way the version of scala-logging is getting kicked out for an older version?,1/10/17,1
pomadchin,so we need to try to recreate things and potentially that may be a problem of emr logger version or smth else,1/10/17,1
echeipesh,"We should def try to recreate this. The test is to trigger logging before serialization, then attempt to use it again post serialization.",1/10/17,1
lossyrob,@MichiB7_twitter one quick fix thing to try in the meantime is to use Kryo serialization if you_Ñére not already doing so,1/10/17,1
MichiB7_twitter,"@lossyrob Sorry Rob, I'm new in this project and in GeoTrellis. How do I find out which serialization is used and how can I set it? ",1/10/17,1
MichiB7_twitter,"@lossyrob In our project we are using Kryo registration and after we registered the `org.slf4j.impl.Log4jLoggerAdapter` we got the NPE. So I think we are already using Kryo serialization.",1/10/17,1
pomadchin,"@MichiB7_twitter during the spark conext creation you can setup spark configuration for this context: ```scala  val conf =     new SparkConf()      .setAppName(""ChattaDemo"")      .set(""spark.serializer"", ""org.apache.spark.serializer.KryoSerializer"")      .set(""spark.kryo.registrator"", ""geotrellis.spark.io.kryo.KryoRegistrator"")    implicit val sparkContext = new SparkContext(conf)```",1/10/17,1
pomadchin,aha,1/10/17,1
pomadchin,interesting,1/10/17,1
lossyrob,@MichiB7_twitter sorry yeah it_Ñés a Spark thing. You can set it into the SparkConf like @pomadchin mentioned or on the `spark-submit` side via a `_ÑÓconf`http://spark.apache.org/docs/latest/configuration.html#compression-and-serialization,1/10/17,1
pomadchin,@MichiB7_twitter you said that you registered it; that means that you had kryo serialization exceptions?,1/10/17,1
pomadchin,and can you throw here usage example of this function you use? ,1/10/17,1
pomadchin,```rdd.map(Ndvi)``` ?,1/10/17,1
MichiB7_twitter,"@pomadchin Yes, I registered the 'org.slf4j.impl.Log4jLoggerAdapter' class. Before that I had a Kryo not registered exception",1/10/17,1
pomadchin,ok,1/10/17,1
echeipesh,Thats kinda strange. What was the exception? Not every class needs to be registered with kryo for it to function unless `spark.kryo.registrationRequired=true`,1/10/17,1
MichiB7_twitter,"`val ndviPixels: RDD[(ProjectedExtent, Tile)] = sceneRdd.mapValues(Ndvi(_))`",1/10/17,1
pomadchin,yes i thought about it,1/10/17,1
pomadchin,@echeipesh i can try to check it on landsat emr demo,1/10/17,1
MichiB7_twitter,"@echeipesh Yes, we use `.set(""spark.kryo.registrationRequired"", ""true"")`",1/10/17,1
echeipesh,"Ok, that makes sense. Disabling it would change the way the class is indexed but it shouldn_Ñét change any of the machanics of how it is serialized, so nothing else to be said there I think.",1/10/17,1
MichiB7_twitter,"I have an assumption but I have to test it tomorrow in more detail. On a first sight, with EMR 5.0.0 I got no Error. With EMR 5.2.0 the error occurs. But this really only a first sight. I saw this in the cluster logs. I have to test this tomorrow and have a look to the executor logs in detail while the cluster is running. Here in Germany its late, so I will be back tomorrow. **Thank you for your help!**",1/10/17,1
lossyrob,"Ok, very interesting about the version causing the error. Guten Nacht!",1/10/17,1
pomadchin,w0w,1/10/17,1
pomadchin,it_Ñés extremely useful,1/10/17,1
pomadchin,@lossyrob log4j versions mismatch makes sense,1/10/17,1
pomadchin,@MichiB7_twitter i double checked on emr 5.2.0,1/11/17,1
pomadchin,looks fine ,1/11/17,1
pomadchin,hm,1/11/17,1
pomadchin,what i tried to do: https://github.com/pomadchin/geotrellis-landsat-emr-demo/tree/test/logger-issue,1/11/17,1
pomadchin,cluster details: https://github.com/pomadchin/geotrellis-landsat-emr-demo/blob/test/logger-issue/Makefile#L48-L63,1/11/17,1
pomadchin,command: https://github.com/pomadchin/geotrellis-landsat-emr-demo/blob/37ec5ceab23db1df24776a3042bb5ae43c3b0fa5/server/src/main/scala/demo/DemoServiceActor.scala#L293-L295,1/11/17,1
pomadchin,i did a additional mem suffle to be sure we serialized and deserialized everything,1/11/17,1
pomadchin,NDVI function: https://github.com/pomadchin/geotrellis-landsat-emr-demo/blob/37ec5ceab23db1df24776a3042bb5ae43c3b0fa5/server/src/main/scala/demo/NDVI.scala,1/11/17,1
pomadchin,i did two conversions (explicit to Int and after that into Float) to be 100% sure that we change from int to floating point type,1/11/17,1
pomadchin,[![Screen Shot 2017-01-11 at 03.50.01.png](https://files.gitter.im/geotrellis/geotrellis/1PUK/thumb/Screen-Shot-2017-01-11-at-03.50.01.png)](https://files.gitter.im/geotrellis/geotrellis/1PUK/Screen-Shot-2017-01-11-at-03.50.01.png),1/11/17,1
pomadchin,[![Screen Shot 2017-01-11 at 03.50.11.png](https://files.gitter.im/geotrellis/geotrellis/FmIi/thumb/Screen-Shot-2017-01-11-at-03.50.11.png)](https://files.gitter.im/geotrellis/geotrellis/FmIi/Screen-Shot-2017-01-11-at-03.50.11.png),1/11/17,1
pomadchin,[commit](https://github.com/pomadchin/geotrellis-landsat-emr-demo/commit/37ec5ceab23db1df24776a3042bb5ae43c3b0fa5) with diff to see everything i mentioned above in code,1/11/17,1
pomadchin,@MichiB7_twitter what_Ñés you emr configuration? how do you build everything? are you shure that everything is fine with deps in your project (it_Ñés important),1/11/17,1
pomadchin,"server depshttps://gist.github.com/pomadchin/e263c2cd3da9857221ff9ccd495e14df",1/11/17,1
pomadchin,mb i missed smth? ,1/11/17,1
pomadchin,how do you lunch spark job? cluster mode or client mode? can you throw an example of command you launch,1/11/17,1
pomadchin,`*_*` ping me tomorrow I_Ñéll try to help you,1/11/17,1
prahim,"```val tilePng =          tile          .renderPng(ColorRamps.GreenToRedOrange)          .bytesval inputStream: ByteArrayInputStream = new ByteArrayInputStream(tilePng)s3Client.putObject(thumbnailS3KeyPrefix + thumbnailS3FileName, inputStream, tilePng.length.toLong)```The code above puts transparent images in s3 -- is there a problem with the way I am writing it? Before I renderPng, I did verify that the tile has data.",1/11/17,1
metasim,Is there a specific reason why `geotrellis.vector.io.json.JsonFeatureCollectionMap` isn't `Serializable`? ,1/11/17,1
lossyrob,"@metasim nope, that_Ñés an oversight",1/11/17,1
metasim,Wanting to  Spark-broadcast it.,1/11/17,1
metasim,...might be able to work around it.,1/11/17,1
echeipesh,"You could use our `KryoWrapper` for the moment, we use it to serialize things that aren_Ñét: https://github.com/locationtech/geotrellis/blob/master/s3/src/main/scala/geotrellis/spark/io/s3/S3RDDReader.scala#L67",1/11/17,1
metasim,"Oh, wicked!",1/11/17,1
echeipesh,Fair warning is its _ÑÒhacky_Ñù but hasn_Ñét really blown up on us yet.,1/11/17,1
metasim,It's better than broadcasting the JSON text and having my transformation reparse the data on each node.,1/11/17,1
metasim,"I'm really glad there's now a `DataType.withNoData(Option[Double])` method, but I'm confused about the semantics described by the ScalaDoc:> In case where [[DataType]] is not Double noDataValue will be coerced to that type.If my underlying type is `IntCellType` and I call `(t: IntCellType).withNoData(Some(Double.NaN))`, what's the expected behavior?",1/11/17,1
metasim,What I'm wanting is something akin to `DataType.withDefaultNoData()`,1/11/17,1
metasim,Because I don't always know (without pattern matching) If I'm dealing with integral or continuous tiles.,1/11/17,1
lossyrob,"@metasim expected behaviour is Double.NaN.toInt, which will give you 0",1/11/17,1
lossyrob,"We don_Ñét have that method, but I understand what you_Ñére looking for",1/11/17,1
lossyrob,i think,1/11/17,1
lossyrob,"you want it to be `Byte.MinValue` if it_Ñés ByteCellType, `Short.MinValue` if it_Ñés ShortCellType, in general for `XXCellType` you want it to return `XXConstantNoDataCellType`",1/11/17,1
metasim,"Gotcha. As an outsider, I felt like there was some API smell given that the parameter type was `Option[Double]` (hope you don't mind my saying).",1/11/17,1
prahim,"hey I am still having issues with rendering png with a colorRamp ....I get the following exception:```key not found: -1155259137java.util.NoSuchElementException: key not found: -1155259137	at scala.collection.MapLike$class.default(MapLike.scala:228)	at scala.collection.AbstractMap.default(Map.scala:59)	at scala.collection.MapLike$class.apply(MapLike.scala:141)	at scala.collection.AbstractMap.apply(Map.scala:59)	at geotrellis.raster.render.IntCachedColorMap$$anonfun$mapColorsToIndex$1.apply$mcVI$sp(ColorMap.scala:317)	at geotrellis.raster.histogram.FastMapHistogram.foreachValue$mcI$sp(FastMapHistogram.scala:264)	at geotrellis.raster.render.IntCachedColorMap.mapColorsToIndex(ColorMap.scala:317)	at geotrellis.raster.render.png.IndexedPngEncoding.convertColorMap(PngColorEncoding.scala:49)	at geotrellis.raster.render.PngRenderMethods$class.renderPng(PngRenderMethods.scala:33)```if I don't pass in a color ramp and call renderPng() I get an image and no exception... am I missing a step?",1/11/17,1
metasim,Don't have a betters suggestion tho.,1/11/17,1
lossyrob,"@metasim , yeah I_Ñéll agree..the option there is to allow for unsetting NoData, pass in None and it gives back the raw type. Def don_Ñét mind point out smells, in fact encourage it",1/11/17,1
lossyrob,"@prahim yes, there_Ñés a key difference that I think is perhaps not explained well in the documentation",1/11/17,1
lossyrob,wait some questions to ensure I know what_Ñés going on,1/11/17,1
lossyrob,or let me look into the stack trace first,1/11/17,1
prahim,@lossyrob sounds good thanks-- let me know if I can provide you more info,1/11/17,1
lossyrob,what is the datatype of the tile?,1/11/17,1
prahim,geotrellis.raster.Tile,1/11/17,1
lossyrob,"sorry, I mean is it Byte, Short, etc",1/11/17,1
lossyrob,yeah what I was thinking was incorrect. looking more into it_Ñ_.,1/11/17,1
prahim,"I am not sure I am using  the collectionLayerReader to get the tiles for a layer then stitching them together```val tileCollection: Seq[(SpatialKey, Tile)] with Metadata[TileLayerMetadata[SpatialKey]] =        collectionReader.read[SpatialKey, Tile, TileLayerMetadata[SpatialKey]](layerId)```How do I know what the datatype is?",1/11/17,1
lossyrob,`tileCollection.metadata.cellType`,1/11/17,1
prahim,float32,1/11/17,1
lossyrob,thanks,1/11/17,1
lossyrob,can you paste what the code around the `renderPng` call looks like?,1/11/17,1
prahim,"```val tile = tileCollection          .stitch          .mask(tileCollection.metadata.extent, Rasterizer.Options(includePartial = true, sampleType = PixelIsPoint))          .tile          .resample(128, 128)          .renderPng(ColorRamps.GreenToRedOrange)          .bytes```",1/11/17,1
lossyrob,thanks,1/11/17,1
lossyrob,"```scalaval tile = tileCollection          .stitch          .mask(tileCollection.metadata.extent, Rasterizer.Options(includePartial = true, sampleType = PixelIsPoint))          .tile          .resample(128, 128)val histogram = tile.histogramDoubleval bytes =     tile          .renderPng(ColorMap.fromQuantileBreaks(histogram, ColorRamps.GreenToRedOrange))          .bytes```",1/11/17,1
lossyrob,can you try that?,1/11/17,1
prahim,yay! it worked!,1/11/17,1
lossyrob,awesome. ,1/11/17,1
lossyrob,yeah there_Ñés a pretty dumb bug in the code (written by yours truely),1/11/17,1
lossyrob,That_Ñés been in there for almost a year,1/11/17,1
lossyrob,so thanks for uncovering that!,1/11/17,1
prahim,lol thank you -- yay at least it was caught ,1/11/17,1
lossyrob,true true,1/11/17,1
lossyrob,:),1/11/17,1
prahim,Does geotrellis provide a way to crop out NODATA? I see the crop methods but they require an extent-- is there another function I should use?,1/12/17,1
lossyrob,"you mean, crop the image so that there is at least one nodata cell on each border row/col?",1/12/17,1
prahim,yes -- does a function for that exist?,1/12/17,1
lossyrob,"no, but it wouldn_Ñét be too difficult to write",1/12/17,1
prahim,"would this work: find the dimensions of the tile, find out which columns and rows have no data and then call crop with the new column and rows? ",1/12/17,1
lossyrob,"yeah, that_Ñés an approach for sure",1/12/17,1
prahim,cool okay I'll try that out-- thank you,1/12/17,1
lossyrob,"one I would go with probably is, some mutable ugly for loop :)",1/12/17,1
lossyrob,"well, why don_Ñét you work out your approach and then I_Ñéll maybe give an alternate if it_Ñés any better :)",1/12/17,1
prahim,haha okay I will ping you if it doesn't work out -- thanks,1/12/17,1
lossyrob,This is how I_Ñéd approach it,1/12/17,1
lossyrob,"```scalaval raster: Raster[Tile] = ???var (colmin, colmax, rowmin, rowmax) = (Int.MaxValue, Int.MinValue, Int.MaxValue, Int.MinValue)cfor(0)(_ < row, _ + 1) { row =>  cfor(0)(_ < col, _ + 1) { col =>    if(isData(tile.getDouble(col, row))) {      colmin = math.min(colmin, col)      colmax = math.max(colmax, col)            rowmin = math.min(rowmin, row)      rowmax = math.max(rowmax, row)          }  }}val re = raster.rasterExtentval CellSize(cw, ch) = re.cellSizeval xmin = re.gridColToMap(colmin) - cw/2val xmax = re.gridColToMap(colmax) + cw/2val ymin = re.gridColToMap(rowmax) - ch/2val ymax = re.gridColToMap(rowmin) + ch/2val cropExtent = Extent(xmin, xmax, ymin, ymax)val result = raster.crop(cropExtent)```",1/12/17,1
lossyrob,(could be `get` instead of `getDouble` if the cell type isn_Ñét floating point),1/12/17,1
prahim,oh cool thank you -- I ended up having access to the extent I needed but it's nice to have a general solution,1/12/17,1
lossyrob,:+1:,1/12/17,1
metasim,Is there a `mapalgebra.local` class already defined that simply takes the first non-`NoData` value from a sequence of `Tile`s?,1/13/17,1
echeipesh,"I assume for a given pixel x,y? If so I don_Ñét think there is. The tile merge methods kind of have the same effect, by trying to burn multiple tiles into one, only overwriting `NoData`.",1/13/17,1
metasim,thx,1/13/17,1
pomadchin,@MichiB7_twitter hi! created tests to check logger serialization in tiles: https://github.com/locationtech/geotrellis/pull/1960/files,1/13/17,1
lossyrob,@metasim would be a straightforward [TileReducer](https://github.com/locationtech/geotrellis/blob/master/raster/src/main/scala/geotrellis/raster/mapalgebra/local/TileReducer.scala) based method,1/13/17,1
metasim,@lossyrob Perfect!,1/13/17,1
metasim,"@lossyrob , in your example above :point_up: [January 12, 2017 2:57 PM](https://gitter.im/geotrellis/geotrellis?at=5877dfa6873d96e16d5de9d2), what are the units of `x` and `y`?",1/13/17,1
metasim,"Or to put it another way, what are the units of ""map coordinates"" in GeoTrellis? I would have guessed they are CRS dependent, but a `RasterExtent` doesn't have a CRS.",1/13/17,1
lossyrob,"there_Ñés assumed implicit knowledge of CRS. i.e. we don_Ñét track CRS within the types, it_Ñés the client responsibility to not combine things of different CRSs",1/13/17,1
metasim,"So in the example above, how does the `gridToMap` come up with a transformation?",1/13/17,1
lossyrob,Very simple logic based on the extent,1/13/17,1
lossyrob,https://github.com/locationtech/geotrellis/blob/master/raster/src/main/scala/geotrellis/raster/RasterExtent.scala#L126-L131,1/13/17,1
metasim,"I just submitted PR #1964, but am getting an Eclipse ""ip-validation"" error, something I've not had a problem with before. I signed the CLA last fall.",1/16/17,1
metasim,Has something changed with the process?,1/16/17,1
lossyrob,yes. You_Ñéll have to _ÑÒsign_Ñù your commits with -s,1/16/17,1
lossyrob,hm I see you did sign,1/16/17,1
lossyrob,checking,1/16/17,1
lossyrob,it looks like it_Ñés from an email that doesn_Ñét have a cla,1/16/17,1
lossyrob,is it signed with a different email addy/,1/16/17,1
lossyrob,that email doesn_Ñét pass the email validation tool. perhaps you need to sign with the address that is registered with the eclipse account?,1/16/17,1
schBen,Hi! Some time ago I asked how to read JPEG2000 files (Sentinel2 data) using GeoTrellis. I would like to re-use my GeoTrellis-based stuff (based on RDD[Tile] etc.) that I wrote for Landsat data for the new Sentinel2 data. @pomadchin you then recommended converting everything to geotiff first. Is this still true today or are there any new possibilities in geotrellis? ;),1/16/17,1
lossyrob,"We haven_Ñét coded up a JPEG reader yet, so GDAL is still the way we go our sentinal processes. I know someone tried to use the ImageIO java reader to work with Sentinal (if that person wants to pipe in that would be useful), but that would be a worthwhile venture for someone wanting to create a reader...",1/16/17,1
lossyrob,a good GoogleSummerofCode projects perhaps,1/16/17,1
kbshah1981,@lossyrob  How Do we Create Server Assembly  ?,1/16/17,1
kthompson,We tried a number of java libraries to read in Sentinel2 JPEG2000 files and ended up going with gdal as there were issues with all of the other libraries.,1/16/17,1
owcm,"Working with ShapefileReader, looked through the documentation, but did see a way to get the CRS information for the feature being read in.  Is there a way to get this info? Thanks in Advance.",1/16/17,1
schBen,@lossyrob @kthompson Okay thank you! Then we'll try to make it work with GDAL as well.,1/17/17,1
mehtaharsh1995,"How Do I create  server Assembly?",1/17/17,1
mehtaharsh1995,"I want to display geotiff files while using spark server(spark submit --name ""name"" {path of server Assembly jar} local catalog), which I have ingested, And after, I want to perform subtraction on it ( r1-r2) , Because, Substraction operation is performed via server-assembly.jar  (Which is created in ""landsat server emr"" github project by @lossyrob @pomadchin  ), I copied your server folder, and performed to build server assembly jar , But, It didn't create. And, another problem is, for ingesting, I have used ETL.ingest(projectedExtent,spatialkey,tile) and in server-assembly.jar you used concept of times, I think So, it will give error like FIleAttributeStore not found (because of time is not presented in ingest method), So how do i resolve this issue? ",1/17/17,1
metasim,"@pomadchin Is Parquet support something that's still in the works? I'm currently working on flattening tiles into Spark DataSets, to then be serialized to Parquet, and would hate to repeat work that's already going on.",1/17/17,1
lossyrob,"@metasim we def have done some work on Parquet and datasets, and it would most likely be worth talking about ways to collaborate/knowledge share. Currently the Azaveans are having a yearly offsite meeting, but tomorrow we_Ñéll be back in the office and more responsive, and would be able to talk it out",1/17/17,1
metasim,Super!,1/17/17,1
lossyrob,"@mehtaharsh1995 today we won_Ñét be too responsive, but we_Ñéll try to address your problems tomorrow - it would be good to have error messages and specific commands/code that caused them to debug",1/17/17,1
mehtaharsh1995,@lossyrob thanks ,1/17/17,1
dorukozturk,"Hi, I am working with geotrellis-landsat-emr-demo and having a problem,  I was able to succesfully launched a cluster but ingest step is failing, btw what is a sane way to show you my logs? Just paste here?",1/17/17,1
metasim,@dorukozturk I'd suggest a Gist.,1/17/17,1
dorukozturk,"@metasim Thanks, great idea !!!",1/17/17,1
dorukozturk,ok here is my stderr,1/17/17,1
dorukozturk,https://gist.github.com/dorukozturk/b3c3f85d7ff625c1ab6d873f7f53b2b3,1/17/17,1
metasim,"@dorukozturk I'm not an expert at these things, but I think you're going to need to look at the YARN logs for `application_1484665861452_0004`.",1/17/17,1
dorukozturk,"ok, that is good advice. Because this error was not something that I saw locally. Thanks for the help.",1/17/17,1
mehtaharsh1995,"When I used Etl.ingest[TemporalProjectedExtent, SpaceTimeKey, Tile] , It will show error like SpaceTimeKey  could not cast to SpatialKey. How do I resolve?",1/18/17,1
echeipesh,@mehtaharsh1995 the ETL plugs have to resort to casts because they_Ñére configured at run-time. Double check that you_Ñére specyfing spacetime enabled input/output profile. If you look at the stack trace you might see a clue as to which profile you_Ñére actually invoking.,1/18/17,1
dorukozturk,@echeipesh what would cause downloads of landsat scenes to be failed? I am getting this from the landsat-emr demo ,1/18/17,1
dorukozturk,"17/01/18 15:45:18 INFO TemporalMultibandLandsatInput: Found 0 landsat images17/01/18 15:45:18 ERROR ApplicationMaster: User class threw exception: java.lang.IllegalArgumentException: requirement failed: Number of partitions (0) must be positive.java.lang.IllegalArgumentException: requirement failed: Number of partitions (0) must be positive.	at scala.Predef$.require(Predef.scala:224)	at org.apache.spark.rdd.RDD$$anonfun$coalesce$1.apply(RDD.scala:441)	at org.apache.spark.rdd.RDD$$anonfun$coalesce$1.apply(RDD.scala:440)	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)	at org.apache.spark.rdd.RDD.withScope(RDD.scala:358)	at org.apache.spark.rdd.RDD.coalesce(RDD.scala:440)	at org.apache.spark.rdd.RDD$$anonfun$repartition$1.apply(RDD.scala:414)	at org.apache.spark.rdd.RDD$$anonfun$repartition$1.apply(RDD.scala:414)	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)	at org.apache.spark.rdd.RDD.withScope(RDD.scala:358)	at org.apache.spark.rdd.RDD.repartition(RDD.scala:413)	at demo.etl.landsat.LandsatInput.fetch(LandsatInput.scala:80)	at demo.etl.landsat.TemporalMultibandLandsatInput.apply(TemporalMultibandLandsatInput.scala:32)	at demo.LandsatIngestMain$$anonfun$main$1.apply(LandsatIngest.scala:69)	at demo.LandsatIngestMain$$anonfun$main$1.apply(LandsatIngest.scala:65)	at scala.collection.immutable.List.foreach(List.scala:381)	at demo.LandsatIngestMain$.main(LandsatIngest.scala:65)	at demo.LandsatIngestMain.main(LandsatIngest.scala)	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)	at java.lang.reflect.Method.invoke(Method.java:498)	at org.apache.spark.deploy.yarn.ApplicationMaster$$anon$2.run(ApplicationMaster.scala:627)17/01/18 15:45:18 INFO ApplicationMaster: Final app status: FAILED, exitCode: 15, (reason: User class threw exception: java.lang.IllegalArgumentException: requirement failed: Number of partitions (0) must be positive.)",1/18/17,1
pomadchin,"@mehtaharsh1995 in addition to Eugenes words, here is an example of temporal json definition: ```son      ""backend"": {        ""type"": ""accumulo"",        ""path"": ""gtintegrationtemporal"",        ""profile"": ""accumulo-local""      },      ""reprojectMethod"": ""buffered"",      ""tileSize"": 256,      ""pyramid"": true,      ""resampleMethod"": ""nearest-neighbor"",      ""keyIndexMethod"": {        ""type"": ""zorder"",        ""timeTag"": ""ISO_TIME"",        ""timeFormat"": ""yyyy-MM-dd'T'HH:mm:ss"",        ""temporalResolution"": 1314000000      },      ""layoutScheme"": ""zoomed"",      ""crs"": ""EPSG:3857""    ```",1/18/17,1
pomadchin,@dorukozturk just no scenes for requested period of time / other requirements  (?),1/18/17,1
dorukozturk,"I was using the defaults but I can double check now, export LAYER_NAME := japan-typhoonexport START_DATE := 2015-07-01export END_DATE := 2015-11-30export BBOX := 135.35,33.23,143.01,41.1export MAX_CLOUD_COVERAGE := 30.0",1/18/17,1
kunal0895,"Hello, I am currently trying to perform an ingest, but with a little modification. I have a geotiff file with a timetag, and I want to use TemporalProjectedExtent and SpaceTimeKey as mentioned in the docs in the etl function. I am trying to fetch from local file system. What change is needed to be made in the sample ETL file?",1/18/17,1
metasim,@kunal0895  Did you check this out?: http://geotrellis.readthedocs.io/en/latest/tutorials/etl-tutorial/,1/18/17,1
kunal0895,"Getting an error like, SpaceTimeKey could not be cast to SpatialKey.",1/18/17,1
kunal0895,Like the one I just saw above.,1/18/17,1
kunal0895,"@metasim , yes I saw that.",1/18/17,1
pomadchin,@kunal0895 see above msg smth like that should work :),1/18/17,1
pomadchin,"```json    ""keyIndexMethod"": {        ""type"": ""zorder"",        ""timeTag"": ""ISO_TIME"",        ""timeFormat"": ""yyyy-MM-dd'T'HH:mm:ss"",        ""temporalResolution"": 1314000000      }```",1/18/17,1
kunal0895,Okay,1/18/17,1
kunal0895,Thanks,1/18/17,1
kunal0895,What does ISO TIME stand for? I get an error like There is no tag ISO_TIME in the geotiff header,1/18/17,1
pomadchin,@kunal0895 eh it_Ñés just an example of tag; so choose tag name you want and set it up,1/18/17,1
pomadchin,basically that inforamtion should be included into geotiff metadata,1/18/17,1
kunal0895,What information from the geotiff metadata should specifically be mentioned in the timeTag? @pomadchin ,1/18/17,1
pomadchin,"`timeTag` is the geotiff tag name with time this tiff is associated with`timeFormat` is the format of date in the time tag specified ",1/18/17,1
pomadchin,temporal reolution means layer temporal resolution ,1/18/17,1
pomadchin,"like layer for a day, month year, several year, other slices types in long format",1/18/17,1
kunal0895,Okay. So i would get that from the metadata of the tif file?,1/18/17,1
kunal0895,"Sorry about too many questions, am kind of new to all this.",1/18/17,1
pomadchin,timeTag is the name of geotiff tag where you keep your time; so your input tiles should have some tag with temporal information about it,1/18/17,1
kunal0895,Okay..Thanks @pomadchin  :D,1/18/17,1
kunal0895,"While displaying it on the local server, I get an error saying Attribute Times not found",1/18/17,1
kunal0895,"I did the ingest, and now i want to display on server, so i created server assembly and then ran spark-submit, but it says Attribute times not found for layer LayerName",1/18/17,1
metasim,"For the purposes of tracking data lineage/provenance, we'd like to be able to have metadata associated with the tiles in `RDD[K, MultibandTile]`.  Something along the lines of `RDD[K, MultibandTile with Metadata[M]]` (i.e `Metadata` inside the `RDD`), where `Metadata[M]` is propogated automatically through type-retaining transformations. A simple example would be `RDD[K, MultibandTile with Metadata[Path]]` for tracking the file from which a tile originated.Another option might be to sub-class `MultibandTile` and add the metadata as a field (like `GeoTiffData`), but I don't see an easy way of doing that without modifying the GeoTrellis source.Is there any precidence for handling this sort of thing?",1/18/17,1
pomadchin,"@metasim we have a tile feature type for these purposes: https://github.com/locationtech/geotrellis/blob/master/raster/src/main/scala/geotrellis/raster/TileFeature.scala",1/18/17,1
pomadchin,"but, there can be a problem in how do you want to build pyramid during your ingest process",1/18/17,1
pomadchin,i have an example of how i tried to store some lc related metadata per tile or even per pixel ,1/18/17,1
metasim,Cool!,1/18/17,1
pomadchin,"@metasim * https://github.com/geotrellis/geotrellis-landsat-emr-demo/pull/14/files* https://github.com/geotrellis/geotrellis-landsat-emr-demo/pull/15/files",1/18/17,1
pomadchin,its a bit outdated; but it_Ñés the idea how to use TileFeature type (just a pair of smth <: CellGrid and any Data related to it),1/18/17,1
metasim,"Interesting.... when the tile using the `TileFeature` for it's `CellGrid` is split, equalized, etc. is the data payload propogated?",1/18/17,1
pomadchin,mm what do you mean O: ?,1/18/17,1
metasim,I think [this pattern](https://github.com/geotrellis/geotrellis-landsat-emr-demo/pull/15/files#diff-72dc947378f92511012017e944c74c4fR7) answers my question: you have to handle the propagation yourself.,1/18/17,1
pomadchin,ahahah,1/18/17,1
pomadchin,yes,1/18/17,1
pomadchin,sry for misunderstanding the question; the main problem is the generic `D` type,1/18/17,1
pomadchin,so you need to define your logic to work with it,1/18/17,1
metasim,"still, this is helpful. the other option I was considering was attaching it to the key, but that has similar issues.",1/18/17,1
pomadchin,definitely,1/18/17,1
metasim,"Wow...... I just discovered how I can implement my own GeoTrellis transformation semantics over `TileFeature[C,D]` via custom context/view bounds, such as `V <: CellGrid: (? => SplitMethods[V])`.  I was already impressed with GeoTrellis, but this knocked my socks off! This works!!```val trackedTiles: RDD[(TemporalProjectedExtent, TileFeature[MultibandTile, Path])] = ???implicit class withTileFeatureMethods(val self: TileFeature[MultibandTile, Path])    extends MethodExtensions[TileFeature[MultibandTile, Path]]     with MultibandTileFeatureSplitMethods // <-- my custom implementationval split = trackedTiles.split(128, 128) // <-- compiles!!!```Hats off to whoever designed this!",1/18/17,1
metasim,(Until now I didn't realize that Scala would fulfill a view bound via an `implicit class`),1/18/17,1
echeipesh,"Yes, view bounds seem to have gone out of favor, but this seemed like a good way to package the extension methods. Pretty exciting that its being discovered.",1/18/17,1
metasim,:boom: ,1/18/17,1
echeipesh,You also mentioned yesterday that you started looking at Datasets?,1/18/17,1
metasim,"I understand you're using the kind-projector, but it still took me a bit of squinting to figure out what the `?` mapped to (i.e. `V`).",1/18/17,1
metasim,"Yes, we want to columnize tiles.",1/18/17,1
metasim,"If it's a multiband tile, we want that column to be an array of the band values in a cell.",1/18/17,1
metasim,(i.e. one row per cell),1/18/17,1
echeipesh,"Ah, so you specifically want to break out the pixels",1/18/17,1
metasim,yeh,1/18/17,1
echeipesh,I_Ñém guessing thats to support some kind of use of SparkML?,1/18/17,1
metasim,Indeed.,1/18/17,1
metasim,"Actually, now that I think of it, we don't want `Array[Double]`, but the ML `Vector` type.",1/18/17,1
metasim,but that could be handled via a `map` or something.,1/18/17,1
metasim,"This is what concerns me: http://stackoverflow.com/a/39442829/296509",1/18/17,1
metasim,"Based on my cursory probing, either a bunch of GeoTrellis classes need to be `case class`es, or a bunch of `Encoder`s  will needed.",1/18/17,1
metasim,The real bummer is that some encoding errors only happen at runtime.,1/18/17,1
echeipesh,"Hopefully there is a way to map the RDD into primitives and then construct the Dataset from that, there should be a hook into that.",1/18/17,1
metasim,"Yeh, I'm currently doing it with a big `(t: RDD[(TemporalProjectedExtent, MultibandTile)]).flatMap(...).toDS`",1/18/17,1
metasim,Where `flatMap` generates instances of a `case class` container.,1/18/17,1
echeipesh,"So you_Ñére flattening the tile into arrays, which is fine, which GT datatypes are you trying to include, the keys ?",1/18/17,1
pomadchin,@metasim do you use kryo to seralize `Tile`s ? ,1/18/17,1
pomadchin,as i had lots of prolblems with blobs serialization ,1/18/17,1
pomadchin,https://github.com/locationtech/geotrellis/pull/1950,1/18/17,1
pomadchin,I_Ñém talking about using datasets **,1/18/17,1
metasim,@pomadchin Can't use kryo.... the cell values have to be coerced and exposed as primitive `Double`s.,1/18/17,1
metasim,... for it to work with SparkML.,1/18/17,1
pomadchin,good to know; thank you (!); do you have some opensourced prototype of your work? that could be helpful ,1/18/17,1
metasim,"So even if the underlying cell type is `BitCellType`, it ends up having to be a (valid!) `Double`for the ML algorithms to work.",1/18/17,1
metasim,So `NoData` handling is a massive hassle.... bascially have to filter it out. `NaN` will blow up most ML algos.,1/18/17,1
metasim,"@pomadchin Not yet.. the code isn't much more than this:```cfor(0)(_ < tile.cols, _ + 1) { col __ê  cfor(0)(_ < tile.rows, _ + 1) { row __ê    val label = tile.get(col, row)    if (!isNoData(label)) {      val pos = re.gridToMap(col, row)      val samples = Array.ofDim[Double](sceneTile.bandCount)      cfor(0)(_ < sceneTile.bandCount, _ + 1) { band =>        val pix = sceneTile.band(band).getDouble(col, row)        samples(band) = pix      }      rows = rows :+ (pos, label, samples)    }  }}```",1/18/17,1
metasim,where `tile` is my target and `sceneTile` are the features (in DS-speak),1/18/17,1
metasim,"That said, I'm going to look into the Spark `Encoder[_]` API and see what I can come up with.",1/18/17,1
metasim,"But that's probably not going to get me anywhere since I want a pixel per row, instead of all the pixels in one row.",1/18/17,1
echeipesh,"Hmm, is it worth the pain though? If you_Ñére just producing `Vector`s for SparkML why try to preserve the related types? Is that something that carries through to the output?",1/18/17,1
metasim,"Not really, except that I like the auto-generated column names and the marshalling to/from Parquet.",1/18/17,1
metasim,Mostly an exercise in figuring out what's generalizable and what's not.,1/18/17,1
metasim,Still wrapping my brain around the problem.,1/18/17,1
echeipesh,Makes sense. Have you played much with DataFrames ? An intuition that I have is that it might be more useful to provide a mapping to DataFrame structs from GT types (since we seem to be forced to map to primitives anyway),1/18/17,1
metasim,Right.... I've been using them (here) interchangeably. I usually start with `DataSet`s but eventually bail on the types. (and use `DataFrame`s).,1/18/17,1
metasim,Not giving up hope tho. GT tracks so much type info I hate to give it up if I don't have to.,1/18/17,1
schBen,"@lossyrob To use GDAL with GeoTrellis, we looked into geotrellis-gdal. For our project, we moved it from sbt to maven, updated dependencies to GeoTrellis 1.0, and added some small fixes. We are close to making it work :) We would like to share the results on github then. But we don't really know where it belongs now :-) What would you prefer as the groupId for this project / build? Originally it was ""com.azavea.geotrellis"". Should we just keep that or better use our own groupId now that we changed the build tool? Or something else? Any questions let us know.",1/19/17,1
pomadchin,@schBen thank you! `*_*` is it possible to use sbt :D ? ,1/19/17,1
pomadchin,as it can be confusing that all gt stuff uses sbt but only geotrellis gdal uses maven,1/19/17,1
pomadchin,just a thought :))),1/19/17,1
pomadchin,btw thanks for creating landsat util issue,1/19/17,1
schBen,"@pomadchin well not at the moment... we did it in maven because that fits into our own project ;-) so that's why I asked if we should give it a new groupId for now, so that we don't confuse everybody :-)  maybe we can bring it together later, but for now we'd just like to share it somewhere",1/19/17,1
schBen,@pomadchin no problem :-),1/19/17,1
metasim,"@schBen What were the ""small fixes""? Could youy just submit a PR against the original repo?",1/19/17,1
metasim,"I'm also interested in this library, but a) our build is based on SBT, b) keeping the original up to date serves the broader community better than if the capability fragmented.",1/19/17,1
metasim,(IMHO),1/19/17,1
lossyrob,"@schBen I agree with metasim, not fragmenting is good. Changing the build tool is a bit inconvenient, but I also don_Ñét want to turn down good work over an sbt/maven war",1/19/17,1
lossyrob,is it up on github as is? I_Ñéd love to take a look,1/19/17,1
lossyrob,"@metasim: r.e. your _ÑÒI just discovered_Ñ__Ñù comment - I_Ñém doing a talk on scala design patterns in GeoTrellis on Tuesday, and plan on going over that technique of type contraints using context bounds, and the benefit. Mind if I put that quote on a slide? :)",1/19/17,1
schBen,"@metasim @lossyrob Okay, good. No it's not on github yet, we just wanted to know your thoughts for now. But we'll of course let you know then!",1/20/17,1
metasim,@lossyrob Go for it!,1/20/17,1
metasim,"As an aside, I can understand why the `<%` syntax might have gone out of favor (due to context bounds being a better generalization), but I never understood why the view bound _pattern_ has been considered deprecated (or maybe they've been unintentially conflated).",1/20/17,1
lossyrob,"@metasim, yeah, I_Ñém also curious about that. I haven_Ñét read anything that indicated why it was deprecated",1/20/17,1
metasim,This is all I know: https://github.com/scala/scala/pull/2909,1/20/17,1
pomadchin,"AFAIK that was caused also by just ContextBounds introducing and that it_Ñés a more generic sugar ëË\_( ó†)_/ëË ```scala// oldschooldef foo[T <% Ordered[T]](x: T, y: T) = x < y// newschooltype OrderedView[T] = T => Ordered[T]def foo[T: OrderedView](x: T, y: T) = x < y```",1/20/17,1
owcm,"Working with ShapefileReader, looked through the documentation, but did see a way to get the CRS information for the features being read in.  Is there a way to get this info?  Is there a way to reproject the features read in.  Thanks in advance.",1/23/17,1
pomadchin,"@owcm yo! https://github.com/locationtech/geotrellis/blob/master/shapefile/src/test/scala/geotrellis/shapefile/ShapeFileReaderSpec.scala#L23-L34this may help you",1/23/17,1
pomadchin,if its a part of data; you may easily get it,1/23/17,1
owcm,"@pomadchin  Thanks, this gets me the attributes for each feature which is good.   But I was looking for the Shapefile projection.  Geojson format files have a projection(CRS) tag for example.   If there was not a CRS available for the shape file read in I was wanting to apply a CRS  to the feature collection I read in.  I read in the Shapefile as in the  example you sent me ```val features = ShapeFileReader.readMultiPolygonFeatures(path)```",1/23/17,1
echeipesh,"@owcm We did some work recently reading shapefiles and RDDs, sounds like it might be useful here: https://github.com/azavea/geowave-geomesa-comparative-analysis/blob/master/empirical-data/common/src/main/scala/com/azavea/ingest/common/shp/HydrateRDD.scala (reading the them) https://github.com/azavea/geowave-geomesa-comparative-analysis/blob/master/empirical-data/common/src/main/scala/com/azavea/ingest/common/Reproject.scala (reproject simplefeature)",1/23/17,1
echeipesh,Basically the CRS should be hanging off the `SimpleFeatureType` ala ` val srcCrs = sf.getType.getCoordinateReferenceSystem`,1/23/17,1
metasim,"I'm getting one of these:```java.lang.UnsupportedOperationException: No Encoder found for geotrellis.proj4.CRS- field (class: ""geotrellis.proj4.CRS"", name: ""crs"")- root class: ""geotrellis.spark.TemporalProjectedExtent""  org.apache.spark.sql.catalyst.ScalaReflection$.org$apache$spark$sql$catalyst$ScalaReflection$$serializerFor(ScalaReflection.scala:598)```from some code that looks like this:```import org.apache.spark.sql.Encodersimplicit val crsEnc = Encoders.kryo[geotrellis.proj4.CRS]implicit val tpeEnc = Encoders.product[geotrellis.spark.TemporalProjectedExtent]implicit val extTupleEnc = Encoders.tuple(tpeEnc, crsEnc)inputFiles.toDF```",1/23/17,1
metasim,Anyone been able to get any form of spark `Encoders` to work?,1/23/17,1
pomadchin,@metasim i used a hack => used kryo for everything,1/23/17,1
metasim,Then why doesn't line 2 work?,1/23/17,1
pomadchin,but that should work,1/23/17,1
pomadchin,hmmm,1/23/17,1
pomadchin,i remember that i had smth with implicits ordering,1/23/17,1
metasim,Is implicit resolution the way you communicate with spark?,1/23/17,1
metasim,or do you have to register the encoders?,1/23/17,1
pomadchin,you can have a look into my datasets pr,1/23/17,1
pomadchin,nope,1/23/17,1
metasim,"(oh, right). forgot about that.",1/23/17,1
pomadchin,just pass it into correct scope,1/23/17,1
schBen,Hi! @lossyrob @metasim @pomadchin We published the updates we propose for geotrellis-gdal. See https://github.com/geotrellis/geotrellis-gdal/pull/1 ,1/24/17,1
pomadchin,@schBen awesome! Thank you! \+ we have lots of prs against scala landsat util and everything would be merged in soon ,1/24/17,1
pomadchin,:tada: :tada: :tada: :tada:,1/24/17,1
metasim,:clap: ,1/24/17,1
hjaekel,@pomadchin We upgraded the scala-landsat-util to the new sat-api. Are there any plans to transform the scala-landsat-util to scala-sat-util? We need  the same functionality for Sentinel (thats why we are also working on geotrellis-gdal) and we are currently discussing how to proceed.,1/24/17,1
echeipesh,"@hjaekel we didn_Ñét really have plans, but that makes a lot of sense. We_Ñéd have to figure out how to deliver it, a mini library for each supported catalog probably (?). If you want to spike out that code we_Ñéll definatly find a home for it in that project.",1/24/17,1
pomadchin,"@hjaekel yes, noticed; great prs ^^_Ñé",1/24/17,1
echeipesh,"^ Ah, I see the PR.",1/24/17,1
metasim,"@hjaekel To be super general, we should consider what it would take to interface via NASA's Central Metadata Repository (CMR), which intends to be the centralized point of search and access (either via download links or data product ordering). The catalog is quite extensive.https://cmr.earthdata.nasa.gov/search/site/search_api_docs.htmlIt won't handle the cases where the data is mirrored by AWS or Google Earth Engine, etc.",1/24/17,1
metasim,"You can manually see what's available here:https://search.earthdata.nasa.gov",1/24/17,1
metasim,"Not trying to encourage scope creep, but having a Scala API over that system would be significant. I've considered implementing it several times myself, but keep getting pulled off to other more pressing things.",1/24/17,1
metasim,"We've talked to some folks at NASA about it, and it appears that NASA & USGS (maybe NOAA) are actually committed to getting a centralized system be the go-to place for discovering data assets.",1/24/17,1
echeipesh,"That NASA query API doesn_Ñét look too intense at first glance, not sure how many return types are there. Also knowing the AWS/Google mirror is pretty useful because to get the bytes cheap.",1/24/17,1
hjaekel,@metasim I will look at it,1/24/17,1
pomadchin,I tried to do smth like that but for landsat api to get tiles directly from usgs (as amazon and google have not complete sets of lc scenes); but it was ugly (i needed to auth and to download scene :D),1/24/17,1
metasim,"I did the same with USGS, even generating SOAP classes via XSD. Indeed messy. CMR is much cleaner.",1/24/17,1
metasim,At least from looking at the docs.,1/24/17,1
metasim,"Also generated bindings for MODIS/MODAPS; cleaner, but very limited.",1/24/17,1
hjaekel,"I think an abstract search api with the possibility to add custom catalogs as suggested by @echeipesh would be the most flexible approach. Downloading is also an issue, especially when different platforms offer the same data in different formats.",1/24/17,1
metasim,"Fortunately, if architected properly, the separation between ""what's available"" and ""how do I get it"" allows for multiple ""fetch"" backends. Once you know the granule or scene ID from a CMR search, you can use that to poke AWS/Google/Azure to see if they have it available, and fall back to the authoratative source if necessary.",1/24/17,1
metasim,"In our case, the multi-spectral products hosted by AWS, etc. aren't really useful because they haven't been calibrated to the quality degree we need, so our use cases lean toward getting from the official sources anyway.",1/24/17,1
hjaekel,@metasim maybe downloading should be separated from the search,1/24/17,1
hjaekel,It seems to bring its own complexity,1/24/17,1
metasim,"Yeh, that's what I'm trying to say.",1/24/17,1
metasim,"The results you get from CMR are of two forms (generally): URI of where to download the data, or URI and other metadata on how to order what you want (e.g. you want some higher-level data product that hasn't been computted yet).",1/24/17,1
metasim,"In the latter case you have to either poll the system for completion, which then gives you the download URI, or you have to have an email handler to receive the compute status messages.",1/24/17,1
metasim,"IMO, getting the searching API right is the critical part.",1/24/17,1
metasim,The download API is probably a moving target as this industry evolves,1/24/17,1
hjaekel,"Yes, it's a moving target. Here in Europe there are several initiatives for building up new distribution services",1/24/17,1
metasim,"Here in the USA, our government likes to reinvent the wheel at least 4 time before acknowledging any commonality in mission ;-) So CMR is actually some measureable progress!",1/24/17,1
lossyrob,@metasim great comments. Is there code out in the wild around reading from the CMR a la scala-landsat-util? I_Ñéd be interested in contributing if there is.,1/24/17,1
lossyrob,"and if not, collaborating on building it :)",1/24/17,1
metasim,"Short answer: no. Where I left off with it was trying to figure out which of the [multiple output schema specifications](https://cmr.earthdata.nasa.gov/search/site/search_api_docs.html#supported-result-formats) it supports would be most amenable to  Scala boilerplate generation. I was starting to think that just hand writing would be the right approach... it's not even clear to me if the request side has a schema. USGS EarthExplorer had a SOAP 1.0 API which, while ugly, allowed generation of a scalaxb API.",1/24/17,1
metasim,"I'm not an expert at theses REST-y JavaScript-y things, so I never know if something doesn't exist because I don't know where to look, or if it just doesn't exist.",1/24/17,1
metasim,So I got stalled by analysis paralysis :/,1/24/17,1
lossyrob,"Gotcha. We have some work that a lib like that would be great for, so I think we_Ñéll end up building it one way or the other. When that starts up I_Ñéll make sure to loop you in (and anyone else here who is interested)",1/24/17,1
metasim,"If someone has a strong opinion or guidance on how to approach writing an API against a public REST API such as this, I'd like to hear it, and would be willing to do the leg work of getting a prototype running.",1/24/17,1
lossyrob,"for more broad open imagery sources, we_Ñére working on  an API to catalog more generally, which will end up having a `scala-landsat-util`-like library in front of it, so hopefully that will be helpful",1/24/17,1
metasim,"IOW, if someone says ""write the search calls from scratch, and rely on the KML output spec"" or whatever, I'd go from there.",1/24/17,1
lossyrob,"awesome, good to know. We should put our heads together on a prototype, I think you_Ñéll find some strong opinions here :)",1/24/17,1
metasim,awesome,1/24/17,1
dorukozturk,Hi any timeline on updating the landsat-emr demo? I saw there is a pr for fixing the scala-landsat-util which is breaks the ingestion step for landsat-emr-demo right now. We are trying to do a proof-of-concept demo on a cluster. We want to use the landsat emr demo as a starting point.,1/24/17,1
owcm,@echeipesh @pomadchin   Thanks for your help on the shapefile reprojection issue.  Your examples helped me implement a solution. ,1/24/17,1
diegonc,"Hello, just cloned geotrellis-sbt-template and I'm trying to follow the quickstart example in the console but cannot import geotrellis.raster.op.focal._; what happened to the package? has it been moved?```scala> import geotrellis.raster.op.focal._<console>:17: error: object op is not a member of package geotrellis.raster       import geotrellis.raster.op.focal._```",1/24/17,1
lossyrob,@diegonc  looks like the quickstart needs to be updated. That functionality exists in `geotrellis.raster.mapalgebra.focal._`,1/24/17,1
lossyrob,"@dorukozturk we will be updating this soon, hopefully I can get to this this weekend. Any help updating would be appreciated!",1/24/17,1
dorukozturk,@lossyrob  Sounds great. If I can get it working I will be happy to update it. ,1/24/17,1
lossyrob,thanks!,1/24/17,1
diegonc,"thanks @lossyrob !, I'll try with that package",1/24/17,1
dorukozturk,I successfully ingested some layers on accumulo (emr). What is the emr step submission script should look like for serving tiles? Is there an example? I see there is a local-tile-server target in the makefile but not for accumulo emr. ,1/24/17,1
dorukozturk,https://gist.github.com/dorukozturk/f04e686a5058a92a3ec85c16c6cbe205,1/24/17,1
dorukozturk,can somebody help me with this?,1/24/17,1
pomadchin,"@dorukozturk look at commands: ```make create-clustermake ingest```",1/24/17,1
dorukozturk,I got that parts,1/24/17,1
dorukozturk,serving is the issue now for me :),1/24/17,1
pomadchin,aaand you want to do same but local?,1/24/17,1
dorukozturk,remote,1/24/17,1
dorukozturk,this gist did not work for me,1/24/17,1
dorukozturk,I am getting format errors,1/24/17,1
dorukozturk,It is going to look something like the gist I sent though right?,1/24/17,1
dorukozturk,I was wondering if you guys have a sample step submission script for emr accumulo tile server,1/24/17,1
pomadchin,"yes, so the problem here that you should predict server address;",1/24/17,1
dorukozturk,"ok maybe I could not clearly stated my problem. Let me try again, I am working with the landsat-emr example, my ingestion step went through, so I did make LIMIT=5 ingest, it ran for 10 minutes and successfully completed, now I want to do the serve step on EMR, but the makefile only have a local-tile-server option. I was wondering if there is an example for a tile-server for EMR accumulo. Looking at the examples it should look like the gist file that I just pasted. But I could not make it work, do you guys have a sample submission script for accumulo tile server on emr?",1/24/17,1
pomadchin,"sry i was interrupted :D  >> continue the mesg abovewe run server in master; https://github.com/geotrellis/geotrellis-landsat-emr-demo/blob/master/scripts/emr/bootstrap-demo.shits a part of our bootstrap",1/24/17,1
dorukozturk,oh so tile server is already up an running ,1/24/17,1
pomadchin,"there sometimes may happen problems that for some reason smth breaks and it wont start; so to resolve it you can manually follow steps from this bootstrap script:```# Start Static Web Serveraws s3 cp $SITE_TGZ /tmp/site.tgzsudo chmod 644 /var/www/html/*sudo chmod 755 /var/www/htmlsudo tar -xzf /tmp/site.tgz -C /var/www/htmlsudo mkdir -p /tmp/catalog/attributessudo mkdir -p /tmp/catalog-cachesudo chmod -R 777 /tmp/catalogsudo chmod -R 777 /tmp/catalog-cache```",1/24/17,1
pomadchin,"yes, its up",1/24/17,1
pomadchin,i should see it in yarn,1/24/17,1
dorukozturk,"ok now it is clear, thank you very much :)",1/24/17,1
pomadchin,;),1/24/17,1
pomadchin,feel free to ask questions,1/24/17,1
dorukozturk,thank you !!!!!!!,1/24/17,1
pomadchin,if you_Ñéll figure out why sometimes it skips these steps that would be great :D i had no chance to debug it deeply,1/24/17,1
dorukozturk,"ok, I would like to contribute, once I get it up and running",1/24/17,1
dorukozturk,"ok, that worked. Thanks @pomadchin ",1/24/17,1
diegonc,"```[info] org.locationtech.geotrellis:geotrellis-spark-etl_2.11:1.0.0-SNAPSHOT [S][info]   +-com.github.fge:json-schema-validator:2.2.6[info]   | +-com.github.fge:json-schema-core:1.2.5[info]   | | +-com.github.fge:jackson-coreutils:1.8[info]   | | | +-com.fasterxml.jackson.core:jackson-databind:2.2.3 (evicted by: 2.6.6)[info]   | | | +-com.fasterxml.jackson.core:jackson-databind:2.6.6[info]   | | | | +-com.fasterxml.jackson.core:jackson-annotations:2.6.0[info]   | | | | +-com.fasterxml.jackson.core:jackson-core:2.6.6[info]   | | | | [info]   | | | +-com.github.fge:msg-simple:1.1[info]   | | | | +-com.github.fge:btf:1.2[info]   | | | | | +-com.google.code.findbugs:jsr305:2.0.1 (evicted by: 3.0.0)[info]   | | | | | +-com.google.code.findbugs:jsr305:3.0.0[info]   | | | | | [info]   | | | | +-com.google.code.findbugs:jsr305:2.0.1 (evicted by: 3.0.0)[info]   | | | | +-com.google.code.findbugs:jsr305:3.0.0[info]   | | | | [info]   | | | +-com.google.code.findbugs:jsr305:2.0.1 (evicted by: 3.0.0)[info]   | | | +-com.google.code.findbugs:jsr305:3.0.0[info]   | | | +-com.google.guava:guava:16.0.1```I found an issue with geotrellis-spark-etl trying to ingest some geotiff files. It depends on json-schema-validator which transitively brings guava 16. Problem is that hadoop has guava 11 in its classpath producing a method not found error when calling Queue::newArrayDeque",1/26/17,1
diegonc,"this is when using the assembled jar from github repo, master branch",1/26/17,1
diegonc,anybody seeing the same issue?,1/26/17,1
pomadchin,"@diegonc you can shade guava: ```scalaassemblyShadeRules in assembly := {  val shadePackage = ""com.azavea.shaded.demo""  Seq(    ShadeRule.rename(""com.google.common.**"" -> s""$shadePackage.google.common.@1"")      .inLibrary(        ""com.azavea.geotrellis"" %% ""geotrellis-cassandra"" % Version.geotrellis,        ""com.github.fge"" % ""json-schema-validator"" % ""2.2.6""      ).inAll  )}```",1/26/17,1
pomadchin,"the problem is classpath specific, that_Ñés why it_Ñés not in the geotrellis-etl build.sbt file",1/26/17,1
diegonc,"ok, thanks @pomadchin ",1/26/17,1
diegonc,"hm, I don't think I'm getting what that is doing...",1/26/17,1
diegonc,does it rename the guava classes and its references in the given libraries?,1/26/17,1
pomadchin,"yes, so it renames classes an refs for these to deps",1/26/17,1
pomadchin,others would use refs from hadoop classpath if it would be necessary,1/26/17,1
diegonc,right,1/26/17,1
diegonc,I'll try it :),1/26/17,1
pomadchin,if you are interested in more details of this shading process: it_Ñés backed by jarjar: https://code.google.com/archive/p/jarjar/wikis/CommandLineDocs.wiki,1/26/17,1
pomadchin,:D) it would work (worked for emr),1/26/17,1
metasim,"@pomadchin A curiosity I'm pondering: Is there any Scala type-system reason that constructs such as [this (MultibandTileFeatureMergeMethods)](https://github.com/geotrellis/geotrellis-landsat-emr-demo/pull/14/files#diff-3bd52e071ea9357fb1178148422552fc) can't be made generic w.r.t. `D` in `TileFeature[T,D]`, assuming the semantics you want are to just copy over the `D` values unchanged?",1/26/17,1
pomadchin,"@metasim its possible but you need to specify some constrains on the D type; and if its trait you cant use context bounds as trait has no constructorsyou can pass them as implicit values i implemented in a such way as it was just easier and faster :D smth like a proof of concept ",1/26/17,1
metasim,But `D` has no constraints in `TileFeature`?,1/26/17,1
pomadchin,no,1/26/17,1
metasim,Is it because you need to be able to copy them in Spark?,1/26/17,1
pomadchin,so if D is generic you need to specify operations on this D and to make in at least `Mergable`,1/26/17,1
pomadchin,hm mb i misunderstood your question? ,1/26/17,1
metasim,"Actually, I think you were thinking more generically about my question.",1/26/17,1
metasim,I was assuming simple copy of the `TileFeature.data` field..... which is some ways is just a `Mergable` strategy,1/26/17,1
pomadchin,"Ah; nope nope if D is generic compiler knows nothing about it",1/26/17,1
metasim,Like you did [here](https://github.com/geotrellis/geotrellis-landsat-emr-demo/pull/14/files#diff-3bd52e071ea9357fb1178148422552fcR54),1/26/17,1
metasim,"(and [here](https://github.com/geotrellis/geotrellis-landsat-emr-demo/pull/14/files#diff-8b3f0205b5736506c2486b079a717399R79), etc.)",1/26/17,1
pomadchin,ah its possible but i was thinking about doing smth smart with D type there so here is implemented only a workaround,1/26/17,1
metasim,"Gotcha.... at any rate, it has some promise, via typeclasses.",1/26/17,1
metasim,"Metadata propagation is at the core of what we've been thinking about, and don't want to have to implement any more `{Singleband|Multiband}TileFeature.*Methods` than we have to.",1/26/17,1
pomadchin,its another problem; abstraction over Tile and MultibandTile types,1/26/17,1
pomadchin,we have it in our backlog O: ,1/26/17,1
metasim,"Yeh, rather than `CellGrid`",1/26/17,1
metasim,(which doesn't expose enough),1/26/17,1
pomadchin,@metasim https://github.com/geotrellis/geotrellis-integration-tests-tool/blob/master/src/main/scala/geotrellis/test/TestEnvironment.scala i tried to asbtract over V type,1/26/17,1
pomadchin,in terms of gt type system ,1/26/17,1
pomadchin,"so basically that would be next important change in geotrellis :D two options: creating a new union type or to resolve it somehow smart using shapeless or smth like that",1/26/17,1
metasim,Dotty has union types..... :),1/26/17,1
metasim,"(Please, no shapeless... makes my head hurt)",1/26/17,1
metasim,"Wow, you've got a lot going on there.",1/26/17,1
pomadchin,Ahahah scala allows you to use umboxed union types too using curry howard isomorphism; not sure that would be prettier though :d ,1/26/17,1
pomadchin,unboxed *,1/26/17,1
metasim,I actually tried [this approach](https://milessabin.com/blog/2011/06/09/scala-union-types-curry-howard/) in an application but noone could read my code :/,1/26/17,1
metasim,Pitchforks showed up in my office.,1/26/17,1
pomadchin,ye its a good approach,1/26/17,1
pomadchin,:D,1/26/17,1
pomadchin,id recommend you to use shapeless instead or try to figure out smth else ,1/26/17,1
pomadchin,:D :D ,1/26/17,1
pomadchin,Avro codec definition for data type of tile feature: https://github.com/geotrellis/geotrellis-landsat-emr-demo/pull/15/files#diff-9b90346481daa91ed923d53d2eeee99cR12,1/26/17,1
pomadchin,No constrains on V here (in reader) except avro codec and classtag: https://github.com/locationtech/geotrellis/blob/master/spark/src/main/scala/geotrellis/spark/io/hadoop/HadoopLayerReader.scala#L45-L56,1/26/17,1
metasim,Is there a public build with [this in it](https://github.com/locationtech/geotrellis/commit/2330f79c826209273d878c3d467430c72e4c10ad#diff-eaa4e4074ea994f0eadcaf1c2f6e0db7)? Doesn't appear to be in 1.0.0.,1/26/17,1
lossyrob,"if the locationtech snapshot build is doing it_Ñés job, it should be in 1.0.0-SNAPSHOT in the LT snapshot repo (should read 1.1.0 but haven_Ñét made that switch yet)",1/26/17,1
lossyrob,looks like it_Ñés publishing,1/26/17,1
metasim,:+1: ,1/27/17,1
metasim,@pomadchin In addition to the `SchemaConverters.toSqlType` you found we'll also need [`SchemaConverters.createConverterToSQL`](https://github.com/databricks/spark-avro/blob/branch-3.1/src/main/scala/com/databricks/spark/avro/SchemaConverters.scala#L140). But it's `private[avro]` :(,1/27/17,1
metasim,(Working on a hack for the PoC.),1/27/17,1
pomadchin,think that the fastest worakaround would be to create `DefaultSource` in `package com.databricks.spark.avro` or to duplicate this code ):,1/27/17,1
pomadchin,after the working proof would be designed there would be time for the code hygene :D,1/27/17,1
pomadchin,"@metasim btw, i believe we can use reflection to call private functions;",1/27/17,1
metasim,[![Screen Shot 2017-01-27 at 12.30.43 PM.png](https://files.gitter.im/geotrellis/geotrellis/r8uD/thumb/Screen-Shot-2017-01-27-at-12.30.43-PM.png)](https://files.gitter.im/geotrellis/geotrellis/r8uD/Screen-Shot-2017-01-27-at-12.30.43-PM.png),1/27/17,1
metasim,This works: ,1/27/17,1
pomadchin,:D :D ,1/27/17,1
metasim,:boom: ,1/27/17,1
metasim,"I've got a successfully resolved instance of `AvroRecordCodec[(TemporalProjectedExtent, MultibandTile)]`. The `.schema` result is correct, but when I call the `.encode` method I get the following:```Exception in thread ""main"" java.lang.RuntimeException: No formats found to support class geotrellis.raster.ByteConstantTile	at scala.sys.package$.error(package.scala:27)	at geotrellis.spark.io.avro.AvroUnionCodec.findFormat(AvroUnionCodec.scala:53)	at geotrellis.spark.io.avro.AvroUnionCodec.encode(AvroUnionCodec.scala:35)	at geotrellis.spark.io.avro.codecs.TileCodecs$$anon$9$$anonfun$encode$1.apply(TileCodecs.scala:349)	at geotrellis.spark.io.avro.codecs.TileCodecs$$anon$9$$anonfun$encode$1.apply(TileCodecs.scala:349)	at geotrellis.spark.io.avro.codecs.TileCodecs$$anon$9.encode(TileCodecs.scala:349)	at geotrellis.spark.io.avro.codecs.TileCodecs$$anon$9.encode(TileCodecs.scala:340)	at geotrellis.spark.io.avro.AvroRecordCodec.encode(AvroRecordCodec.scala:32)...```It smells a little like a GeoTrellis bug to me, but wasn't 100% confident. Any gut reaction to it? ",1/27/17,1
metasim,(I'm using `ByteConstantTile` only out of testing convenience),1/27/17,1
pomadchin,@metasim can you give a code snippet?,1/27/17,1
metasim,just a sec,1/27/17,1
metasim,"It's [in here](https://github.com/s22s/avro2spark/blob/master/src/main/scala/astraea/demo/DatabricksAvro.scala#L47-L63), but let me reduce it down to a smaller test case.",1/27/17,1
metasim,https://gist.github.com/metasim/a7b05a5f1b6012be8691cc8358e6a34f,1/27/17,1
metasim,(GeoTrellis version 1.0.0),1/27/17,1
pomadchin,Yes!,1/27/17,1
pomadchin,it_Ñés kind of a bug,1/27/17,1
pomadchin,hm,1/27/17,1
pomadchin,looks like we need to implement avro codecs for all constant types,1/27/17,1
pomadchin,thank you,1/27/17,1
pomadchin,i_Ñéll push a workaround is a sec to that repo,1/27/17,1
metasim,want anything filed?,1/27/17,1
pomadchin,so we just don_Ñét have codecs for constant tiles weird :O,1/27/17,1
metasim,Why wouldn't the `Tile` codec handle it?,1/27/17,1
metasim,A schema is generated OK.,1/27/17,1
metasim,Seems like the union matching code might be at fault.,1/27/17,1
metasim,Except that you probably want a constant tile back when you `decode`,1/27/17,1
pomadchin,https://github.com/s22s/avro2spark/pull/3,1/27/17,1
pomadchin,"no, everything is fine",1/27/17,1
pomadchin,types can be generated; but codecs should be used and found in runtime ,1/27/17,1
pomadchin,that_Ñés why it looks confusing,1/27/17,1
pomadchin,I created issue: https://github.com/locationtech/geotrellis/issues/1992,1/27/17,1
metasim,cool,1/27/17,1
metasim,thanks for looking at it.... saved me from a rabbit hole,1/27/17,1
pomadchin,:D),1/27/17,1
metasim,"@pomadchin I kinda got a super inefficient version working; having to create a new encoder for each instance due to Avro Schema serialization errors from Spark:https://github.com/s22s/avro2spark/blob/master/src/main/scala/astraea/demo/DatabricksAvro.scala#L81",1/27/17,1
metasim,"Will continue to work on it, but wanted to point out where I was. ",1/27/17,1
lossyrob,:clap: ,1/27/17,1
pomadchin,:tada: ye; prototype works and without lots of pain (it seems to me),1/27/17,1
kunal0895,How do I create an RDD from a geotiff image using Spark and Geotrellis?,1/29/17,1
kunal0895,"Read the docs but couldnt get much, any help would be appreciated.",1/29/17,1
kunal0895,I have done the ingest successfully and I have the catalog ,1/29/17,1
kunal0895,"It has the metadata, but i dont have any idea as to how i can load it into an RDD",1/29/17,1
kunal0895,Load it as RDD* from local FS,1/29/17,1
kunal0895,"Can anyone help? I read this https://groups.google.com/forum/#!msg/geotrellis-user/8iRLBcaDXV4/Qh2v6-wVGQAJ but instead of HDFS, if i want to use the local FS, what should i do?",1/29/17,1
pomadchin,"Yo @kunal0895 ; we have FileLayerWriter and Reader; (if you are using etl, use ""file"" as an output backend type)",1/29/17,1
kunal0895,Okay @pomadchin ,1/29/17,1
kunal0895,"Yes, using ETL",1/29/17,1
pomadchin,"@kunal0895 we have example projects: https://github.com/geotrellis/geotrellis-chatta-demo (uses file backend by default and our new collections api, that's why it depends on gt master branch) and https://github.com/geotrellis/geotrellis-landsat-emr-demo but it's a bit out of date",1/29/17,1
kunal0895,Okay..thanks @pomadchin ,1/29/17,1
lossyrob,I get this type of error when working with the most up to date akka-http and spark local: https://github.com/akka/akka/issues/21596,1/29/17,1
lossyrob,(part of updating scala-landsat-util),1/29/17,1
lossyrob,"it comes from shading. If I remove assembly shading, it doesn_Ñét error.",1/29/17,1
lossyrob,very weird. not clear how to avoid while also shading.,1/29/17,1
pomadchin,@lossyrob https://github.com/pomadchin/geotrellis-chatta-demo/blob/master/geotrellis/build.sbt,1/29/17,1
pomadchin,i resolved it,1/29/17,1
pomadchin,so the problem that akka http depends on guava too,1/29/17,1
lossyrob,so your saying do the exclude instead of the shade,1/29/17,1
pomadchin,nopt,1/29/17,1
pomadchin,sh,1/29/17,1
pomadchin,wait,1/29/17,1
pomadchin,im searching for link,1/29/17,1
pomadchin,here it is https://github.com/geotrellis/geotrellis-chatta-demo/blob/master/geotrellis/build.sbt,1/29/17,1
pomadchin,^^,1/29/17,1
pomadchin,ajja depends on guava so in local case we eed to shade all deps in project,1/29/17,1
pomadchin,"correct shading rule: ```scalaassemblyShadeRules in assembly := {  val shadePackage = ""com.azavea.shaded.demo""  Seq(    ShadeRule.rename(""com.google.common.**"" -> s""$shadePackage.google.common.@1"").inProject  )}```",1/29/17,1
lossyrob,landsat emr demo already shades,1/29/17,1
pomadchin,"lc emr demo has ```scalaassemblyShadeRules in assembly := {  val shadePackage = ""com.azavea.shaded.demo""  Seq(    ShadeRule.rename(""com.google.common.**"" -> s""$shadePackage.google.common.@1"")      .inLibrary(        ""com.azavea.geotrellis"" %% ""geotrellis-cassandra"" % Version.geotrellis,        ""com.github.fge"" % ""json-schema-validator"" % ""2.2.6""      ).inAll  )}```",1/29/17,1
pomadchin,"try this version: ```scalaassemblyShadeRules in assembly := {  val shadePackage = ""com.azavea.shaded.demo""  Seq(    ShadeRule.rename(""com.google.common.**"" -> s""$shadePackage.google.common.@1"").inProject  )}```",1/29/17,1
lossyrob,trying,1/29/17,1
pomadchin,"ye, hacky think that akka fails due to bad guava links",1/29/17,1
lossyrob,that worked. so it_Ñés that akka depends on guava?,1/29/17,1
pomadchin,yep!,1/29/17,1
pomadchin,that's weird,1/29/17,1
lossyrob,cool,1/29/17,1
pomadchin,^^',1/29/17,1
pomadchin,i think we need to include this problem into docs (why do we need shading sometimes / always / `ëË\_( ó†)_/ëË`) ,1/29/17,1
lossyrob,that_Ñés a good idea. it_Ñés just going to keep biting people,1/30/17,1
lossyrob,"@dorukozturk updated landsat emr demo, would appreciate a review: https://github.com/geotrellis/geotrellis-landsat-emr-demo/pull/24",1/30/17,1
pomadchin,:tada: :tada: :tada: yeeeeeeeee,1/30/17,1
lossyrob,"I pushed a scala-landsat-util 1.0.0 to bintray as well, with all the PRs merged",1/30/17,1
pomadchin,:tada: :tada: :tada: ,1/30/17,1
hjaekel,:clap: thank you @lossyrob for releasing scala-landsat-util 1.0.0 ,1/30/17,1
hjaekel,"in my last PR which updated to GeoTrellis 1.0.0, I forgot to update the README.md",1/30/17,1
hjaekel,I will submit a new PR soon,1/30/17,1
hjaekel,here it is azavea/scala-landsat-util#18 ,1/30/17,1
fosskers,Guava is causing me problems now too,1/30/17,1
lossyrob,@hjaekel merged,1/30/17,1
hjaekel,great!,1/30/17,1
rgwozdz,@lossyrob - attempting to configure GeoTrellis with accumulo and came across your Gists at https://gist.github.com/lossyrob/20cf6d30fe02a7c6abcc; do those still apply to current version of GeoTrellis?,1/30/17,1
pomadchin,"@rgwozdz +- geotrellis can work on default accumulo / spark / hadoop settings",1/30/17,1
rgwozdz,"ok, thanks very much",1/30/17,1
pomadchin,"my recommendation - don_Ñét copy paste them, as 2 years passed and some config options can be a bit different; as libs were updated",1/30/17,1
rgwozdz,I'm interested in leveraging [geotrellis-viewer](https://github.com/dwins/geotrellis-viewer) but I notice it is referencing `spark-assembly.jar` which I can't seem to find in Spark 2.0 (though it exists in older versions of Spark).  Anyone aware of a workaround for  the requirement of this jar?,1/30/17,1
pomadchin,"@rgwozdz we have updated demos: * https://github.com/geotrellis/geotrellis-chatta-demo* https://github.com/geotrellis/geotrellis-landsat-emr-demo/pull/24",1/30/17,1
rgwozdz,"@pomadchin -  I very much appreciate the links.  Probably very obvious already, but I am very new to Scala and haven't developed in Java for a solid 6 years.  I very much want to wrap my head around building an REST API that leverages GeoTrellis - so I plan to try get oriented with these demos.  If there are any other resources you recommend that might help me, I would be very interested to review them",1/30/17,1
lossyrob,I_Ñéd recommend https://github.com/geotrellis/geotrellis-landsat-tutorial as an easier onramp to get acquainted ,1/30/17,1
lossyrob,"It needs to be updated to use akka-http and GeoTrellis 1.0, but generally won_Ñét change much for those updates",1/30/17,1
rgwozdz,"Attempting to `make build` geotrellis-chatta-demo, getting unresolved dependency exception:`[error] (*:update) sbt.ResolveException: unresolved dependency: org.locationtech.geotrellis#geotrellis-spark-etl_2.11;1.0.0-SNAPSHOT: not found`anyone seen this before?",1/31/17,1
pomadchin,@rgwozdz you need to clone geotrellis and run `./scripts/publish-local.sh`,1/31/17,1
pomadchin,https://github.com/locationtech/geotrellis/blob/master/scripts/publish-local.sh,1/31/17,1
rgwozdz,"will do, thank u",1/31/17,1
rgwozdz,"@pomadchin - I've made some progress with geotrellis-chatta-demo, but my first ingest bombed half-way through.  Now when I run `make ingest` I get:`Exception in thread ""main"" java.lang.IllegalArgumentException: requirement failed: Layer(name = ""DevelopedLand"", zoom = 13) already exists`So I guess I have to remove the older Layer?  how would I go about that?",1/31/17,1
lossyrob,You can delete the `path` directory for the output https://github.com/geotrellis/geotrellis-chatta-demo/blob/master/geotrellis/conf/output.json#L4,1/31/17,1
rgwozdz,"@lossyrob - thanks very much!  Now I get it - I was looking for the Layer in Accumulo or HDFS, and thus couldn't find it.  Its coming together now.  Thanks for the quick response",1/31/17,1
rgwozdz,"cannot tell if this is hanging or not:`hadoop@vagrant:/srv/geotrellis-chatta-demo$ make servercd geotrellis && java -noverify -cp target/scala-2.11/geotrellis-chatta-demo-assembly-0.1-SNAPSHOT.jar geotrellis.chatta.Main`Should there be more output?When I `curl 'http://localhost:8777/static/index.html'` I get  `curl: (52) Empty reply from server`",1/31/17,1
pomadchin,eh not sure,1/31/17,1
pomadchin,what do you see here http://localhost:8777 ?,1/31/17,1
rgwozdz,"This site can_Ñét be reachedThe webpage at http://localhost:8777/ might be temporarily down or it may have moved permanently to a new web address.",1/31/17,1
pomadchin,"nope, i double checked last pr i did",1/31/17,1
pomadchin,it works; locally,1/31/17,1
pomadchin,but i guess there can the following problem,1/31/17,1
pomadchin,https://github.com/geotrellis/geotrellis-chatta-demo/blob/master/geotrellis/src/main/resources/application.conf#L4,1/31/17,1
pomadchin,akka http can be binded on localhost and awaits queries from localhost; try to change it to smth more speific,1/31/17,1
pomadchin,for example vagrant,1/31/17,1
rgwozdz,"hmm, ok.  Definitely feel like I am missing something with respect to the webserver used in the geotrellis demos.  I had a similar problem yesterday on the ""geotrellis-landsat-tutorial"".  I am running on Ubuntu 16.04;  in most past development I have leveraged Node.js to start a web server on a given port and I haven't had to change any other configuration to reach it.  So i am probably missing some critical config",1/31/17,1
rgwozdz,@pomadchin  are there any logs for the included web-server?,1/31/17,1
pomadchin,https://github.com/geotrellis/geotrellis-chatta-demo/blob/master/geotrellis/src/main/resources/application.conf#L6,1/31/17,1
pomadchin,only these wich are custom,1/31/17,1
pomadchin,timings for routes calls,1/31/17,1
pomadchin,you can change logger level to warn to see more akka logs,1/31/17,1
pomadchin,http://doc.akka.io/docs/akka/current/java/logging.html,1/31/17,1
lossyrob,@rgwozdz did you run that curl command in the vagrant box or outside? might debug if it_Ñés a host->vm config issue,2/1/17,1
rgwozdz,@lossyrob - what ended up working was installing Nginx and having it forward incoming requests on port 80 to http://127.0.0.1:8777,2/1/17,1
lossyrob,hmmm. perhaps it was a `127.0.0.1` vs `localhost` thing? I_Ñéve seen spray routes behave poorly when the hostname is different than what it wants ,2/1/17,1
lossyrob,but glad you got it working,2/1/17,1
rgwozdz,"However, the geotrellis-chatta-demo server is now logging these 500 errors:`[ERROR] [02/01/2017 00:16:40.050] [chatta-demo-akka.actor.default-dispatcher-6] [akka.actor.ActorSystemImpl(chatta-demo)] Error during processing of request: 'Layer Layer(name = ""ImperviousSurfacesBarrenLandsOpenWater"", zoom = 9) not found in the catalog'. Completing with 500 Internal Server Error response.`",2/1/17,1
rgwozdz,But I know that I successfully ingested the layers,2/1/17,1
lossyrob,you can see that layer_Ñés files in the [configured file path?](https://github.com/geotrellis/geotrellis-chatta-demo/blob/master/geotrellis/src/main/resources/application.conf#L40),2/1/17,1
rgwozdz,"ah, that's must be it - forgot to change that after a reclone",2/1/17,1
rgwozdz,"any changes to application.conf must be followed by another `make build`, correct?",2/1/17,1
lossyrob,yes,2/1/17,1
lossyrob,it gets packaged in with the jar,2/1/17,1
rgwozdz,"that was it - serving the data now, excellent!  Thanks very much",2/1/17,1
lossyrob,no problem!,2/1/17,1
lossyrob,@pomadchin: what we were talking about as far as analysis area of focal operations -,2/1/17,1
lossyrob,https://github.com/locationtech/geotrellis/blob/master/spark/src/main/scala/geotrellis/spark/mapalgebra/focal/FocalOperation.scala#L33 is where the `mapValues` on the buffered tiles passes the analysis area (`Some(gridBounds)` which comes from the BufferedTile) to the focal operation,2/1/17,1
lossyrob,https://github.com/locationtech/geotrellis/blob/master/raster/src/main/scala/geotrellis/raster/mapalgebra/focal/FocalCalculation.scala#L39-L46 is where you can see the analysis area in the focal calc base class,2/1/17,1
lossyrob,"there are two strategies for focal operations in `geotrellis-raster`; cellwise and cursor. Basically cellwise is faster but can_Ñét be applied in all cases, cursor is much more general and can do things like apply arbitrary masks for arbitrary neighborhood shapes. For Cellwise, you can see how analysis area is applied: https://github.com/locationtech/geotrellis/blob/master/raster/src/main/scala/geotrellis/raster/mapalgebra/focal/FocalStrategy.scala#L204-L206",2/1/17,1
lossyrob,for cursor you can see it here https://github.com/locationtech/geotrellis/blob/master/raster/src/main/scala/geotrellis/raster/mapalgebra/focal/Cursor.scala#L72-L78,2/1/17,1
pomadchin,@lossyrob great! thanks,2/1/17,1
rgwozdz,"Doing the `make build` for geotrellis-chatta-demo and seems to be hung on `[info] Including: geotrellis-cassandra_2.11.jar` for a very long time. Sound irregular?Also - unable to kill the build, terminal frozen",2/1/17,1
rgwozdz,"hmm, nm, I think its that my ssh session died",2/1/17,1
pomadchin,:D,2/1/17,1
rgwozdz,keeps happening though :worried: ,2/1/17,1
pomadchin,hah,2/2/17,1
pomadchin,):,2/2/17,1
pomadchin,not sure; should be fine,2/2/17,1
pomadchin,mb memory limits (ram)?,2/2/17,1
pomadchin,or disk space,2/2/17,1
lossyrob,"if it_Ñés the docker container, usually that happens with low mem",2/2/17,1
rgwozdz,"Ok, here is a clue - I was trying to build it in a Vagrant Ubuntu directory that is shared with the host (Macbook) OS.  Doesn't seem to like that.  When I build in a non-shared directory, its all good",2/2/17,1
rgwozdz,"So, assuming I can't figure out a work around, I'm curious what you might suggest for a development setup.  My general approach with other languages/frameworks is to put code in a shared directory;  this allows me to use the IDE on my macbook, but build/test/debug that same code on the Ubuntu server (I'm attempting to mimic the final deployment server).  Perhaps that approach isn't an option here.I've not really leveraged Docker before, but perhaps that is the way forward?",2/2/17,1
pomadchin,yes try docker for mac / docker,2/2/17,1
lossyrob,+1 docker. Much better way to interact with specific environments. You can mount volumnes into the container like you would with a vm,2/2/17,1
pomadchin,\+ docker related questions you can throw here :D and even try geodocker environment : https://github.com/geodocker/geodocker ADS :D ,2/2/17,1
diegonc,"I was reading that KeyIndexMethod configuration in output.json has some time related parameters. `MultibandIngest` source says that it calls `Etl.ingest[ProjectedExtent, SpatialKey, MultibandTile](args)`So, no time neither in the extent nor key.Does that mean I need to write my own main class to ingest geotiff with timestamps?Is `Etl.ingest[TemporalProjectedExtent, SpaceTimeKey, MultibandTile](args)` enough?",2/2/17,1
lossyrob,Yes it should be,2/2/17,1
diegonc,"Ok, thanks!",2/2/17,1
hjaekel,"Did anyone see an exception like this on an executor?```java.lang.RuntimeException: org.apache.spark.SparkException: Failed to register classes with Kryo	at org.apache.spark.serializer.KryoSerializer.newKryo(KryoSerializer.scala:129)	at org.apache.spark.serializer.KryoSerializerInstance.borrowKryo(KryoSerializer.scala:274)	[...]Caused by: java.lang.ClassNotFoundException: de.gaf.ric.workflow.RicKryoRegistrator	at java.net.URLClassLoader.findClass(URLClassLoader.java:381)```I.e. my KryoRegistrator is not found on the executor, but it is definitively included in my uber jar. Is KryoSerializer using the wrong ClassLoader?",2/2/17,1
lossyrob,"interesting. I haven_Ñét seen that before, where the class is definitely there and it throws like that. I_Ñéve only seen that when I_Ñéve moved the registrator around and forgot to update the spark conf",2/2/17,1
hjaekel,this occured after the migration to spark 2.0.2,2/2/17,1
hjaekel,strange,2/2/17,1
diegonc,"HadoopLayerReader goes above my understanding. I'm trying to call its `read[SpaceTimeKey, MultibandTile, TileLayerMetadata[SpaceTimeKey]]` method but the compiler complains that `could not find implicit value for evidence parameter of type spray.json.JsonFormat[geotrellis.spark.SpaceTimeKey]`",2/2/17,1
diegonc,I'm not sure what's going on,2/2/17,1
pomadchin,"```scalaimport geotrellis.spark._import geotrellis.spark.io._import geotrellis.spark.io.hadoop._```",2/2/17,1
diegonc,also it declares about three parameters but in its test only  the layer id is provided,2/2/17,1
diegonc,"oh, let me try. Scala implicits are a double edged sword :stuck_out_tongue: ",2/2/17,1
pomadchin,"yes, `could not find implicit value for evidence parameter of type spray.json.JsonFormat[geotrellis.spark.SpaceTimeKey]` means that you have no json codec to encode/decode `SpaceTimeKey` in the scope",2/2/17,1
diegonc,"yay, I was missing the io._ import",2/2/17,1
pomadchin,here are examples with valid implicits: http://geotrellis.readthedocs.io/en/latest/guide/tile-backends/,2/2/17,1
diegonc,thanks,2/2/17,1
diegonc,"About the parameters, I was not reading the `extends ..` bit of the class declaration xD",2/2/17,1
diegonc,I'm following the SpaceTimeKey -> SpatialKey example from the spark section to compute an average over time,2/3/17,1
diegonc,"I came up with a function `fn` of type `MultibandTileLayerRDD[SpaceTimeKey] -> RDD[(SpatialKey, MultibandTile)]`",2/3/17,1
diegonc,"and now I want to keep the metadata using withContext`rdd.withContext(fn)` but I get a mixed TileLayerRDD where metadata is still parametrized with SpaceTimeKey",2/3/17,1
diegonc,how should I map the context to go from `TileLayerMetadata[SpaceTimeKey]` to `TileLayerMetadata[SpatialKey]`?,2/3/17,1
pomadchin,there is `mapContext` function too,2/3/17,1
diegonc,I've got this: `.mapContext { m => m }` but I don't know what to do with `m`,2/3/17,1
diegonc,"hmm, apparently only the bounds depend on the key type and I can map the bounds with a function `KeyBound[A] -> KeyBound[B]`",2/3/17,1
diegonc,"and KeyBounds looks like a pair of keys...hmm, maybe I can solve the puzzle :smile: ",2/3/17,1
diegonc,"```.mapContext { m =>        TileLayerMetadata[SpatialKey](            m.cellType,            m.layout,            m.extent,            m.crs,            m.bounds.map { kb => KeyBounds(kb.minKey.getComponent[SpatialKey]                                          ,kb.maxKey.getComponent[SpatialKey])})      }```that compiles :grin: ",2/3/17,1
pomadchin,"@diegonc ```scalardd.mapContext { m =>   m.copy(bounds = m.bounds.map { kb =>     KeyBounds(      kb.minKey.getComponent[SpatialKey],      kb.maxKey.getComponent[SpatialKey]    )  }) }```",2/3/17,1
diegonc,"oh, that looks way better; more succint",2/3/17,1
diegonc,thanks!,2/3/17,1
diegonc,"I just noticed both `TemporalGeoTiffHadoopInput ` and `TemporalMultibandGeoTiffHadoopInput ` declare a format of `""temporal-geotiff""` whereas the spatial only singleband and multiband inputs have distinct format strings",2/3/17,1
diegonc,wouldn't that cause trouble? like using the singleband input instead of the multiband,2/3/17,1
diegonc,"on the other hand, the plugin is looked up with```val plugins = combinedModule.findSubclassOf[InputPlugin[I, V]]```the types could tell the cases apart. But I don't know enough Scala to assert that",2/3/17,1
dorukozturk,"what is the best way to test accumulo ingestion locally? I got the geodocker-accumulo and did a docker-compose up, when I go to localhost:50095 I see the accumulo instance but when I try to spark-submit for etl and try to confiugre my backend-profiles to accumulo it is raising sh 17/02/03 16:39:06 WARN ThriftUtil: Failed to open transport to 2349ffdc6b65:9997 . I feel like I am not passing the zookeepers parameter correctly. Have you guys seen this error before?",2/3/17,1
dorukozturk,"{    ""backend-profiles"": [        {      	    ""name"": ""accumulo-local"",      	    ""type"": ""accumulo"",            ""zookeepers"": ""localhost"",            ""instance"": ""accumulo"",            ""user"": ""root"",            ""password"": ""GisPwd""        }    ]}",2/3/17,1
rgwozdz,Anyone using IntelliJ to remote-debug geotrellis-chatta-demo?  I can get IntelliJ it to start the demo but breakpoints are not working.  wondering if the -noverify option might have something to do with it (?),2/3/17,1
kthompson,i am looking at `geotrellis.spark.ingest.Ingest` and trying to determine if there is a way to parallelize the layer writing in this code. Does that happen to exist? or any thoughts on the feasability of trying to do that?,2/4/17,1
pomadchin,@rgwozdz never tried; -noverify flag is required due to shading; for some reason with akka in deps after shading bytecode is not 100% valid(?) its a question and noverfiy is a workaround,2/4/17,1
dorukozturk,@pomadchin hey I am glad you are online :) can you help me with my backend-profiles problem? What is the format of passing my zookeeper host:port?,2/4/17,1
pomadchin,@kthompson by default accumulo hbase and hadoop writes are done by mr jobs or parelilized some how different; accumulo (socket write strategy) s3 and cassandra threads number can be setuped in configuration: https://github.com/locationtech/geotrellis/blob/master/accumulo/src/main/resources/reference.conf#L19,2/4/17,1
pomadchin,checkout other rederence.conf files to be sure,2/4/17,1
pomadchin,@dorukozturk use dns container name,2/4/17,1
pomadchin,"if its via docker compose, i beleve the best way would be to create in your hosts reference: `<ipaddresss> <internal accumulo name>`",2/4/17,1
dorukozturk,what about in the backend-profiles.json file,2/4/17,1
dorukozturk,should that be the dns?,2/4/17,1
pomadchin,no needs in passing ports; yes that should be dns,2/4/17,1
pomadchin,@rgwozdz not the best idea to debug scala using java methods :)),2/4/17,1
dorukozturk,I still could not make it work. When I do docker-compose up for geodocker-accumulo and then docker ps I see that zookeeper is running on localhost:2181 but when I pass that in my backend-profiles it is not working.  ,2/4/17,1
pomadchin,"```telnet localhost 2181```",2/4/17,1
pomadchin,from the machine / os / vm you are trying to run ingest,2/4/17,1
pomadchin,"if its not avaible that explains everything; thought i dont know the whole picture: where are you trying to run ingest from; what ports forwarded, etc",2/4/17,1
dorukozturk,ok so I am trying to do everything locally on my laptop. I am trying to mimic AWS deployment so that is why I want accumulo so that I can try to ingest 1 layer and see how it is behaving,2/4/17,1
dorukozturk,so I decided to go for the geodocker-accumulo,2/4/17,1
dorukozturk,so it will be all in my local machine (ingest and everything),2/4/17,1
dorukozturk,so when I do docker-compose up I can go to http://localhost:50095/master and http://localhost:50070/ so both hdfs and accumulo are running. You are saying maybe my zookeeper config is borked?,2/4/17,1
dorukozturk,"ok something is wrong with my zookeeper ""echo ruok | nc localhost 2181"" this is not outputting anything.Thanks for narrowing down the problem. I know now it is a zookeeper issue",2/4/17,1
pomadchin,though if accumulo works zookeeper should work too,2/4/17,1
pomadchin,how many tservers it shows?,2/4/17,1
pomadchin,anyway you have to be carefull with dns names runing jobs on your host machine,2/4/17,1
pomadchin,as you_Ñéll be trying to access zookeeper at localhost,2/4/17,1
pomadchin,and it would give you internal accumulo nodes addresses,2/4/17,1
pomadchin,you would have to define in hosts correct aliases,2/4/17,1
pomadchin,"\+ following ports should be forwarder from tserver:`9999 9997 50091 4560 12234`",2/4/17,1
pomadchin,if you still want to run from your host machine,2/4/17,1
dorukozturk,"himmm, so this is this result of my docker ps https://gist.github.com/dorukozturk/58a1ebbfeb60d366923a7f7c82480e2e",2/4/17,1
pomadchin,i mean what do you see in accumulo ui,2/4/17,1
pomadchin,how many tservers it shows to you?,2/4/17,1
pomadchin,\+ there is a good reference to accumulo ports: https://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.3.4/bk_HDP_Reference_Guide/content/accumulo-ports.html ,2/4/17,1
pomadchin,be sure that all accumulo services forward ports on your host machine,2/4/17,1
dorukozturk,"ok I feel little lost, time to go read stuff :)",2/4/17,1
pomadchin,":)) say docker is a sort of vm; each container ~ a sep machine; when they laucnhed via docker-compose they all in the same network; that_Ñés why all services are visible for each otherwhen you are forwarding ports you make them avail on your machine: `docker run -p <p1>:<p2> ` means that you set `localhost:p1` to listen `dockercontaineraddress:p2` ",2/4/17,1
dorukozturk,okay I cannot access any of the forwarded ports but 50095,2/4/17,1
pomadchin,`ports` section in docker compose forwards ports,2/4/17,1
pomadchin,if you are using our default docker compose than nothing is forwarded,2/4/17,1
pomadchin,https://github.com/geodocker/geodocker-accumulo/blob/master/docker-compose.yml#L42-L43,2/4/17,1
pomadchin,example of forwarding ui,2/4/17,1
pomadchin,tablet server is not avail from host at all: https://github.com/geodocker/geodocker-accumulo/blob/master/docker-compose.yml#L47-L55,2/4/17,1
dorukozturk,"okay so I have to forward them, I thought this was a canned example but it is more of a template ",2/4/17,1
dorukozturk,now I see,2/4/17,1
pomadchin,its just for another usecase,2/4/17,1
dorukozturk,"okay now I see, I just need to forward the ports ",2/4/17,1
pomadchin,with geodocker i prefer to laucnh spark in docker; to mount neccesary jars / dirs into spark container and to launch jobs from this spark container,2/4/17,1
pomadchin,uis only forwarded in this example; to have a simple access via browser,2/4/17,1
dorukozturk,"okay now I see so in theory if I can forward all the tserver ports and everything correctly(make them accessible in my host machine) I should be able to do this, thanks for your guidance",2/4/17,1
pomadchin,\+ double check what zookeeper would have; potentially it may have _internal_ tserver addresses,2/4/17,1
pomadchin,that means that during ingest you would resolve zookeeper; but after that it would try to reach _internal_ accumulo addresses,2/4/17,1
pomadchin,smth like `hash:port`,2/4/17,1
pomadchin,here you may try to add `hash localhost` record into `hosts` file of your host machine,2/4/17,1
pomadchin,hey @metasim was this proposal to use avro 2 sql successful?,2/4/17,1
rgwozdz,"@pomadchin - can you recommend ways to debug/step-through Scala code;  sorry, I'm still a noob...even telling me what terms to search for would be useful",2/4/17,1
pomadchin,"@rgwozdz 1. a bit misterioud why you have problems with runing debugger; mb logs are not included or noverfify falg is evil? you can check it on some prototype project. i had a success in trying to debug this way ~ a year ago but it was working not as expected.2. so scala has types => that means that you (in theory) control your code using type system so there is no need in debugging this way",2/4/17,1
pomadchin,but if you still need to debug some runtime values => printlns ):,2/4/17,1
pomadchin,there also was a project called ~`scala-debugger` but i have no experience with it,2/4/17,1
pomadchin,another thiig you may try - to run repl from some point of your app,2/4/17,1
pomadchin,i don_Ñét remember how to do it but let me google,2/4/17,1
pomadchin,http://stackoverflow.com/questions/2160355/drop-into-interpreter-during-arbitrary-scala-code-location/2161749#2161749,2/4/17,1
rgwozdz,"hmm, ok.  Yeah it is a mystery.  The remote debugger on both Eclipse and Scala is able reach out and start the geotrellis-chatta-demo, but breakpoints don't work.  I guess printlns are options, but not quite as efficient",2/4/17,1
pomadchin,mb you have an old intelij idea version?,2/4/17,1
rgwozdz,brand new,2/4/17,1
rgwozdz,I must not have it configured correctly.,2/4/17,1
pomadchin,`ëË\_( ó†)_/ëË` if you really need debugger try to run repl ): but im not very helpful in this debugging question,2/4/17,1
rgwozdz,"ok, well thanks very much for trying",2/4/17,1
rgwozdz,"ha, just figured it out!  Wasn't setting the ""src"" folder as ""sources root""",2/4/17,1
pomadchin,w0w it works finally?,2/4/17,1
rgwozdz,"yuh, stopping at breakpoints",2/4/17,1
pomadchin,"hm; how did you import project using intelij idea?file -> from existing sources -> as sbt project?",2/4/17,1
rgwozdz,Just did a File -> open and pointed to geotrellis-chatta-demo directory,2/4/17,1
pomadchin,ok the correct way (i believe so otherwise i have problems with imports and syntax higlight): File -> new -> project from existing sources -> as sbt checkbox,2/4/17,1
rgwozdz,"then right clicked on the ""src""  and Mark Directory as -> Sources Root",2/4/17,1
rgwozdz,"Ok, I will try that",2/4/17,1
pomadchin,cool; great that it worked,2/4/17,1
diegonc,"regarding the ""temporal-geotiff"" format string being shared by both multiband and singleband hadoop input modules, it doesn't seem to cause crashes when using the catalog as multiband tiles, although I haven't verified the data read had all the bands",2/4/17,1
pomadchin,@diegonc what is your etl job definition (scala code)?,2/4/17,1
diegonc,"@pomadchin I copied MultibandIngest, but changed the `Etl.ingest` call to```Etl.ingest[TemporalProjectedExtent, SpaceTimeKey, MultibandTile](args)```then in input.json I've got `""format"": ""temporal-geotiff""`",2/4/17,1
pomadchin,that should and would work,2/4/17,1
diegonc,awesome. I'll add some logging statements next week to double check,2/4/17,1
pomadchin,._. :D ,2/4/17,1
diegonc,that difference between  hadoop's input modules  caught my attention and I'm so newbie when it comes to Scala that every detail scares me :stuck_out_tongue_closed_eyes: ,2/4/17,1
pomadchin,@diegonc here is the logic you are looking for,2/4/17,1
pomadchin,https://github.com/locationtech/geotrellis/blob/master/spark-etl/src/main/scala/geotrellis/spark/etl/Etl.scala#L107-L110,2/4/17,1
pomadchin,"at first it finds everything suitable for your `I, V`; after that searches entry accroding to format you provided in json etl description",2/4/17,1
diegonc,"yup, I've looked there. but I was not sure whether Scala had type erasure and the singleband plugin could be returned instead of /along with the multiband plugin",2/4/17,1
pomadchin,so you thought we are writing a non working code :D ,2/4/17,1
pomadchin,"ok so yes, scala has type erasure, but you can deal with it",2/4/17,1
pomadchin,We are using `TypeTag`s to workaround type erasure,2/4/17,1
diegonc,"oh, good to know",2/4/17,1
pomadchin,there are ways to deal with it smarter (using hlists for example) ,2/4/17,1
diegonc,"so, the distinction between `geotiff` and `multiband-geotiff` is not technically required as there is the type filtering of the plugins collection",2/4/17,1
Leonhalt3141,"Does geotrellis enable to make a class to define PostgreSQL table including geometry polygon?I would like to create the following table to handle SQL query in my Scala code.```scalacase class Geodummy(dummy_id: Int, geom: Polygon)class Geodummys(tag: Tag) extends Table[Geodummy](tag, ""geodummy""){  def dummy_id = column[Int](""dummy_id"")  def geom = column[Polygon](""geom"")  def * = (dummy_id, geom) <> (Geodummy.tupled, Geodummy.unapply)}```",2/5/17,1
schBen,"We have an application that uses GeoTrellis on EMR. It gives us the following error in the executor logs:```17/02/06 13:51:03 ERROR Executor: Exception in task 15.1 in stage 62.0 (TID 554)java.lang.NullPointerException	at org.slf4j.impl.Log4jLoggerAdapter.isWarnEnabled(Log4jLoggerAdapter.java:391)	at geotrellis.raster.ArrayTile$class.convert(ArrayTile.scala:46)	at geotrellis.raster.UShortArrayTile.convert(UShortArrayTile.scala:24)	at geotrellis.raster.UShortArrayTile.convert(UShortArrayTile.scala:24)	at geotrellis.raster.ArrayMultibandTile.convert(ArrayMultibandTile.scala:130)	at de.gaf.demo.algo.index.Ndvi$.apply(Ndvi.scala:27)	at de.gaf.demo.apps.NdviWorkflow$$anonfun$runSparkProcess$1$$anonfun$2.apply(NdviWorkflow.scala:82)	at de.gaf.demo.apps.NdviWorkflow$$anonfun$runSparkProcess$1$$anonfun$2.apply(NdviWorkflow.scala:82)	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$mapValues$1$$anonfun$apply$43$$anonfun$apply$44.apply(PairRDDFunctions.scala:759)	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$mapValues$1$$anonfun$apply$43$$anonfun$apply$44.apply(PairRDDFunctions.scala:759)	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)	at org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:192)	at org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:63)	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:79)	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:47)	at org.apache.spark.scheduler.Task.run(Task.scala:86)	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)	at java.lang.Thread.run(Thread.java:745)```@pomadchin @lossyrob Maybe you remember we reported that some weeks ago, see the discussion:https://gitter.im/geotrellis/geotrellis?at=5874d43961fac5a03dbd122fNow we still get this error. To investigate, we created a demo app we'd like to share with you:[ndvi-demo](https://github.com/gaf-ag/ndvi-demo.app)This leads to the same error if we build it and run it on EMR. We know that the error disapppears if we don't use RDD persistence[persist](https://github.com/gaf-ag/ndvi-demo.app/blob/master/ndvi-demo/src/main/scala/de/gaf/demo/apps/NdviWorkflow.scala#L78)[unpersist](https://github.com/gaf-ag/ndvi-demo.app/blob/master/ndvi-demo/src/main/scala/de/gaf/demo/apps/NdviWorkflow.scala#L91)Still we're stuck with that. Would you have any suggestions? We also invite you to try out our demo app if you'd like to reproduce that :-)",2/6/17,1
metasim,"@lossyrob Not immediately useful to GT, but worth keeping on the radar: https://github.com/haraldk/TwelveMonkeys",2/6/17,1
metasim,"@pomadchin I've had to focus on business things the past week, only getting fractured minutes to look at the avro2spark work. To do it correctly I think we need to go through the `org.apache.spark.sql.Encoder[T]` API. I reached out to the GeoMesa guys to see how much of the Catalyst work they did overlapped with the `Encoder[T]` API. While there is some overlap, it's not an exact match (but got some good pointers into their source). The current approach to writing the `Encoder`s reminds me of writing a javac annotation processor plugin (or Scala macros before quasiquotes) where you have to procedurally construct an AST for future evaluation. While it's a [public API](https://github.com/apache/spark/blob/master/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/encoders/ExpressionEncoder.scala), I've not found any examples of how to make use of it outside of Catalyst itself. My strategy is to take it in small steps.",2/6/17,1
lossyrob,"@metasim looked into that a bit ago, good to keep tabs on it. Also in that line is scrimage https://github.com/sksamuel/scrimage",2/6/17,1
pomadchin,"@schBen interesting, thx for catching it; though we have tests: https://github.com/locationtech/geotrellis/pull/1960/files can you make them fail?",2/6/17,1
lossyrob,"@schBen interesting. I would assume it_Ñés a deserialization error with Kryo, since the caching would cause that. Are you using a serialization option with persistance?",2/6/17,1
lossyrob,ah I see the links to the calls,2/6/17,1
lossyrob,so not using `_SER` option,2/6/17,1
lossyrob,which I thought wouldn_Ñét serialize on persistance,2/6/17,1
schBen,"hi rob. I have tried that same application with `MEMORY_ONLY_SER`,  and then I don't have this error",2/6/17,1
lossyrob,ah so perhaps when writing to disk it always serializes,2/6/17,1
lossyrob,@Leonhalt3141 we have a `geotrellis-slick` project that works with PostGIS,2/6/17,1
lossyrob,"we_Ñéve used it a lot, but to be honest I_Ñém unclear if it itself would provide what you_Ñére looking for",2/6/17,1
lossyrob,This for example uses it for a table with a Geometry: https://github.com/azavea/raster-foundry/blob/develop/app-backend/database/src/main/scala/com/azavea/rf/database/tables/Scenes.scala,2/6/17,1
lossyrob,@moradology might have a better clue,2/6/17,1
schBen,"@lossyrob I'm not sure, since there is also `MEMORY_AND_DISK_SER`",2/6/17,1
lossyrob,"trying to read the spark code to verify, tough to track how it happens",2/6/17,1
schBen,"at least  for ""DISK_ONLY"", there is no  ""SER"" option",2/6/17,1
lossyrob,"right. I just think that because it_Ñés flushing to disk, there is serialization implied ",2/6/17,1
lossyrob,not sure how you avoid serialization when saving to disk,2/6/17,1
moradology,@Leonhalt3141 let me know if you run into any specific problems or questions. it seems like you've used slick before and might already understand your next steps,2/6/17,1
schBen,@lossyrob that sounds logical,2/6/17,1
schBen,"but if that's right and the error has something to do with serialization, why does it disappear at `MEMORY_ONLY_SER`",2/6/17,1
lossyrob,good question,2/6/17,1
lossyrob,could it be a case where the way it is ser/deser to/from disk is different then memory?,2/6/17,1
lossyrob,"so persisting MEMORY_ONLY and MEMORY_ONLY_SER don_Ñét fail, but MEMORY_AND_DISK, MEMORY_AND_DISK_SER, and DISK_ONLY fail?",2/6/17,1
schBen,I will check it once again for all the cases,2/6/17,1
schBen,"@lossyrob I made a quick test. `MEMORY_ONLY` and `MEMORY_AND_DISK` had failed tasks. `DISK_ONLY`, `MEMORY_AND_DISK_SER` and `MEMORY_ONLY_SER` didn't.",2/6/17,1
lossyrob,interesting. so it_Ñés actually the non-serialization memory case that is failing,2/6/17,1
lossyrob,"so if it stores _ÑÉwithout serialization_Ñé, how does that affect the transient vals of an instance?",2/6/17,1
pomadchin,mb it_Ñés a _spark feature_ ?,2/6/17,1
schBen,"@lossyrob yes, it seems so. @pomadchin well at least I couldn't find this feature in the manual  :sweat_smile:",2/6/17,1
schBen,http://spark.apache.org/docs/latest/programming-guide.html#rdd-persistence,2/6/17,1
lossyrob,and you said this was only occuring after an upgrade to spark?,2/6/17,1
schBen,"yes, we only had it after upgrading Spark",2/6/17,1
lossyrob,from what to what?,2/6/17,1
pomadchin,"@schBen ye, that is why it is a _feature_",2/6/17,1
schBen,from 1.6.1 to 2.0.2,2/6/17,1
lossyrob,@pomadchin lol. the sarcasm is thick,2/6/17,1
lossyrob,ok interesting,2/6/17,1
lossyrob,it seems like it might be a spark bug,2/6/17,1
schBen,okay. or we just do something weird in our application,2/6/17,1
schBen,at least something you're not supposed to do with spark,2/6/17,1
lossyrob,seems like pretty standard usage though...,2/6/17,1
lossyrob,I wish I could find where in code that caching happens to get a better idea of what is actually going on,2/6/17,1
lossyrob,"`persist` on RDD simply marks it to be persisted, and some other mechanism does the actual persisting (I_Ñém looking in the DAGScheduler currently)",2/6/17,1
hjaekel,@lossyrob what do you think of our way of parallelizing the batch jobs [ParSeq](https://github.com/gaf-ag/ndvi-demo.app/blob/master/ndvi-demo/src/main/scala/de/gaf/demo/apps/NdviWorkflow.scala#L69)?,2/6/17,1
schBen,yeah I understood that RDDs are _marked_ for persistence first. And they get actually persisted when you call the first action on them. ,2/6/17,1
lossyrob,"@hjaekel I think that the way your doing it is executing jobs in parallel on a spark master, which can cause a bottleneck just based on how spark parallelizes job execution",2/6/17,1
echeipesh,^ https://spark.apache.org/docs/latest/job-scheduling.html#scheduling-within-an-application,2/6/17,1
lossyrob,we_Ñéve run into similar questions. @echeipesh has said there_Ñés a way to increase parallelism via configuration. ,2/6/17,1
lossyrob,"there_Ñés also potentially a way to ingest multiple layers as one RDD, which I_Ñéve explored a bit but haven_Ñét sat down and fought through coding",2/6/17,1
echeipesh,"Basically you can submit multiple jobs in parallel and thats cool, but its up to the scheduler to decide how the tasks get scheduled and executed. So for instance you have to configure to do fair scheduling to round robin through all the jobs (executing them incrementally). I believe the default behavior is to wait for each individual job to be complated.",2/6/17,1
echeipesh,"I seem to recall that when we were submitting jobs to Mesos in fine-grained scheduling mode it would actually run them in parallel and dynamically scale the executors, not sure if that_Ñés still the case.",2/6/17,1
hjaekel,"ok, we already saw that bottleneck on the master. we chose FIFO scheduling so that all the processed results do not arrive at the same time on the master, but that does not work out very well",2/6/17,1
echeipesh,collecting the results is blowing the heap ?,2/6/17,1
hjaekel,could be. we used a master with -Xmx51G,2/6/17,1
schBen,"but don't Spark's ""out-of-heapspace"" exceptions look different? we had something like that before",2/6/17,1
echeipesh,"well, there is lots of heaps, so if you run out of heap on executors it fails different from when the master runs out of heap. I_Ñém not sure what @hjaekel is trying to optimize for. Is it memory usage on master or trying to increase job parallism ?",2/6/17,1
lossyrob,have you tried the FAIR scheduler and found problems?,2/6/17,1
hjaekel,"with spark 1.6.1, we chose a configuration with 10 executors on 2 workers and then we scaled the master so that no OutOfMemoryErrors occured. If we increase the number of executors, we also have to increase parallelism, otherwise executors are idle. In that case, we also have to increase memory on the master",2/6/17,1
lossyrob,"a bit confused, the number of executors shouldn_Ñét effect the memory of the master",2/6/17,1
lossyrob,because the work executing on the executors doesn_Ñét play into master memory,2/6/17,1
lossyrob,"the thing we_Ñéve found is when we have a lot of tasks (like 50000) we can end up running out of mem on the master because it has to track the DAG in a really complex and heavy graph, but I_Ñém not sure about the situation you_Ñére talking about",2/6/17,1
hjaekel,"the `stitch` is executed on the master, and if you have too many parallel tasks, the master has to execute many parallel stitches",2/6/17,1
schBen,"that's for sure an issue... but we thought it would not be directly linked to the ""persist"" thing",2/6/17,1
lossyrob,"ah gotcha. If you_Ñére stitching, you_Ñére doing large collects_Ñ_I would wonder if there_Ñéd be a way around that",2/6/17,1
lossyrob,true I don_Ñét think the persist thing and the parallelization issue are tied. Though I_Ñém confused enough about the persist bug that I wouldn_Ñét say anything definitively...,2/6/17,1
schBen,okay sure :-) ,2/6/17,1
kthompson,"Hi I posted this PR, I am looking for some feedback  on it. https://github.com/locationtech/geotrellis/pull/2001",2/6/17,1
lossyrob,"Thanks @kthompson! Could you put a description of the motivations for this feature, and what exactly it does, in the PR description?",2/6/17,1
kthompson,@lossyrob sure thing,2/6/17,1
Leonhalt3141,"@lossyrob @moradology  Thank you for your kind reply. Actually, I would like to manage satellite imageries with PostGIS. To know coverage location of image, I need to set a column of geometry Polygon in the table. But I don_Ñét know how to  define Polygon in Scala codes.  Is there any class for PostGIS Polygon geometry, or just set as String type and convert to geometry each time?",2/7/17,1
mattfullerton,"Hi everyone, was redirected here from GitHub (https://github.com/geotrellis/geotrellis-landsat-tutorial/issues/5#issuecomment-277842088) - to get back on that, I also get the failure doing a boring lynx localhost:8089 (I had to change the port in my case), except I can see the 404 first appearing and then I get the Resource could not be found thing.",2/7/17,1
mattfullerton,"To be honest though this is partly just a symptom of a larger understanding problem of mine; I was familiar with the geotrellis catalog form from ca. October 2015, when the viewer/sample catalog were still in the main repo. As far as I can tell, a lot has changed since then. This landsat tutorial outputs a big catalog but actually no master JSON file explain what data stores should be in there, which I would have been expecting. Is there an overview anywhere of the current catalog format and how one goes about building one ""by hand"" (i.e. setting up the catalog JSON and outputting converted files from gtloader). My overall aim is to serve a catalog from s3 for the geotrellis-admin project. But I am lacking info on what exactly should be sitting in S3 and thought I would start with the output of the landsat tutorial project.",2/7/17,1
echeipesh,@Leonhalt3141 The types for columns and their projections is what `geotrellis-slick` adds. You can see the use of geometry type without projection here (https://github.com/locationtech/geotrellis/blob/master/slick/src/test/scala/geotrellis/slick/PostgisSpec.scala#L52) and with projection SRID here (https://github.com/locationtech/geotrellis/blob/master/slick/src/test/scala/geotrellis/slick/PostGisProjectionSupportSpec.scala#L38). The latter will satisffy SRID constraints on a PostGIS geometry column. (There is support for Polygon in similar manner),2/7/17,1
echeipesh,@mattfullerton Did @pomadchin suggestion to change `localhost` to `0.0.0.0` in `Server.scala` make any difference for you?,2/7/17,1
echeipesh,The catalog really has changed a lot since 0.9. We basically used to store images of arbitrary size and then clip them to extract the rasters we needed. The catalog/layer concept right now is more of an image pyramid which stores tiles on a regular grid and you can find the metadata we store in the `_attributes` directory.,2/7/17,1
echeipesh,"There really isn_Ñét a fisible manual way to create this pyramid, you either do the process which is shown in `IngestImage.scala`, where you have a lot of control over the data, or you can use the spark ETL wrapper around that process discussed here (http://geotrellis.readthedocs.io/en/latest/tutorials/etl-tutorial/). ",2/7/17,1
mattfullerton,OK. Thanks for the explanation. But the output from the tutorial is a bona fide catalog which I could put as is in an S3 bucket and explorer using geotrellis-admim?,2/7/17,1
mattfullerton,@echeipesh Just tried localhost->0.0.0.0 - no difference,2/7/17,1
mattfullerton,"To be clear, I'm almost certain I'm communicating with the Spray app - its just not returning me anything",2/7/17,1
mattfullerton,"(also tried /static, /static/index.html, /index.html...)",2/7/17,1
echeipesh,"Yep, you_Ñéd have to switch from using `FileLayerWrter` to `S3LayerWriter` (and realted `S3AttributeStore`, `S3LayerManager`) https://github.com/geotrellis/geotrellis-landsat-tutorial/blob/master/src/main/scala/tutorial/IngestImage.scala#L89",2/7/17,1
mattfullerton,I can't just copy the files across?,2/7/17,1
echeipesh,"The matadata is going to be slightly off, its going to be talking about file locations on the file system vs S3 buckets. Its probably easier to re-write them rather than going to change the metadata by hand.",2/7/17,1
mattfullerton,"ahha, I see",2/7/17,1
mattfullerton,"I am getting this btw., in case its helpful:```[info] Running tutorial.Serve [info] [INFO] [02/07/2017 14:20:38.879] [tutorial-system-akka.actor.default-dispatcher-3] [akka://tutorial-system/user/IO-HTTP/listener-0] Bound to /0.0.0.0:8089[info] [INFO] [02/07/2017 14:20:38.886] [tutorial-system-akka.actor.default-dispatcher-5] [akka://tutorial-system/deadLetters] Message [akka.io.Tcp$Bound] from Actor[akka://tutorial-system/user/IO-HTTP/listener-0#-1186893658] to Actor[akka://tutorial-system/deadLetters] was not delivered. [1] dead letters encountered. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.[info] [INFO] [02/07/2017 14:21:18.810] [tutorial-system-akka.actor.default-dispatcher-12] [akka://tutorial-system/system/IO-TCP/selectors/$a/2] Message [akka.io.Tcp$ConfirmedClose$] from Actor[akka://tutorial-system/user/IO-HTTP/listener-0/1#-1840302958] to Actor[akka://tutorial-system/system/IO-TCP/selectors/$a/2#398254234] was not delivered. [2] dead letters encountered. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.```",2/7/17,1
echeipesh,"Yep, thats the normal akka-http startup log messages.",2/7/17,1
mattfullerton,Including all the dead letter stuff?,2/7/17,1
echeipesh,"Ah, that does look very suspect. Let me kick it to compare and sanity check.",2/7/17,1
pomadchin,@mattfullerton you are trying to launch server in a VM?,2/7/17,1
mattfullerton,I am running this on a single machine,2/7/17,1
mattfullerton,Ubuntu,2/7/17,1
pomadchin,can you show your hosts file?,2/7/17,1
mattfullerton,"```127.0.0.1 localhost# The following lines are desirable for IPv6 capable hosts::1 ip6-localhost ip6-loopbackfe00::0 ip6-localnetff00::0 ip6-mcastprefixff02::1 ip6-allnodesff02::2 ip6-allroutersff02::3 ip6-allhosts```",2/7/17,1
echeipesh,curl 'http://localhost:8089/ndvi/10/910/402',2/7/17,1
mattfullerton,"a large amount of junk, which I assume is good",2/7/17,1
mattfullerton,"so if I can get the tiles, why can I not get the index/map?",2/7/17,1
mattfullerton,"PNG, to be more specific about the junk",2/7/17,1
echeipesh,"Yea, that_Ñés the PNG. So I_Ñém just remembering that this demo doesn_Ñét actually serve the html files, just the TMS server. Can you open the `index.html` in a browser from the file system ?",2/7/17,1
mattfullerton,ah its a JS app I guess,2/7/17,1
echeipesh,yep :),2/7/17,1
pomadchin,:D,2/7/17,1
pomadchin,:D ,2/7/17,1
pomadchin,wooh,2/7/17,1
echeipesh,Added to the static file endpoints to the demo since this is the second time this kind of confusion happened: https://github.com/geotrellis/geotrellis-landsat-tutorial/commit/24d729ee330d5df57f8611089949d6052f91de84,2/7/17,1
mattfullerton,ah beautiful,2/7/17,1
mattfullerton,"So, I am happy, I think: https://data.hawadawa.com/geotrellisviewer/",2/7/17,1
mattfullerton,I'm getting lots of 404s but I think that's just where its looking for tiles outside of the landsat image area,2/7/17,1
mattfullerton,So to go a few steps back; what is the status of python-geotrellis and creating catalog data from geotiffs as used to be possible with gtloader (if I'm remembering correctly) - no longer supported?,2/7/17,1
mattfullerton,@pomadchin @echeipesh Thanks very much for getting me up and running. You're very responsive and helpful around here... :smile: ,2/7/17,1
lossyrob, python-geotrellis is not used for ingesting data after 0.9. The `spark-etl` project is used for ingesting data now: http://geotrellis.readthedocs.io/en/latest/tutorials/etl-tutorial/,2/7/17,1
lossyrob,"we are currently working on python bindings, which will include python-scriptable methods of ingesting data",2/7/17,1
lossyrob,"also, we have a GeoTiff reader and writer now that is performant and featureful, so we generally use GeoTiffs as the flat file format for rasters.",2/7/17,1
mattfullerton,"Ah - that means if you write the server/app right, you can interchange catalog<->GeoTiffs?",2/7/17,1
mattfullerton,My colleague would have been so pleased if I told her we are forced to use Spark,2/7/17,1
lossyrob,"You can write a server that draws directly from GeoTiffs. You can also write the ingest (preprossing), which stores the data internally as Avro-serialized data, and write a server that reads thigns out without spark (either by single tile or collections of tiles that are read in parallel and fit in memory)",2/7/17,1
mattfullerton,"OK, great. I think I need to go off and read and experiment before I ask any more questions - cheers!",2/7/17,1
lossyrob,"cool, let us know when you have more. cheers :)",2/7/17,1
metasim,"@echeipesh @pomadchin @lossyrob I had a breakthrough:https://github.com/s22s/avro2spark/pull/4#issuecomment-278434943",2/8/17,1
lossyrob,that_Ñés looking good,2/8/17,1
lossyrob,what_Ñés all the nulls in the data frame?,2/8/17,1
lossyrob,"`[WrappedArray([[3,3,[B@2bb31455,-128],null,null,null,null,null,null,null], [[3,3,[B@3c16528d,-128],null,null,null,null,null,null,null]` etc",2/8/17,1
metasim,It's an oddity of how the Databricks `spark-avro` library deals with `UNION` types.,2/8/17,1
lossyrob,ah,2/8/17,1
lossyrob,so those are all the tile types that the thing is not,2/8/17,1
metasim,"yep.... so to make it more usable we're likely to need to a) rethink the structure of the GT Avro codecs, or b) have some additional GT specific transformation functions to clean all that up.That said, there are still a a couple of opportunities to get that output a little better structured, just based on the Avro codec.",2/8/17,1
metasim,"I'm just starting to grok the way the Catalyst trees work, which was where most of my time was spent in figuring this out. So I'm pretty pumped ;-)",2/8/17,1
metasim,Next step is to dive deeper into that `WrappedArray` bit.,2/8/17,1
lossyrob,how does the encoder stuff play into catalyst?,2/8/17,1
metasim,"Classes [like this](https://github.com/s22s/avro2spark/blob/master/src/main/scala/astraea/demo/AvroDerivedSparkEncoder.scala#L69) implement traits that are part of Catalyst's ""Logical Plan"" infrastructure. In a way somewhat similar to the RDD API where you effectively define a program for future execution via transformations, the Catalyst expression API allows you to construct a tree of expressions that are to be executed (in the future) in each partition.",2/8/17,1
lossyrob,"whoa. looks pretty intense, I_Ñéll have to take some time to read into it",2/8/17,1
metasim,"Further (like the GeoMesa guys did), we could implement transformation rules the optimize the logical plan into a physical plan.",2/8/17,1
lossyrob,or would be a good thing to doc in a writeup :),2/8/17,1
metasim,An example would be to only selectively serialize a subpart of the Avro structure if only certain columns are desired.,2/8/17,1
metasim,Yep.... once I feel I've exhausted what I can do with it I'd appreciate being able to do a code walkthrough with you guys.,2/8/17,1
lossyrob,that would be awesome,2/8/17,1
metasim,"(it has been intense.... tested my patience for progress, for sure)",2/8/17,1
lossyrob,well good on you for getting to breakthroughs. I feel like we_Ñére cheering you on from the gitter sidelines haha _ÑÒgo! go!_Ñù :racehorse: ,2/8/17,1
metasim,Awwww shucks :blush: ,2/8/17,1
david-2012,"Hi @pomadchin, @lossyrob Currently, I am using some sequence of PointFeature[Double] to interpolate a raster in a specific extent.In geotrellis, there are some methods called like ordinaryKriging. However, the method return a whole tile.Is there any solution to convert a tile with specific extent to a TileLayerRDD without saving it to the file and ingesting from the file.Thanks for your comments!",2/9/17,1
lossyrob,"Hi @david-2012, is the problem that you cannot fit the single tile in memory? If not, there is a way you can go directly from a Tile to a TileLayerRDD",2/9/17,1
david-2012,"@lossyrob «_Ô I can fit the single tile in memory and write it into tiff. However, I'd like to know if there is some way to convert the single tile to a TileLayerRDD in memory, without interacting with disk.",2/9/17,1
metasim,"@david-2012 Is there something more to it going on, such that `TileLayerRDD(spark.makeRDD(myTile), myMetadata)` doesn't work?",2/9/17,1
lossyrob,"the thing would be that, you would need to tile to a layout or else the SpatialKey won_Ñét be there",2/9/17,1
lossyrob,"`spark.makeRDD(myTile): RDD[Tile]`, need `RDD[(SpatialKey, Tile)]`",2/9/17,1
lossyrob,you can do that with `tileToLayout`,2/9/17,1
lossyrob,"so if you have the extent of the tile, you can do",2/9/17,1
lossyrob,"`val rdd = sc.makeRDD((ProjectedExtent(extent, crs), Tile))`",2/9/17,1
lossyrob,you_Ñéll need the crs too,2/9/17,1
lossyrob,from there it_Ñés imitating what ETL does,2/9/17,1
metasim,"ah, right... good to know [this exists](https://github.com/locationtech/geotrellis/blob/a00c35b928e96083188d91734252d66574e3b4a7/spark/src/main/scala/geotrellis/spark/tiling/TilerMethods.scala#L55).",2/9/17,1
lossyrob,"```scalaval layoutScheme = ZoomedLayoutScheme(crs)val md = rdd.collectMetadata(layoutScheme)val result: TileLayerRDD[SpatialKey] = ContextRDD(rdd.tileToLayout[SpatialKey](md), md)```",2/9/17,1
lossyrob,there should probably be a better API around that,2/9/17,1
lossyrob,"if you feel like making an issue about that, that would be appreciated :)",2/9/17,1
metasim,#2006 ,2/9/17,1
lossyrob,:+1:,2/9/17,1
xarmatzis_twitter,"Good morning, what is the best ide or anything else to develop a project with geotrellis? I have tried VS code but all things have to be done manually  and now I am trying IntelliJ IDEA. What r u using? ",2/10/17,1
david-2012,I'm using IDEA on Mac. Found that it is very nice and powerful.,2/10/17,1
pomadchin,@xarmatzis_twitter you can use any ide with scala support or any text editor with scala syntax highlight,2/10/17,1
cepkoCC,"Hi everyone! I'm currently trying to write a service for a Spark cluster that converts vector data (Double: latitude, Double: longitude, Double: value) into map tiles. I'm having a hard time finding the exact workflow to make this happen even though it appears to be fairly straightforward, so I feel like I must be missing something essential. Using the Kernel tutorial and https://github.com/geotrellis/geotrellis-landsat-tutorial/blob/master/src/main/scala/tutorial/IngestImage.scala I've managed to represent some dummy data as an array of Features, but actually getting that array into an RDD that `Pyramid.uplevels` accepts has been troublesome.",2/10/17,1
xarmatzis_twitter,I am in the same page as u. Could someone provide us few examples....?,2/10/17,1
cepkoCC,"https://gist.github.com/cepkoCC/b706fb1947354e97beeb160b1eebbe2dThis is where I'm at after removing all of the code that just didn't work, am I making some assumptions that are blocking me from moving forward?",2/10/17,1
echeipesh,@cepkoCC Looks like you_Ñére just trying to do a kernel density over the points? If so we have a specific feature to do that to an RDD of points and the test should be pretty helpful as an example: https://github.com/locationtech/geotrellis/blob/master/spark/src/test/scala/geotrellis/spark/density/RDDKernelDensitySpec.scala#L76,2/10/17,1
cepkoCC,oh awesome! I'll give this a try,2/10/17,1
lossyrob,"For conversion of vector to raster, there_Ñés kernel density, and also just straight rasterization",2/10/17,1
lossyrob,"for example, here_Ñés a method that counts points per cell and rasterizes https://github.com/locationtech/geotrellis/blob/master/doc-examples/src/main/scala/geotrellis/doc/examples/raster/RasterExamples.scala#L24",2/10/17,1
lossyrob,that_Ñés more of a manual rasterization,2/10/17,1
lossyrob,we also have methods for rasterizing any geometry,2/10/17,1
lossyrob,"that functionality is unfortunately not well documented, but you can see the test here https://github.com/locationtech/geotrellis/blob/master/raster-test/src/test/scala/geotrellis/raster/rasterize/RasterizeMethodsSpec.scala#L26",2/10/17,1
lossyrob,that_Ñés better for polygon rasterizaiton. for Points just doing a RasterExtent.mapToGrid transformation to get the cell values is easy,2/10/17,1
lossyrob,as the count points example shows,2/10/17,1
lossyrob,"to do something like that in an RDD, it_Ñés pretty straight forward, but would require some setup to create the appropriate RDD with metadata, then parallize over potential intersecting spatial keys, broadcast the points, map to an option tile if there are intersecting points (with the tile being the rasterized version), and then flatten to get rid of Nones. So there_Ñés a clear path for something like that, but would take some thinking through. The kernel density is a good place to look for that sort of algorithm",2/10/17,1
lossyrob,@xarmatzis_twitter does that point you in the right direction?,2/10/17,1
xarmatzis_twitter,"@lossyrob  well, yes. But, I need really help in that. I am developing GIS applications for 7 years and with geotrellis I am feeling like I am back to school..... ",2/10/17,1
lossyrob,have you done any scala before? spark?,2/10/17,1
xarmatzis_twitter,"yes a little bit both, but from a demo to real application it is very far distance.... What do you think?",2/10/17,1
lossyrob,yeah that makes sense,2/10/17,1
lossyrob,"It_Ñés a lot to take in, so it would be best to try and figure out what the smaller steps are to try and get there",2/10/17,1
lossyrob,"where are you at right now with it? have you got spark set up, can run through pre-existing demos?",2/10/17,1
lossyrob,https://github.com/geotrellis/geotrellis-landsat-tutorial is probably the easiest one to go through,2/10/17,1
xarmatzis_twitter,"I am working on that right now, but when you try to change the input data , nothing works...",2/10/17,1
cepkoCC,^ This is the link I've been using to try to get where I'm trying to go,2/10/17,1
lossyrob,"The usual case is to start with raster, and ingest raster. It seems like you_Ñére trying to move from point data to raster",2/10/17,1
lossyrob,what sort of use case is this?,2/10/17,1
xarmatzis_twitter,e.g. if I change the geotif from multiband to singleband nothing works,2/10/17,1
xarmatzis_twitter,@lossyrob  could you provide us a build.sbt template with the latest build of geotrellis?,2/10/17,1
lossyrob,"if you change from multiband to single band, there_Ñés a number of types that would have to be changed",2/10/17,1
lossyrob,"it would take some changes to the codebase, but getting it to work with single band is possible and would be a good thing to try to work through",2/10/17,1
xarmatzis_twitter,yes I have changed the MultibandTile to Tile and it was not working,2/10/17,1
lossyrob,"when you say, _ÑÉlatest build.sbt_Ñé, what do you mean?",2/10/17,1
xarmatzis_twitter,latest version of geotrellis inside a the build.sbt,2/10/17,1
xarmatzis_twitter,so the dependencies are all set up to the right version,2/10/17,1
lossyrob,https://github.com/locationtech/geotrellis#getting-started,2/10/17,1
lossyrob,"you depend on the various subprojects, and the current release is 1.0.0",2/10/17,1
xarmatzis_twitter,and the resolver how do I use it?,2/10/17,1
lossyrob,"it will be on maven central, so you won_Ñét have to add a resolver",2/10/17,1
lossyrob,you only have to add a locationtech repo resolver if your using a snapshot,2/10/17,1
xarmatzis_twitter,which version of scala should I use?,2/10/17,1
lossyrob,2.11.8,2/10/17,1
echeipesh,https://github.com/geotrellis/geotrellis-landsat-tutorial has been updated to geotrellis version 1.0.0 and scala 2.11,2/10/17,1
cepkoCC,https://gist.github.com/cepkoCC/88852b6f3e06e1b5daae0ee6133170da,2/10/17,1
cepkoCC,"So I'm still trying to figure this pipeline out, I can't seem to get the data structured correctly for Pyramid to accept my tiles",2/10/17,1
cepkoCC,"The error message I'm getting says that uplevels expects `(rdd: org.apache.spark.rdd.RDD[(K, V)] with geotrellis.spark.Metadata[M], layoutScheme: geotrellis.spark.tiling.LayoutScheme, startZoom: Int, options: geotrellis.spark.pyramid.Pyramid.Options)`but gets `(org.apache.spark.rdd.RDD[(geotrellis.spark.SpatialKey, geotrellis.raster.Tile)], geotrellis.spark.tiling.ZoomedLayoutScheme, Int, geotrellis.raster.resample.Bilinear.type)`",2/10/17,1
cepkoCC,All I'm really looking for is a way to go from the generated features to map tiles,2/10/17,1
echeipesh,this is the problem right here: https://gist.github.com/cepkoCC/88852b6f3e06e1b5daae0ee6133170da#file-main-scala-L61 You are upcasting the type to just an RDD,2/10/17,1
cepkoCC,"ah ok, what should I be doing instead?",2/10/17,1
echeipesh,"If you look at the return type of the method it actually gives `RDD[(SpatialKey, Tile)] with Metadata[TileLayerMetadata[SpatialKey]]`",2/10/17,1
echeipesh,https://github.com/locationtech/geotrellis/blob/a00c35b928e96083188d91734252d66574e3b4a7/spark/src/main/scala/geotrellis/spark/density/RDDIntKernelDensityMethods.scala#L30,2/10/17,1
cepkoCC,so I'm losing the metadata,2/10/17,1
echeipesh,"yeap, you can be explicit and use the full type or you can not specify it and let scala infer it from the method return type.",2/10/17,1
cepkoCC,"awesome, now I'm actually creating some binary tiles!",2/10/17,1
cepkoCC,thanks for helping me get this far,2/10/17,1
echeipesh,sweet :) and you_Ñére welcome.,2/10/17,1
cepkoCC,https://github.com/locationtech/geotrellis/issues/1199,2/10/17,1
cepkoCC,did you ever find a solution for this bug?,2/10/17,1
cepkoCC,because I think I'm running into it right now,2/10/17,1
lossyrob,yeah the issue was just to create a more meaningful error message,2/10/17,1
lossyrob,"what is happening is that, the extent you are creating is invalid",2/10/17,1
lossyrob,so there_Ñés something in the Extent creation logic that is probably off (is my guess),2/10/17,1
cepkoCC,I would almost guarantee it,2/10/17,1
lossyrob,whats the stack trace?,2/10/17,1
cepkoCC,"```[error] (run-main-0) geotrellis.raster.GeoAttrsError: invalid cols: 0geotrellis.raster.GeoAttrsError: invalid cols: 0        at geotrellis.raster.RasterExtent.<init>(RasterExtent.scala:77)        at geotrellis.raster.RasterExtent$.apply(RasterExtent.scala:279)        at geotrellis.spark.pyramid.Pyramid$.up(Pyramid.scala:52)        at geotrellis.spark.pyramid.Pyramid$.runLevel$1(Pyramid.scala:181)        at geotrellis.spark.pyramid.Pyramid$.upLevels(Pyramid.scala:188)        at geotrellis.spark.pyramid.Pyramid$.upLevels(Pyramid.scala:211)        at geotrellis.spark.pyramid.Pyramid$.upLevels(Pyramid.scala:221)        at demo.Main$.main(Main.scala:64)        at demo.Main.main(Main.scala)        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)        at java.lang.reflect.Method.invoke(Method.java:498)```",2/10/17,1
cepkoCC,I think it's just an Extent,2/10/17,1
lossyrob,"the points/extent are in LatLng, correct?",2/10/17,1
lossyrob,"the zoomed layout scheme is in web mercator   `val layoutScheme = ZoomedLayoutScheme(WebMercator, tileSize = 256)`",2/10/17,1
lossyrob,so you_Ñéd have to reproject your points to web mercator for that to work,2/10/17,1
cepkoCC,ah ok,2/10/17,1
cepkoCC,let me see if I can make that happen,2/10/17,1
lossyrob,"pts.map(_.reproject(LatLng, WebMercator))",2/10/17,1
lossyrob,ah but they are point features,2/10/17,1
lossyrob,"so, a little different",2/10/17,1
lossyrob,feature has a `mapGeom` method that will map into the geom and modify,2/10/17,1
echeipesh,you can reproject `Feature` https://github.com/locationtech/geotrellis/blob/a00c35b928e96083188d91734252d66574e3b4a7/vector/src/main/scala/geotrellis/vector/reproject/Implicits.scala#L112,2/10/17,1
lossyrob,oh,2/10/17,1
cepkoCC,"Still having a lot of trouble with the NullPointer error:https://gist.github.com/cepkoCC/cfd46aec21edcf5cedde1e7e3748df6fhttps://gist.github.com/cepkoCC/b917141c191554e65fca08f5b1654ce7",2/10/17,1
cepkoCC,"I've had a few revelation about how this process should work, but I still feel like I'm missing something essential",2/10/17,1
lossyrob,you_Ñére using 1.0.0?,2/11/17,1
lossyrob,that null pointer exception seems like a legit bug,2/11/17,1
lossyrob,you could avoid the delete call if you_Ñére writing new layers,2/11/17,1
lossyrob,from the line numbers looks like 0.10,2/11/17,1
lossyrob,https://github.com/locationtech/geotrellis/blob/_old/v0.10.3/spark/src/main/scala/geotrellis/spark/io/file/FileLayerDeleter.scala#L33-L36,2/11/17,1
lossyrob,"this does not check that the file exists, and according to the javadocs for `listFiles`: > Returns null if this abstract pathname does not denote a directory, or if an I/O error occurs.",2/11/17,1
lossyrob,"So, that_Ñés our bad (!) thanks for finding that bug",2/11/17,1
lossyrob,Issue created: https://github.com/locationtech/geotrellis/issues/2011,2/11/17,1
lossyrob,Pretty low hanging fruit if anyone wants an easy issue to contribute with!,2/11/17,1
lossyrob,We_Ñéll release in a bugfix version (1.0.1) probably sooner rather than later.,2/11/17,1
lossyrob,(i.e. target end of Feb unless people have needs for this functionality beforehand),2/11/17,1
lossyrob,"@cepkoCC note that you could blow away your catalog directory, and that should fix things",2/11/17,1
cepkoCC,"@lossyrob I can pick up that ticket, seems like a good way to start contributing",2/13/17,1
lossyrob,:+1::+1:,2/13/17,1
cepkoCC,I'm still having 0 col errors in the pyramiding process though,2/13/17,1
cepkoCC,https://gist.github.com/cepkoCC/4fac2797c33249be3acb161bf8300065,2/13/17,1
lossyrob,"```scalavar pts = (for (i <- 1 to 10000) yield randomPointFeature(extent)).toList    pts.map(ReprojectFeature(_).reproject(LatLng, WebMercator))```",2/13/17,1
lossyrob,in scala we prefer immutability always,2/13/17,1
cepkoCC,that was my impression,2/13/17,1
lossyrob,so that map and reproject each return a thing,2/13/17,1
lossyrob,a new collection,2/13/17,1
lossyrob,that collection isn_Ñét being captured,2/13/17,1
cepkoCC,"hm ok, so `pts` is still in LatLng",2/13/17,1
lossyrob,"```scalaval pts =     (for (i <- 1 to 10000) yield randomPointFeature(extent))        .map(ReprojectFeature(_).reproject(LatLng, WebMercator))        .toList```",2/13/17,1
lossyrob,oops,2/13/17,1
lossyrob,fixed formatting,2/13/17,1
cepkoCC,"that worked, but now after parallelizing it, `kernelDensity` doesn't seem to be a member function anymore",2/13/17,1
cepkoCC,"`value kernelDensity is nota member of org.apache.spark.rdd.RDD[geotrellis.vector.Feature[geotrellis.vector.Geometry,Double]]`",2/13/17,1
lossyrob,What is `ReprojectFeature`?,2/13/17,1
lossyrob,"kernelDensity needs a Point, you have a Geometry",2/13/17,1
lossyrob,ah your using the implicit class,2/13/17,1
lossyrob,as an actual thing. but there should be a point specific version,2/13/17,1
lossyrob,so just remove that,2/13/17,1
lossyrob,"```scalaval pts =     (for (i <- 1 to 10000) yield randomPointFeature(extent))        .map(_.reproject(LatLng, WebMercator))        .toList```",2/13/17,1
lossyrob,as long as you import `geotrellis.vector._`,2/13/17,1
cepkoCC,would the fact that I skip over writing the layer cause the `invalid cols` error?,2/13/17,1
lossyrob,"no that_Ñés the reprojection error, I think",2/13/17,1
cepkoCC,I can't imagine where a reprojection error would stem from at this point,2/13/17,1
cepkoCC,I updated my gist with the current state of my program/logs,2/13/17,1
cepkoCC,could it be the Metadata?,2/13/17,1
lossyrob,"that is odd, it does go through a set of zoom levels",2/13/17,1
lossyrob,yes it could be,2/13/17,1
cepkoCC,is there a way I can generate Metadata over the array without getting the kernel density first?,2/13/17,1
lossyrob,  val extent = CRSWorldExtent(LatLng).worldExtent,2/13/17,1
lossyrob,change to webm,2/13/17,1
cepkoCC,no change,2/13/17,1
cepkoCC,"even without reprojection and WebMercator everywhere, there still seems to be something standing in the way of an understanding between the data structure and Pyramid",2/13/17,1
cepkoCC,"ok, the problem gets a little stranger",2/14/17,1
cepkoCC,"I started fiddling around with the maxzoom settings in Pyramid.upLevels, and as soon as I cross zoom level 7, I start getting ""0 col"" errors",2/14/17,1
cepkoCC,"but from 6 down, I don't seem to be running into any real problems",2/14/17,1
pomadchin,"@cepkoCC sry, not following 100% your pyramiding steps; so the problem with levels > 6?",2/14/17,1
metasim,"@lossyrob @pomadchin @echeipesh Just wanted to let you know I'm still making plodding but measureable progress on the ""avro2spark"" work. I've figured out how to keep avro marshalling from happening for each extracted field, but currently having to revisit the nuances of how UNION types are treated by Databricks spark-avro.",2/14/17,1
pomadchin,@metasim yo! today vewed changes you did; looks promising,2/14/17,1
pomadchin,what_Ñés the problem right now? mb i can help you,2/14/17,1
metasim,"@pomadchin Just pushed my latest to the `recursive-encoding` branch. The problem has to do with a depth misalignment between the *Spark* schema translated by `spark-avro` and the actual Avro `GenericRecord`instances generated in the translation when `UNION` types are involved. The Spark schema has an extra `StructType` in the hierarchy declaring the various union options, whereas the Avro `GenericRecord` instances don't have any intermediate representation (because it supports union properly). I'm having to revisit some of my assumptions to be more flexible in this regard. When dealing with two different schemas and three different data formats, it's easy to get my wires crossed.",2/14/17,1
pomadchin,@metasim have you posted the problematic example? or can you post it some where? to dive quickly ,2/14/17,1
metasim,Yes. Look at `should handle Tile` in `astraea.demo.AvroDerivedSparkEncoderSpec` ,2/14/17,1
pomadchin,great!,2/14/17,1
metasim,@pomadchin [This background material](https://github.com/databricks/spark-avro#supported-types-for-avro---spark-sql-conversion) might be helpful.,2/14/17,1
metasim,"@pomadchin You'll probably want to apply this:```diff --git a/src/main/scala/astraea/demo/AvroDerivedSparkEncoder.scala b/src/main/scala/astraea/demo/AvroDerivedSparkEncoder.scalaindex 13b64c5..a59410a 100644--- a/src/main/scala/astraea/demo/AvroDerivedSparkEncoder.scala+++ b/src/main/scala/astraea/demo/AvroDerivedSparkEncoder.scala@@ -116,12 +116,7 @@ object AvroDerivedSparkEncoder {      override protected def nullSafeEval(input: Any): InternalRow = {       val avroRecord = input.asInstanceOf[GenericRecord]-      // We can't rely on the already computed Spark schema (as declared by DataType)-      // because UNION types result in a change in structure-      val sqlType = SchemaConverters.toSqlType(avroRecord.getSchema)---      convertRecord(avroRecord, sqlType.dataType.asInstanceOf[StructType])+      convertRecord(avroRecord, sparkSchema)     }   }```",2/14/17,1
metasim,"It won't fix it, just make tracking down the problem less confusing (it was a hack that actually just pushes the problem down the pipeline).",2/14/17,1
metasim,"Here's a GT+Avro specific question: Is [this approach to handling unions](https://github.com/locationtech/geotrellis/blob/master/spark/src/main/scala/geotrellis/spark/io/avro/AvroUnionCodec.scala#L41-L48), where you search for the specific format based on name, canonical Avro? Or is there another approach whereby you maintain the fact that the actual encoded data was a member of a union? If you take the result of `encode` and call `getSchema`, you'll see that all information about it being a member of a union has been removed. This is basically the problem I'm trying to resolve (the encoder schema doesn't match the converted object schema).",2/14/17,1
dowjones226,hello,2/14/17,1
dowjones226,just have a quick question re: the status of the geotrellis-osm-elevation project,2/14/17,1
dowjones226,"it looks like the last commit was 9 months ago, just want to see whether this project is still being maintained or updated",2/14/17,1
lossyrob,"Hi @dowjones226, that project was part of a Facebook Open Academy project, and has fallen into deprecation. However the functionality around it has continued on in GeoTrellis and other projects. What were you looking at it to do?",2/14/17,1
karalas,"Hi all, I am trying to do an ingest to accumulo but I am getting an io error at hdfswritestrategy of accumulowritestrategy (mkdirs failed to create /geotrellis-ingest/...). Maybe it has to do with accumulo permissions on hdfs, but I don't know how to fix this and I have been stuck for a long time. Furthermore, is it possible to see a catalog in accumulo (e.g. with command line or through monitor)? Any help would be greatly appreciated!",2/15/17,1
pomadchin,hi @karalas looks like permissions,2/15/17,1
pomadchin,"can you create this folder manually? ```hadoop fs -mkdir /geotrellis-ingest/```",2/15/17,1
pomadchin,"mb your user you are laucnhing your job, has no permissions to write on hdfs? or to create folders",2/15/17,1
pomadchin,"it is possible, there is accumulo shell: `accumulo shell`to scan table you can use: `scan -t <tablename>`",2/15/17,1
pomadchin,metadata table called `metadata`,2/15/17,1
karalas,"Hi @pomadchin, I have already run this command and also set 755 permission on this folder but I am still getting the same error.",2/15/17,1
pomadchin,what hdfs accumulo uses?,2/15/17,1
pomadchin,are you sure it is correct hdfs?,2/15/17,1
pomadchin,can you create whatever dir using job?,2/15/17,1
pomadchin,just write a simple spark job which creates only one dir,2/15/17,1
pomadchin,and you can get information about host / hdfs / configuration out of SparkContext ,2/15/17,1
pomadchin,you can also try to use SocketWriteStrategy instead of HDFS just to be sure that accumulo works correct,2/15/17,1
pomadchin,"besides, accumulo ui seems to work correct?",2/15/17,1
karalas,"Yes I have tried also with socketwritestrategy and I am not getting this error. I can see the created table in accumulo monitor. But then I don't know how to retrieve the ingested image to show on a wms. Although, I don't understand what socketwritestrategy does and that's why I was trying to implement with hdfswritestrategy.",2/15/17,1
pomadchin,to retrieve image you have to use `AccumuloLayerReader` and `AccumuloValueReader` ,2/15/17,1
pomadchin,http://geotrellis.readthedocs.io/en/latest/guide/tile-backends.html,2/15/17,1
pomadchin,we store geotiffs accroding to layout scheme you ingested; so your geotiff would be splited into tiles on some layout,2/15/17,1
pomadchin,"have a look at our demos: * https://github.com/geotrellis/geotrellis-chatta-demo* https://github.com/geotrellis/geotrellis-landsat-emr-demo",2/15/17,1
pomadchin,@schBen I am trying to reproduce your persistence error; but can_Ñét do it; what_Ñés your emr cluster configuration?,2/15/17,1
pomadchin,have you tried to update up to spark 2.1.0 and to use latest emr?,2/15/17,1
karalas,"Thank you @pomadchin  for your quick response! I downloaded and installed Accumulo 1.8.0 (along with hadoop-2.7.3 and zookeeper-3.4.9) separately. Accumulo and hdfs are running on their corresponding monitor ui, so I am assuming that I have set them correctly (although i get a message that the secondary namenodes are starting at [0.0.0.0]).  Is there another way to confirm the proper installation and the questions you asked me (what hdfs accumulo, correct hdfs, ..)?Actually, I was looking at https://github.com/geotrellis/geotrellis-landsat-tutorial and more specifically at IngestImage.scala. I was trying to modify this script in order to ingest a Sentinel-2 image to Accumulo. This way, i utilized the corresponding code for Accumulo from here http://geotrellis.readthedocs.io/en/latest/guide/tile-backends.html (and replaced FileAttributeStore, FileLayerWriter, FileLayerManager in the original script). As I have told you, this script ran with SocketWriteStrategy(), but with HdfsWriteStrategy, I get the ""mkdirs failed to create /geotrellis-ingest/..."" IO error. Then, i tried to run the server and viewer code from  https://github.com/geotrellis/geotrellis-landsat-emr-demo for my case. The codes run without error, but in the end I was not able to see my image on localhost. I understand that these questions are quite abstract, but i am new to all these technologies and maybe i am missing something fundamental. I would like also to clarify if Geotrellis has the ability to perform vector zonalstats on rasters (e.g., mean for the raster pixels that the vector covers, or mean for a vector in time-series rasters). Thank you very much in advance.",2/15/17,1
pomadchin,ah,2/15/17,1
pomadchin,the problem can be in Accumulo version,2/15/17,1
pomadchin,we don_Ñét support 1.8.0 100% as there could be some api changes / whatever,2/15/17,1
pomadchin,"have you tried to run it local, using file backend?",2/15/17,1
schBen,"Hello @pomadchin Thank you for coming back to this. It was an EMR-5.2.0 cluster with 2 workers (all m4.4xlarge) and Spark 2.0.2. No, I haven't tried it with newer versions as far as I know.",2/15/17,1
pomadchin,"Im trying all modes on emr 5.2.0; 4 nodes, m3.2xlarge",2/15/17,1
schBen,"So this is still an open issue for us, we just put it aside because we didn't know where to look now ",2/15/17,1
schBen,okay,2/15/17,1
pomadchin,can you review branch? ,2/15/17,1
pomadchin,i tried to port everyhting to reproduce bug: https://github.com/pomadchin/geotrellis-landsat-emr-demo/tree/fix/persistence-bug/ingest,2/15/17,1
pomadchin,create-cluster: https://github.com/pomadchin/geotrellis-landsat-emr-demo/blob/fix/persistence-bug/Makefile#L48-L64,2/15/17,1
pomadchin,persistence itself: https://github.com/pomadchin/geotrellis-landsat-emr-demo/blob/fix/persistence-bug/ingest/src/main/scala/demo/LandsatIngest.scala#L129,2/15/17,1
schBen,"hey this is great thank you!! I will sure look into this, I just don't know if I still have time today. But it looks really helpful ;-)",2/15/17,1
pomadchin,"@schBen doublecheck everything ^^ as i can_Ñét reproduce this issue; I_Ñéll try to reduce nodes amount, mb smth here",2/15/17,1
schBen,alright! :thumbsup: ,2/15/17,1
lossyrob,"@metasim @echeipesh wrote that bit, so would have the best answer.",2/15/17,1
karalas,"@pomadchin Unfortunately,  I get the same error with Accumulo v.1.7.2 as well. Maybe I'm missing something in accumulo-site xml? I have only added <value>hdfs://localhost:9000/accumulo</value> under  <name>instance.volumes</name>. I cannot run it local either. I use as the following parameters:input.json{  ""format"": ""multiband-geotiff"",  ""name"": ""example"",  ""cache"": ""NONE"",  ""backend"": {    ""type"": ""hadoop"",    ""path"": ""file:///tmp/rasters""  }}output.json{    ""backend"": {        ""type"": ""accumulo"",        ""path"": ""file:///tmp/catalog/"",		""profile"":""accumulo-local""    },    ""reprojectMethod"": ""buffered"",    ""pyramid"": true,    ""tileSize"": 256,    ""keyIndexMethod"": {        ""type"": ""zorder""    },    ""resampleMethod"": ""cubic-spline"",    ""layoutScheme"": ""zoomed"",    ""crs"": ""EPSG:3857""}backend-profiles.json{  ""backend-profiles"": [{    ""name"": ""accumulo-local"",    ""type"": ""accumulo"",    ""zookeepers"": ""localhost:2181"",    ""instance"": ""karalas"",    ""user"": ""root"",    ""password"": ""123""  }]}and run spark-submit    --class geotrellis.spark.etl.MultibandIngest    --master 'local[*]'    --driver-memory 8G    geotrellis-spark-etl-assembly-1.0.0-SNAPSHOT.jar    --input ""file:///tmp/json/input.json""    --output ""file:///tmp/json/output.json""    --backend-profiles ""file:///tmp/json/backend-profiles.json""As a result, a get the following error:input validation error:com.github.fge.jsonschema.core.report.ListProcessingReport: failure--- BEGIN MESSAGES ---error: instance type (object) does not match any allowed primitive type (allowed: [""array""])    level: ""error""    schema: {""loadingURI"":""#"",""pointer"":""""}    instance: {""pointer"":""""}    domain: ""validation""    keyword: ""type""    found: ""object""    expected: [""array""]---  END MESSAGES  ---",2/15/17,1
pomadchin,"incorrect jsons",2/15/17,1
pomadchin,"incorect input.json (in yout example it is object, should be array of objects):```json[     {        ""format"":""multiband-geotiff"",      ""name"":""example"",      ""cache"":""NONE"",      ""backend"":{           ""type"":""hadoop"",         ""path"":""file:///tmp/rasters""      }   }]```",2/15/17,1
pomadchin,"output.json looks correct: ```json{     ""backend"":{        ""type"":""accumulo"",      ""path"":""file:///tmp/catalog/"",      ""profile"":""accumulo-local""   },   ""reprojectMethod"":""buffered"",   ""pyramid"":true,   ""tileSize"":256,   ""keyIndexMethod"":{        ""type"":""zorder""   },   ""resampleMethod"":""cubic-spline"",   ""layoutScheme"":""zoomed"",   ""crs"":""EPSG:3857""}```",2/15/17,1
pomadchin,"backend-profiles.json looks correct too: ```json{     ""backend-profiles"":[        {           ""name"":""accumulo-local"",         ""type"":""accumulo"",         ""zookeepers"":""localhost:2181"",         ""instance"":""karalas"",         ""user"":""root"",         ""password"":""123""      }   ]}```",2/15/17,1
pomadchin,"so the typo i believe was in input.json: it should be array of objects, not a single object",2/15/17,1
pomadchin,"btw, in gitter, there are same markdown rules as in github `.md`: \```{lang}<code> \```",2/15/17,1
pomadchin,let me know if everything works with accumulo 1.7.x for you,2/15/17,1
echeipesh,"@metasim Ah, interesting. The normal workflow for Avro is actually to go from schema to encoders/decorders (through code-gen). The thing we did with geotrellis where we assemble encoders/decoders at compile time is not a standard solution. The lookup for subclass (a member of the union) being compatabable with the schema for the superclass (a union) is part of the Avro spec, so we actually can change that implementation to return full union without breaking any contracts. I still need to look at your code a bit closer to fully grok how this is breaking things though.",2/15/17,1
prahim,"Hi, I was wondering if there was a way to construct a `LayerQuery` to only retrieve tiles that contain data using the `CollectionLayerReader`",2/15/17,1
hjaekel,"hey @pomadchin , we tried your port to reprocuce our persistence bug, and yes we could reproduce it with your code. We used a cluster with a m4.4xlarge master and two m4.4xlarge workers. we started with `spark-submit --master yarn --driver-memory 51G --driver-cores 15 --num-executors 10 --executor-cores 3 --executor-memory 9G --deploy-mode client /mnt/ingest-assembly-0.1.0.jar MEMORY_AND_DISK`",2/15/17,1
hjaekel,Where is the cluster definition that your Makefile uses?,2/15/17,1
pomadchin,@hjaekel https://github.com/pomadchin/geotrellis-landsat-emr-demo/blob/fix/persistence-bug/Makefile#L48,2/15/17,1
pomadchin,https://github.com/pomadchin/geotrellis-landsat-emr-demo/blob/fix/persistence-bug/Makefile#L84,2/15/17,1
pomadchin,https://github.com/pomadchin/geotrellis-landsat-emr-demo/blob/fix/persistence-bug/scripts/configurations.json,2/15/17,1
pomadchin,and emr vars: https://github.com/pomadchin/geotrellis-landsat-emr-demo/blob/fix/persistence-bug/config-emr.mk,2/15/17,1
pomadchin,so you can do: make upload-code && make create-cluster && make ingest,2/15/17,1
hjaekel,"ok, I will try your cluster configuration. usually we use Python scripts for that purpose. Last time I used Makefiles is maybe 15 years ago :smile: ",2/15/17,1
pomadchin,`ëË\_( ó†)_/ëË`,2/15/17,1
pomadchin,w0w,2/15/17,1
pomadchin,reproduced,2/15/17,1
pomadchin,gotcha,2/15/17,1
pomadchin,interesting,2/15/17,1
pomadchin,"```spark-submit --conf spark.dynamicAllocation.enabled=true --master yarn --driver-memory 4G --executor-memory 4G --deploy-mode client /tmp/ingest-assembly-0.1.0.jar MEMORY_AND_DISK```",2/15/17,1
pomadchin,hm weird; i didn_Ñét notice it or there were no exceptions using `aws step`,2/15/17,1
hjaekel,great,2/15/17,1
hjaekel,"I could not reproduce it on your cluster (without Makefiles...), but I will try your spark-submit",2/15/17,1
karalas,"Hi, @pomadchin thanks for the advice. I installed Accumulo 1.7.2, but the problem persists:```17/02/15 20:34:30 ERROR Utils: Aborting taskjava.io.IOException: Mkdirs failed to create /geotrellis-ingest/f05a47c2-663c-4614-9a6a-98dff28855fd-jd1oEIdHPT/_temporary/0/_temporary/attempt_20170215203429_0027_r_000001_0 (exists=false, cwd=file:/tmp)	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:455)	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440)	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911)	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:892)	at org.apache.accumulo.core.file.rfile.RFileOperations.openWriter(RFileOperations.java:126)	at org.apache.accumulo.core.file.rfile.RFileOperations.openWriter(RFileOperations.java:106)	at org.apache.accumulo.core.file.DispatchingFileFactory.openWriter(DispatchingFileFactory.java:78)	at org.apache.accumulo.core.client.mapreduce.AccumuloFileOutputFormat$1.write(AccumuloFileOutputFormat.java:172)	at org.apache.accumulo.core.client.mapreduce.AccumuloFileOutputFormat$1.write(AccumuloFileOutputFormat.java:152)	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopDataset$1$$anonfun$12$$anonfun$apply$4.apply$mcV$sp(PairRDDFunctions.scala:1125)	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopDataset$1$$anonfun$12$$anonfun$apply$4.apply(PairRDDFunctions.scala:1123)	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopDataset$1$$anonfun$12$$anonfun$apply$4.apply(PairRDDFunctions.scala:1123)	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1341)	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1131)	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1102)	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)	at org.apache.spark.scheduler.Task.run(Task.scala:99)	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)	at java.lang.Thread.run(Thread.java:745)```",2/15/17,1
pomadchin,@karalas looks like hdfs settings; can you try geodocker-accumulo?,2/15/17,1
pomadchin,what os do u use,2/15/17,1
karalas,centos7,2/15/17,1
karalas,"I haven't tried geodocker, maybe i should give it a try..i hope the installation to be easier!",2/15/17,1
pomadchin,@karalas https://github.com/geodocker/geodocker-accumulo,2/15/17,1
echeipesh,"@prahim That_Ñés not really possible since the query planning happens before any I/O, this process of course doesn_Ñét know if tile it will fetch do or do not have any data. However, if the tile is all NoData values it will be very compressable and it can be filtered out as the first step. If this is something that is very important one can imagine achieving this using storgage that has some storage side processing, like Accumulo. (I don_Ñét kow how commited to this you are however).",2/15/17,1
echeipesh,@karalas What is your cluster environment? You may need to pay attention to the user that starts the spark job. It is somewhat common for the HDFS to be only accessible by either `hdfs` user or members of `hadoop` group. Likely you are starting spark as some other user and it is being denied. This kind of mismatch is most often the cause of these errors.,2/15/17,1
echeipesh,Looking over the user related games played in the geodocker images would also be good reference as @pomadchin is saying.,2/15/17,1
metasim,"@echeipesh Thanks for that background info. I'm going to look some more into a work around. The breakage is assoicated with assumptions the `spark-avro` library makes, where, at least when you use the library for IO, it maintains a wrapper around the resolved union with a `StructType` with nullable elements.",2/15/17,1
echeipesh,"Ah gotcha, it materialized the union so to speak.",2/15/17,1
echeipesh,Which I guess makes it harder to create a decoder from `StructType` to `T`,2/15/17,1
metasim,"Yeh, that's probably a better way of putting it.",2/15/17,1
metasim,"I can detect the case when I need the union materialized, but couldn't figure out how to manually create that outer wrapper.",2/15/17,1
metasim,"The `org.apache.avro.generic.GenericData`class has methods to manually create `Record`, `Array`, `Fixed`, `EnumSymbol`, etc. but nothing for a union.",2/15/17,1
echeipesh,"@cepkoCC Ok, we may have given you incomplete guidance. I believe if you changed the line 61 from `val ld = LayoutDefinition(CRSWorldExtent(WebMercator).worldExtent, tl)` to `val ld = layoutScheme.levelForZoom(10)` everything will totally work and I can totally explain what went sideways.",2/15/17,1
echeipesh,"@metasim what do you mean by _ÑÒouter wrapper_Ñù exactly, I_Ñém not sure I_Ñém synced.",2/15/17,1
metasim,"Yeh, I'm struggling for the right word.",2/15/17,1
metasim,"I'm working on a very simplistic example, that I hope will help explain the situation.",2/15/17,1
metasim,I'll ping you when I have it.,2/15/17,1
echeipesh,"sweet, I will delay my questions until then  :)",2/15/17,1
metasim,"@echeipesh, I think the best way to see what's going on is to run [this test](https://github.com/s22s/avro2spark/blob/master/src/test/scala/astraea/demo/AvroDerivedSparkEncoderSpec.scala#L57) and pay close attention to the error messages. You'll see that the Spark schema from `spark-avro` represents each union option as a nullable field with names like `member0`, `member1`, etc. If we knew which of the Avro union items matched we might be able to reconstruct that field, but I'm not seeing an easy way of determining the index of the schema that matched in `AvroUnionCodec`.",2/15/17,1
pomadchin,"@hjaekel yes, could reproduce on a job with not enough memory; it forces spark to shuffle mem => and yes error persists on Spark 2.1.0 too",2/16/17,1
pomadchin,thx for the report; looks like the fastest way to resolve it would be to create our own Logging trait,2/16/17,1
pomadchin,my thouhgt that it uses old and not serializable underlying slf4j from the spark classpath,2/16/17,1
hjaekel,"thank you @pomadchin  for investigatin this, great job! I think spark 2.1.0 uses slf4j 1.7.16 which is one year old, and 1.7.23 is the latest version. you think about creating a new trait like `org.apache.spark.internal.Logging`? I think this would solve the issue. How can we help?",2/16/17,1
pomadchin,"@hjaekel it_Ñés fine, yes, we_Ñéll return to our own trait usage i suppose",2/16/17,1
pomadchin,I_Ñéll create a PR today; just finishing investigation and trying to create unit tests,2/16/17,1
pomadchin,"@hjaekel btw, can you try `spark.driver.userClassPathFirst` flag usage?",2/16/17,1
pomadchin,Would it work in your case?,2/16/17,1
hjaekel,I never tried that flag,2/16/17,1
pomadchin,https://spark.apache.org/docs/latest/configuration.html,2/16/17,1
pomadchin,"yes, it is a new experimental flag, which forces spark to use deps from your extra fat jar (means no provied deps allowed)",2/16/17,1
hjaekel,but slf4j is not included in my fat jar :confused: What will it change?,2/16/17,1
pomadchin,in theory we can force jar to have one slf4j dep and to force spark to use it;,2/16/17,1
hjaekel,"please ping @schBen  or me when you PR is ready, maybe we can try with your changes",2/16/17,1
pomadchin,sure,2/16/17,1
hjaekel,"there will be a GeoTrellis 1.0.1 release soon, right? I hope your fix will be included :smile: ",2/16/17,1
pomadchin,@hjaekel ok; im still wondering why https://github.com/locationtech/geotrellis/pull/1960/files work,2/16/17,1
hjaekel,"me too. I really tried to add an test that fails, but I was not successful. here is what I tried:```package geotrellis.sparkimport java.io.{ByteArrayInputStream, ByteArrayOutputStream, ObjectInputStream, ObjectOutputStream}import geotrellis.proj4._import geotrellis.raster.testkit.{RasterMatchers, TileBuilders}import geotrellis.raster.{ArrayTile, DoubleCellType, DoubleConstantNoDataArrayTile, DoubleRawArrayTile, IntCellType, IntConstantNoDataArrayTile, IntRawArrayTile, Tile}import geotrellis.spark.util.KryoSerializerimport geotrellis.vector.{Extent, ProjectedExtent}import org.apache.spark.rdd.RDDimport org.apache.spark.storage.StorageLevelimport org.scalatest._class PersistenceTests extends FunSuite with Matchers with RasterMatchers with TestEnvironment with TileBuilders {  test(""Memory Serialization"") {    def intToDouble(tile: (ProjectedExtent, ArrayTile)): (ProjectedExtent, DoubleRawArrayTile) = {      (tile._1, tile._2.convert(DoubleCellType).asInstanceOf[DoubleRawArrayTile])    }    def doubleToInt(tile: (ProjectedExtent, ArrayTile)): (ProjectedExtent, IntRawArrayTile) = {      (tile._1, tile._2.convert(IntCellType).asInstanceOf[IntRawArrayTile])    }    val tile1: ArrayTile =      createTile(        Array( 1,1,1, 2,2,2, 3,3,          1,1,1, 2,2,2, 3,3,          4,4,4, 5,5,5, 6,6,          4,4,4, 5,5,5, 6,6 ),        8, 4)    val e1 = Extent(0.0, 0.0, 8.0, 10.0)    val tile2: ArrayTile =      createTile(        Array( 1,1,          1,1 ),        2, 2)    val e2 = Extent(0.0, 0.0, 2.0, 2.0)    val manyRdds: Seq[RDD[(ProjectedExtent, ArrayTile)]] = (1 to 1000) map ( _ =>      sc.parallelize(Seq((ProjectedExtent(e1, LatLng), tile1), (ProjectedExtent(e2, LatLng), tile2)))    )    manyRdds foreach (_.persist(StorageLevel.MEMORY_AND_DISK))    val doublerdds: Seq[RDD[(ProjectedExtent, DoubleRawArrayTile)]] = manyRdds.map(_.map(intToDouble))    val finalrdds: Seq[RDD[(ProjectedExtent, IntRawArrayTile)]] = doublerdds.map(_.map(doubleToInt))    finalrdds foreach (_.count)    finalrdds foreach (_.count)    val doublerdds2: Seq[RDD[(ProjectedExtent, DoubleRawArrayTile)]] = manyRdds.map(_.map(intToDouble))    val finalrdds2: Seq[RDD[(ProjectedExtent, IntRawArrayTile)]] = doublerdds2.map(_.map(doubleToInt))    finalrdds2 foreach (_.count)    finalrdds2 foreach (_.count)    //rdd.persist(StorageLevel.MEMORY_AND_DISK).count  }}```",2/16/17,1
pomadchin,me too,2/16/17,1
pomadchin,working on the same thing,2/16/17,1
pomadchin,AH,2/16/17,1
pomadchin,got it,2/16/17,1
pomadchin,"PR would be ready, and it would be included into 1.0.1 ;)",2/16/17,1
schBen,@pomadchin So could you make the test fail?,2/16/17,1
pomadchin,eh i thought so but no; trying to dig into spark test suits,2/16/17,1
schBen,ok ok :),2/16/17,1
diegonc,"hello, if I ingest a temporal-geotiff input, can I use value readers to get a list of tiles given an x/y/z? (i.e. all the history of a tile)",2/16/17,1
diegonc,it looks like I need to specify a time besides x/y to construct a key for the read method,2/16/17,1
diegonc,not sure if the best bet would be to compute a bounding box for the tile and use a layer reader instead,2/16/17,1
xarmatzis_twitter,One really very fast question,2/16/17,1
xarmatzis_twitter,"I need a really large csv file with lat, long fields to be geoprocessed like intersecting etc... what is the best procedure?",2/16/17,1
pomadchin,"@diegonc sure you can, just use `SpaceTimeKey` instead of `SpatialKey` in value readers; why not (see landsat emr demo for example)? see docs about space time key constructors `SpaceTimeKey(col, row, instant)`ValueReaders declarations are generic: https://github.com/locationtech/geotrellis/blob/master/spark/src/main/scala/geotrellis/spark/io/hadoop/HadoopValueReader.scala#L45",2/16/17,1
pomadchin,@xarmatzis_twitter you ask a very generic question o: ,2/16/17,1
pomadchin,you can do whatever you want in spark; you can load everything into mem into some typed objects and make any operations; ,2/16/17,1
diegonc,"I figured they are generic, but if I use `SpaceTimeKey`, don't I need to set a fixed time? which would get me only one tile",2/16/17,1
pomadchin,"than therare two ways: yes, define bounding box and use layer reader, if you want to read a slice",2/16/17,1
pomadchin,as it would be a `layer`,2/16/17,1
pomadchin,"the second: just populate space time key basing on your spatial key and some temopral range, after that use value reader",2/16/17,1
pomadchin,"you can try to use collections api, that would be faster for smaller datasets; but if your time range is not huge than keys populating can work faster",2/16/17,1
pomadchin,"hm, so the question you ask is rather good; not sure that we can consider using time ranges for value readers",2/16/17,1
pomadchin,@diegonc https://github.com/locationtech/geotrellis/blob/master/spark/src/main/scala/geotrellis/spark/io/hadoop/HadoopCollectionLayerReader.scala,2/16/17,1
pomadchin,to control threads you can use conf files: https://github.com/locationtech/geotrellis/blob/master/spark/src/main/resources/reference.conf#L21,2/16/17,1
pomadchin,so be careful :D ,2/16/17,1
diegonc,hehe,2/16/17,1
pomadchin,`default` means `Runtime.getRuntime.availableProcessors`,2/16/17,1
pomadchin,@xarmatzis_twitter so on what step do you have problems? O:,2/16/17,1
xarmatzis_twitter,@pomadchin let's say that I can read the csv file,2/16/17,1
xarmatzis_twitter,"``` val sparkSession = SparkSession.builder.    master(""local"")    .appName(""read csv example"")    .getOrCreate()  val csvInPath = ""data/demo.csv""  val df: DataFrame = sparkSession.read.option(""header"",""true"").            csv(csvInPath)```",2/16/17,1
xarmatzis_twitter,"now the df which is dataframe has some columns like lat, lon",2/16/17,1
pomadchin,"so you can map over this data frame and convert everything into Points ~```scaladf.map { row => geotrellis.vector.Point(row.getDouble(0), row.getDouble(1)) }```and after that your custom logic on top of it",2/16/17,1
hjaekel,"@pomadchin I tried the setting `spark.driver.userClassPathFirst`. This setting is only available in cluster mode (usually we use client mode). In cluster mode with `spark.driver.userClassPathFirst=false` I can reproduce the logger error, with `spark.driver.userClassPathFirst=true` I get strange errors (`java.lang.ClassCastException: org.apache.spark.deploy.yarn.YarnSparkHadoopUtil cannot be cast to org.apache.spark.deploy.SparkHadoopUtil`). It seems that the classpath is messed up so that Spark cannot find Yarn classes.",2/16/17,1
hjaekel,maybe that is why this setting is still experimental,2/16/17,1
pomadchin,@hjaekel eh ): so the unit test is not ready,2/16/17,1
pomadchin,i tried to use explicitly spark persistence engine: https://github.com/apache/spark/blob/master/core/src/test/scala/org/apache/spark/storage/BlockManagerSuite.scala,2/16/17,1
pomadchin,@diegonc feel free about questions :D collections api is still not a wide used thing ,2/16/17,1
pomadchin,you can find examples in chatta demo,2/16/17,1
pomadchin,it looks like +- the original rdd api,2/16/17,1
diegonc,"@pomadchin thanks! I'll go with bounding box first and see how it works because that's what I believe to understand better haha I'll have a look at collections after the demo.btw, to give some context,  I have a layer, say `history`, where I loaded singleband geotiffs for different points in time and I want to serve another layer, say `statistics`, with aggregated data which is computed on the fly based on all the tiles of `history` on a given x/y; I'll probably do ""pyramiding"" on the fly too (within some reasonable limits)",2/16/17,1
pomadchin,"@diegonc yep, you can do it; hm and why do you need pyramiding on the fly? you mean resampling?",2/16/17,1
pomadchin,"I am asking, because it was a frequent question how to overzoom some certain tile ^^_Ñé : https://github.com/locationtech/geotrellis/issues/1993",2/16/17,1
xarmatzis_twitter,"if I add spark-sql in my dependencies I get an error ""main"" java.lang.NoClassDefFoundError: com/google/common/collect/Maps ",2/16/17,1
xarmatzis_twitter,what is that now....?,2/16/17,1
pomadchin,hm,2/16/17,1
pomadchin,how do you add this dep? (:,2/16/17,1
xarmatzis_twitter,"  ""org.apache.spark""  %% ""spark-core""    % ""2.0.1"",    ""org.apache.spark"" %% ""spark-sql"" % ""2.0.1"",    ""io.spray""          %% ""spray-routing"" % ""1.3.3"",    ""io.spray""          %% ""spray-can""     % ""1.3.3"",    ""org.apache.hadoop""  % ""hadoop-client"" % ""2.6.1"",    ""org.locationtech.geotrellis"" %% ""geotrellis-spark"" % Version.gtVersion,    ""com.google.guava"" % ""guava"" % ""16.0.1""",2/16/17,1
pomadchin,can you try to kick out guava?,2/16/17,1
pomadchin,or you have to use it for some reasons?,2/16/17,1
diegonc,"@pomadchin because I store the data in only one zoom level, so when the client ask for lower zoom levels I need to scale the results of aggregating in the original levelactually, maybe that's not called pyramiding :smile: ",2/16/17,1
diegonc,awesome! the ticket look very useful. although I'll be doing  it in the other direction,2/16/17,1
pomadchin,@xarmatzis_twitter as hadoop-client already depends on guava 11 (afaik),2/16/17,1
xarmatzis_twitter,"Yes, I think so",2/16/17,1
xarmatzis_twitter,"if I remove guava, it can't compile... What can I do?",2/16/17,1
pomadchin,._.,2/16/17,1
pomadchin,"you can try: ```scala""org.apache.hadoop"" % ""hadoop-client"" % ""2.6.1"" excludeAll ExclusionRule(""com.google.guava""),""com.google.guava"" % ""guava"" % ""16.0.1""```",2/16/17,1
pomadchin,and why it can_Ñét compile? geotrellis does not use guava explicitly,2/16/17,1
xarmatzis_twitter,Now it does.... thx,2/16/17,1
xarmatzis_twitter,"Have I ever mention how much I hate sbt, scala and spark.....????????",2/16/17,1
pomadchin,"@xarmatzis_twitter oops, we love here functional programming; if you don_Ñét like scala you can try our experimental python bindings for fp haters :)",2/16/17,1
xarmatzis_twitter,I hate even more python...... do we have anything in R?,2/16/17,1
hjaekel,"@xarmatzis_twitter I agree with @pomadchin , we love Scala and functional programming. Why are you using  sbt, scala and spark if you don't like it?",2/16/17,1
xarmatzis_twitter,how can I use else Geotrellis?,2/16/17,1
hjaekel,"then you cannot use GeoTrellis, but you can try to reach your goals with other libraries",2/16/17,1
pomadchin,"@xarmatzis_twitter or you can write R bindings for people who don_Ñét like Scala; (there is rscala for R lang, to call Scala from R language) and there is R Spark with R lang support obviously",2/16/17,1
xarmatzis_twitter,"Well, after a month of research, I end up that the projects that geotrellis have demo  that are more close to the project that I am developing",2/16/17,1
xarmatzis_twitter,sorry if I was rude...,2/16/17,1
pomadchin,@hjaekel @schBen there is a PR: https://github.com/locationtech/geotrellis/pull/2017 can you try it?,2/16/17,1
pomadchin,https://github.com/locationtech/geotrellis/pull/2017/files#diff-367956a5978b95348ba8a6f6c27916beR12,2/16/17,1
hjaekel,"@pomadchin it's just the `@transient` that you added to `LazyLogging`, right? Looks good. We will test it in the next days.",2/16/17,1
pomadchin,"@hjaekel yes, just transient; i could not reproduce logger npes :))",2/16/17,1
hjaekel,"@pomadchin we could not reproduce them in unit tests, too. They just occur while executing bigger processes with limited memory",2/16/17,1
pomadchin,@hjaekel true; that_Ñés how i could reproduce it on emr; i meant that with this PR i can catch npes on emr,2/16/17,1
pomadchin,looks like mb some data loss (?),2/16/17,1
prahim,"```val filteredTiles: RDD[(SpatialKey, Tile)] = tileLayerRdd.filter(!_._2.isNoDataTile)            val newMetadata: (Int, TileLayerMetadata[SpatialKey]) =              TileLayerMetadata.fromRdd[SpatialKey, Tile, SpatialKey](filteredTiles, WebMercator, layoutScheme)```I am trying to create metadata from the filtered rdd but I get the following errors:```Error:(393, 70) No implicit view available from geotrellis.spark.SpatialKey => geotrellis.spark.tiling.TilerKeyMethods[geotrellis.spark.SpatialKey,geotrellis.spark.SpatialKey].              TileLayerMetadata.fromRdd[SpatialKey, Tile, SpatialKey](filteredTiles, WebMercator, layoutScheme)Error:(393, 70) not enough arguments for method fromRdd: (implicit evidence$17: geotrellis.spark.SpatialKey => geotrellis.spark.tiling.TilerKeyMethods[geotrellis.spark.SpatialKey,geotrellis.spark.SpatialKey], implicit evidence$18: geotrellis.spark.SpatialComponent[geotrellis.spark.SpatialKey], implicit evidence$19: geotrellis.spark.Boundable[geotrellis.spark.SpatialKey])(Int, geotrellis.spark.TileLayerMetadata[geotrellis.spark.SpatialKey]).Unspecified value parameters evidence$17, evidence$18, evidence$19.              TileLayerMetadata.fromRdd[SpatialKey, Tile, SpatialKey](filteredTiles, WebMercator, layoutScheme)```am I missing an import? thank you.",2/16/17,1
pomadchin,"@prahim it should be used like:```scalafromRdd[  {ProjectedExtent|TemporalProjectedExtent},   Tile,   {SpatialKey|SpaceTimeKey}]```",2/16/17,1
pomadchin,"usually this function called on sourceTilesRdd: ```scalardd: RDD[({ProjectedExtent|TemporalProjectedExtent}, Tile)]```",2/16/17,1
pomadchin,@prahim what are you trying to do? mb we can help you with smth?,2/16/17,1
karalas,"@echeipesh  @pomadchin  I was finally able to do the ingest to Accumulo by explicitly defining  ```HdfsWriteStrategy(""hdfs://localhost:9000/geotrellis-ingest"")```. Now I can see my table and its entries in Accumulo monitor.  As the next step, I would like to see my ingested image in a wms. So, I used the server and viewer of https://github.com/geotrellis/geotrellis-landsat-emr-demo, but finally i get an error ```Cross-Origin Request Blocked: The Same Origin Policy disallows reading the remote resource at http://localhost:8899/catalog. (Reason: CORS header 'Access-Control-Allow-Origin' missing).``` on a web client, or ```[iaas-system-akka.actor.default-dispatcher-5] [akka.actor.ActorSystemImpl(iaas-system)] Error during processing of request: 'Attribute times not found for layer Layer(name = ""sentinel2-tfl-samples"", zoom = 0)'. Completing with 500``` on  akka-http server. Any help would be much appreciated!",2/16/17,1
karalas,I tried to put the cors headers in ```router.scala``` but i had no luck.,2/16/17,1
karalas,By adding ```var  settings = CorsSettings.defaultSettings.copy(allowGenericHttpRequests = true)``` and then ```cors(settings)``` in the corresponding lines.,2/16/17,1
lossyrob,"@prahim hmm, you are trying to avoid getting tiles without data? Those tiles should actually just not be in the store - the ingest should avoid writing tiles that are all NoData",2/16/17,1
pomadchin,"@karalas launch server smth like: ```spark-submit --master yarn-client \     --driver-memory 5G --driver-cores 4 \     --executor-cores 2 --executor-memory 5G \     --conf spark.dynamicAllocation.enabled=true \     SERVER_JAR accumulo accumulo `hostname` root secret```",2/16/17,1
pomadchin,"it supports different backends, so it depends on how you launched it",2/16/17,1
pomadchin,so what was your try (command you launched server)? :),2/16/17,1
echeipesh,"@karalas That is not always enough. Some browsers trigger CORS content negotiation. We use `""ch.megard"" %% ""akka-http-cors"" % _ÑÒ0.1.10_Ñù` to get the cors direcectives for the emr demo: https://github.com/geotrellis/geotrellis-landsat-emr-demo/blob/a7ef452bca79b1ab0c6c788e298de67a87bcdc27/server/src/main/scala/demo/Router.scala#L66",2/16/17,1
prahim,@lossyrob really? it seems that ingest is writing nodata tiles to the store ... ,2/16/17,1
prahim,@pomadchin Thank you.. let me try that out,2/16/17,1
lossyrob,"ah, really. Perhaps by default it doesn_Ñét remove nodata tiles, but that_Ñés a common step to filter before writing",2/16/17,1
prahim,is there a better way to filter it than the way I am currently doing it? @lossyrob ,2/16/17,1
lossyrob,you can do it when you are writing the layer into the backend,2/16/17,1
lossyrob,you can check the tile values for a method_Ñ_one sec,2/16/17,1
lossyrob,https://github.com/locationtech/geotrellis/blob/master/raster/src/main/scala/geotrellis/raster/Tile.scala#L223,2/16/17,1
prahim,hmm I think that is what I am doing ```tileLayerRdd.filter(!_._2.isNoDataTile)``` @lossyrob ,2/16/17,1
lossyrob,"yeah. Are you doing that on the fetch side? That will still bring down the tiles. When the layers are initially saved, if the same sort of filter is done, it_Ñéll avoid storing them at all",2/16/17,1
prahim,"Yea it's done on the write side-- I am not sure how to make the new metadata to match my filtered rdd though ...  it can't find the definition for rdd if I do `fromRdd[ProjectedExtent, Tile, SpatialKey]` ... it finds it if I do `fromRdd[SpatialKey, Tile, SpatialKey]` but then has errors regarding implicits",2/16/17,1
lossyrob,`tileLayerRdd.withContext(rdd => rdd.filter(!_._2.isNoDataTile))`,2/16/17,1
prahim,thank you!!!,2/16/17,1
lossyrob,no problem!,2/16/17,1
prahim,now if I read it out I would expect metadataReader.read(layerId).spatialKeys to only contain spatialKeys for the tiles I saved -- is that not true?,2/16/17,1
prahim,I have 29 tiles I saved but I have thousands of spatialKeys when I read the metadata back,2/16/17,1
lossyrob,"The metadata will describe an area that can cover more keys, but only those keys that exist will be read in",2/16/17,1
prahim,so if I use the `CollectionLayerReader` it should only read for the existing keys?,2/16/17,1
lossyrob,yup,2/16/17,1
prahim,hmm it doesn't automatically do it if I just call collectionlayerReader.read(layerId) but we don't do anything with the spatial keys when reading ... I tried to add some logic by getting the metadata.spatialKeys and only reading those keys but all keys including keys that don't exist come back in that array,2/16/17,1
prahim,should I set filterIndexOnly ? what would that do,2/16/17,1
prahim,I think there is a problem with the way we are storing our spatial keys. I will modify that. Thank you for your help!,2/16/17,1
lossyrob,Hm interesting,2/16/17,1
lossyrob,The spatial keys that are stored should only be the ones that are written,2/16/17,1
lossyrob,filterIndexOnly will bring back some false positives for bounding box/polygon queries,2/16/17,1
prahim,we were deriving the spatialkeys based on the extent so it was storing more than actually existed,2/16/17,1
prahim,I fixed that issue and now we are only storing the spatialkeys that actually exist ... the CollectionLayerReader didn't read based off the spatialkeys though ...,2/16/17,1
lossyrob,"how do you mean, based off the spatial keys?",2/16/17,1
prahim,"I have 29 spatial keys but it was trying to fetch tiles for over 2000 spatialkeys .. If I explicitly read for each spatial key then it only reads for the 29 existing keys like:```val spatialKeys: Array[SpatialKey] = tileLayerMetadata.spatialKeysval tilesSeq = spatialKeys.map { spatialKey =>        (spatialKey, cachingTileReader.read(layerId, spatialKey))      }```",2/16/17,1
prahim,now the metadata returns the right set of keys after fixing the storing,2/16/17,1
prahim,but if I just use collectionLayerReader.read(layerId) it tries to read for keys that don't exist,2/16/17,1
prahim,it is using the `S3CollectionReader` btw,2/16/17,1
lossyrob,"ah it tries to read keys that don_Ñét exist, but doesn_Ñét actually read them",2/16/17,1
prahim,yea so it 404s,2/16/17,1
lossyrob,and you_Ñére trying to avoid that. for performance?,2/16/17,1
prahim,yes! too slow,2/16/17,1
lossyrob,gotcha,2/16/17,1
prahim,it that an improvement that could get added?,2/16/17,1
lossyrob,I_Ñém wondering why there_Ñés so many missing spatial keys?,2/16/17,1
lossyrob,is it one layer that has sparsely populated areas?,2/16/17,1
prahim,yes there's data in the upper left corner and lower right corner and nothing in the middle,2/16/17,1
lossyrob,"yeah, that_Ñés a use case we haven_Ñét thought through enough TBH",2/16/17,1
lossyrob,and they shouldn_Ñét be 2 different layers,2/16/17,1
lossyrob,?,2/16/17,1
prahim,yea we treat it as one layer for how we use the information,2/16/17,1
lossyrob,it_Ñés a good point,2/16/17,1
lossyrob,"and there are solutions, but the ones I can immediately think of go a bit deep",2/16/17,1
echeipesh,"yeap, potentially using the `HadoopValueReader` with `s3a://` URI would be a way around this, because it stores mapfiles which are sorted, packed and indexed. But using it may be a little tricky because of common dependency clashes with EMR. Also its not a well benchmarked approach. But in theory it would be more efficient for these very sparse layers.",2/16/17,1
lossyrob,"Potentially we could store key range metadata optionally. Which would be an unnecessary hit for full layers, but a good fit for sparse layers",2/16/17,1
lossyrob,Sparse layer support could be a feature for 1.1,2/16/17,1
prahim,Yay! Does 1.1 have an estimated release date?,2/16/17,1
lossyrob,End of march or april,2/16/17,1
lossyrob,and we_Ñéd appreciate help on it :),2/16/17,1
lossyrob,"If you want to create a github issue, we can start a discussion there",2/16/17,1
prahim,okay sounds good ... we need a way to get ours working right now.. I can submit a PR ,2/16/17,1
lossyrob,I can think of a workaround,2/16/17,1
lossyrob,so if you know the extents of the sparse sets,2/16/17,1
lossyrob,you can store that in the attribute metadata,2/16/17,1
lossyrob,"and if you pull that down, use that as part of a MultiPolygon Intersection layer filter",2/16/17,1
prahim,ohh ,2/16/17,1
prahim,can I generate a multipolygon based on spatial keys?,2/16/17,1
prahim,instead of extent?,2/16/17,1
lossyrob,extent has a `.toPolygon`,2/16/17,1
lossyrob,"if you string a sequence of them together, you can create a `MultiPolygon` out of them",2/16/17,1
prahim,"I don't know how to get the extents of the sparse sets -- I just have the extent of the whole thing.... I ended up parallelizing the reading with ```val tilesSeq = spatialKeys.par.map { spatialKey =>        (spatialKey, cachingTileReader.read(layerId, spatialKey))      }```it seems pretty fast ",2/16/17,1
xarmatzis_twitter,"Hi, I would like to ask a question, if I have a Seq[FeaturePoint[T]] how can I group by a property of that sequence?",2/17/17,1
xarmatzis_twitter,"``` val kv = List.map(_.mapData(_.ID)) ??????``` ",2/17/17,1
xarmatzis_twitter,I just want a list of unique IDs,2/17/17,1
xarmatzis_twitter,"Sorry, it was easy ",2/17/17,1
xarmatzis_twitter,"```val kv: Seq[Int] = List.map(_.data.ID).distinct```",2/17/17,1
xarmatzis_twitter,Hi I also have one quick question... What is SymDifference?,2/17/17,1
xarmatzis_twitter,https://github.com/locationtech/geotrellis/blob/a00c35b928e96083188d91734252d66574e3b4a7/vector-test/src/test/scala/spec/geotrellis/vector/PolygonSpec.scala#L358 ,2/17/17,1
xarmatzis_twitter,I need to clip a polygon geometry with a Seq[MultiPolygon]  ,2/17/17,1
pomadchin, SymDifference ~ symmetric difference,2/17/17,1
diegonc,"```java.util.NoSuchElementException: key not found: 663864319        at scala.collection.MapLike$class.default(MapLike.scala:228)        at scala.collection.AbstractMap.default(Map.scala:59)        at scala.collection.MapLike$class.apply(MapLike.scala:141)        at scala.collection.AbstractMap.apply(Map.scala:59)        at geotrellis.raster.render.IntCachedColorMap$$anonfun$mapColorsToIndex$1.apply$mcVI$sp(ColorMap.scala:264)        at geotrellis.raster.histogram.FastMapHistogram.foreachValue$mcI$sp(FastMapHistogram.scala:264)        at geotrellis.raster.render.IntCachedColorMap.mapColorsToIndex(ColorMap.scala:264)        at geotrellis.raster.render.png.IndexedPngEncoding.convertColorMap(PngColorEncoding.scala:49)        at geotrellis.raster.render.PngRenderMethods$class.renderPng(PngRenderMethods.scala:49)        at geotrellis.raster.package$withTileMethods.renderPng(package.scala:53)        at geotrellis.raster.render.PngRenderMethods$class.renderPng(PngRenderMethods.scala:57)        at geotrellis.raster.package$withTileMethods.renderPng(package.scala:53)```while calling `raster.tile.band(band).renderPng(ColorRamps.BlueToRed)` where raster is a MultibandRaster coming from stitch, floating point cell typedoes the exception ring a bell? I can't figure why this is happening :(",2/17/17,1
karalas,"Hi all, I am trying to run Chatta example, but I can't. When I run ```make build``` in the root folder I get the following errors:```cd geotrellis && ./sbt assembly[info] Loading project definition from /home/kkaralas/Documents/Geotrellis projects/geotrellis-chatta-demo-master/geotrellis/project[info] Set current project to geotrellis-chatta-demo (in build file:/home/kkaralas/Documents/Geotrellis%20projects/geotrellis-chatta-demo-master/geotrellis/)[info] Compiling 10 Scala sources to /home/kkaralas/Documents/Geotrellis projects/geotrellis-chatta-demo-master/geotrellis/target/scala-2.11/classes...[error] /home/kkaralas/Documents/Geotrellis projects/geotrellis-chatta-demo-master/geotrellis/src/main/scala/geotrellis/chatta/ChattaServiceRouter.scala:75: value convert is not a member of Seq[(geotrellis.spark.SpatialKey, geotrellis.raster.Tile)] with geotrellis.spark.Metadata[geotrellis.spark.TileLayerMetadata[geotrellis.spark.SpatialKey]][error]                   reader.read[SpatialKey, Tile, TileLayerMetadata[SpatialKey]](layerId(layer)).convert(ShortConstantNoDataCellType) * weight[error]                                                                                                ^[error] /home/kkaralas/Documents/Geotrellis projects/geotrellis-chatta-demo-master/geotrellis/src/main/scala/geotrellis/chatta/Model.scala:33: value * is not a member of Seq[(geotrellis.spark.SpatialKey, geotrellis.raster.Tile)] with geotrellis.spark.Metadata[geotrellis.spark.TileLayerMetadata[geotrellis.spark.SpatialKey]][error]           reader.read[SpatialKey, Tile, TileLayerMetadata[SpatialKey]](layer, new LayerQuery[SpatialKey, TileLayerMetadata[SpatialKey]].where(Intersects(bounds))) * weight[error]                                                                                                                                                                    ^[error] /home/kkaralas/Documents/Geotrellis projects/geotrellis-chatta-demo-master/geotrellis/src/main/scala/geotrellis/chatta/Model.scala:35: type mismatch;[error]  found   : Seq[Nothing][error]  required: ?{def localAdd: ?}[error] Note that implicit conversions are not applicable because they are ambiguous:[error]  both method withTileSeqMethods in package raster of type (self: Traversable[geotrellis.raster.Tile])geotrellis.raster.withTileSeqMethods[error]  and method withLocalTileRDDSeqMethods in trait Implicits of type [K](self: Traversable[org.apache.spark.rdd.RDD[(K, geotrellis.raster.Tile)]])(implicit keyClassTag: scala.reflect.ClassTag[K])geotrellis.spark.withLocalTileRDDSeqMethods[K][error]  are possible conversion functions from Seq[Nothing] to ?{def localAdd: ?}[error]         .toSeq[error]          ^[error] /home/kkaralas/Documents/Geotrellis projects/geotrellis-chatta-demo-master/geotrellis/src/main/scala/geotrellis/chatta/Model.scala:36: value localAdd is not a member of Seq[Nothing][error] possible cause: maybe a semicolon is missing before `value localAdd'?[error]         .localAdd[error]          ^[error] /home/kkaralas/Documents/Geotrellis projects/geotrellis-chatta-demo-master/geotrellis/src/main/scala/geotrellis/chatta/Model.scala:49: value mask is not a member of Seq[(geotrellis.spark.SpatialKey, geotrellis.raster.Tile)] with geotrellis.spark.Metadata[geotrellis.spark.TileLayerMetadata[geotrellis.spark.SpatialKey]][error]           val masked = raster.mask(polygon)[error]                               ^[error] 5 errors found[error] (compile:compileIncremental) Compilation failed[error] Total time: 7 s, completed Feb 17, 2017 7:38:37 PMmake: *** [build] Error 1```(I have converted also ```val gtVersion = ""1.0.0""```in ```build.sbt``` ). What am I doing wrongly? Thank you",2/17/17,1
pomadchin,@karalas you have to use latest geotrellis from master branch ):,2/17/17,1
pomadchin,chatta demo depends on some imortant collection api improvements :),2/17/17,1
karalas,yes,2/17/17,1
pomadchin,just publish geotrellis master branch; and build chatta demo; it should work,2/17/17,1
karalas,"@pomadchin  I am sorry, but I don't understand what is ""publish geotrellis master branch"". I have downloaded geotrellis master branch locally and then I ran ```./sbt ""project spark-etl"" assembly``` in order to produce the dependencies and the corresponding jar.",2/17/17,1
pomadchin,"oh; publish, in terms of java / scala world means to publish into local artefacts repository; sbt has a special command called `publish`; to publsih gt it_Ñés enough to run [this](https://github.com/locationtech/geotrellis/blob/master/scripts/publish-local.sh) script",2/17/17,1
pomadchin,"the whole build process for you would look like: ```bashgit clone https://github.com/locationtech/geotrellis && \cd geotrellis && ./scripts/publish-local.sh```",2/17/17,1
pomadchin,"gt would be build and published into local repo: ```bashls ~/.ivy2/local/org.locationtech.geotrellis/```",2/17/17,1
lossyrob,chatta demo doesn_Ñét run on 1.0?,2/17/17,1
diegonc,"about the color ramp, I was able to make it work using `histogramDouble` and `DoubleColorMap`btw, aren't the branches below reversed? i.e. taking an Int histogram for the floating point case. Or am I off again? :stuck_out_tongue: ```  def renderPng(colorRamp: ColorRamp): Png = {    if(self.cellType.isFloatingPoint) {      val histogram = self.histogram      val quantileBreaks = histogram.quantileBreaks(colorRamp.numStops)      renderPng(new IntColorMap(quantileBreaks.zip(colorRamp.colors).toMap).cache(histogram))    } else {      val histogram = self.histogramDouble      renderPng(ColorMap.fromQuantileBreaks(histogram, colorRamp))    }  }```",2/17/17,1
pomadchin,"@lossyrob nope, due to collections api extension",2/17/17,1
echeipesh,"@diegonc yeah, that kind of looks like a bug _Ñ_ is it biting you right now ?",2/17/17,1
diegonc,"yep, I hit it a few hours ago. But I could workaround it by implementing it with the `Double` version of the code; so nothing serious :smile: ",2/17/17,1
echeipesh,"hehe, would you be interested in putting together a PR with the unit test and a fix ?",2/17/17,1
diegonc,"sure, it will have to wait until next week though. I'm on a tight schedule right now :worried: ",2/17/17,1
echeipesh,"Ah no worries. We might clean it up by then, trying to get 1.0.1 with some bug fixes soonish.",2/17/17,1
xarmatzis_twitter,@karalas Did u run chatta demo successfully? Do u want any help?,2/18/17,1
diegonc,"I'm observing a weird behaviour in Spark tasks from the save stage of ETL. Using the web UI, I can get a thread dump and there's a lot of recursion when writing to Hadoop; one sample stack trace is in the linked pastebin below",2/18/17,1
diegonc,http://pastebin.com/GQbfExnX,2/18/17,1
diegonc,It's comes from the toVector call in HadoopRDDWriter.scala#L99,2/18/17,1
diegonc,is that normal?,2/18/17,1
pomadchin,"sure, the more stacktraces you have the better result would be (no, it's a joke) ",2/18/17,1
pomadchin,:d ,2/18/17,1
diegonc,:),2/18/17,1
pomadchin,"in curious, what spark cluster do you have? (Versions of spark and hadoop) ",2/18/17,1
pomadchin,and what serialization do you use during the ingest process?,2/18/17,1
diegonc,spark 2.1.0 and hadoop 2.7.3,2/18/17,1
pomadchin,looks like serializer could not drive a correct type ,2/18/17,1
pomadchin,can you try with spark 2.0.2?,2/18/17,1
pomadchin,"and ye serialization, try both - with and without kryo",2/18/17,1
pomadchin,and a small example of a failing code,2/18/17,1
diegonc,"serializer...I think it's Kryo. spark context is created with `SparkUtils.createSparkContext(""TemporalSinglebandIngest"", new SparkConf(true))`",2/18/17,1
diegonc,I'll try spark 2.0.2,2/18/17,1
pomadchin,and scala 2.11.8?,2/18/17,1
pomadchin,do you use some special additional deps in your fat jar assembly?,2/18/17,1
diegonc,"yes, scala 2.11.8. deps are:```""org.locationtech.geotrellis"" %% ""geotrellis-spark-etl"" % ""1.0.0""""org.apache.spark"" %% ""spark-core"" % ""2.1.0"" % ""provided""```",2/18/17,1
diegonc,"btw, this is with tile size of 128",2/18/17,1
rgwozdz,"Can anyone direct me to an example of how to do a ""programattic"" ingest?  Use case is a REST API in which a POST request uploads a tiff and the controller needs to trigger an ingest.",2/19/17,1
pomadchin,@rgwozdz i think [this](https://github.com/geotrellis/geotrellis-landsat-tutorial/blob/master/src/main/scala/tutorial/IngestImage.scala) would be helpful for you,2/19/17,1
rgwozdz,Awesome!  thanks very much,2/19/17,1
pomadchin,"but be carefull; if you ingest a tiff into some layer and after that to ingest another tiff (that means a sort of an update operation), you need to predefine the whole key space for this layer; and after ingesting the first tiff to use LayerUpdater for the backend you use",2/19/17,1
rgwozdz,"ok, thanks - I imagine that addition note just saved me quite a bit of head scratching",2/19/17,1
pomadchin,https://github.com/locationtech/geotrellis/blob/master/spark/src/main/scala/geotrellis/spark/io/LayerWriter.scala#L43 ,2/19/17,1
pomadchin,you can pass here KeyIndex explicitly with KeySpace bounds you want,2/19/17,1
pomadchin,keyspace in this context means layer key bounds,2/19/17,1
pomadchin,"it_Ñés _ÑÒkeybounds of all possible tiles on this layer""",2/19/17,1
pomadchin,it_Ñés necessary for layer _ÑÒupdate_Ñù ~ for correct new tiles indexing,2/19/17,1
rgwozdz,"ok, much appreciated.  The is new territory for me, so its a big help",2/19/17,1
dorukozturk,hi I am getting User class threw exception: java.lang.RuntimeException: Failed to connect to zookeeper () within 2x zookeeper timeout period 30000  on the emr when I try to ingest something. I did the echo ruok thing and get back the expected response from the master machine. Does this mean the bootstrap script for some reason failed?,2/21/17,1
pomadchin,"@dorukozturk in case of accumulo we don't work explicitly with zookeepers, it's a driver work as you could notice; so yes, smth is wrong with your cluster setup.",2/21/17,1
dorukozturk,"ok, I see. I will try to ssh into the master machine and run the bootstrap.sh manually just to see",2/21/17,1
dorukozturk,thank you,2/21/17,1
pomadchin,It's your own or you used our?,2/21/17,1
dorukozturk,I slightly modified the emr demo,2/21/17,1
dorukozturk,but your demo was working so I am guessing I am skipping somehing in the bootstrapping step. Because the cluster get launched but when I submit my ingest job I get that timeout error,2/21/17,1
dorukozturk,where would be the trace if the bootstrap step is failed,2/21/17,1
dorukozturk,?,2/21/17,1
pomadchin,on master at first (emr master logs),2/21/17,1
pomadchin,do you see accumulo ui?,2/21/17,1
pomadchin,http://master-address:50095/,2/21/17,1
Charmatzis,geotrellis with R ,2/21/17,1
Charmatzis,https://github.com/jeffreyhanson/geotrellis ,2/21/17,1
Charmatzis,very impressive !!!!,2/21/17,1
pomadchin,"yes, this guy wrote here about his work; but his benchmark doesn_Ñét include spark support; and gt is a _distributed_ framework :)",2/21/17,1
Charmatzis,"Hi, to everyone. How can I create a array of Feature Points?",2/21/17,1
Charmatzis,"```val featureList: Array[(Point, (Int, Int, Int, Int, Timestamp))] = points.map { p => (geotrellis.vector.Point(p.LON, p.LAT), (p.SHIP_ID, p.COURSE, p.HEADING, p.SPEED, p.TIMESTAMP)) }```",2/21/17,1
Charmatzis,I got it ,2/21/17,1
Charmatzis,"```val featurePoints = points.map{ p => (PointFeature(Point(p.LON, p.LAT), p))}```",2/21/17,1
lossyrob,well done :),2/21/17,1
Charmatzis,thx!!!,2/21/17,1
Charmatzis,Well now I am trying to calculate the distance detween these points.,2/21/17,1
Charmatzis,I know that I can do something like that.... ,2/21/17,1
Charmatzis,"```pointsFeatureCollection.zip(collection.tail).map(x=>x._1.geom.distance(x._2.geom))```",2/21/17,1
Charmatzis,"but the Point does not have crs to calculate the right distance, or am I wrong?",2/21/17,1
pomadchin,this distance would be 2-dimensional euclidean distance; probably you need a sort of a real world distance?,2/21/17,1
Charmatzis,"I see, so if my points are in wgs'84 i have first to reprojected to let's say WebMercator",2/21/17,1
pomadchin,"hm, i think it_Ñés possible ofc to calculate that distance using plain geometries but it would be rather rough and you need to reproject points into some certain projection",2/21/17,1
pomadchin,the correct way would be to use `JTS.orthodromicDistance` function and / or `GeodeticCalculator`,2/21/17,1
pomadchin,"Mb that is what you want:* http://docs.geotools.org/stable/userguide/library/referencing/calculator.html* http://docs.geotools.org/stable/userguide/library/api/jts.html",2/21/17,1
Charmatzis,"yes, i understand what you are saying. thank you!",2/21/17,1
lossyrob,this might be helpful https://github.com/locationtech/geotrellis/blob/master/spark/src/main/scala/geotrellis/spark/tiling/ZoomedLayoutScheme.scala#L71,2/21/17,1
Charmatzis,@lossyrob thx! I used a different aproach,2/21/17,1
Charmatzis,"```    def deg2rad(deg: Double) = deg * Math.PI / 180.0    def rad2deg(rad: Double) = rad / Math.PI * 180.0    def getDistanceMeters(lat1: Double, lon1: Double, lat2: Double, lon2: Double) = {      val theta = lon1 - lon2      val dist = Math.sin(deg2rad(lat1)) * Math.sin(deg2rad(lat2)) + Math.cos(deg2rad(lat1)) *        Math.cos(deg2rad(lat2)) * Math.cos(deg2rad(theta))      Math.abs(        Math.round(          rad2deg(Math.acos(dist)) * 60 * 1.1515 * 1.609344 * 1000)      )    }    def getDistanceBetween2Points(begin:Point, end:Point)= {      getDistanceMeters(begin.y, begin.x, end.y, end.x)    }```",2/21/17,1
dorukozturk,"@pomadchin Hey, so I really do not understand what is wrong. So when I use your guys' input, output backend-profiles json files (for landsat type) it works on emr. But when I use my json files i see ""User class threw exception: java.lang.RuntimeException: Failed to connect to zookeeper () within 2x zookeeper timeout period 30000"". I tried both s3 and hdfs inputs and they both give me the same error. Little bit more information: I am trying to use the spark-etl assembly and trying to do a temporal ingest. Btw the temporal ingest works locally so there is nothing wrong with my tiffs( everything is there including tags). Any ideas?  ",2/21/17,1
pomadchin,double check zookeeper address,2/21/17,1
pomadchin,i used a hack / feature for emr; in code im setting zookeeper address = hdfs namenode address,2/21/17,1
dorukozturk,"oh ok, so in my case it is empty",2/21/17,1
pomadchin,yes,2/21/17,1
pomadchin,let me point you to the code,2/21/17,1
pomadchin,https://github.com/geotrellis/geotrellis-landsat-emr-demo/blob/master/ingest/src/main/scala/demo/etl/landsat/package.scala#L50-L98,2/21/17,1
dorukozturk,ok now it makes sense. What about getting the namenode address? when I do netstat it does not show the application names ,2/21/17,1
pomadchin,ah in emr case that would be just master node address,2/21/17,1
pomadchin,so took that address from `sc.hadoopConfiguration`,2/21/17,1
pomadchin,it takes address from `core-site.xml` as its in EMR Spark classpath,2/21/17,1
dorukozturk,"yepp, that worked !!! Thanks @pomadchin ",2/21/17,1
pomadchin,@dorukozturk ;) glad that finally it worked,2/21/17,1
metasim,"@echeipesh Just FYI: I just added support and tests for `SpatialKey`, `SpaceTimeKey`, `KeyValueRecordCodec`, and `VectorTile` to `AvroDerivedSparkEncoderSpec`.",2/21/17,1
dorukozturk,are there any examples for writing the catalog to s3? What does the backend-profiles and output.json look like for s3 case?,2/21/17,1
dorukozturk,https://github.com/locationtech/geotrellis/blob/master/docs/guide/etl.rst  ok it is here ,2/21/17,1
dorukozturk,sorry,2/21/17,1
karalas,"Hi all, I am having trouble running the Chatta demo. I have managed to ```make ingest``` the files using the Cassandra as background and I can see the corresponding catalogs under ```/data/chatta-demo```, but when I run ```make server``` I cannot see the desired images on the map on localhost getting the following error:```fatal: Not a git repository (or any of the parent directories): .gitcd geotrellis && java -cp target/scala-2.11/geotrellis-chatta-demo-assembly-0.1-SNAPSHOT.jar geotrellis.chatta.Mainlog4j:WARN No appenders could be found for logger (com.datastax.driver.core.SystemProperties).log4j:WARN Please initialize the log4j system properly.log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.[ERROR] [02/21/2017 19:49:46.569] [chatta-demo-akka.actor.default-dispatcher-12] [akka.actor.ActorSystemImpl(chatta-demo)] Error during processing of request: 'Layer Layer(name = ""ImperviousSurfacesBarrenLandsOpenWater"", zoom = 9) not found in the catalog'. Completing with 500 Internal Server Error response.```",2/21/17,1
pomadchin,@karalas what is ls `/data/chatta-demo` ?,2/21/17,1
pomadchin,think you are mixing smth; you said that you used cassandra as a backend; but you see tiles here? `/data/chatta-demo` ?,2/21/17,1
pomadchin,that is true only if you used file backend o:,2/21/17,1
pomadchin,there is a setting https://github.com/geotrellis/geotrellis-chatta-demo/blob/master/geotrellis/src/main/resources/application.conf#L5 to specify backend you want to use; settigns examples for each backend you can notice below (link for cassandra ): https://github.com/geotrellis/geotrellis-chatta-demo/blob/master/geotrellis/src/main/resources/application.conf#L21-L32,2/21/17,1
dorukozturk,"hi, is this the right place to ask our geopyspark questions ?",2/22/17,1
pomadchin,@dorukozturk i think yes; feel free to write your questions :),2/22/17,1
dorukozturk,"ok I am getting this geotrellis.spark.io.package$ValueNotFoundError: Value with key SpatialKey(4,7) not found for layer Layer(name = ""srtm"", zoom = 4) when I try to run the examples/s3-tile-server.py",2/22/17,1
dorukozturk,I ingested couple of sample srtm files to s3 and I am trying to create a s3 tile server,2/22/17,1
pomadchin,@dorukozturk I_Ñéll ping @echeipesh or @jbouffard ,2/22/17,1
dorukozturk,ok thank you very much !!!,2/22/17,1
jbouffard,"@dorukozturk Unfortunately, there are no current examples of the most up-to-date version of GeoPySpark. Can I see an example of what your code looks like?",2/22/17,1
dorukozturk,"hey, I just changed the bucket name prefix and layer name on this guy : https://github.com/locationtech-labs/geopyspark/blob/master/examples/s3_tile_server.py",2/22/17,1
dorukozturk,to match with my credentials,2/22/17,1
jbouffard,Are you using the current master?,2/22/17,1
dorukozturk,yepp,2/22/17,1
dorukozturk,just checked out fresh :D,2/22/17,1
jbouffard,Hmm... Could you please post the sack trace that you're getting?,2/22/17,1
dorukozturk,sure,2/22/17,1
dorukozturk,"Traceback (most recent call last):  File ""/home/dorukozturk/.virtualenvs/geopyspark/lib/python3.5/site-packages/flask/app.py"", line 1982, in wsgi_app    response = self.full_dispatch_request()  File ""/home/dorukozturk/.virtualenvs/geopyspark/lib/python3.5/site-packages/flask/app.py"", line 1614, in full_dispatch_request    rv = self.handle_user_exception(e)  File ""/home/dorukozturk/.virtualenvs/geopyspark/lib/python3.5/site-packages/flask/app.py"", line 1517, in handle_user_exception    reraise(exc_type, exc_value, tb)  File ""/home/dorukozturk/.virtualenvs/geopyspark/lib/python3.5/site-packages/flask/_compat.py"", line 33, in reraise    raise value  File ""/home/dorukozturk/.virtualenvs/geopyspark/lib/python3.5/site-packages/flask/app.py"", line 1612, in full_dispatch_request    rv = self.dispatch_request()  File ""/home/dorukozturk/.virtualenvs/geopyspark/lib/python3.5/site-packages/flask/app.py"", line 1598, in dispatch_request    return self.view_functions[rule.endpoint](**req.view_args)  File ""/home/dorukozturk/Desktop/Tasks/geopyspark/examples/s3_tile_server.py"", line 20, in tile    tup = value_reader.readSpatialSingleband(layer_name, zoom, x, y)  File ""/usr/local/spark/python/lib/py4j-0.10.1-src.zip/py4j/java_gateway.py"", line 933, in __call__    answer, self.gateway_client, self.target_id, self.name)  File ""/usr/local/spark/python/lib/py4j-0.10.1-src.zip/py4j/protocol.py"", line 312, in get_return_value    format(target_id, ""."", name), value)py4j.protocol.Py4JJavaError: An error occurred while calling o23.readSpatialSingleband.: geotrellis.spark.io.package$ValueNotFoundError: Value with key SpatialKey(3,9) not found for layer Layer(name = ""srtm"", zoom = 4)	at geotrellis.spark.io.s3.S3ValueReader$$anon$1.read(S3ValueReader.scala:54)	at geopyspark.geotrellis.io.ValueReaderWrapper.readSpatialSingleband(ValueReaderFactory.scala:40)	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)	at java.lang.reflect.Method.invoke(Method.java:498)	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237)	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)	at py4j.Gateway.invoke(Gateway.java:280)	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:128)	at py4j.commands.CallCommand.execute(CallCommand.java:79)	at py4j.GatewayConnection.run(GatewayConnection.java:211)	at java.lang.Thread.run(Thread.java:745)",2/22/17,1
dorukozturk,and this is what my called urls look like http://localhost:5000/6/14/25.png,2/22/17,1
jbouffard,Does the server start at all?,2/22/17,1
jbouffard,"When I run it, I can get the server to start locally",2/22/17,1
Charmatzis,"Hi, guys!!!! how can save a seq of MultiLineFeatures in one file as GeoJson?",2/22/17,1
pomadchin,"@Charmatzis ```scala// write that string to file however you wantval str: String = seq.map(_.geom).toGeoJson```",2/22/17,1
dorukozturk,yepp the server starts,2/22/17,1
dorukozturk,Running on http://127.0.0.1:5000/  it prints out this,2/22/17,1
dorukozturk,and I use this command to start it PYSPARK_PYTHON=python PYSPARK_DRIVER_PYTHON=python spark-submit --master 'local[*]' --jars geopyspark-backend/geotrellis/target/scala-2.11/geotrellis-backend-assembly-0.1.0.jar examples/s3_tile_server.py  and I am on a virtualenv with python3,2/22/17,1
dorukozturk,one thing that is weird is on s3 my attributes file looks like _attributes. Just to make sure I copied the contents to attributes but still the same error,2/22/17,1
pomadchin,@dorukozturk that_Ñés a correct attributes folder name on s3,2/22/17,1
dorukozturk,"oh ok good to know, ",2/22/17,1
Charmatzis,"@pomadchin  that serializes only the geometries, what about the data?",2/22/17,1
dorukozturk,@jbouffard do you have some example map application where you hit the tile server? I just modified the landsat-demo's html file to hit the port 5000,2/22/17,1
pomadchin,@Charmatzis it_Ñés up to you; write your own codec to serialize the data,2/22/17,1
jbouffard,"@dorukozturk We do, but they're not written using GeoPySpark. I can get the server to start, but it appears to not be working when geojson.io. I'll have to look at it a bit more in a little bit.",2/22/17,1
jbouffard,"Also, thanks for reporting this!",2/22/17,1
dorukozturk,"sure, I appreciate your help. ",2/22/17,1
Charmatzis,@pomadchin as I see here https://github.com/locationtech/geotrellis/blob/a00c35b928e96083188d91734252d66574e3b4a7/vector-test/src/test/scala/spec/geotrellis/vector/io/json/FeatureFormatsSpec.scala#L82 there is a JsonFeatureCollection object that can add my features and serialize it in geojson. Or am I wrong?,2/22/17,1
pomadchin,`JsonFeatureCollection` is a wrapper on top of `List[JsValue]` ,2/22/17,1
pomadchin,"`MultilineFeature` is a `Feature[MultiLine, D]` in fact it_Ñés a tuple of `MultiLine` and `D`, `MultiLine` can be converted to json as we know its type. `D` is your type; so you have to think how to convert it to json your own",2/22/17,1
pomadchin,"what error do you have when you are trying to do ?```scalaseq.toGeoJson ```",2/22/17,1
pomadchin,@Charmatzis you would see smth like `missing JsonWriter`,2/22/17,1
Charmatzis,"yeap, I saw it. but I try to write down an ""implicit object MyClassJsonFormat extends RootJsonWriter[MyClass]""",2/22/17,1
Charmatzis,thx,2/22/17,1
pomadchin,is it a case class?,2/22/17,1
pomadchin,sry for making you pain with spray json usage ): in 1.1 we are going to get rid of it (hopefully) ,2/22/17,1
pomadchin,if it_Ñés a case class than it can derive codec itself; semiautomatic derivation: https://github.com/locationtech/geotrellis/blob/a00c35b928e96083188d91734252d66574e3b4a7/vector-test/src/test/scala/spec/geotrellis/vector/io/json/FeatureFormatsSpec.scala#L170-L171,2/22/17,1
jbouffard,"@dorukozturk So I was able to get it to work, but the issue I had probably isn't what you're experiencing. I talked to @jamesmcclain who wrote the script, and he said that the ingest had to be done in a specific way.",2/22/17,1
jamesmcclain,"Specifically, the ingest has to be done in a way so that the z, x, and y of the TMS tiles match with the zoom level and SpatialKey(x,y) of your layer",2/22/17,1
jamesmcclain,"I think that if you do an ingest with target CRS WebMercator and 256x256 tiles, that should hopefully work",2/22/17,1
echeipesh,@dorukozturk `_attributes` is the actual way they are stored on S3 bucket. You must have done an ETL job to ingest that layer ?,2/22/17,1
dorukozturk,yepp,2/22/17,1
dorukozturk,"{""crs"": ""EPSG:3857"", ""pyramid"": true, ""tileSize"": 256, ""reprojectMethod"": ""buffered"", ""keyIndexMethod"": {""type"": ""zorder""}, ""layoutScheme"": ""zoomed"", ""resampleMethod"": ""cubic-spline"", ""backend"": {""path"": ""s3://kitware-geotrellis-demo/catalog"", ""type"": ""s3""}} this is my output file ",2/22/17,1
jamesmcclain,That looks okay to me,2/22/17,1
dorukozturk,@echeipesh I used spark-etl assembly to ingest the laer,2/22/17,1
dorukozturk,*layer,2/22/17,1
echeipesh,that looks like a correct output definition too,2/22/17,1
dorukozturk,"yeah everything looks correct to me, the zoom levels, x, y they all look reasonable",2/22/17,1
dorukozturk,I just cant put them on the map. ,2/22/17,1
echeipesh,"Any chance this is S3 credentials issue, are you about to `aws s3 ls` that bucket from shell ?",2/22/17,1
pomadchin,@dorukozturk have you tried a common scala server?,2/22/17,1
pomadchin,using scala gt api,2/22/17,1
dorukozturk,I will do that If I have to but I feel much more comfortable using python :),2/22/17,1
dorukozturk,I can do aws s3 ls and see all the buckets,2/22/17,1
dorukozturk,https://github.com/locationtech-labs/geopyspark/pull/17 so what does the tile request look like in this example,2/22/17,1
jbouffard,You mean to view the tile?,2/22/17,1
dorukozturk,"yepp, I feel like I am doing something wrong requesting the til",2/22/17,1
dorukozturk,e,2/22/17,1
dorukozturk,localhost:5000/z/x/y.png should be the format right?,2/22/17,1
jbouffard,"Close. You'd want to go to geojson.io or a similar site, and then click on Meta -> Add map layer",2/22/17,1
jbouffard,Then you'll want to enter in this url: http://localhost:5000/{z}/{x}/{y}.png,2/22/17,1
echeipesh,"```GET /5/8/12.png HTTP/1.1Host: localhost:5000Connection: keep-aliveUser-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.87 Safari/537.36Accept: image/webp,image/*,*/*;q=0.8Referer: http://geojson.io/Accept-Encoding: gzip, deflate, sdch, brAccept-Language: en-US,en;q=0.8,ru;q=0.6```Is an example request",2/22/17,1
pomadchin,"try these :D if it works it means you are requesting missing keys* localhost:5000/0/0/0.png* localhost:5000/1/1/0.png* localhost:5000/1/0/1.png* localhost:5000/1/1/1.png",2/22/17,1
jbouffard,I'm actually getting this on the terminal at certain zooms:,2/22/17,1
jbouffard,"```py4j.protocol.Py4JJavaError: An error occurred while calling o22.readSpatialSingleband.: geotrellis.spark.io.package$ValueNotFoundError: Value with key SpatialKey(3,1) not found for layer Layer(name = ""nlcd-zoomed"", zoom = 3)	at geotrellis.spark.io.s3.S3ValueReader$$anon$1.read(S3ValueReader.scala:54)	at geopyspark.geotrellis.io.ValueReaderWrapper.readSpatialSingleband(ValueReaderFactory.scala:40)	at sun.reflect.GeneratedMethodAccessor14.invoke(Unknown Source)	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)	at java.lang.reflect.Method.invoke(Method.java:498)	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237)	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)	at py4j.Gateway.invoke(Gateway.java:280)	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)	at py4j.commands.CallCommand.execute(CallCommand.java:79)	at py4j.GatewayConnection.run(GatewayConnection.java:214)	at java.lang.Thread.run(Thread.java:745```",2/22/17,1
dorukozturk,that is kind of normal though right because it is trying to request all the tiles in the viewport,2/22/17,1
echeipesh,"yep, thats normal, some tiles are just not part of that layer",2/22/17,1
dorukozturk,but in my case I am going to the location of the layer and get that error,2/22/17,1
jbouffard,"Ah, I see. Sorry about that. I'm actually somewhat new to tile servers.",2/22/17,1
dorukozturk,btw thank you very much guys you are very responsive and helpful,2/22/17,1
jbouffard,Don't mention it. Thank you for trying out GeoPySpark.,2/22/17,1
echeipesh,"Yes, thanks for trying our stuff out. I_Ñém going to try to use another public catalog that we have and see if that produces a problem.",2/22/17,1
dorukozturk,that would be fantastic if we can try the same catalog ,2/22/17,1
pomadchin,"@metasim to follow avro2spark project: it should fail in tests, correct?",2/22/17,1
metasim,Yes,2/22/17,1
metasim,two tests should fail,2/22/17,1
pomadchin,yes,2/22/17,1
metasim,Both are with unions.,2/22/17,1
metasim,"BTW, adding more documentation  now.",2/22/17,1
dorukozturk,do yu guys have any public catalog that we can both try with geopyspark? (for validation purposes),2/22/17,1
metasim,"@pomadchin FYI, just pushed an update.",2/22/17,1
pomadchin,@metasim :+1:,2/22/17,1
jbouffard,@dorukozturk I can look into making one public for us to work on.,2/22/17,1
jbouffard,Are the rasters in your layer `Singleband` or `Multiband`?,2/22/17,1
dorukozturk,"Hey, we can try singleband. Actually I was just testing it with the srtm dem tiles. I downloaded 4 of them from here : http://dwtkns.com/srtm/ and ingested them",2/22/17,1
dorukozturk,and it took 3 minutes for me to ingest with a relatively small cluster,2/22/17,1
jbouffard,"Okay. Only `Singleband`s will work for right now, since `Multiband`s will require a bit of extra work.",2/22/17,1
jbouffard,Alright. I'll try to ingest a few in a little bit. Have you had any luck getting them on the server?,2/22/17,1
dorukozturk,you mean the srtm tiffs,2/22/17,1
jbouffard,"Yeah, I wasn't sure if you tried to use them in the example.",2/22/17,1
dorukozturk,"I did, those 4 srtm tiff files are the ones that I tried to serve with s3-tile-server example if that is the question",2/22/17,1
jbouffard,"Oh, okay. I see now.. I'm going to start the ingest soon.",2/22/17,1
dorukozturk,Is there a very small file that you guys ingested and used geopyspark to create the tile server and validated it is working? I can give that layer a try. NLCD is little too big for me to handle right now.,2/22/17,1
pomadchin,I was mistaken about version we are planing to get rid of spray json // it's gt 2.0 as it's api breaking change and we follow semver // thx @lossyrob for pointing to that :0 ,2/22/17,1
metasim,"@pomadchin Which json lib will you move to? (alas, there are so many!)",2/22/17,1
pomadchin,@metasim thinking about Circe *_* ,2/22/17,1
metasim,cool,2/22/17,1
pomadchin,"but that would happen in a long long future, so even don't think about it",2/22/17,1
pomadchin,):,2/22/17,1
metasim,Have you guys considered switching to TypesafeConfig for the ETL configuration files (instead of generic JSON)?,2/22/17,1
pomadchin,https://github.com/locationtech/geotrellis/issues/1767,2/22/17,1
pomadchin,"No, that would be an extended Json support",2/22/17,1
fosskers,not necessarily too far in the future @pomadchin ,2/22/17,1
prahim,I have a question regarding TileLayerMetadata.extent -- what is it representing? I assumed it was the extent of the area that contained data but the extent changed when filtering out nodata tiles which would mean my assumption was incorrect.,2/22/17,1
fosskers,Major versions are allowed to happen quickly,2/22/17,1
pomadchin,@fosskers ahaha,2/22/17,1
pomadchin,@metasim that was our first try - with typesafe config; after that we figured out that json is more convinient,2/22/17,1
metasim,"Huh.... convenient for the user, or the interpretation code?",2/22/17,1
pomadchin,Both ,2/22/17,1
pomadchin,But it's discussable :d ,2/22/17,1
metasim,"Cool, because you have me bewildered. Another time tho :)",2/22/17,1
pomadchin,"ahah, my hand is for json : p and we can have a validation scheme for it ",2/22/17,1
fosskers,how about YML?,2/22/17,1
metasim,@fosskers are you trolling?,2/22/17,1
fosskers,"not at all, if it were a choice between the two, I think YML is much cleaner than JSON",2/22/17,1
pomadchin,)':,2/22/17,1
metasim,"+1 for validation, but I love being able to have variable expansion and imports.",2/22/17,1
metasim,And with the [right tricks](https://github.com/ElderResearch/ssc) you can have type safety.,2/22/17,1
fosskers,how about some non-turing-complete typed config? https://github.com/Gabriel439/Haskell-Dhall-Library,2/22/17,1
fosskers,"(guarantees it terminates, can't infinite loop within config)",2/22/17,1
lossyrob,trollin,2/22/17,1
pomadchin,:d ,2/22/17,1
lossyrob,@prahim It should represent the extent of the actual data,2/22/17,1
lossyrob,how did you filter nodata tiles?,2/22/17,1
fosskers,"yml was serious, Dhall was half. It's still pretty alpha and probably won't hit critical mass",2/22/17,1
prahim,"hey @lossyrob ! I did it with you last week: ```val filteredTiles: ContextRDD[SpatialKey, Tile, TileLayerMetadata[SpatialKey]] =              tileLayerRdd.withContext(rdd => rdd.filter(!_._2.isNoDataTile))```",2/22/17,1
lossyrob,"hi! you're just passing forward the context (which is the metadata), so the extent shouldn_Ñét change at all",2/22/17,1
prahim,so the problem now is that when we read back the tiles stitched up with the extent it is misaligned ... if I put back the nodata tiles everything is fine,2/22/17,1
prahim,yea so the extent didn't change so it was not aligned correctly then I udpated the extent and it aligned,2/22/17,1
prahim,but that doesn't make sense,2/22/17,1
prahim,"```            val newMetadata = filteredTiles.metadata.updateBounds(              Bounds(minSpatialKey, maxSpatialKey)            )```",2/22/17,1
prahim,where minSpatialKey and maxSpatialKey are from the filteredTiles new spatialKey set ,2/22/17,1
lossyrob,the extent shouldn_Ñét be used during stitching,2/22/17,1
lossyrob,I don_Ñét see the bounds used either,2/22/17,1
lossyrob,what_Ñés the method of stitching them?,2/22/17,1
prahim,hmm so what is making it misalign?,2/22/17,1
prahim,"```tiles:  Seq[(SpatialKey, Tile)] with Metadata[M] =???crs: CRS = ???val extent = tiles.metadata.getComponent[Extent]val stitchedTile = tiles.stitch stitchedTile        .crop(extent)        .reproject(tiles.metadata.getComponent[CRS], crs)        .tile```",2/22/17,1
prahim,`stitchedTile.extent` does not match `extent` here,2/22/17,1
lossyrob,"that would make sense, if nodata tiles that are accounted for in metadata.extent would be removed, the stitch calculates some true extent of the tiles",2/22/17,1
lossyrob,so I would sort of expect that to work,2/22/17,1
lossyrob,what do you mean by misaligned?,2/22/17,1
prahim,like the stitched image doesn't fit in the extent provided -- it is out of place,2/22/17,1
lossyrob,what extent provided?,2/22/17,1
prahim,the extent returned is `extent` above and the image is `stitchedTile` and when put on a map it doesn't look correct,2/22/17,1
prahim,"but when nodata tiles are not filtered out, it works fine",2/22/17,1
lossyrob,yeah they will not be the same,2/22/17,1
lossyrob,because you are removing data,2/22/17,1
lossyrob,so the stitched extent is going to reflect the actual data there,2/22/17,1
prahim,but I am removing nodata data why does the extent of the data change?,2/22/17,1
lossyrob,because those data tiles are gone,2/22/17,1
lossyrob,so the layer extent is not the extent of the data that is acutally data,2/22/17,1
lossyrob,it_Ñés the extent of the raster that was incoming before tileToLayout was called,2/22/17,1
lossyrob,it doesn_Ñét go in and check if border pixels are NoData,2/22/17,1
prahim,"okay so the extent is just the envelope of the tiles regardless of whether the tiles contain ""data""",2/22/17,1
prahim,"Is there a better way than calculating min and max spatial keys after filtering and calling```            val newMetadata = filteredTiles.metadata.updateBounds(              Bounds(minSpatialKey, maxSpatialKey)            )```to update the metadata before writing? ",2/22/17,1
prahim,I saw functions to generate metadata `fromRDD` but I have implicit errors,2/22/17,1
lossyrob,"well, the extent isn_Ñét just of the tiles. It_Ñés of the original rasters. So if you read in a set of geotiffs to ingest, it computes the bounding box of the whole set. That_Ñés the extent (after any reprojection). It might not fit exactly the tile boundaries. But it also doesn_Ñét account for NoData values (and sparse rasters for that matter, it just is one bounding box that holds all of your original raster data)",2/22/17,1
lossyrob,are you doing this filter on the ingest side?,2/22/17,1
lossyrob,thinking of if there_Ñés a way to do it earlier,2/22/17,1
lossyrob,"there_Ñés actually not, which is a bit unfortunate",2/22/17,1
prahim,"hmm okay ... yea ingest side .. so I tried```val sourceTiles: RDD[(ProjectedExtent, Tile)] =            sc.parallelize(Seq(layer.location))              .map(fetchGeoTiff(_))              .map { geoTiff =>                val reprojected: Raster[Tile] = geoTiff.raster.reproject(geoTiff.crs, WebMercator, Bilinear)                val projectedExtent = ProjectedExtent(reprojected.extent, WebMercator)                (projectedExtent, reprojected.tile)              }          val filteredSourceTiles = sourceTiles.filter(!_._2.isNoDataTile)geotrellis.spark.ingest.Ingest[ProjectedExtent, SpatialKey](            filteredSourceTiles,            ... ) { (tileLayerRdd, zoom) =>```to your point to do it earlier but it didn't filter anything out ... could you help me understand why",2/22/17,1
lossyrob,"so you would compute the new bounds, and then you could use the `mapTransform` on the GridBounds of those keys to update the extent",2/22/17,1
lossyrob,"that filter step, would filter out only if the entire geotiff is nodata",2/22/17,1
lossyrob,which isn_Ñét the case,2/22/17,1
lossyrob,"it_Ñés when you split the tiles into tiles that fit into the grid layout, that you end up with nodata tiles",2/22/17,1
lossyrob,so you have to filter after you_Ñéve cut the tiles,2/22/17,1
prahim,ah okay,2/22/17,1
lossyrob,but then the metadata becomes wrong,2/22/17,1
lossyrob,so you have to recreate it,2/22/17,1
lossyrob,which is wonky. I wish we had a cut tiles method that would take this into account,2/22/17,1
prahim,yes please ,2/22/17,1
lossyrob,:),2/22/17,1
prahim,right now I am calculating the min and max spatial keys and then creating the new bounds and calling updateBounds on the metadata which ends up calling mapTransform(GridBounds),2/22/17,1
lossyrob,oh. right then that should work,2/22/17,1
prahim,okay then I think the point I was missing was what the extent really is .. now it makes sense,2/22/17,1
lossyrob,:+1: ,2/22/17,1
prahim,thank you!,2/22/17,1
lossyrob,no prob! thanks for working that out here,2/22/17,1
jbouffard,"@dorukozturk I was able to get the tiles up with the data srtm. There's issues with it, but it does work. Here's the code that I used",2/22/17,1
jbouffard,https://gist.github.com/jbouffard/ff980e60565851c7847ce2302d559968,2/22/17,1
jbouffard,"Sorry for the sloppy code, I'll work on this more tomorrow, but I need to run now.",2/22/17,1
dorukozturk,@jbouffard that worked. Thank you very much. I will dive into your code to see what you have changed. ,2/23/17,1
Charmatzis,> if it_Ñés a case class than it can derive codec itself; semiautomatic derivation: https://github.com/locationtech/geotrellis/blob/a00c35b928e96083188d91734252d66574e3b4a7/vector-test/src/test/scala/spec/geotrellis/vector/io/json/FeatureFormatsSpec.scala#L170-L171,2/23/17,1
Charmatzis,What is jsonFormat2 ?,2/23/17,1
pomadchin,it derives fromat codec for case clases automatically,2/23/17,1
Charmatzis,well I am stuck again,2/23/17,1
Charmatzis,the error is ,2/23/17,1
Charmatzis,"Error:(120, 15) value toGeoJson is not a member of scala.collection.immutable.Seq[geotrellis.vector.Feature[geotrellis.vector.MultiLine,demo.ShipRoute]] Note: implicit object ShipRouteJsonFormat is not applicable here because it comes after the application point and it lacks an explicit result type    val t = g.toGeoJson() ",2/23/17,1
Charmatzis,which ShipRoute is ,2/23/17,1
Charmatzis,"```case class ShipRoute(Ship_Id: Int, StartTime: Timestamp, EndTime:Timestamp)```",2/23/17,1
Charmatzis,also I have created an object ShipRoute,2/23/17,1
Charmatzis,"```object ShipRoute {  implicit object ShipRouteJsonFormat extends RootJsonWriter[ShipRoute] {    val format = new SimpleDateFormat(""MM/dd/yyyy HH:mm"")    def write(h: ShipRoute): JsValue =      JsObject(        ""Ship_Id"" -> JsString(h.Ship_Id.toString),        ""StatTime"" -> JsString(format.format(h.StartTime)),        ""EndTime"" -> JsString(format.format(h.EndTime))      )  }}```",2/23/17,1
pomadchin,can you create a gist?,2/23/17,1
pomadchin,i need to see all imports you have in the scope,2/23/17,1
pomadchin,have you imported that implicit somewhere? `import ShipRoute._`,2/23/17,1
pomadchin,_before_ applying function `toGeoJson`,2/23/17,1
Charmatzis,"yes I did, I will send a link from gist",2/23/17,1
Charmatzis,thx,2/23/17,1
Charmatzis,https://gist.github.com/Charmatzis/9fcfd191a28e3f83ed56918a216c093b ,2/23/17,1
pomadchin,@Charmatzis it_Ñés not compilable code o: can you fix f1 and f2 definitions?,2/23/17,1
Charmatzis,"it works now, I had to put first the object and class ShipRoute and then the object Main",2/23/17,1
pomadchin,ok,2/23/17,1
Charmatzis,"Sorry, for the trouble ",2/23/17,1
jbouffard,"@dorukozturk Don't mention it! I'm glad that worked for you. It seems that there's going to have to be a bit of fine tuning when trying to run a tile server with python, at least with the libraries we've used thus far. I'll probably have to investigate this more to get a better understanding of how to implement something like this in the future.",2/23/17,1
jbouffard,"Please feel free to let us know if you run into anymore problems! Either on here, the mailing list, or by making an issue. And of course, if there's a change that you make that you feel would improve the project, don't hesitate to make a PR!",2/23/17,1
dorukozturk,@jbouffard sounds perfect. Thanks again,2/23/17,1
dorukozturk,@jbouffard Any strategy on running the geopyspark tile server on EMR? :) ,2/23/17,1
pomadchin,@dorukozturk ahhhh,2/23/17,1
pomadchin,:D ,2/23/17,1
dorukozturk,am I in a bad place now :D,2/23/17,1
pomadchin,:D ,2/23/17,1
dorukozturk,I am guessing time to go back to scala :D,2/23/17,1
pomadchin,I always suggest it :D but you can do the way it is more convinient for you,2/23/17,1
pomadchin,"\+ you use geopy spark, it_Ñés awesome",2/23/17,1
pomadchin,"ok, trying to use",2/23/17,1
pomadchin,:D ,2/23/17,1
dorukozturk,ok so once I have the code for the tileserver am I going to create an assembly and do a spark-submit just like the ingest? ,2/23/17,1
pomadchin,are you talking about scala server?,2/23/17,1
dorukozturk,yepp,2/23/17,1
pomadchin,yep,2/23/17,1
pomadchin,here is an example: https://github.com/geotrellis/geotrellis-landsat-emr-demo/blob/master/scripts/emr/bootstrap-demo.sh#L57-L61,2/23/17,1
dorukozturk,"so I can go to the landsat-demo modify that tile server to read from s3 create an assembly from it and submit the job, this is going to be the steps right?",2/23/17,1
pomadchin,basically it supports s3,2/23/17,1
pomadchin,"let me throw you some pointers, sec",2/23/17,1
pomadchin,https://github.com/geotrellis/geotrellis-landsat-emr-demo/blob/master/server/src/main/scala/demo/Main.scala#L50-L54,2/23/17,1
pomadchin,"```bashspark-submit --master yarn-client \     --driver-memory 5G --driver-cores 4 \     --executor-cores 2 --executor-memory 5G \     --conf spark.dynamicAllocation.enabled=true \     /tmp/tile-server.jar s3 key prefix```",2/23/17,1
dorukozturk,"great, thanks for the info ",2/23/17,1
dorukozturk,Ok let me describe our use case: We have ~1tb of singleband ndvi bands. We want to overlay and we also want to demonstrate we can get some data for a subregion(it can be small). Simply we want to do a .collect on the rdd and give an array back to the user once they select a region. For the .collect task should I still use scala? I saw that geopyspark has some ways to collect that data back but I am guessing ultimately i want to run that also on emr. so should I stick with scala?,2/23/17,1
dorukozturk,https://github.com/locationtech-labs/geopyspark/blob/master/geopyspark/tests/s3_layer_test.py#L38,2/23/17,1
dorukozturk,this is what we want :),2/23/17,1
pomadchin,Im not sure what_Ñés going on here with geopy spark and emr,2/23/17,1
pomadchin,._.,2/23/17,1
pomadchin,@jbouffard would answer your questions :D ,2/23/17,1
jbouffard,"@dorukozturk You should be able to do that with GeoPySpark. However, depending one the `RDD` size, it might be best not to do `collect` on it. This is the case for python and scala. I've actually never worked with anything on emr, so I can't say for sure which language would be better there.",2/23/17,1
jbouffard,"Unfortunately, that test is outdated and we don't have anything recent, but https://github.com/locationtech-labs/geopyspark/blob/master/geopyspark/geotrellis/catalog.py is what you're going to want to look at.",2/23/17,1
jbouffard,"You can still do everything in that test with one of those `_Catalog` classes, but it'll be a lot neater.",2/23/17,1
jbouffard,"Sorry for the lack of documentation. We're still pre-alpha, so we decided not to document anything until we decide on a final implementation.",2/23/17,1
pomadchin,"yes, the collect method would collect all data on your driver; so be carefull with it_Ñés usage",2/23/17,1
dorukozturk,ok thanks for the help again I will try to implement something reasonable,2/23/17,1
karalas,"Hi, I have managed to run the Chatta demo locally with ```file``` as a backend. I try now to run the example with my own images, but I get  errors when I ```make server``` like the following:```[ERROR] [02/24/2017 16:47:31.038] [chatta-demo-akka.actor.default-dispatcher-11] [akka.actor.ActorSystemImpl(chatta-demo)] Error during processing of request: 'Value with key SpatialKey(284,192) not found for layer Layer(name = ""mask"", zoom = 9)'. Completing with 500 Internal Server Error response.```I do not have a mask for my images, just an ndvi and an ndwi image. To be honest, I do not understand the code very well, and especially  the code in ```ChattaServiceRouter.scala``` where there is for example:```val maskTile =            timedCreate(              ""tms"",              ""ChattaServiceRouter(124)::maskTile start"",              ""ChattaServiceRouter(124)::maskTile end"") {              tileReader.reader[SpatialKey, Tile](LayerId(""mask"", zoom)).read(key).convert(ShortConstantNoDataCellType).mutable            }```Maybe this part has to do with my error, or no? Thank you for any help.",2/24/17,1
pomadchin,it_Ñés a correct err message,2/24/17,1
pomadchin,probably we need to make it 404 instead of a weird 500 ._.,2/24/17,1
karalas,"@pomadchin  ok, and how do I have to interpret it? Thank you",2/24/17,1
pomadchin,interpret it as 404 :),2/24/17,1
pomadchin,not found,2/24/17,1
pomadchin,i recommend you in leaflet to jump directly on your tiles ingested area,2/24/17,1
pomadchin,https://github.com/geotrellis/geotrellis-chatta-demo/blob/master/static/js/application.js#L33,2/24/17,1
pomadchin,"if you don_Ñét need to mask, than just remove all mentions of masking",2/24/17,1
pomadchin,if you would experience problems just write them here (:,2/24/17,1
karalas,"I have already set this line to point my area. It is true that I have not a ""mask"" layer, why do I get this error, since I have not ingested a ""mask"" now? And can you explain me what this line means: ```""ChattaServiceRouter(124)::maskTile start""```?",2/24/17,1
pomadchin,just a message for an extended chatta demo logger with timings,2/24/17,1
pomadchin,ChattaServiceRouter(line)::{val/def}name,2/24/17,1
pomadchin,https://github.com/geotrellis/geotrellis-chatta-demo/blob/master/geotrellis/src/main/scala/geotrellis/chatta/LazyLogging.scala#L19,2/24/17,1
karalas,ah ok thanks!,2/24/17,1
pomadchin,to make it work set [this](https://github.com/geotrellis/geotrellis-chatta-demo/blob/master/geotrellis/src/main/resources/application.conf#L6) parameter to `true`,2/24/17,1
karalas,"@pomadchin  If I remove all mentions of masking, there still exists in ```ChattaServiceRouter.scala``` ```bytes``` val as an input of ```HttpResponse``` . What can I do in order ```bytes``` not to be depended  on ```maskedTile```? I am sorry for this kind of questions, but I am really struggling to understand.. Thanks ",2/24/17,1
pomadchin,@karalas show me the line you are talking about,2/24/17,1
dorukozturk,is it possible to use the spark-etl assembly and udpate a layer?,2/24/17,1
acoronadoiruegas,"Greetings to all, I have a multiband layer, where one of the bands corresponds to the land use class, I want to generate rectangles of 100 x 100 pixels. Where all pixels are of the same class. I think focalMean can serve, but you guys recommend me? Subsequently generate statistics of each region, but the main problem is to generate this list of homogeneous rectangles (or indexes to the rectangles). Of course there is no problem if there is overlap between the rectangles. That's why I think a local operation, like FocalMean, with a 100 x 100 neighborhood could be useful. Thanks",2/24/17,1
pomadchin,"@dorukozturk yo, nope, that would be supported in future releases ): you have to write code yourself ",2/24/17,1
acoronadoiruegas,"What I mean, only come to my mind solutions, which seem inefficient, due to my ignorance of geotrellis. For example, extract only the class of my interest, say class 38, and only where there is such a value there will be data, in the rest will be NODATA, then I apply FocalMean of a neighborhood of 100 x 100 and in all those pixels where there is a value exactly Equal to 38, these will be the centers of the desired regions, there will be multiple contiguous centers, but that is not a problem at all. But it seems to me somewhat inefficient.",2/24/17,1
Sharathmk99,"Hi All. I have problem statement like, there is 50GB image data from Drone and I want to stitch the images.. Can I use ODM for this? If yes, please suggest system configuration and can I leverage GPU for processing??. Please help......",2/25/17,1
dorukozturk,Do you guys have documentation on submitting large ingest jobs and best practices? I saw that using gdal_retile helps with parallelizing the process more. Are there any other tricks like that? I understand this configs vary based on the cluster and the size/format of the dataset. I am curious about tuning my application. For example numPartitions and partitions how do they differ and how do you set them? Should you use 100000 or 10 or 100? Or this is kind of a spark question more than a geotrellis question :),2/25/17,1
metasim,[![Screen Shot 2017-02-25 at 4.29.55 PM.png](https://files.gitter.im/geotrellis/geotrellis/KRVc/thumb/Screen-Shot-2017-02-25-at-4.29.55-PM.png)](https://files.gitter.im/geotrellis/geotrellis/KRVc/Screen-Shot-2017-02-25-at-4.29.55-PM.png),2/25/17,1
metasim,@echeipesh @pomadchin @lossyrob  :-D ,2/25/17,1
lossyrob,"@acoronadoiruegas I_Ñém having trouble figuring out your exact problem. You want to, for each cell, generate a 100x100 area? Or you want to generate, for each pixel, the mean value of only values that are of a certain classification?",2/26/17,1
lossyrob,"@Sharathmk99 If you have raw drone imagery that needs to be stitched and orthorectified, ODM is a good tool for that. They would be the best people to ask, as I haven_Ñét used ODM enough to be able to help. If you_Ñére looking to mosiac or deal with the GeoTiffs that would come out of something like ODM, we_Ñéd be able to help with that",2/26/17,1
lossyrob,@dorukozturk we actually don_Ñét need gdal_retile anymore with the release of 1.0_Ñ_we can use the `maxTileSize` option in ETL (and the winowed GeoTiff reading functionality) to just do what gdal_retile was doing for us on the fly during the ingest.,2/26/17,1
lossyrob,"As for the partition size thing: you_Ñére hitting on a very important part of running ingest jobs. I do believe it_Ñés sort of a general spark thing, but something that we deal with a lot",2/26/17,1
lossyrob,"how many cores, how much memory per executor? How many partitions? These are generally the questions we end up trying to answer, and it often takes some trial and error",2/26/17,1
lossyrob,I take the landsat EMR demo config as a general guideline when dealing with EC2 instances,2/26/17,1
lossyrob,https://github.com/geotrellis/geotrellis-landsat-emr-demo/blob/master/config-ingest.mk#L8-L12,2/26/17,1
lossyrob,and set partitions by the size of the job,2/26/17,1
lossyrob,"I_Ñéve had to set 50K partitions before, often times less",2/26/17,1
lossyrob,"if you describe the job, I can probably try to think of what I would do, and explain my logic",2/26/17,1
lossyrob,@metasim :tada: :tada:,2/26/17,1
karalas,@pomadchin  https://github.com/geotrellis/geotrellis-chatta-demo/blob/master/geotrellis/src/main/scala/geotrellis/chatta/ChattaServiceRouter.scala#L194,2/26/17,1
dorukozturk,@lossyrob Thanks for your clarification. So here is our use case: We have 8000 single band spatial files (sizes vary from 100 mb to 1 gb) and the total size is ~1.5 tb. We want to ingest them to an s3 catalog (we read from s3 as well). For the cluster we do not really have a limit. We actually want to run this job in hours not days :). That is the only limit. Like if we can ingest it in 8-10 hours that will be perfect. Is it enough to just provide maxTileSize on the input.json or do I need to change something on the etl project? My other question is about the cluster. So when I launch a 8 node cluster equipped with 16 cores each I get a lot of cores but only 9 of them were used despite almost all the memory were used during the ingest. Is that normal? I am guessing usage of all the memory is good but using only 10 percent of the cores seemed wrong to me. It would be fantastic if you can share set of configs that you use during an ingest similar to mine. ,2/26/17,1
lossyrob,What's the instance type? Your on aws right? Are you running EMR?,2/26/17,1
lossyrob,What's your current executor config for memory and cores? ,2/26/17,1
Sharathmk99,@lossyrob Thank you very much for the information.,2/26/17,1
dorukozturk,"@lossyrob i am on emr, i ran a cluster of 8 nodes with m3 2xlarge instances and i set my executor memory to 20g and cores to 16 which was the max of the machines. I am planning around 50 or 100 nodes to do the ingest",2/26/17,1
lossyrob,We use the same instances,2/26/17,1
lossyrob,"The key is the executor memory and cores. If the executors ask for too much mem, YARN won_Ñét allocate as much of them on the machine as you_Ñéd like",2/26/17,1
lossyrob,"By using executors with 2 cores, memory set to 4200M, and yarn overhead to 700M, you should be able to see most of the CPUs get utilized",2/26/17,1
lossyrob,"that should work out no matter what the # of nodes in your cluster are. If it doesn_Ñét, either you_Ñéll end up with YARN killing your containers because they take up too much mem - in which case you should push up the # of partitions (in which case you have to push up the memory, and eat the lack of CPU power), or if the # of containers isn_Ñét expected, in which you have to push down the memory to get more containers allocated",2/26/17,1
dorukozturk,ok that is very useful. Thank you very much.,2/27/17,1
lossyrob,"you should definitly use the `maxTileSize` option (https://github.com/locationtech/geotrellis/blob/master/spark-etl/src/main/scala/geotrellis/spark/etl/config/Input.scala#L32, in Input JSON https://github.com/locationtech/geotrellis/blob/master/docs/guide/etl.rst#input-json) and set to something like 1024",2/27/17,1
lossyrob,then repartition to something like 15K to start (that_Ñés a guess),2/27/17,1
lossyrob,"the way I do partition is, rule of thumb, you want your tasks to execute in around 500ms",2/27/17,1
lossyrob,that comes from databricks,2/27/17,1
lossyrob,"so if your seeing something like 5s, 10s task execution, you need to bump the partitions up",2/27/17,1
lossyrob,"sometimes I can_Ñét get it to 500ms, and that_Ñés OK, just a rule of thumb. Sometimes it only gets to 1 or 2 sec",2/27/17,1
dorukozturk,"ok, that is very informative. What about the size of the cluster? Are there guidelines like ( I am just making up) if your total data is 500 G you need at least 100 G of ram or something like that?",2/27/17,1
lossyrob,"if you partition it right, then the size of the cluster only dictates speed",2/27/17,1
lossyrob,"because it_Ñés a matter of, can the cluster work on 100 partitions at a time or 1000",2/27/17,1
lossyrob,i.e. a job that can is configured correctly should be able to run on 3 nodes along with 300 nodes in the same config,2/27/17,1
dorukozturk,I see. And you said it is good if each task is around 500ms and you see that on the spark ui?,2/27/17,1
lossyrob,"Yup. If you go click into a job, into a stage, you_Ñéll get some statistics on min max and average task execution time",2/27/17,1
lossyrob,so I aim for the average to be 500ms,2/27/17,1
lossyrob,"oftentimes it_Ñés more like 1s, but that_Ñés what I go for at least",2/27/17,1
lossyrob,one important thing to note,2/27/17,1
lossyrob,"when you make many tasks, say like 50K",2/27/17,1
lossyrob,or above,2/27/17,1
lossyrob,you start to put a whole lot of pressure on the memory of the driver,2/27/17,1
lossyrob,because it has to track the DAG and all the tasks and everything,2/27/17,1
lossyrob,"and when the driver runs out of memory, it_Ñés a really weird and hard to parse error in YARN",2/27/17,1
lossyrob,so for large jobs I_Ñéll usually just make the master node a m3.2xlarge and give it a heap full of memory,2/27/17,1
dorukozturk,"cool, when I specify numpartitions and maxtilesize I got this: geotrellis.raster.io.geotiff.reader.MalformedGeoTiffException: incorrect byte order",2/27/17,1
lossyrob,that happens when it tries to read a non-geotiff as a geotiff. ,2/27/17,1
lossyrob,so is there a file it could be trying to read in the input directory that_Ñés not a geotiff,2/27/17,1
dorukozturk,yes that is the case ,2/27/17,1
lossyrob,That_Ñés actually a bug in here: https://github.com/locationtech/geotrellis/blob/master/s3/src/main/scala/geotrellis/spark/io/s3/TiffTagsS3InputFormat.scala#L27. More like a missing feature. That should be using the tiff tag extensions to filter out non-tiff files,2/27/17,1
lossyrob,here is where it is used,2/27/17,1
lossyrob,https://github.com/locationtech/geotrellis/blob/master/s3/src/main/scala/geotrellis/spark/io/s3/S3GeoTiffRDD.scala#L107-L112,2/27/17,1
lossyrob,"I_Ñéll write up an issue, unless you want to do the honors :)",2/27/17,1
dorukozturk,sure I will be happy to :) ,2/27/17,1
lossyrob,thanks!,2/27/17,1
lossyrob,"as far as a temp workaround, I don_Ñét know if you would be able to get rid of those files. or else you could just make the change in a local geotrellis clone and publish local (and then a PR would be handy)",2/27/17,1
lossyrob,basically have to mimic what happens here:https://github.com/locationtech/geotrellis/blob/master/s3/src/main/scala/geotrellis/spark/io/s3/S3InputFormat.scala#L61-L67,2/27/17,1
dorukozturk,cool tomorrow first job I ll try. (Probably early morning) ,2/27/17,1
lossyrob,ah actually,2/27/17,1
lossyrob,I might be mistaken,2/27/17,1
lossyrob,yeah,2/27/17,1
lossyrob,because it_Ñés an S3InputFormat,2/27/17,1
lossyrob,that should just happen for the TiffTagS3InputFormat,2/27/17,1
lossyrob,so that_Ñés actually not the solution. hmm_Ñ_.,2/27/17,1
lossyrob,should filter on the extensions listed here https://github.com/locationtech/geotrellis/blob/master/s3/src/main/scala/geotrellis/spark/io/s3/S3GeoTiffRDD.scala#L59,2/27/17,1
lossyrob,my bad,2/27/17,1
lossyrob,sometimes things get weird if you don_Ñét end your _ÑÒdirectory_Ñù keys with a `/`,2/27/17,1
lossyrob,so for example sending in prefix=`some/dir/of/files` instead of `some/dir/of/files/`,2/27/17,1
lossyrob,should try the latter,2/27/17,1
pomadchin,@karalas [here](https://github.com/geotrellis/geotrellis-chatta-demo/blob/master/geotrellis/src/main/scala/geotrellis/chatta/ChattaServiceRouter.scala#L190) use `tile` instead of `maskedTile`,2/27/17,1
pomadchin,@metasim :tada: :tada: :tada:,2/27/17,1
dorukozturk,"hi, have you seen this before on aws: ",2/27/17,1
dorukozturk, com.amazonaws.services.s3.model.AmazonS3Exception: The requested range is not satisfiable (Service: Amazon S3; Status Code: 416; Error Code: InvalidRange;,2/27/17,1
dorukozturk,This happens when I try to ingest and I could not find a solution to this,2/27/17,1
pomadchin,interesting,2/27/17,1
pomadchin,looks like generated key range is out of order?,2/27/17,1
pomadchin,can you catch error and throw here some output? (index ranges),2/27/17,1
dorukozturk,which would be the console to see that? ,2/27/17,1
pomadchin,you are using etl?,2/27/17,1
dorukozturk,yepp,2/27/17,1
pomadchin,what_Ñés the code?,2/27/17,1
pomadchin,can you point me? or you just build geotrellis-etl assembly,2/27/17,1
dorukozturk,I built the etl assembly,2/27/17,1
dorukozturk,and this is a singlebandingest,2/27/17,1
dorukozturk,this is my whole trace in the yarn ,2/27/17,1
dorukozturk,https://gist.github.com/dorukozturk/d188dbac49433bc08f725dc8c873a6b4,2/27/17,1
dorukozturk,ok when I did not pass maxTileSize it started to run,2/27/17,1
lossyrob,Interesting. Could be a bug in the window reader. Looks like it's trying to read a buye,2/27/17,1
lossyrob,*byte range that doesn't exist,2/27/17,1
dorukozturk,"when I did not use maxtilesize I got this  User class threw exception: org.apache.spark.SparkException: Job aborted due to stage failure: Task 63 in stage 0.0 failed 4 times, most recent failure: Lost task 63.3 in stage 0.0 (TID 83, ip-172-31-46-228.us-west-2.compute.internal): ExecutorLostFailure (executor 42 exited caused by one of the running tasks) Reason: Container marked as failed: container_1488202636886_0006_01_000925 on host: ip-172-31-46-228.us-west-2.compute.internal. Exit status: 137. Diagnostics: Container killed on request. Exit code is 137Container exited with a non-zero exit code 137Killed by external signal ",2/27/17,1
dorukozturk,does that mean the cluster is not beefy enough ?,2/27/17,1
dorukozturk,it has 10 m3 2xlarge worker nodes,2/27/17,1
lossyrob,Container being killed never have to do with the size of the cluster. Only to do with container (executor) configuration,2/27/17,1
lossyrob,And your going to need maxtilesize,2/27/17,1
lossyrob,How large in col/row/bands is the larger geotiffs?,2/27/17,1
pomadchin,and the size of the whole input (just to know what you are trying to deal with) // in GBs,2/27/17,1
acoronadoiruegas,[![100x100-Rectangles.png](https://files.gitter.im/geotrellis/geotrellis/INIq/thumb/100x100-Rectangles.png)](https://files.gitter.im/geotrellis/geotrellis/INIq/100x100-Rectangles.png),2/27/17,1
acoronadoiruegas,"Hi @lossyrob, I want, for each cell, to generate an area 100x100, with the condition that the generated regions belong to the same class.In the image you can see valid examples of generated cells.",2/27/17,1
metasim,I'm getting OOME (in the sbt process) when running the GT test suite. Do you guys have any sbt environment variables or `.sbtrc` files defined in your development environment that bump up the memory?,2/27/17,1
pomadchin,@metasim `./sbt -J-Xmx2G` is a maximum thing we do for travis ci,2/27/17,1
metasim,:+1: ,2/27/17,1
metasim," Is there an equivalent of [this](https://bintray.com/azavea/geotrellis/geotrellis-spark) (sha-tagged builds) since the transition to LocationTech? I need a recent version of GT (has all tile avro codecs), but version-stable.",2/27/17,1
pomadchin,w8 a sec i_Ñéll double check,2/27/17,1
pomadchin,"yes, just use our snapshot repo",2/27/17,1
pomadchin,https://repo.locationtech.org/content/repositories/geotrellis-snapshots,2/27/17,1
metasim,thx!,2/27/17,1
pomadchin,https://repo.locationtech.org/content/repositories/geotrellis-snapshots/org/locationtech/geotrellis/geotrellis-spark_2.11/ is a proof link that everything is up to date,2/27/17,1
metasim,awesomesauce,2/27/17,1
metasim,"oh, wait, these aren't ""version stable""....",2/27/17,1
pomadchin,ahaha,2/27/17,1
pomadchin,ye,2/27/17,1
metasim,the old sha-tagged builds were stable,2/27/17,1
pomadchin,just snapshot sliding,2/27/17,1
pomadchin,eh ): acidently even bintray doesn_Ñét appreciate people who publish sha artefacts,2/27/17,1
pomadchin,the correct way is to have releases and snapshots,2/27/17,1
metasim,shoot. that was a helpful feature,2/27/17,1
pomadchin,ye,2/27/17,1
pomadchin,i see what you are talking about,2/27/17,1
pomadchin,w8 before pubslish i want to create a pr,2/27/17,1
metasim,when's the next release? ;-),2/27/17,1
pomadchin,you_Ñére talking about 1.0.1 ?,2/27/17,1
pomadchin,SOOON,2/27/17,1
pomadchin,:D ,2/27/17,1
pomadchin,@metasim looks like for avro union types it_Ñés the only way to get fields by id,2/27/17,1
pomadchin,the only way to get fieds is by id**,2/27/17,1
pomadchin,hm,2/27/17,1
metasim,@pomadchin not sure what you are referencing.,2/27/17,1
pomadchin,ah nothing :D,2/27/17,1
pomadchin,forget about it,2/27/17,1
metasim,'k,2/27/17,1
metasim,What's the difference between `1.0.0-SNAPSHOT` and `1.1.0-PC-SNAPSHOT`?,2/27/17,1
metasim,"What's ""PC""?",2/27/17,1
pomadchin,PointCloud: https://github.com/locationtech/geotrellis/tree/milestone/pointcloud,2/27/17,1
pomadchin,snapshot with geotrellis-pointcloud project and pdal support,2/27/17,1
metasim,So can I interpret `1.0.0-SNAPSHOT` as reallly being `1.0.1-SNAPSHOT` (the release candidate for 1.0.1)?,2/27/17,1
pomadchin,"I can_Ñét be 100% sure, but it looks like it",2/27/17,1
pomadchin,"i guess we need to double check on breaking api changes; but looks like there were no breaking API prs, so it_Ñés 1.0.1",2/27/17,1
metasim,OK. I'm just used the the approach to versioning where you bump the point version right after a release. i.e. `x.y.z-SNAPSHOT` < `x.y.z` < `x.y.(z+1)-SNAPSHOT`.,2/27/17,1
metasim,(c.f. [semver](http://semver.org/#spec-item-11)),2/27/17,1
pomadchin,"yes, we decided to follow this way",2/27/17,1
pomadchin,.1 would be bug fix release; it looks like nearly all prs were fixes (eh sry about inconsistent answers // a bit busy): https://github.com/locationtech/geotrellis/pulls?utf8=%E2%9C%93&q=is%3Apr%20is%3Aall,2/27/17,1
pomadchin,:D :D ,2/27/17,1
metasim,"@echeipesh @pomadchin Interestingly, [this library](https://github.com/sksamuel/avro4s#coproducts) uses shapeless `Coproduct`s to encode the Avro unions.",2/27/17,1
pomadchin,@metasim because Avro union is a list :o probably that's why it's a good abstraction ,2/27/17,1
pomadchin,though very interesting approach :O,2/27/17,1
Charmatzis,"Hi, guys!!! I have one really quick question. Why the above code gives me an error?",2/28/17,1
Charmatzis,"``` var lines: Array[Line] = v.map(x=>x.geom)        val fullLine = lines.reduceLeft((k, l)=> k.union(l))```",2/28/17,1
Charmatzis,"Error:(110, 57) type mismatch; found   : geotrellis.vector.LineOneDimensionUnionResult required: geotrellis.vector.Line        val fullLine = lines.reduceLeft((k, l)=> k.union(l))",2/28/17,1
Charmatzis,"I just want an ay of Lines to be union into one, how can I achive that?",2/28/17,1
pomadchin,"Probably that would help:```scalalines.reduceLeft((k, l)=> k.union(l).geom)```",2/28/17,1
pomadchin,ah no that won_Ñét work,2/28/17,1
Charmatzis,"yeap, it doesn't resolve",2/28/17,1
pomadchin,what type you expect after reduce?,2/28/17,1
pomadchin,Multiline?,2/28/17,1
Charmatzis,"well it doesn't matter, all I have to do, is to union line segments into one Line, if the result is MultiLine I have no problem...",2/28/17,1
pomadchin,"workaround (not safe): ```scalalines.reduceLeft((k, l)=> k.union(l).asMultiLine.get)```",2/28/17,1
Charmatzis,"it doen't work ""type mismatch""",2/28/17,1
Charmatzis,"well, even that it doen't work....```",2/28/17,1
Charmatzis,"```val fullLine = lines.reduceLeft((k, l)=>  k.length + l.length) ```",2/28/17,1
pomadchin,"that should not work, because of the `reduce` function definition.",2/28/17,1
pomadchin,as your accumulator type is different; reduce takes first element of the *list it's applied to* as accumulator,2/28/17,1
Charmatzis,"well yes, I take the first element length (Double)  and added to the second length (Double)",2/28/17,1
pomadchin,the first element of _lines_ array; it_Ñés an original definition of reduce function (fold but with acc as fst elem of a list),2/28/17,1
pomadchin,we have to use fold there to init own accumulator,2/28/17,1
pomadchin,"```scala  val lines: Array[Line] = ???  val fullLine: Option[MultiLine] =     lines      .foldLeft(Option(MultiLine.EMPTY)){ (acc, e)=>         acc.flatMap(_.union(e).asMultiLine)       }```",2/28/17,1
Charmatzis,well that it doen't give me the right result since it gives the same lines as a the lines object,2/28/17,1
Charmatzis,it gives bach line segments,2/28/17,1
pomadchin,hm what do you need in result and what do you want to do with these lines? im just out of context o:,2/28/17,1
pomadchin,:D ,2/28/17,1
pomadchin,"```scalaval r = lines(0).union(lines(1)) ``` gives you a correct result?",2/28/17,1
Charmatzis,"well no,  it also gives me the wrong result.",2/28/17,1
Charmatzis,"the lines are an array of continuous line segments, that thing i want to create a line with no replecation of the points that intersect",2/28/17,1
Charmatzis,I believe I have to go to the point level of the line and then create a line,2/28/17,1
Charmatzis,"let's say val lines = Array[Line(0.0 0.0, 1.0 1.0), Line(1.0 1.0, 2.0 2.0)]the union result will be Line(0.0 0.0, 1.0 1.0, 2.0 2.0)",2/28/17,1
Charmatzis,Well I believe I have found a solution ,2/28/17,1
Charmatzis,"```        val fullLine =  lines.foldLeft(Array(lines(0).points.head)){ (k, l)=>              k :+ l.points.last        }        var l = Line(fullLine).toGeoJson()```",2/28/17,1
karalas,"Hello, I have a problem with Chatta demo when I ingest an external GeoTiff image. When I write the newly ingested image in ```layers``` in https://github.com/geotrellis/geotrellis-chatta-demo/blob/master/static/config.json, I keep getting an error like this:```[ERROR] [02/28/2017 15:11:59.172] [test-demo-akka.actor.default-dispatcher-4] [akka.actor.ActorSystemImpl(test-demo)] Error during processing of request: 'Value with key SpatialKey(133,202) not found for layer Layer(name = ""ndvi"", zoom = 9)'. Completing with 500 Internal Server Error response.```, where ```ndvi``` is the ```""name""``` of the newly ingested image (not binary) defined in ```input.json```. ",2/28/17,1
pomadchin,"@karalas hey, looks like you just have no these keys",2/28/17,1
pomadchin,it_Ñés normal,2/28/17,1
karalas,I thought that the keys were automatically created when I do the ingest process,2/28/17,1
karalas,"and read then for example the ```SpatialKey(x,y)```",2/28/17,1
pomadchin,"ah ._. so the problem of these lines: you have a set of tiffs you want to ingest, right? and all tiffs have own geospatial reference; duting the ingest process app grabs all the input tifs, cut them into tiles into some layout and assignes SpatialKey to each chunk (tile) on a certain zoom level; that means that there can be missing keys",2/28/17,1
pomadchin,"what you can do: you can have a look into your metadata, to look at the layer key bounds and to go through all these keys to figure out what was ingested or not.",2/28/17,1
pomadchin,"in case of succesfull ingest, and good tiffs ingest; you can check it just looking into the correct place on leaflet map",2/28/17,1
pomadchin,everything that_Ñés not included throws 500,2/28/17,1
pomadchin,"i can fix that error to be 404, if it_Ñés confusing for you",2/28/17,1
pomadchin,"@karalas btw, can you give some input tiles, and describe what are you doing? O:",2/28/17,1
karalas,"@pomadchin thanks for the answer. In the ```attributes``` folder, I can see that there are the corresponding .jsons for my new image from zoom level 0 to 14. The corresponding line where we ""assign SpatialKey to each chunk (tile) on a certain zoom level"" as you say above in Chatta demo is here https://github.com/geotrellis/geotrellis-chatta-demo/blob/master/geotrellis/src/main/scala/geotrellis/chatta/ChattaServiceRouter.scala#L126? ",2/28/17,1
pomadchin,i meant that it happens during the ingest,2/28/17,1
pomadchin,"so mmmm this error means that you tried to read: ```scalatileReader.reader[SpatialKey, Tile](layerId).read(key)```and it throwed you `ValueNotFoundError`",2/28/17,1
pomadchin,just means that there is no such `key` in your catalog,2/28/17,1
pomadchin,O: is it makes sense or still not clear what happens?,2/28/17,1
pomadchin,hm,2/28/17,1
karalas,"Yes, thanks for the detailed answer, it makes sense. I try to understand now why this ```key``` has not been created (since the created folder/catalog with the zoom levels and the metadata folder seem ok)",2/28/17,1
pomadchin,can you print here your metadata?,2/28/17,1
pomadchin,"```scalaattributeStore.readMetadata[TileLayerMetadata[SpatialKey]](layerId)```",2/28/17,1
pomadchin,"even: ```scalaprintln(  attributeStore    .readMetadata[TileLayerMetadata[SpatialKey]](id)    .bounds)```",2/28/17,1
pomadchin,if it_Ñés `ChattaServiceRouter` you can find `def getMetaData(id: LayerId): TileLayerMetadata[SpatialKey]` function,2/28/17,1
dorukozturk,"what is the tifftag that is missing from my files so that it raises this: ```scala  def segmentCount: Int =    if (hasStripStorage) {      (this        &|-> TiffTags._basicTags        ^|-> BasicTags._stripByteCounts get) match {        case Some(stripByteCounts) =>          stripByteCounts.size        case None =>          throw new MalformedGeoTiffException(""No StripByteCount information."")      }```",2/28/17,1
karalas,"@pomadchin  I get something like this:```TileLayerMetadata(float32raw,GridExtent(Extent(-2.0037508342789244E7, -2.0037508342789244E7, 2.0037508342789244E7, 2.0037508342789244E7),305.748113140705,305.748113140705),Extent(2337682.610111591, 4946621.804837849, 2484596.9367464115, 5094082.562120422),geotrellis.proj4.CRS$$anon$1@a942e1e1,KeyBounds(SpatialKey(285,190),SpatialKey(287,192)))```for ```getMetaData(LayerId(""ndvi"", 9))``` for example.",2/28/17,1
pomadchin,"Ok so your ayer keybounds are: ```KeyBounds(  SpatialKey(285,190),  SpatialKey(287,192))```",2/28/17,1
karalas,yes,2/28/17,1
pomadchin,"and you are tyring to read `SpatialKey(133,202) `",2/28/17,1
pomadchin,"try to read smth like: `SpatialKey({285-286}, {190-192})` // it_Ñés not a minus its a range; basically you have just 6 keys for your layer",2/28/17,1
pomadchin,and some of them can miss,2/28/17,1
pomadchin,on a map that would mean just to look at the correct place on map,2/28/17,1
rgwozdz,"Haven't found this in docs, but wondering if  there is a way to validate GeoTiffs (prior to ingestion).  I see there is a method `readGeoTiffInfo` that seems to check for some core GeoTiff information.  Is this perhaps the best way to ensure a file submitted for ingestion is indeed a valid GeoTiff?",2/28/17,1
rgwozdz,Perhaps better to just leverage: https://github.com/geotrellis/geotrellis-gdal?,2/28/17,1
metasim,"@rgwozdz I think the code you want is in `geotrellis.raster.io.geotiff.reader.GeoTiffReader#readGeoTiffInfo`. Unfortunately, that function is private.",2/28/17,1
metasim,Maybe the team could be convinced to make it public?,2/28/17,1
rgwozdz,"Actually I just came across the gdal package in GeoTrellis, so I'll try gdalinfo first",2/28/17,1
rgwozdz,Darn - I'm having trouble compiling geotrellis-gdal;  the dependencies still point to azavea,2/28/17,1
lossyrob,"someone updated it in a pull request, which I have been really bad about getting around to merging :/",2/28/17,1
lossyrob,https://github.com/geotrellis/geotrellis-gdal/pull/1,2/28/17,1
lossyrob,you can try compiling that,2/28/17,1
lossyrob,@dorukozturk looks like StripByteCount https://github.com/locationtech/geotrellis/blob/master/raster/src/main/scala/geotrellis/raster/io/geotiff/tags/codes/TagCodes.scala#L40,2/28/17,1
lossyrob,http://www.awaresystems.be/imaging/tiff/tifftags/stripbytecounts.html,2/28/17,1
dorukozturk,@lossyrob cool. So now I am trying to find a smart way to check that out. My tiffs are in s3 and I will do a gdalinfo on them now hoping to see something like this: http://gis.stackexchange.com/questions/34198/adding-georeferencing-data-into-tiff-image/34201,2/28/17,1
dorukozturk,If I do not see a warning I do not know what to do. Is there a way for me to skip the files that are failing in an easy way?,2/28/17,1
metasim,FYI: I'm going to try to fix #2033 ,2/28/17,1
lossyrob,:+1:,2/28/17,1
lossyrob,@dorukozturk interesting. are these images that will be generated in the future and might have this problem? or could you do a one time processing to fix it and be good with that?,2/28/17,1
dorukozturk,I can do a one time processing to fix them,2/28/17,1
lossyrob,"this is a bit hacky, but if you pump them through a gdal translate, they_Ñéll come out with the correct tags",2/28/17,1
lossyrob,"you could use a bootstrap script that will install gdal on EMR nodes, and actually run it through spark",2/28/17,1
lossyrob,or some other way to apply it to the geotiff files and rewrite them,2/28/17,1
lossyrob,"or, you could add the workaround to geotrellis - if the tag isn_Ñét there, compute the stripbytecount by the imagelength like GDAL does",2/28/17,1
dorukozturk,"actually our files were created using gdalwarp, we converted hdf files to tiffs",2/28/17,1
dorukozturk,I think I will try the gdalinfo thing first to see if there are any warnings on any file,2/28/17,1
lossyrob,odd that gdalwarp would create a bad geotiff like that,2/28/17,1
dorukozturk,that is exactly why I am so confused,2/28/17,1
dorukozturk,maybe some of the hdfs were malformed to begin with I really don't know right now.,2/28/17,1
metasim,"geotrellis compiles fine in sbt, but not in IntelliJ. Is this just me, or does this ring a bell?:```geotrellis/spark/src/test/scala/geotrellis/spark/SerializationTests.scala    Error:Error:line (24)object testkit is not a member of package geotrellis.sparkimport geotrellis.spark.testkit._```",2/28/17,1
pomadchin,"@metasim to be honest there would be much more problems rather than this one, if you_Ñéll try to build it with intelij idea",2/28/17,1
metasim,"so, no running tests from within Idea?",2/28/17,1
pomadchin,i think that testkit project can_Ñét be resolved as intelij idea can_Ñét deal with it: https://github.com/locationtech/geotrellis/blob/master/spark/build.sbt#L17-L21,2/28/17,1
pomadchin,was you at least succeded in just compiling gt?,2/28/17,1
pomadchin,tests is another step :D ,2/28/17,1
metasim,I can compile from command line.,2/28/17,1
metasim,Just want to be able to run one of the codec tests,2/28/17,1
metasim,in Idea,2/28/17,1
pomadchin,use sbt,2/28/17,1
pomadchin,:D ,2/28/17,1
pomadchin,i never could make it work in intelij idea;,2/28/17,1
metasim,:/,2/28/17,1
metasim,OK: `spark/testOnly *ExtentCodecsSpec`,2/28/17,1
metasim,Seems to me there's got to be a way of constructing the build without using `internalDependencyClasspath` ,2/28/17,1
pomadchin,"to make it faster: ```$ ./sbt$ project spark$ test-only com.blabla.blabla```",2/28/17,1
pomadchin,"yes, there is a way, to move tests into a separate project",2/28/17,1
lossyrob,"before internalDependencyClasspath, we did the separate projects for tests, like raster-test etc",2/28/17,1
pomadchin,and to create spark-test,2/28/17,1
pomadchin,@lossyrob is it possible to refactor all projects to use ` internalDependencyClasspath`?,2/28/17,1
lossyrob,yeah it_Ñés been on the docket,2/28/17,1
pomadchin,"though i repeat myself again, i don_Ñét think it_Ñés the only problem; i can even compile gt in idea :D",2/28/17,1
metasim,So y'all aint so keen on getting it to play nice with IntelliJ?,2/28/17,1
pomadchin,no :D and Rob had a really bad experience with intelij idea :D :D ,2/28/17,1
lossyrob,I think it wouldn_Ñét be smart to shut out intellij users,2/28/17,1
lossyrob,oye grisha,2/28/17,1
lossyrob,"If there_Ñés a way to get it easier to use with intelliJ, I say that_Ñés a good thing",2/28/17,1
metasim,"Seriously, this speaks to me of a hack at a symptom, not a solution to some underlying problem.",2/28/17,1
pomadchin,"heh, i can try to play around with it, but i don_Ñét think it_Ñés really important",2/28/17,1
metasim,"Since I brought it up, I might take a look later.",2/28/17,1
lossyrob,that would be great - I don_Ñét know enough about intellij to be useful i think,2/28/17,1
metasim,It really just comes down to not using obscure sbt settings.,2/28/17,1
pomadchin,"eh, don_Ñét be excited about it too much, as `shapeless` is always highlighted with red",2/28/17,1
lossyrob,the separate test projects are a pain - just muddy up the codebase,2/28/17,1
pomadchin,and our several internal imports are always red,2/28/17,1
metasim,"If there's a cyclic build dependency, that speaks to me of an alternative configuration needed. (without separate test projects)",2/28/17,1
metasim,The `shapeless` hightlights are a different matter.,2/28/17,1
lossyrob,"so i_Ñéd like to keep that. but if it comes down to _ÑÉworks with IDEs vs has single projects with code and tests_Ñé, we can make that decision",2/28/17,1
lossyrob,but if there_Ñés another way that would be best,2/28/17,1
pomadchin,++,2/28/17,1
pomadchin,"btw, be carefull with our generated things: https://github.com/locationtech/geotrellis/blob/master/project/Boilerplate.scala",2/28/17,1
metasim,:+1: ,2/28/17,1
pomadchin,sometimes idea should be manually pointed to track managed_src,2/28/17,1
metasim,I think they might have fixed that.,2/28/17,1
metasim,I haven't had to manually do that in a while.,2/28/17,1
metasim,(the bug was that the parent of `managed_src` was excluded),2/28/17,1
pomadchin,:+1:,2/28/17,1
pomadchin,@metasim i figured it out: you can manually add it as a dep into your project,2/28/17,1
pomadchin,looks like it works,2/28/17,1
metasim,did you just add `spark-testkit`?,2/28/17,1
metasim,"When I did, I got a funky compiler error.",2/28/17,1
pomadchin,hah you_Ñére correct,2/28/17,1
pomadchin,we tried.,2/28/17,1
pomadchin,:D ,2/28/17,1
pomadchin,but I think we have chances; half a year ago i had much more errors during compilation step,2/28/17,1
metasim,@echeipesh I'm interested in knowing if you approve of the approach I took in PR #2034.,2/28/17,1
metasim,"Is there a reason certain packages are missing from the `aggregate` setting in the `root` project, e.g. `util`?https://github.com/locationtech/geotrellis/blob/master/build.sbt#L75",2/28/17,1
lossyrob,"util would be brought in transitively, but no I don_Ñét think there_Ñés any good reason to explicitly exclude it (in that it would be excluded from a `test` command on the root)",2/28/17,1
lossyrob,"the project structure/builds stuff could def could use a fresh set of eyes on it, so thanks for these questions/fixes",2/28/17,1
metasim,"Ok, good... sbt handles transtivity and aggregation differently depending on the command being executed.",2/28/17,1
karalas,"Hello all, I am trying to run the Landsat emr demo locally. I am able to run all the commands successfully till the ```npm start```, where after the ```Express server listening on port 3000``` I get a long message resulting in ```ERROR in Use of const in strict mode. @ ./components/Leaflet.js 54:0-30ERROR in Use of const in strict mode. @ ./components/Leaflet.js 50:0-35ERROR in Use of const in strict mode. @ ./components/Leaflet.js 52:0-45ERROR in Use of const in strict mode. @ ./~/bootstrap-webpack/index.js 1:0-66ERROR in Use of const in strict mode. @ ./components/charts/TimeSeries.js 58:0-52Child html-webpack-plugin for ""index.html"":         Asset    Size  Chunks       Chunk Names    index.html  577 kB       0           chunk    {0} index.html 540 kB [rendered]        [0] ./~/html-webpack-plugin/lib/loader.js!./~/html-webpack-plugin/default_index.ejs 538 bytes {0} [built]        [1] ./~/lodash/lodash.js 540 kB {0} [built]        [2] (webpack)/buildin/module.js 251 bytes {0} [built]Error: ENOENT, stat '/home/kkaralas/Downloads/geotrellis-landsat-emr-demo-master/viewer/client/index.html'```This file does not exist in the specified folder indeed .",3/1/17,1
pomadchin,yo @karalas do you use https://github.com/geotrellis/geotrellis-landsat-emr-demo repo?,3/1/17,1
pomadchin,how do you run npm commands? from a viewer folder?,3/1/17,1
pomadchin,npm install passes wihtout errors?,3/1/17,1
karalas,"Yes, I run them from viewer folder, but they install with a lot of warnings```npm WARN package.json geotrellis-viewer@0.0.2 No repository field.npm WARN engine less@2.7.2: wanted: {""node"":"">=0.12""} (current: {""node"":""0.10.30"",""npm"":""1.4.21""})npm WARN engine loader-utils@1.0.2: wanted: {""node"":"">=4.0.0""} (current: {""node"":""0.10.30"",""npm"":""1.4.21""})npm WARN engine loader-utils@1.0.2: wanted: {""node"":"">=4.0.0""} (current: {""node"":""0.10.30"",""npm"":""1.4.21""})npm WARN engine loader-utils@1.0.2: wanted: {""node"":"">=4.0.0""} (current: {""node"":""0.10.30"",""npm"":""1.4.21""})npm WARN engine postcss@5.2.15: wanted: {""node"":"">=0.12""} (current: {""node"":""0.10.30"",""npm"":""1.4.21""})npm WARN engine html-minifier@3.4.0: wanted: {""node"":"">=4""} (current: {""node"":""0.10.30"",""npm"":""1.4.21""})npm WARN engine clean-css@4.0.8: wanted: {""node"":"">= 4.0""} (current: {""node"":""0.10.30"",""npm"":""1.4.21""})npm WARN engine normalize-url@1.9.0: wanted: {""node"":"">=4""} (current: {""node"":""0.10.30"",""npm"":""1.4.21""})npm WARN engine vendors@1.0.1: wanted: {""node"":"">=0.11.0""} (current: {""node"":""0.10.30"",""npm"":""1.4.21""})npm WARN optional dep failed, continuing fsevents@1.1.1npm WARN engine deep-extend@0.4.1: wanted: {""node"":"">=0.12.0"",""iojs"":"">=1.0.0""} (current: {""node"":""0.10.30"",""npm"":""1.4.21""})npm WARN optional dep failed, continuing fsevents@1.1.1npm WARN deprecated minimatch@2.0.10: Please update to minimatch 3.0.2 or higher to avoid a RegExp DoS issuenpm WARN engine request@2.79.0: wanted: {""node"":"">= 4""} (current: {""node"":""0.10.30"",""npm"":""1.4.21""})npm WARN engine form-data@2.1.2: wanted: {""node"":"">= 0.12""} (current: {""node"":""0.10.30"",""npm"":""1.4.21""})npm WARN engine hawk@3.1.3: wanted: {""node"":"">=0.10.32""} (current: {""node"":""0.10.30"",""npm"":""1.4.21""})npm WARN engine esprima@3.1.3: wanted: {""node"":"">=4""} (current: {""node"":""0.10.30"",""npm"":""1.4.21""})npm WARN engine boom@2.10.1: wanted: {""node"":"">=0.10.40""} (current: {""node"":""0.10.30"",""npm"":""1.4.21""})npm WARN engine cryptiles@2.0.5: wanted: {""node"":"">=0.10.40""} (current: {""node"":""0.10.30"",""npm"":""1.4.21""})npm WARN engine hoek@2.16.3: wanted: {""node"":"">=0.10.40""} (current: {""node"":""0.10.30"",""npm"":""1.4.21""})```",3/1/17,1
pomadchin,ok; ill double check it on a clean machine; because on my it works,3/1/17,1
karalas,I am on centos7,3/1/17,1
pomadchin,ah,3/1/17,1
pomadchin,im on mac os x ,3/1/17,1
pomadchin,:D ,3/1/17,1
pomadchin,ok; ,3/1/17,1
pomadchin,i can check it on ubuntu; if that would work it would mean that smth wrong is in your deps ,3/1/17,1
karalas,ok,3/1/17,1
pomadchin,and i can recommend you to try some docker image with npm installed inside,3/1/17,1
karalas,how can I test it with docker?,3/1/17,1
pomadchin,i think you can use node image https://hub.docker.com/_/node/,3/1/17,1
pomadchin,"somehow like this:```bash$ docker run -it -p 3000:3000 -v $PWD/viewer:/viewer node:latest bash# at this point you would be inside your container$ cd /viewer; npm install && npm start```",3/1/17,1
pomadchin,"@karalas i checked: installed only nodejs, npm, node legacy (alias node for nodjs command) it works",3/1/17,1
pomadchin,what versions of libs do you have?,3/1/17,1
pomadchin,"```bash$ npm --version3.5.2$ nodejs --versionv4.2.6```",3/1/17,1
karalas,@pomadchin  thank you very much for your help! I was able to run the demo successfully with the docker solution. My ```npm --version``` is ```1.4.21``` and my ```node --version``` is ```v0.10.30```.,3/1/17,1
pomadchin,eh glad that docker solution worked well :)),3/1/17,1
pomadchin,you can try to install `npm 3.5.2` and nodejs `4.2.6` on centos and to try the same,3/1/17,1
pomadchin,i think it would help,3/1/17,1
metasim,"@lossyrob @pomadchin In case you're interested, there's another approach to the ""testkit"" cyclic dependency issue, so I'll give it a try at some point to see if it makes IntelliJ happier:https://gitter.im/sbt/sbt?at=58b6367f1465c46a56cb312e",3/1/17,1
pomadchin,@metasim cool; crossed fingers,3/1/17,1
metasim,So far looking good.,3/1/17,1
pomadchin,you want to say it works?,3/1/17,1
pomadchin,very cool,3/1/17,1
pomadchin,would you create a PR / issue with description? ^^',3/1/17,1
metasim,"In the middle of it now. Will submit a PR once I get it all working, as I'm also having to fix some mis-uses of the `provided` scope and ivy specifiers like `test->test` vs `test`.",3/1/17,1
pomadchin,thanks for that cleanup,3/1/17,1
metasim,I'm used to it. ;-),3/1/17,1
metasim,End up filling that role on a lot of projects.,3/1/17,1
metasim,Most of my SO cred points come from answers to CMake questions :/,3/1/17,1
metasim,"My policy about `provided` is to almost never use it in a library, but give that discretion to the end-user application build. It's the perogative of the application to determine what's going to be available automatically at runtime vs. collected transitively via the build.",3/1/17,1
metasim,"If youre library uses `provided`, then you're basically dictating to the end-user application how they do deployment (or manually maintain their build).",3/1/17,1
metasim,"hadoop & spark are arguably an exception, but in our internal code we don't use `provided` except for the application/cli sub-module.",3/1/17,1
pomadchin,"yes, that's why spark and hadoop are provided libraries",3/1/17,1
pomadchin,"we don't need them for an sbt assembly jar, ofc if you don't have a real reason for it",3/1/17,1
metasim,Some of our utilities actually want spark/hadoop linked in at runtime.,3/1/17,1
metasim,(i.e. when we're not doing `spark-submit`),3/1/17,1
pomadchin,when you don't use spark-submit than you need to provide binaries of version you prefer,3/1/17,1
pomadchin,and to build uber assebly for spark submit you have to exclude hadoop and spark deps as both these deps are useless for a job ._. looks like not the best solution,3/1/17,1
metasim,Done. See #2036. Works in IntelliJ with one manual operation.,3/1/17,1
pomadchin,@metasim thank you; great,3/1/17,1
dorukozturk,"hey, am I being impatient or a hanging task on the spark ui is a bad thing",3/1/17,1
pomadchin,"@dorukozturk you are correct, that_Ñés bad",3/1/17,1
pomadchin,:D,3/1/17,1
metasim,"@dorukozturk If nothing looks supicous in the logs, check for GC thrashing ",3/1/17,1
dorukozturk,how do I do that?,3/1/17,1
dorukozturk,I could not find anything suspicios in the logs,3/1/17,1
metasim,I usually connect with jvisualvm,3/1/17,1
dorukozturk,instances are not using allocated resources ,3/1/17,1
metasim,On one of the Spark UI pages there's something about how much CPU is spent doing GC,3/1/17,1
lossyrob,@dorukozturk are you using windowed reading (maxTileSize) on multband tiffs?,3/1/17,1
metasim,"The host resources, or the JVM allocated heap space?",3/1/17,1
lossyrob,we found a pretty bad problem with that,3/1/17,1
lossyrob,that eats memory like a beast,3/1/17,1
dorukozturk,I sshed into the machine did a htop and it is not maxing memory or cpu,3/1/17,1
dorukozturk,I use windowed reading on singleband tiffs,3/1/17,1
lossyrob,hm,3/1/17,1
metasim,`jstat` is another option if you don't have graphical console access.,3/1/17,1
metasim, But it can be overwhelming.,3/1/17,1
lossyrob,where does it hang,3/1/17,1
lossyrob,"what step, what_Ñés the set of operations in the stage where there_Ñés a hanging task",3/1/17,1
lossyrob,"is it the last task that hangs in that stage (i.e. there_Ñés only one specifically that is hanging), or does it hang somehwere in the middle of completion",3/1/17,1
lossyrob,"wondering how far it gets, to detect whether it_Ñés a problem reading the geotiff or if it_Ñés already been in memory, etc",3/1/17,1
dorukozturk,repartition at S3GeoTiffRDD.scala:147,3/1/17,1
dorukozturk,this is the stage that it fails,3/1/17,1
dorukozturk,322/7930,3/1/17,1
dorukozturk,it has been hanging there for 2 hours,3/1/17,1
dorukozturk,at 322,3/1/17,1
dorukozturk,I think it is a file read thing,3/1/17,1
dorukozturk,I think I will just to a gdal_retile on all of those and give that a try,3/1/17,1
lossyrob,hm. that hasn_Ñét even tried to read in the whole GeoTiff yet,3/1/17,1
lossyrob,that_Ñés just working on the TiffTags,3/1/17,1
dorukozturk,some of my tiffs are 2 G in size ,3/1/17,1
lossyrob,so something is definitely up there,3/1/17,1
lossyrob,https://github.com/locationtech/geotrellis/blob/master/s3/src/main/scala/geotrellis/spark/io/s3/S3GeoTiffRDD.scala#L147 this here hasn_Ñét even tried to read the geotiff data,3/1/17,1
dorukozturk,I grep ed the logs on s3 and there is nothing there indicating an error/warning,3/1/17,1
lossyrob,just the tags,3/1/17,1
dorukozturk,himm so you think retile will solve my problems?,3/1/17,1
lossyrob,"perhaps. but if GDAL is somehow writing tiles that we can_Ñét read, there might be a problem otherwise",3/1/17,1
lossyrob,"it hadn_Ñét gotten to the stage where memory should be an issue, which would be where the tiling would really help if the window stuff isn_Ñét working",3/1/17,1
dorukozturk,so is there a way for me to figure out what files is breaking ?,3/1/17,1
lossyrob,"@dorukozturk I would say logging - you could print out file names in a map of the RDD before that repartition, and then see what was the last files to print out before hanging. Then try to manually read those files (just the TiffTags) and see if you get a hang outside of spark",3/1/17,1
dorukozturk,Great will try that !!! Thanks @lossyrob ,3/2/17,1
rgwozdz,@lossyrob - I'm struggling bit with how to leverage/integrate https://github.com/geotrellis/gdal-scala into an application that I am building that already leverages Geotrellis.  Should a `sbt publish-local` at the root of `gdal-scala` be enough to leverage the code in another application on the same server?,3/2/17,1
pomadchin,"@rgwozdz yes, i think you have to publish it; i don_Ñét think gdal-scala was published somewhere",3/2/17,1
Charmatzis,Hi guys! I have a very really good (stupid) question..... ,3/2/17,1
Charmatzis,"I can see that I can do geometric operations with geometries, like polygon.intersects(polygon), but how can I check if a whole layer (e.g. geojson file) intersects another layer?",3/2/17,1
Charmatzis,should I use something like ,3/2/17,1
Charmatzis,"```geojson.map(t=> geojson2.map(_.intersects(t.geom))```",3/2/17,1
pomadchin,i think it depends on your geojson types,3/2/17,1
pomadchin,what geometries do you have in you geojsons?,3/2/17,1
Charmatzis,well it is featureCollection with Multipolygons,3/2/17,1
pomadchin,i think you can do intersections of two mulipolygon collections,3/2/17,1
pomadchin,"```scala  val a: MultiPolygon = ???  val b: MultiPolygon = ???  Seq(a, b).intersect(Seq(a, b))```",3/2/17,1
dorukozturk,"Hi I am trying to do a check on my tiffs on s3. So the ultimate goal is to see whether geotrellis can read them or not. what would be the best way to do that? Checking the tags maybe? I tried a gdalinfo on all layers and gdal can read from s3 now, it took around 20 minutes for the whole 8000 files but there are no errors on the gdalinfo output. I  want to do the same thing for a geotrellis reader. Can you guide me a little to the right direction?",3/2/17,1
karalas,"Hello, I have preprocessed a singleband GeoTiff image with gdal to add a TIFFTAG_DATETIME (```gdal_edit -mo TIFFTAG_DATETIME=""2017:03:02 12:00:00"" A.tif```) and then I try to ingest it to the local filesystem via the ETL tool (``` Etl.ingest[TemporalProjectedExtent, SpaceTimeKey, Tile](args) ```).  I get the following error:```Exception in thread ""main"" java.lang.RuntimeException: Unable to find input module of type 'hadoop' for format `geotiff for input key type 'geotrellis.spark.TemporalProjectedExtent' and tile type 'geotrellis.raster.Tile'```My ```input.json``` is as follows:``` [  {    ""name"": ""test"",    ""format"": ""geotiff"",    ""backend"": {      ""type"": ""hadoop"",      ""path"": ""file:///data/A.tif""    },    ""cache"": ""NONE""  }]```",3/2/17,1
karalas,Maybe am I missing something? Thank you,3/2/17,1
dorukozturk,I believe format will be temporal-geotiff not geotiff,3/2/17,1
dorukozturk,in your input.json file,3/2/17,1
karalas,"@dorukozturk  Thank you, I have also tried with ```temporal-geotiff```, but then I get an error like this:```Exception in thread ""main"" java.lang.ClassCastException: geotrellis.spark.SpaceTimeKey cannot be cast to geotrellis.spark.SpatialKey```",3/2/17,1
karalas,"Also, why the ```""type"": ""hadoop""```, since the file is a GeoTiff on local filesystem?",3/2/17,1
metasim,`hadoop` will work for both local fs and hdfs.,3/2/17,1
metasim,You could use the local FS backend instead if you want.,3/2/17,1
karalas,"Forget my issue, it's ok now, thanks!",3/2/17,1
schBen,"I also have a question about ETL config: in the geotrellis-landsat-emr-demo, you specify ""landsat"" as a backend type for the input. [input](https://github.com/geotrellis/geotrellis-landsat-emr-demo/blob/master/conf/input-local.json)Where is this type specified in the code?",3/2/17,1
pomadchin,@schBen you need to implement module for it: https://github.com/geotrellis/geotrellis-landsat-emr-demo/blob/master/ingest/src/main/scala/demo/etl/landsat/TemporalMultibandLandsatInput.scala,3/2/17,1
pomadchin,so landsat-emr demo is also a demonstartion of how to implement smth for our etl stuff o: though still not brilliant api (at all) :D ,3/2/17,1
pomadchin,It_Ñés enabled here: https://github.com/geotrellis/geotrellis-landsat-emr-demo/blob/master/ingest/src/main/scala/demo/LandsatIngest.scala#L67,3/2/17,1
schBen,"@pomadchin okay I saw this input class but I thought it just selects the input process (fetch, tile, partition...)",3/2/17,1
schBen,"so thats specified by ""name"" and ""format"" in the input config",3/2/17,1
schBen,"so, I mean why do I have to write ""landsat"" instead of ""hadoop"" in the backend part",3/2/17,1
schBen,and what does that trigger exactly?,3/2/17,1
pomadchin,"we are talking about input class, that means it _reads_ data into spark memory, it is not hdfs because tiles are not on hdfs, we use landsat util to load it from aws into spark memory O:",3/2/17,1
pomadchin,https://github.com/geotrellis/geotrellis-landsat-emr-demo/blob/master/ingest/src/main/scala/demo/LandsatIngest.scala#L68-L69,3/2/17,1
pomadchin,it is used just there,3/2/17,1
schBen,hmm okay I have seen that but it's not completely clear to me ;),3/2/17,1
schBen,so is this property definded here?,3/2/17,1
schBen,https://github.com/geotrellis/geotrellis-landsat-emr-demo/blob/master/ingest/src/main/scala/demo/etl/landsat/LandsatInput.scala#L19,3/2/17,1
pomadchin,"oh, yes :D)",3/2/17,1
pomadchin,"hm, i think there is a bug (lul)",3/2/17,1
schBen,so this is then given twice in the config?,3/2/17,1
pomadchin,twice?,3/2/17,1
schBen,just to be sure,3/2/17,1
schBen,https://github.com/geotrellis/geotrellis-landsat-emr-demo/blob/master/conf/input-local.json#L3,3/2/17,1
schBen,https://github.com/geotrellis/geotrellis-landsat-emr-demo/blob/master/conf/input-local.json#L6,3/2/17,1
pomadchin,only here: https://github.com/geotrellis/geotrellis-landsat-emr-demo/blob/master/conf/input-local.json#L6,3/2/17,1
pomadchin,[this](https://github.com/geotrellis/geotrellis-landsat-emr-demo/blob/master/conf/input-local.json#L3) line can be whatever you want,3/2/17,1
pomadchin,just any random string,3/2/17,1
schBen,ah okay!!,3/2/17,1
pomadchin,"it_Ñés used as a name if the ingest dataset, layername otherwords",3/2/17,1
schBen,yes it's in the manual :D,3/2/17,1
schBen,but I was confused,3/2/17,1
schBen,"okay... so ""landsat"" is the input method. ",3/2/17,1
schBen,and then there is this trait here https://github.com/geotrellis/geotrellis-landsat-emr-demo/blob/master/ingest/src/main/scala/demo/etl/landsat/EtlConfLandsatMethods.scala,3/2/17,1
schBen,"which uses a method getPath https://github.com/geotrellis/geotrellis-landsat-emr-demo/blob/master/ingest/src/main/scala/demo/etl/landsat/EtlConfLandsatMethods.scala#L61 , which requires us to use the UserDefinedPath . And thats our ""landsat-util-configuration-string"". Is that correct? :)",3/2/17,1
pomadchin,yep,3/2/17,1
schBen,ahhh... okay. well that's crystal clear then. thank you ;),3/2/17,1
pomadchin,;),3/2/17,1
dorukozturk,I think the order here is mixed https://github.com/locationtech/geotrellis/blob/master/spark-etl/src/main/scala/geotrellis/spark/etl/config/json/ConfigFormats.scala#L243-L244,3/2/17,1
pomadchin,"@dorukozturk ._. indeed, though it_Ñés for writes, thanks for the catch! do you want to create a PR?",3/2/17,1
lossyrob,"This is really great, @andypetrella_Ñés team is doing awesome work with GeoTrellis! https://blog.kensu.io/2017/03/scalable-geospatial-data-analysis-with-geotrellis-spark-sparkling-water-and-the-spark-notebook/",3/2/17,1
dorukozturk,"@pomadchin I can create a pr, sure",3/2/17,1
schBen,"@pomadchin Sorry I have to come back to the etl-input-config :)So the property ""name"" is just the name of the catalog layer, that's clear. But the ""format"" and ""backend""(""type"") keys seem to have no effect in the landsat-example case / UserDefinedPath. That is, this config also works fine with the landsat demo:```[  {    ""name"": ""justatest"",    ""format"": """",    ""backend"": {      ""type"": """",      ""path"": ""--cache catalog-cache --bandsWanted 4,3,2,5,QA --bbox -43.9883,-22.3221,-42.7695,-21.0435 --startDate 2015-06-01 --endDate 2015-06-10 --limit 1""    },    ""cache"": ""NONE""  }]```I ask because I want to provide my own input module, probably also user defined. So I don't need these properties?",3/3/17,1
Charmatzis,"Hi, I am still working on intersecting and I get this kind of error",3/3/17,1
Charmatzis,"""This method does not support GeometryCollection arguments""",3/3/17,1
Charmatzis,"```t:Feature[Line, Smth]= ??????geom1 : Seq[MultiPolygon]=?????geom1 .intersects(t.geom)```",3/3/17,1
pomadchin,"@schBen yes, that_Ñés why i was talking about a bug; it_Ñés called here explicitly: https://github.com/geotrellis/geotrellis-landsat-emr-demo/blob/master/ingest/src/main/scala/demo/LandsatIngest.scala#L68-L69",3/3/17,1
pomadchin,it requires some refactor; as we have to call: https://github.com/geotrellis/geotrellis-landsat-emr-demo/blob/master/ingest/src/main/scala/demo/LandsatIngest.scala#L37,3/3/17,1
pomadchin,I think i can publish a small refactor,3/3/17,1
pomadchin,"but if you have smth less specific than we have in landsat emr demo: replace [this](https://github.com/geotrellis/geotrellis-landsat-emr-demo/blob/master/ingest/src/main/scala/demo/LandsatIngest.scala#L68-L69) with `etl.load[I, V]`",3/3/17,1
schBen,"@pomadchin but this [metadata method](https://github.com/geotrellis/geotrellis-landsat-emr-demo/blob/master/ingest/src/main/scala/demo/LandsatIngest.scala#L37) is called in this [part](https://github.com/geotrellis/geotrellis-landsat-emr-demo/blob/master/ingest/src/main/scala/demo/LandsatIngest.scala#L77), and that seems to be concerned with the output. so you mean it has to be called before? yeah maybe an example would make it clear :)",3/3/17,1
pomadchin,I think i can refator to make it more clear ^^_Ñé not now though ):,3/3/17,1
dorukozturk,"@grisha I want to ""append"" to an already ingested layer. I am using the spark-etl assembly and I see that I need a layer updater. My dataset is global which consists of (8000 files). I see that you have this example(https://gist.github.com/pomadchin/c2c8346ab0ef1ece11cf4432a87d14ec#file-layerupdatersample-scala-L70-L71) my question is since my layer is global can I preset somehow the attribute store and do not deal with updater? So simply can I have 8000 layers on s3 jam all the data on the zoom levels to the same layer and somehow create my json files (I am literally thinking about rysnc)? Since my layer is global the extent should not be a problem right? My question is can I  create the json metadata files by hand (programmatically but not with geotrellis) for a global layer?",3/3/17,1
pomadchin,"@dorukozturk hm, what do you mean by not to deal with layer updater?",3/3/17,1
pomadchin,"can you expand a bit, what_Ñés your idea? o: i just don_Ñét really get what you are trying to do; but ofc, you can preset your attr store, and it_Ñés done with layer writer // as you don_Ñét have a layer in your attr store before write",3/3/17,1
lossyrob,@dorukozturk the issue there is with the SFC index,3/3/17,1
lossyrob,"each S3 file name is based on the space filling curve index, which plays into the index configuration in the metadata",3/3/17,1
lossyrob,so you_Ñéd have to name every tile appropriately,3/3/17,1
lossyrob,"but can it be done? It could, but it might end up being more difficult",3/3/17,1
lossyrob,What are the things your trying to avoid/problems your having/optimizations your thinking of?,3/3/17,1
dorukozturk,So i have 8000 files and i want to visualize them all. As far as i understand i can either give the s3 bucket path which holds all my tiffs or i can just add have a layer and keep updating it iteratively. Let me ask it this way: can a reader read from the catalog without the layer id? If i have 8000 layers as opposed to 1 layers with 8000 files in it in my catalog can i read them all with a boundary query? I want to ingest my layers one by one and show them all at once:),3/3/17,1
lossyrob,Do the 8000 files overlap?,3/3/17,1
lossyrob,The way to do that would be  inside a tile server,3/3/17,1
lossyrob,"so say you have 8000 layers, 8000 layer IDs",3/3/17,1
lossyrob,"in this tile server that you would code up, I_Ñéd have a structure that related (in memory cache) which layer IDs covered which `z/x/y` tiles",3/3/17,1
lossyrob,"so that per tile request, I can look up which layers I would need to read from",3/3/17,1
lossyrob,"then, pull down the appropriate tiles, mosaic them if needed, color correct, a la all the stuff that Raster Foundry does, and then put it on a map",3/3/17,1
lossyrob,so definitely possible ,3/3/17,1
lossyrob,but requires more coding in the tile server. which is probably where you want to do a lot of the work anyway,3/3/17,1
dorukozturk,Sure so having 8000 ids wont be a problem? And do you have an example for asking only the tiles in the viewport?,3/3/17,1
lossyrob,"Raster Foundry does it, so I_Ñéd say look into that code: https://github.com/azavea/raster-foundry/blob/develop/app-backend/tile/src/main/scala/routes/MosaicRoutes.scala",3/3/17,1
lossyrob,"but it does a whole lot of other things, so it might be hard to parse just that out of the code.",3/3/17,1
dorukozturk,I double checked so my layers do not overlap that means I do not have to deal with mosaicing right?,3/3/17,1
dorukozturk,I just need z/x/y to layer name somehow,3/3/17,1
lossyrob,yeah,3/3/17,1
lossyrob,which isn_Ñét too difficult - each of the layer metadata will have the grid bounds of the layer,3/3/17,1
lossyrob,https://github.com/locationtech/geotrellis/blob/master/spark/src/main/scala/geotrellis/spark/TileLayerMetadata.scala#L49,3/3/17,1
lossyrob,I_Ñéd actually just read the extent,3/3/17,1
lossyrob,and you can compute the grid bounds for each zoom level,3/3/17,1
lossyrob,"maybe event compute the 8000 extents once, run through all the layers and gather layer name to extent, save that off in a JSON file somewhere",3/3/17,1
lossyrob,"then on tile server start up, read that in, compute the grid bounds (statically or on the fly per tile request)",3/3/17,1
lossyrob,"`LayerList.layerFor(zoom, x, y): LayerId` something like that",3/3/17,1
dorukozturk,Makes perfect sense. Rathen than opening up 8000 json files I will be opening one. Ok that sounds reasonable now. Thank you very much,3/3/17,1
andypetrella,"@lossyrob thanks man :-)> This is really great, @andypetrella_Ñés team is doing awesome work with GeoTrellis! https://blog.kensu.io/2017/03/scalable-geospatial-data-analysis-with-geotrellis-spark-sparkling-water-and-the-spark-notebook/",3/4/17,1
andypetrella,"Let us know if there are something we could do better when using the GeoTrellis parts. Also, we consider submitting something at Foss4G on this",3/4/17,1
zainab-ali,"Hello.  I'm experimenting with `geotrellis-vector` as a geometry library, but I'm struggling with some basic operations.  For example, how would I find the shortest distance between a point and a line?  Given that I'm finding it somewhat difficult, is `geotrellis-vector` the right thing to use for this?",3/4/17,1
lossyrob,"@zainab-ali would this work?```scalascala> import geotrellis.vector._import geotrellis.vector._scala> val p = Point(2, 1)p: geotrellis.vector.Point = POINT (2 1)scala> val l = Line((0, 0), (2, 2))l: geotrellis.vector.Line = LINESTRING (0 0, 2 2)scala> l.distance(p)res0: Double = 0.7071067811865476```",3/6/17,1
jmelching,I am looking to use geotrellis-spark to perform a spatial intersects tests between 2 large sets of polygons.  I see https://github.com/locationtech/geotrellis/blob/master/spark/src/main/scala/geotrellis/spark/join/VectorJoin.scala but this seems to be doing a Cartesian product so won't work well for my case.  Is there anything I am missing?  I also wasn't quite sure what this was for: https://github.com/locationtech/geotrellis/blob/master/spark/src/main/scala/geotrellis/spark/join/SpatialJoin.scala,3/6/17,1
zainab-ali,"@lossyrob thanks! That was so basic, I can't believe I missed it.  ",3/6/17,1
echeipesh,@jmelching Those are the two stabs at spatial join we took. `SpatialJoin` was the first attempt where we create a grided partitioner and then working from mapping of partition id to spatial extent it covers. The code in`VectorJoin` was the second attempt where we rely on some of the spark implemintation to make the cartisian join efficient. Looks like we should have documented it a bit better because I_Ñéll have to code dive again to remind myself why it will not produce a full cartisian shuffle when it short circuits `compute` function based on the meta rdd.,3/6/17,1
jmelching,Ok.  I'll try the VectorJoin code on one of our big sets and see what kind of performance i get.,3/6/17,1
echeipesh,"Sounds good, if it is suboptimal we will consider it a bug.",3/6/17,1
jamesmcclain,"@jmelching I am the author of the VectorJoin code, I can answer questions",3/6/17,1
jamesmcclain,The core idea is to compute bounding boxes around partitions of RDDs so that partitions that can be determined ahead of time to be non-overlapping do not have to have their contents compared,3/6/17,1
jamesmcclain,The bounding boxes around partitions are computed [here](https://github.com/locationtech/geotrellis/blob/master/spark/src/main/scala/org/apache/spark/rdd/FilteredCartesianRDD.scala#L41-L42),3/6/17,1
jamesmcclain,The [metapred parameter](https://github.com/locationtech/geotrellis/blob/master/spark/src/main/scala/org/apache/spark/rdd/FilteredCartesianRDD.scala#L35) is a function which answers whether the bounding boxes given as input intersect,3/6/17,1
jamesmcclain,"If the bounding boxes [intersect](https://github.com/locationtech/geotrellis/blob/master/spark/src/main/scala/org/apache/spark/rdd/FilteredCartesianRDD.scala#L68-L72) then their contents are compared, otherwise the set of the respective intersections of all items in the first partition will all items in the second partition is declared [empty](https://github.com/locationtech/geotrellis/blob/master/spark/src/main/scala/org/apache/spark/rdd/FilteredCartesianRDD.scala#L73)",3/6/17,1
jamesmcclain,This code relies on the input being spatially sorted,3/6/17,1
jamesmcclain,In order to get any benefit,3/6/17,1
jamesmcclain,"If there are any issues, we will try to address them.  Thank you",3/6/17,1
joshmarcus,"hey folks!  i am looking to import some MODIS satellite imagery, apply some masks, and do some zonal summary (with the zones defined as raster or shapefile, either way) as part of an exploration.   i also want to try doing that in python.  is there good documentation that might guide my path, both for ingest & for operation?",3/6/17,1
lossyrob,"@joshmarcus the python bit is pretty new/being developed, so potentially you could do that although the documentation will certainly be lacking (and the code changing beneath you)",3/6/17,1
lossyrob,"if you wanted to do it in scala, though, there_Ñés some docs around that",3/6/17,1
joshmarcus,scala totally works too,3/6/17,1
lossyrob,"I think it_Ñés possible in python (we_Ñéve done ETL stuff in python, barely), but would be a wild west sort of thing",3/6/17,1
joshmarcus,:gun:,3/6/17,1
lossyrob,"@mmcfarland has been doing some work on zonal summaries in python using shapely and numpy arrays, so at some point we_Ñéd like to bring that code into pulling tiles out of a GeoTrellis backend and serving those out ;)",3/6/17,1
joshmarcus,yeah for the python thing i would just need two numpy arrays,3/6/17,1
lossyrob,definitely possible. but I haven_Ñét played around with the python stuff to even know how to do it :) perhaps the python people working on that could use this oppurtunity to document some code/tutorial for it,3/6/17,1
lossyrob,"from the scala side, probably a bit more complicated of a demo that would contain the functionality you_Ñéd want is here https://github.com/geotrellis/geotrellis-landsat-emr-demo",3/6/17,1
lossyrob,the `ingest` subproject would be where to start,3/6/17,1
jmelching,@jamesmcclain can you just explain the spatially sorted part?,3/6/17,1
lossyrob,"though because it_Ñés using an API to find landsat scenes, it_Ñés overkill if you just have an S3 bucket of stuff you want to ingest",3/6/17,1
joshmarcus,"ah, right on",3/6/17,1
pomadchin,simpler examples of etl jsons are here: https://github.com/geotrellis/geotrellis-chatta-demo/tree/master/geotrellis/conf,3/6/17,1
lossyrob,"yeah, but might take a bit to grok what etl json is and how it_Ñés used ;)",3/6/17,1
lossyrob,http://geotrellis.readthedocs.io/en/latest/tutorials/etl-tutorial.html,3/6/17,1
lossyrob,should explain some of that,3/6/17,1
lossyrob,"you can build an ETL assembly direct from the `spark-etl` subproject and run it, or include it as a dependency in your own `etl` subproject ",3/6/17,1
lossyrob,"trying to find a good example of that, surprised that it_Ñés not easier to find",3/6/17,1
dorukozturk,"hey guys how can I do a .map here to add multiple layers?```scala  def tilesRoute =    pathPrefix(Segment / IntNumber / IntNumber / IntNumber) { (layer, zoom, x, y) =>      def colorMap(layer: String) = layer match {        case ""nlcd"" => ColorMap.fromStringDouble(""11:476BA0;12:D1DDF9;21:DDC9C9;22:D89382;23:ED0000;24:AA0000;31:B2ADA3;41:68AA63;42:1C6330;43:B5C98E;51:A58C30;52:CCBA7C;71:E2E2C1;72:C9C977;73:99C147;74:77AD93;81:DBD83D;82:AA7028;90:BAD8EA;95:70A3BA"").get        case _ => ColorMap.fromStringDouble(""0.05:ffffe5aa;0.1:f7fcb9ff;0.2:d9f0a3ff;0.3:addd8eff;0.4:78c679ff;0.5:41ab5dff;0.6:238443ff;0.7:006837ff;1:004529ff"").get      }      complete {        Future {          val tileOpt =            readerSet.readSinglebandTile(layer, zoom, x, y)          tileOpt.map { tile =>            val png = Render.renderLayer(tile, colorMap(layer))            pngAsHttpResponse(png)          }        }      }  }```",3/6/17,1
pomadchin,building just etl-assembly would be enough to ingest data without temporal component ):,3/6/17,1
dorukozturk,so lets say I have an array with layer names inside ,3/6/17,1
lossyrob,"true, but it_Ñés a big jump to move from the assembly to using it in code, and it would be good to have a simple example of that",3/6/17,1
lossyrob,since I feel like that_Ñés what we do more than just use the assembly,3/6/17,1
pomadchin,ah: https://github.com/geotrellis/geotrellis-chatta-demo/blob/master/geotrellis/src/main/scala/geotrellis/chatta/ChattaIngest.scala,3/6/17,1
lossyrob,nice,3/6/17,1
joshmarcus,"yeah my use case is directories of tiles, organized by type and date",3/6/17,1
lossyrob,do they have geotiff tags that have the date?,3/6/17,1
lossyrob,it gets slightly more complicated if you need to parse out the time tag from the file path,3/6/17,1
pomadchin,"we have a great issue to allow to encode time / smth else in files path / names; as it_Ñés really common in gis community: https://github.com/locationtech/geotrellis/issues/1958",3/6/17,1
lossyrob,"ah, we do pass it along in S3",3/6/17,1
lossyrob,so yeah we need to do that for the other backends,3/6/17,1
joshmarcus,so they are not geotiffs,3/6/17,1
lossyrob,ah. what are they?,3/6/17,1
lossyrob,"@dorukozturk you could just map over the layer names and retrieve a `Seq[Tile]` or the like, and then `.merge` them",3/6/17,1
joshmarcus,they are HDF4,3/6/17,1
lossyrob,we can_Ñét read that natively,3/6/17,1
pomadchin,oh its modis ye,3/6/17,1
pomadchin,:D ,3/6/17,1
lossyrob,"so you_Ñéd have to `gdal_translate`, or use the `geotrellis-gdal` bindings",3/6/17,1
dorukozturk,@joshmarcus I just ingested 7300 hdf files to s3 so I can help you (you should convert them to tiff),3/6/17,1
joshmarcus,"right, makes sense.  we do extract geotiffs",3/6/17,1
lossyrob,ah nice :),3/6/17,1
joshmarcus,"in some cases, we don't ... but ok, good to know",3/6/17,1
joshmarcus,raw -> geotiff -> ingest,3/6/17,1
pomadchin,mb today / tomoroow i_Ñéll have a look at our geotrellis-gdal; it would allow to ingest hdf5 and jpeg2000,3/6/17,1
lossyrob,"If you know anyone that_Ñés a good fit for this, we might end up with a native HDF4 reader https://fellowship.azavea.com/#/projects?project=scala_native_netcdf_reader",3/6/17,1
joshmarcus,"curious if geotrellis-gdal might make a transient ingest possible, but i will stay clear of that for now",3/6/17,1
joshmarcus,(transient ingest meaning ... don't write out the geotiffs to s3),3/6/17,1
lossyrob,it should. It just needs some love,3/6/17,1
lossyrob,(also https://fellowship.azavea.com/#/projects?project=geotrellis_gdal_integration),3/6/17,1
dorukozturk,"so I am trying something like this: ``` scala  def weldRoute =    pathPrefix(Segment / IntNumber / IntNumber / IntNumber) { (extent, zoom, x, y) =>      def colorMap = ColorMap.fromStringDouble(""0.05:ffffe5aa;0.1:f7fcb9ff;0.2:d9f0a3ff;0.3:addd8eff;0.4:78c679ff;0.5:41ab5dff;0.6:238443ff;0.7:006837ff;1:004529ff"").get      val hardCodedExtent = ""-8766409.899970295,5009377.085697312,-8140237.7642581295,5635549.221409474""      val layers = layerLookup.getIntersectingLayers(hardCodedExtent)      for (layer <- layers ) {        complete {          Future {            val tileOpt =              readerSet.readSinglebandTile(layer, zoom, x, y)            tileOpt.map { tile =>              val png = Render.renderLayer(tile, colorMap)              pngAsHttpResponse(png)            }          }        }      }    }```",3/6/17,1
dorukozturk,so layers is an Array[String] with layer names that I want to overlay,3/6/17,1
joshmarcus,"or HDF5, which is a nice format",3/6/17,1
lossyrob,"you are doing a for loop (which evaluates to `Unit`), so that_Ñés a problem",3/6/17,1
lossyrob,"we can think it through like, what is the input and what is the result you want?",3/6/17,1
dorukozturk,"yepp this is my error ```shError:(51, 18) type mismatch; found   : Unit required: akka.http.scaladsl.server.RequestContext => scala.concurrent.Future[akka.http.scaladsl.server.RouteResult]      for (layer <- layers ) {```",3/6/17,1
lossyrob,seems like you_Ñére trying to write some `Seq[LayerId] => Future[Option[HttpResponse[Png]]]`,3/6/17,1
lossyrob,or something like that,3/6/17,1
dorukozturk,"the input is list of layerids in the catalog,",3/6/17,1
lossyrob,"since we have `pngAsHttpResponse` and `Render.rednerLayer`, what we really want is",3/6/17,1
lossyrob,`Seq[LayerId] => Tile`,3/6/17,1
lossyrob,or,3/6/17,1
lossyrob,`Seq[LayerId] => Future[Tile]` right?,3/6/17,1
dorukozturk,yes,3/6/17,1
lossyrob,and then we could do,3/6/17,1
lossyrob,"```scaladef weldRoute =    pathPrefix(Segment / IntNumber / IntNumber / IntNumber) { (extent, zoom, x, y) =>      def colorMap = ColorMap.fromStringDouble(""0.05:ffffe5aa;0.1:f7fcb9ff;0.2:d9f0a3ff;0.3:addd8eff;0.4:78c679ff;0.5:41ab5dff;0.6:238443ff;0.7:006837ff;1:004529ff"").get      val hardCodedExtent = ""-8766409.899970295,5009377.085697312,-8140237.7642581295,5635549.221409474""      val layers = layerLookup.getIntersectingLayers(hardCodedExtent)        complete {           val t: Future[Option[Tile]] = theFunctionYouNeedToWrite(layers)            t.map { tileOpt =>              tileOpt.map { tile =>                                  val png = Render.renderLayer(tile, colorMap)              pngAsHttpResponse(png)                  }        }      }    }```",3/6/17,1
lossyrob,probably got some parans mixed there,3/6/17,1
dorukozturk,"ok let me experiment, thanks",3/6/17,1
lossyrob,"that function would include retrieving each tile from the catalog (so getting a `Seq[Option[Tile]]` or `Seq[Future[Option[Tile]]]`, and then merging the resulting tiles",3/6/17,1
lossyrob,:+1:,3/6/17,1
dorukozturk,Actually I do not think I have to merge because my layers are not overlapping,3/6/17,1
dorukozturk,I though I could write one more .map,3/6/17,1
lossyrob,"if you_Ñéll be sure that there_Ñés only one, then you can just call `.head` or something",3/6/17,1
lossyrob,"and if that_Ñés the case, `layers` should always only be one element",3/6/17,1
lossyrob,so you might want to check it at that level,3/6/17,1
dorukozturk,"ok I think I did not clearly stated the problem, pseudo code coming :)",3/6/17,1
dorukozturk,"```scala  def weldRoute =    pathPrefix(Segment / IntNumber / IntNumber / IntNumber) { (extent, zoom, x, y) =>      def colorMap = ColorMap.fromStringDouble(""0.05:ffffe5aa;0.1:f7fcb9ff;0.2:d9f0a3ff;0.3:addd8eff;0.4:78c679ff;0.5:41ab5dff;0.6:238443ff;0.7:006837ff;1:004529ff"").get      val hardCodedExtent = ""-8766409.899970295,5009377.085697312,-8140237.7642581295,5635549.221409474""      val layers = layerLookup.getIntersectingLayers(hardCodedExtent)        complete {          Future {            layers.map { layer =>              val tileOpt =                readerSet.readSinglebandTile(layer, zoom, x, y)              tileOpt.map { tile =>                val png = Render.renderLayer(tile, colorMap)                pngAsHttpResponse(png)              }             }          }        }    }```",3/6/17,1
dorukozturk,"this does not compile and I get Error:(52, 18) type mismatch; found   : scala.concurrent.Future[Array[Option[akka.http.scaladsl.model.HttpResponse]]] required: akka.http.scaladsl.marshalling.ToResponseMarshallable          Future {",3/6/17,1
lossyrob,are you trying to return an Array of things?,3/6/17,1
lossyrob,"`Future { layers.map { // _Ñ_ } }` means, give me a Future that is some seqeunce of elements",3/6/17,1
lossyrob,"that compile error is saying that it can_Ñét marshal the type it described (scala.concurrent.Future[Array[Option[akka.http.scaladsl.model.HttpResponse]]]), which I don_Ñét think is what you are shooting for",3/6/17,1
dorukozturk,ok I will take a step back,3/6/17,1
dorukozturk,I ingested lot of layers. In the catalog I have 7000 layer ids. I am trying to write the tile server to visualize them based on the viewport of the map. So each time user pans and zooms I am doing an intersection between all 7000 layers and the viewport of the map. So getIntersectingLayers is returning an Array of Layerid's each time the user pans and zooms. ,3/6/17,1
dorukozturk,I am trying to visualize multiple layers at a time ultimately,3/6/17,1
dorukozturk,and my layers don't overlap so I did not do a mosaic,3/6/17,1
lossyrob,I think I get the premise,3/6/17,1
lossyrob,"And actually if we think on a type level, we can forget the details and work at that level",3/6/17,1
lossyrob,well first question,3/6/17,1
lossyrob,hm,3/6/17,1
lossyrob,why are you using a hard coded extent?,3/6/17,1
dorukozturk,just for testing ultimately it will be dynamic and will be passed with url,3/6/17,1
lossyrob,"what you really want in this tile route (which serves an individual tile based on `z`, `x` and `y`) is the extent of the tile",3/6/17,1
lossyrob,"not the extent of, say, the viewport",3/6/17,1
lossyrob,"because this endpoint gets hit with the purpose of serving out one tile. So at the time of the request, we only care about the exent of the one tile. right?",3/6/17,1
dorukozturk,yes,3/6/17,1
lossyrob,@jmelching I think the problem would arise from this notion: `This code relies on the input being spatially sorted. In order to get any benefit` that @jamesmcclain. @jamesmcclain what_Ñés the process of spatially sorting them before the vector join?,3/6/17,1
jamesmcclain,"Rather than saying ""sorted"" I should have said that it at least needs to be ""quasi-sorted""",3/6/17,1
lossyrob,"@dorukozturk ok, so we need to figure that part out, but we can just stub that as `def getExtentForTile(z: Int, x: Int, y: Int): Extent`",3/6/17,1
jamesmcclain,"Essentially, the less ""void space"" there is in each partition, the better",3/6/17,1
lossyrob,"right, so how do you achieve that?",3/6/17,1
jamesmcclain,Sorting is one way,3/6/17,1
jamesmcclain,"Some kind of hashing scheme would probably work, as well",3/6/17,1
lossyrob,"@dorukozturk then the next thing is calling that layer funciton. If we are only giving it the tile extent, and there is only one layer that should feed that tile, then it should just be one layer",3/6/17,1
lossyrob,"@jamesmcclain ok, the fact that we don_Ñét provide that functionality to make the vector join useful is a little troubling",3/6/17,1
jamesmcclain,Okay,3/6/17,1
lossyrob,we should try to come up with an example or ways to do an efficient vector join even if the rdds are not pre sorted or pre spatially partitioned,3/6/17,1
lossyrob,"we might already have code to do that, @echeipesh we can partition spatially according to some grid layout right?",3/6/17,1
lossyrob,"@dorukozturk but the thing is, even though the images may not be overlapping, two or more could overlap one tile - e.g. one layer might contribute the left upper corner, and the rest NoData, and another layer could contribute to the right lower corner, with the rest nodata",3/6/17,1
lossyrob,so you actually will end up having to merge,3/6/17,1
lossyrob,"even if that merge doesn_Ñét have to make a choice between two _ÑÉdata_Ñé pixels, and only fills in nodata pixels with imagery that contributes those pixels",3/6/17,1
dorukozturk,but zoom level 1 will merge 8000 layers then,3/6/17,1
dorukozturk,wouldn't that be horrible performance wise?,3/6/17,1
lossyrob,di you pyramid your layers?,3/6/17,1
dorukozturk,yepp,3/6/17,1
dorukozturk,0-13,3/6/17,1
lossyrob,"that_Ñés a good point, and you_Ñéd have to work around that",3/6/17,1
lossyrob,that_Ñés why having them all as one layer would be best I guess...,3/6/17,1
lossyrob,hmm,3/6/17,1
dorukozturk,:) I tried and failed doing that so I ingested them one by one :D,3/6/17,1
lossyrob,a crazy idea would be to do another _ÑÉetl_Ñé that would union the layers all together and then save off/pyramid as the combined layer,3/6/17,1
dorukozturk,depends on how crazy :D,3/6/17,1
lossyrob,I don_Ñét think it should be really. You_Ñéd just load up all the RDDs and then do `sc.union`,3/6/17,1
lossyrob,it_Ñés curious that the ingest hung like it did. I wish we could have worked past that - the idea of ingesting 8K of geotiffs is well within the bounds of regular GeoTrellis work,3/6/17,1
dorukozturk,I think the problem is with the windowed read from s3,3/6/17,1
dorukozturk,because the hang happened when I specify maxtilesize,3/6/17,1
lossyrob,"and then otherwise, you would get out of memory?",3/6/17,1
lossyrob,without maxtilesize?,3/6/17,1
dorukozturk,I did not try with all 8000 because when I did not specify maxtilesize It was kind of serial,3/6/17,1
dorukozturk,it was using very little ram,3/6/17,1
lossyrob,there_Ñés other techniques to get around that,3/6/17,1
dorukozturk,I was using like 10% of the cluster memory so I killed it launched 50 small clusters and do the ingest one by one,3/6/17,1
lossyrob,like `split` and `repartition`,3/6/17,1
dorukozturk,we have a demo coming so I was just trying to make it work :D,3/6/17,1
lossyrob,gotcha gotcha,3/6/17,1
lossyrob,I understand that feeling :),3/6/17,1
dorukozturk,"yeah I am very stressed out, our dataa is 1.5 tb",3/6/17,1
dorukozturk,not very big but not small either,3/6/17,1
dorukozturk,I will write a blog post for geotrellis integration once we do the demo,3/6/17,1
Charmatzis,"Hi, what 'toTraversable' does?",3/7/17,1
pomadchin,converts collection to traversable collection; usually it_Ñés an upcast,3/7/17,1
pomadchin,(like it_Ñés already a traversable),3/7/17,1
pomadchin,@Charmatzis http://docs.scala-lang.org/overviews/collections/trait-traversable.html,3/7/17,1
Charmatzis,"thx, I am reading right now.",3/7/17,1
Charmatzis,"So, here https://github.com/geotrellis/vectorpipe/blob/master/src/main/scala/vectorpipe/Vectorpipe.scala#L68 ",3/7/17,1
Charmatzis,when you do 'toTraversable' it is a way to all operations are guaranteed to be performed in a single-threaded manner?,3/7/17,1
pomadchin,what type of the result `c.map(_.geom)` ?,3/7/17,1
Charmatzis,a geom,3/7/17,1
pomadchin,?,3/7/17,1
Charmatzis,maybe line?,3/7/17,1
pomadchin,what type of `c` value ?,3/7/17,1
pomadchin,oh; it_Ñés  `Array[OSMFeature]` ,3/7/17,1
pomadchin,"to be honest, not sure why it_Ñés done here; as Array is already `TraversableLike`",3/7/17,1
Charmatzis,I see,3/7/17,1
pomadchin,you can try to remove it; so it _may_ work,3/7/17,1
Charmatzis,"Is there an example to build a pyramid file set (z/x/y) of png files, like tms structure from a vector data?",3/7/17,1
bjvanlinschoten,"A) Say I have a grid covering the whole earth for each day. In Geotrellis, should this be turned into one big Tile Layer, with a SpaceTimeKey? ",3/7/17,1
bjvanlinschoten,B) Where do I start in transforming an RDD[Array[Double]] per day into tiles? Do I just pick a certain number of tiles to split the grid into myself?,3/7/17,1
lossyrob,"@Charmatzis not yet, we_Ñére still working on that! https://github.com/geotrellis/vectorpipe is where that work is happening",3/7/17,1
lossyrob,"@bjvanlinschoten A) yes, that_Ñés a pretty straightforward use case B) Can you describe the problem a bit more? What are the Double values in this RDD[Array[Double]]? Where is the geospatial information for those values?",3/7/17,1
bjvanlinschoten,"Hey Rob, thanks for your answer. So I have as input one .he5 file per day containing three datasets: - Double value (parts NO2 in atmosphere, grid covering the whole earth)- Latitude- LongitudeI can read these datasets easily in an RDD[Array[Double]] using h5spark. But I'm not sure how to combine/transform this data so I can use it in Geotrellis. I could just split the Double dataset myself and put them in DoubleArrayTiles right? Is that the way to go? How would I combine this with the latitude/longitude data?",3/8/17,1
karalas,"Hi all, I did the ingest via the etl tool (```Etl.ingest[TemporalProjectedExtent, SpaceTimeKey, Tile](args)```) for Landsat EMR demo (for non Landsat images) in ```LandsatIngest.scala```, and then tried to serve and view it, but I get an error like this: ```[iaas-system-akka.actor.default-dispatcher-2] [akka.actor.ActorSystemImpl(iaas-system)] Error during processing of request: 'Attribute times not found for layer Layer(name = ""NDVI"", zoom = 0)'. Completing with 500 Internal Server Error response.``` In ```input.json``` I set ```format:""temporal-geotiff""``` and ```type:""hadoop""``` instead of ```geotiff``` and ```landsat```. What should I do? Thank you",3/8/17,1
pomadchin,@karalas how do you read metadata?,3/8/17,1
karalas,"I do not read them.  Should I use something like this: ```val (_, rasterMetaData) = TileLayerMetadata.fromRdd(inputRdd, FloatingLayoutScheme(512))```, or is there something I can use directly with the ETL tool?",3/8/17,1
pomadchin,"eh, i mean how are your reading it?",3/8/17,1
pomadchin,what line of code causes `Error during processing of request:` ?,3/8/17,1
pomadchin,so metadata depends on a key type; that_Ñés why this problem could appear,3/8/17,1
pomadchin,"```attributeStore  .readMetadata[    TileLayerMetadata[{SpatialKey | SpaceTimeKey}]  ](id)```",3/8/17,1
pomadchin,that may cause these issues,3/8/17,1
karalas,"Ok, so we are talking about the server code, and more specifically this line https://github.com/geotrellis/geotrellis-landsat-emr-demo/blob/0317ebdeb08a0ab0728e54443f31df67b5abf6a4/server/src/main/scala/demo/MetadataReader.scala#L13? I have a ```SpaceTimeKey```, but maybe it's something wrong with the ```JsonFormat``` (line above)? Because I haven't changed anything related to this",3/8/17,1
pomadchin,we are talking about [this](https://github.com/geotrellis/geotrellis-landsat-emr-demo/blob/0317ebdeb08a0ab0728e54443f31df67b5abf6a4/server/src/main/scala/demo/MetadataReader.scala#L14) line,3/8/17,1
pomadchin,"so just remove it, as i believe you don_Ñét have it",3/8/17,1
pomadchin,it_Ñés a custom field saved in attribute store,3/8/17,1
pomadchin,custom fields in attr sotre can be written [this](https://github.com/geotrellis/geotrellis-landsat-emr-demo/blob/0317ebdeb08a0ab0728e54443f31df67b5abf6a4/ingest/src/main/scala/demo/LandsatIngest.scala#L48) way,3/8/17,1
karalas,"Ok thank you, I will examine this",3/8/17,1
karalas,"@pomadchin as you said I don't have it. I have replaced ```def read``` here https://github.com/geotrellis/geotrellis-landsat-emr-demo/blob/0317ebdeb08a0ab0728e54443f31df67b5abf6a4/server/src/main/scala/demo/MetadataReader.scala#L12 with:```def read(layer: LayerId): TileLayerMetadata[SpaceTimeKey] =    attributeStore.readMetadata[TileLayerMetadata[SpaceTimeKey]](layer)```I have also removed the corresponding lines of ```""times""``` from here https://github.com/geotrellis/geotrellis-landsat-emr-demo/blob/master/server/src/main/scala/demo/Router.scala#L199 and here https://github.com/geotrellis/geotrellis-landsat-emr-demo/blob/master/server/src/main/scala/demo/Router.scala#L214 but the same when I try to return the JSON representation of the catalog. Thanks",3/8/17,1
pomadchin,@karalas can you post your code somewhere; as it_Ñés a bit hard to debug it this way ^^',3/8/17,1
pomadchin,i believe just a typo or not redefined function somewhere,3/8/17,1
lossyrob,"@bjvanlinschoten What you would need is an `RDD[Array[(Double, Double, Double)]]`, which would contain the lat, long, and value of each point",3/8/17,1
lossyrob,"from there, you could create a raster layer by choosing a LayoutScheme or TileLayout and rasterizing the points",3/8/17,1
lossyrob,what sort of raster are you trying to create?,3/8/17,1
acoronadoiruegas,"Regards, could you indicate the classes I need to add to create a new focal operation, such as focalMean?",3/8/17,1
acoronadoiruegas,For simplicity we can call it focalMean2,3/8/17,1
lossyrob,on an RDD or just a Tile,3/9/17,1
Charmatzis,Hi guys. I am trying to create  from vector data some TMS local files in png. I am using `Pyramid.upLevels`,3/9/17,1
pomadchin,Yo @Charmatzis what_Ñés the problem? be careful with type constrains this function [has](https://github.com/locationtech/geotrellis/blob/5513313c543e3dff408b508ed9301f1f1cc24f26/spark/src/main/scala/geotrellis/spark/pyramid/Pyramid.scala#L44-L52),3/9/17,1
pomadchin,mostly im talking about rather serious limitation in `V` type usge: `V <: CellGrid`,3/9/17,1
Charmatzis,I really can't understand how can I use `LayoutDefinition` and `TileLayout` so it wil be the same as in WebMercator TMS,3/9/17,1
Charmatzis,if I use ,3/9/17,1
Charmatzis,"``` val layoutMap: Map[Int, LayoutDefinition] =      (1 to 20)        .map { case zoom => (zoom, ZoomedLayoutScheme(WebMercator, 512).levelForZoom(zoom).layout) }        .toMap```",3/9/17,1
Charmatzis,I get for each zoom level the right LayoutDefinition in WebMercator,3/9/17,1
Charmatzis,Is that right?,3/9/17,1
Charmatzis,"Also, I am trying to tiling the kernel density example from your docs site",3/9/17,1
Charmatzis,the data are stored as WGS'84 and I want to reproject them ,3/9/17,1
Charmatzis,"```    val (zoom: Int, reprojected )= TileLayerRDD(tileRdd, metadata).reproject(WebMercator, layoutScheme, Bilinear)```",3/9/17,1
Charmatzis,"but then if I try to get the spatial index (X,Y) the X value is right for the colorado exent, but the Y value is wrong",3/9/17,1
Charmatzis,https://gist.github.com/Charmatzis/b3bf410d83b21f782d59c72f181659c9 ,3/9/17,1
dorukozturk,"hi, my ingest job failed here flatMap at CutTiles.scala:46",3/9/17,1
dorukozturk,and it got stuck again :),3/9/17,1
dorukozturk,have you guys encountered an error on this step before?,3/9/17,1
lossyrob,did any tasks get completed? how many tasks? over the whole 8000?,3/9/17,1
dorukozturk,yes over the whole 8000 I am using 1000000 partitions and it got stuck on 122638,3/9/17,1
lossyrob,I would perhaps recommend using `spark.rdd.compress = true` ,3/9/17,1
lossyrob,do you have the UI up?,3/9/17,1
dorukozturk,not anymore :) ,3/9/17,1
lossyrob,"for the stage it is caught on, there were tasks that were completed for that stage?",3/9/17,1
dorukozturk,I submitted the job again though on a new cluster because I was giving only 15G to the worker nodes and I suspected the shuffle write filled out the space in the worker node,3/9/17,1
dorukozturk,yepp 122638/1000000 was done,3/9/17,1
lossyrob,"ah, yes - did any of the nodes go down or did they get stuck in active? You can see in the hdfs UI of decommissioned nodes, or in the YARN ui, or seeing dead executors",3/9/17,1
dorukozturk,yes some nodes died,3/9/17,1
lossyrob,ok. yeah that sounds like hard disk filled up,3/9/17,1
dorukozturk,so I bumped up the space on each worker node,3/9/17,1
lossyrob,cool,3/9/17,1
dorukozturk,I was able to ingest 1000 though :),3/9/17,1
lossyrob,shouldn_Ñét you get a lot of emphermal storage with your instance though?,3/9/17,1
lossyrob,that_Ñés encouraging ,3/9/17,1
lossyrob,curious about how large the largest shuffle write in the job is,3/9/17,1
dorukozturk,at that point it was 3 G,3/9/17,1
lossyrob,when the whole job completes :),3/9/17,1
dorukozturk,"oh I can provide statistics, I am running on a cluster with 190 nodes",3/9/17,1
dorukozturk,4 TB of ram,3/9/17,1
lossyrob,nice,3/9/17,1
dorukozturk,"if it stucks this time I will wait for your response this time, so I can provide info",3/9/17,1
lossyrob,"so in the spark UI, eventually (perhaps after cut tiles) there will be  a shuffle write size, which represents the actual size of the job (you can_Ñét really know with the compressed file total size)",3/9/17,1
dorukozturk,sure so for example the stage 0 is repartition and total is 485.5 mb,3/9/17,1
dorukozturk,so no matter how I partitioned reduce at TileLayerMetadata.scala:147 step takes 2 hours,3/9/17,1
lossyrob,"there_Ñés not really a shuffle at the start, so you won_Ñét see the large shuffle write until later tiles. If you_Ñére using buffered reproject, the largest shuffle will probably be that",3/9/17,1
lossyrob,interesting that takes 2 hours,3/9/17,1
lossyrob,"there_Ñés a tweak that should make that a lot faster, at least that step",3/9/17,1
dorukozturk,what is the tweak ?,3/9/17,1
lossyrob,to use TiffTags to construct the metadata,3/9/17,1
dorukozturk,but that requires preprocessing and adding the correct tifftags?,3/9/17,1
dorukozturk,or tools like gdalwarp already adds them,3/9/17,1
lossyrob,"the tags are there, it_Ñés just the metadata extraction doesn_Ñét use the streaming stuff to pull only the tag data right now",3/9/17,1
lossyrob,"it reads the images in order to get metadata, which is wasteful",3/9/17,1
dorukozturk,I see,3/9/17,1
lossyrob,what method are you using for ETL? are you calling etl.load?,3/9/17,1
dorukozturk,yepp,3/9/17,1
lossyrob,"well I_Ñéll think through that, but will take a bit to produce something",3/9/17,1
dorukozturk,"ok, sounds good ",3/9/17,1
lossyrob,@Charmatzis when you say Y is wrong - you mean the tile coordinates (SpatialKeys)?,3/9/17,1
lossyrob,"could this be an issue where (x, y) = (lng, lat) vs lat long are getting mixedu p",3/9/17,1
lossyrob,"@acoronadoiruegas for Tiles, this would be a good example to follow off of https://github.com/locationtech/geotrellis/blob/master/raster/src/main/scala/geotrellis/raster/mapalgebra/focal/Max.scala#L28",3/9/17,1
lossyrob,feel free to ask questions about the architecture,3/9/17,1
lossyrob,"for spark, it_Ñés along this line of architecture https://github.com/locationtech/geotrellis/blob/master/spark/src/main/scala/geotrellis/spark/mapalgebra/focal/FocalOperation.scala#L52",3/9/17,1
dorukozturk,after increasing disk space it looks like it is going through and based on my quick estimation it will shuffle write ~600 GB,3/9/17,1
dorukozturk,for the cuttiles step,3/9/17,1
pomadchin,do you use EMR?,3/9/17,1
dorukozturk,yepp,3/9/17,1
pomadchin,you can use ganglia to monitor physical nodes resources,3/9/17,1
dorukozturk,"himm before disk space each node was low and nodes were dying in the cuttiles method I bumped it to 250 GB each node so now I passed the step that it was failing so I  just wanted to share, maybe somebody in the future will be scraping this gitter conversations :D ",3/9/17,1
lossyrob,ha yes. Perhaps a post-mortem after it works would be really useful to that future person :),3/9/17,1
dorukozturk,"I will definitely do that,",3/9/17,1
dorukozturk,"my data size, my configuration, my instance type I will write a blog post about all those things",3/9/17,1
dorukozturk,"""if it works "" :D",3/9/17,1
lossyrob,that would be really great. I think you_Ñére working through some issues that will be very useful for other users to read through,3/9/17,1
lossyrob,haha yes. _ÑÒwhen it works_Ñù ;),3/9/17,1
lossyrob,"there_Ñés no fingers crossed emoji, but I feel like it_Ñés appropriate here",3/9/17,1
dorukozturk,ok it looks like it froze again at cuttiles,3/9/17,1
dorukozturk,and I have the ui opened,3/9/17,1
lossyrob,are nodes dying again?,3/9/17,1
dorukozturk,some executors have this CANNOT FIND ADDRESS,3/9/17,1
lossyrob,"one thing to check is the hadoop UI, which can tell you how much DFS and non-DFS space is used",3/9/17,1
lossyrob,yeah,3/9/17,1
lossyrob,what_Ñés the instance type again?,3/9/17,1
dorukozturk,m4 2xlarge,3/9/17,1
dorukozturk,so the total shuffle write is 176.3 GB at this point,3/9/17,1
lossyrob,"those are EBS only, with m3.2xlarge you get 160x2 SSDs",3/9/17,1
lossyrob,you gave it what sort of volume?,3/9/17,1
lossyrob,what size ebs?,3/9/17,1
dorukozturk,"[{VolumeSpecification={VolumeType=gp2,SizeInGB=250},VolumesPerInstance=1}",3/9/17,1
dorukozturk,both the master and workeers have 250GB,3/9/17,1
dorukozturk,and 41 of the cuttiles failed,3/9/17,1
pomadchin,i had experience when spark just ate all disk space; nodes were just killed sielntly and emr tried to bootstrap new; ._. that_Ñés why everything hanged,3/9/17,1
dorukozturk,so before when I refresh the number was changing all the time and it is stuck now,3/9/17,1
dorukozturk,number of finished task I am not being impatient right?,3/9/17,1
lossyrob,nah you want to always see forward progress,3/9/17,1
dorukozturk,so should I bump them to 1TB or something you think it is a matter of that? Or should I increase the worker memory from 4200 to 8GB maybe and reduce partitions?,3/9/17,1
lossyrob,thinking,3/9/17,1
lossyrob,you really shouldn_Ñét be running out of disk space if it_Ñés that much space,3/9/17,1
lossyrob,you have how many nodes running?,3/9/17,1
dorukozturk,190,3/9/17,1
lossyrob,that_Ñés 47 TB,3/9/17,1
lossyrob,something else is going on,3/9/17,1
dorukozturk,so this is the highest shuffle spill (memory) 5.2,3/9/17,1
lossyrob,can you imgur the UI?,3/9/17,1
lossyrob,for the stages,3/9/17,1
dorukozturk,http://imgur.com/a/vH7HI,3/9/17,1
dorukozturk,maxtilesize is 512 will using 256 reduce shuffle size if that is all about shuffle size? My guts tell me to either use 256 as maxtilesize or bump the memory per executor and drop partitions just to see but if you think that is irrelevant than I really don't know,3/9/17,1
lossyrob,I don_Ñét think it_Ñés the shuffle size,3/9/17,1
lossyrob,I_Ñéve done way more on way smaller clusters,3/9/17,1
lossyrob,it might be the 1mil tasks,3/9/17,1
lossyrob,is there a way to check the stats of the master node?,3/9/17,1
lossyrob,ah actually not the master,3/9/17,1
lossyrob,the driver,3/9/17,1
lossyrob,"so the executors tab should have the driver on there, and the logs should maybe indicate it_Ñés health",3/9/17,1
dorukozturk,"```shDRIVER_MEMORY := 20GDRIVER_CORES := 2EXECUTOR_MEMORY := 4200MEXECUTOR_CORES := 2YARN_OVERHEAD := 1400```",3/9/17,1
dorukozturk,anything odd here?,3/9/17,1
lossyrob,no,3/9/17,1
lossyrob,so executors are dying? is there any error messages you can track down for why the executors are dying?,3/9/17,1
dorukozturk,there are 28 dead executors,3/9/17,1
dorukozturk,I will try to see their logs,3/9/17,1
lossyrob,"sometimes the master logs (which you can get through via the YARN ui, hit the application link at the left of the table, look at logs there) are good to look at. I think they might be the same as the logs that you can get at via the driver logs via the executor list",3/9/17,1
lossyrob,how many partitions did you use for the 1000 files?,3/9/17,1
dorukozturk,100,3/9/17,1
dorukozturk,this log is huge,3/9/17,1
dorukozturk,:D,3/9/17,1
lossyrob,maybe bringing the partition count down to 500K,3/9/17,1
dorukozturk,ok I can give that a try,3/9/17,1
dorukozturk,should I play with the maxtilesize or executor memory u think?,3/9/17,1
lossyrob,"I don_Ñét think you should have to - I was running on those files with that setup, and so it should just be a matter of scaling",3/9/17,1
dorukozturk," ERROR YarnClusterScheduler: Lost executor 490 on ip-172-31-42-18.us-west-2.compute.internal: Container marked as failed: container_1489075159907_0003_01_000522 on host: ip-172-31-42-18.us-west-2.compute.internal. Exit status: 137. Diagnostics: Container killed on request. Exit code is 137",3/9/17,1
dorukozturk,those nodes are running out of memory,3/9/17,1
dorukozturk,so I will bump it from 4200 to 6200 and reduce the partitions from 1000000 to 500000,3/9/17,1
dorukozturk,sounds reasonable,3/9/17,1
dorukozturk,?,3/9/17,1
lossyrob,bumping from 42 to 62 might mean less executors ,3/9/17,1
lossyrob,but it_Ñés worth a shot,3/9/17,1
dorukozturk,ok thanks !!!!,3/9/17,1
lossyrob,"another point to make is that EBS will be potentially much slower than using ephemeral drives, e.g. m3.2xlarges",3/9/17,1
Charmatzis,"@lossyrob  yes > @Charmatzis when you say Y is wrong - you mean the tile coordinates (SpatialKeys)?",3/10/17,1
Charmatzis,"> could this be an issue where (x, y) = (lng, lat) vs lat long are getting mixedu p",3/10/17,1
bgokden,"Hi I have a probably simple question. I want to add transparency to one color in a MultibandTile tiff image, can somebody show me an example or documentation on how to do it?",3/10/17,1
pomadchin,hey @bgokden! transparency can mean a couple of things in terms of geotiff; probably you are talking about no data values?,3/10/17,1
bgokden,"@pomadchin There is data on it. I have an already colored image. It is in RGB colorspace and I would want to convert it to RGBA colorspace while adding Transparency to a specific color like RGB(0, 128, 128)",3/10/17,1
pomadchin,ah; so you have a multiband tile with only 3 bands,3/10/17,1
pomadchin,i think that would be enought just to ad 4th band,3/10/17,1
pomadchin,o:,3/10/17,1
bgokden,yes,3/10/17,1
bgokden,Yes but to a spesific color,3/10/17,1
bgokden,is there a way to to do it like image.tile.color.map(rgbcolor => add transparency here and return new rgba color),3/10/17,1
pomadchin,"what the result output type should be? 4 band tiff ot smth else?",3/10/17,1
bgokden,result will be a png actually,3/10/17,1
bgokden,but I do other things like reproject rescale,3/10/17,1
bgokden,so we can say it will be a 4 band tiff image.,3/10/17,1
pomadchin,ah sry im a bit busy here ._.,3/10/17,1
bgokden,ok,3/10/17,1
bgokden,The thing is I found out how to do it thanks!,3/10/17,1
pomadchin,w0w great (: glad to hear,3/10/17,1
pomadchin,how you decided to do it?,3/10/17,1
bgokden,"To answer my own question I did it this way:val r = 0    val g = 128    val b = 128    val colorToAlpha = ((r & 0xFF) << 24) | ((g & 0xFF) << 16) | ((b & 0xFF) << 8) | 0xFF    val colorTransparent = ((r & 0xFF) << 24) | ((g & 0xFF) << 16) | ((b & 0xFF) << 8) | 0x00    tiff.tile.color().map(color => if (color == colorToAlpha) { colorTransparent } else {color})",3/10/17,1
pomadchin,hah interesting; ,3/10/17,1
pomadchin,i just remember that we have smth in `geotrellis.raster.render.png` package,3/10/17,1
pomadchin,and to create strategy to make colors transparent during the render to png step // in order to avoid this manual comparison and iteration over all pixels,3/10/17,1
pomadchin,"can_Ñét remember right now what_Ñés there ): sry for it, not too familiar with that part of the codebase :D ",3/10/17,1
avalcepina,thanks we'll look into it,3/10/17,1
Charmatzis,"Hi, guys. I have a `val tileRdd: RDD[(SpatialKey, Tile)] = ??????` which is in projection WGS'84 . How can I reprojected it in WebMercator? ",3/10/17,1
pomadchin,"hey @Charmatzis you have to add layer metadata; ```scalaval tileRdd: RDD[(SpatialKey, Tile)] with Metadata[TileLayerMetadata[SpatialKey]] = ???tileRdd.reproject(WebMercator)```",3/10/17,1
pomadchin,"to make operations and to keep rdd metadata, you can use `.withContext` method",3/10/17,1
pomadchin,"```scalardd.withContext { _.{someoperation that throws md} }```",3/10/17,1
Charmatzis,"thx, i will try",3/10/17,1
Charmatzis,"@pomadchin  well I do smth like that``` val tl: TileLayout = TileLayout(4, 4, 256, 256)    val ld: LayoutDefinition = LayoutDefinition(extent, tl)    val metadata: TileLayerMetadata[SpatialKey] = TileLayerMetadata(DoubleCellType,  ld,  ld.extent,  LatLng,                    KeyBounds(SpatialKey(0, 0), SpatialKey(ld.layoutCols - 1,  ld.layoutRows - 1))) val layoutScheme: ZoomedLayoutScheme = ZoomedLayoutScheme(WebMercator, tileSize = 256)tileRdd: RDD[(SpatialKey, Tile)] = ??????    val (zoom: Int, reprojected )= TileLayerRDD(tileRdd, metadata).reproject(WebMercator,layoutScheme, Bilinear)```",3/10/17,1
Charmatzis,"but the `reprojected :RDD[(SpatialKey, Tile)]  ` has not reprojected the spatial keys....",3/10/17,1
pomadchin,@Charmatzis 1. you said your init data in wgs: `EPSG:4326` try `CRS.fromName` function,3/10/17,1
echeipesh,"@Charmatzis The spatial keys will/may have new extents they_Ñére covering. They_Ñére now relative to the layout definition returned from `reproject`, which won_Ñét be the same as the one in original `matadata`.",3/10/17,1
dorukozturk,"hey, after waiting 20 hours my job died again but this time I have levels 14 13 12 11 ",3/10/17,1
dorukozturk,please tell me there is an easy way to compute the rest of the levels,3/10/17,1
echeipesh,"Yeah, you can read level 12 with just a layer reader (zoom 11 might have been partial) and call pyramid on that and save them through a writer, give me a sec to mock up an example.",3/10/17,1
dorukozturk,great I appreciate,3/10/17,1
echeipesh,"```scalaobject MockupFinishPyramid {  import geotrellis.spark.io.FilteringLayerReader  import geotrellis.spark.io.s3._  import geotrellis.spark.io.index._  import geotrellis.spark.tiling._  import geotrellis.spark._  import geotrellis.spark.pyramid._  import geotrellis.raster._  val layerName = ""my-name""  val layoutScheme: ZoomedLayoutScheme = ??? // scheme was used in the original ingest  val layerReader: S3LayerReader = S3LayerReader(""bucket"", ""catalog-prefix"")  val layerWriter: S3LayerWriter = S3LayerWriter(""bucket"", ""catalog-prefix"")  val keyIndex: KeyIndexMethod[SpatialKey] = ??? // same as was used in original ingest  val layer = layerReader.read[SpatialKey, MultibandTile, TileLayerMetadata[SpatialKey]](LayerId(layerName, 12))  for ((zoom, rdd) <- Pyramid.levelStream(layer, layoutScheme, 12, 1)) {    // write will force collect and will iterate on RDD    layerWriter.write(LayerId(layerName, zoom), rdd, keyIndex)  }}```Something like this to be expanded to be a job would finish pretty quickly",3/10/17,1
dorukozturk,"ok, thank you very much",3/10/17,1
echeipesh,"although, the layer writer won_Ñét overwrite a layer once the metadata has been saved. You would need to either use a `LayerDeleter` to do it or just blow it away manually (this is for level 11)",3/10/17,1
dorukozturk,ok sounds great ,3/10/17,1
dorukozturk,ok my job failed in  the pyramiding step of level 10 so level 11 is complete,3/10/17,1
echeipesh,even better,3/10/17,1
dorukozturk,the rest is easy I am assuming because there are less and less layers,3/10/17,1
echeipesh,"Yes, each successive layer is 1/4 of size (pixels and tiles) of the one under it.",3/10/17,1
lossyrob,@Charmatzis the reproject on `TileLayerRDD[SpatialKey]` should reproject the spatial keys (i.e. put the keys into the new key space) - it does a reproject of the tiles and then cuts those resulting rasters into the new layout scheme,3/10/17,1
lossyrob,"re: if I use ``` val layoutMap: Map[Int, LayoutDefinition] =      (1 to 20)        .map { case zoom => (zoom, ZoomedLayoutScheme(WebMercator, 512).levelForZoom(zoom).layout) }        .toMap```I get for each zoom level the right LayoutDefinition in WebMercatorIs that right?",3/10/17,1
lossyrob,"Yes that would give you the correct map. why are you constructing that? In a tile server, you should be getting the z, x and y which you can use directly",3/10/17,1
Charmatzis,"@lossyrob well I want to create the z/x/y folders/png , so with an API would get very fast this images. THX, for the code I will try it and get back as soon as possible",3/11/17,1
Charmatzis,"``` val tileRdd: RDD[(SpatialKey, Tile)] =??? val layoutScheme: ZoomedLayoutScheme = ZoomedLayoutScheme(CRS.fromName(""EPSG:3857""), tileSize = 512) val (zoom: Int, reprojected )= tileRdd.reproject(CRS.fromName(""EPSG:3857""),layoutScheme, Bilinear)```Now how can cut the `reprojected` using the `layoutMap` , will I use `Pyramid.upLevels(reprojected, layoutScheme, zoom)`???",3/11/17,1
acoronadoiruegas,@lossyrob Thanks,3/11/17,1
lossyrob,"@Charmatzis I wasn_Ñét sure what you were using the LayerMap for (I just pasted your code from a comment a bit ago_Ñ_), you don_Ñét need to use a layout map for ingesting. The pyramiding will do it based off the scheme, so yes the `Pyramid.upLevels` you pointed to is correct",3/12/17,1
Charmatzis,"Goodmorning, after strugling many days now I finally get it to create a TMS service",3/13/17,1
Charmatzis,"```   def writePng ( baseUri: String, zoom: Int, rdd: RDD[(SpatialKey, Array[Byte])], layout: LayoutDefinition): Unit = {      rdd.foreach( f=> {        val uriStr = baseUri + ""\\"" + zoom.toString + ""\\"" + f._1._1.toString + ""\\"" + (layout.layoutRow -  f._1._2 -1).toString + "".png""        val directory = new File(baseUri + ""\\"" + zoom.toString + ""\\"" + f._1._1.toString)        directory.mkdirs()        val yourFile = new File(uriStr)        yourFile.createNewFile(); // if file already exists will do nothing        val oFile = new FileOutputStream(uriStr, false )        val bos = new BufferedOutputStream(oFile)        bos.write(f._2)        bos.flush()        bos.close()      })    }val writeOp =  Pyramid.upLevels(reprojected, layoutScheme, zoom) { (rdd, z) =>      val newTiles: RDD[(SpatialKey, Array[Byte])] =        rdd.asRasters.flatMap {          case (sp, raster) =>            val tileList = mutable.ListBuffer[(SpatialKey, Array[Byte])]()            var pngBytes: Array[Byte] = raster._1.renderPng(colorMap).bytes            val newEntry = (sp, pngBytes)            tileList += newEntry            tileList        }      writePng(outputPath, z,newTiles,layoutMap(z).layoutCols, layoutScheme.levelForZoom(z).layout )    }```",3/13/17,1
Charmatzis,well the catch is that the  TMS is counting from down to top and spatail keys are counting from top to down,3/13/17,1
Charmatzis,relative links ,3/13/17,1
Charmatzis,https://alastaira.wordpress.com/2011/07/06/converting-tms-tile-coordinates-to-googlebingosm-tile-coordinates/ ,3/13/17,1
Charmatzis,http://www.maptiler.org/google-maps-coordinates-tile-bounds-projection/ ,3/13/17,1
Charmatzis,Why there is the restriction <1 zoom level?,3/13/17,1
Charmatzis,https://github.com/locationtech/geotrellis/blob/b8605e603da6932a36a72aa3a6971dbafaa96406/spark/src/main/scala/geotrellis/spark/tiling/ZoomedLayoutScheme.scala#L41 ,3/13/17,1
Charmatzis,and not zoom level  <0????,3/13/17,1
pomadchin,"@Charmatzis agree with you, zoom level 0 can be here",3/13/17,1
pomadchin,"@Charmatzis you can keep `Pngs` instead of `Array[Byte]`s, and each Png has method `write`",3/13/17,1
Charmatzis,@pomadchin could you consider for the new version creating spatial keys by choosing the tilling system? (Google's/OSM/Bing or TMS),3/13/17,1
pomadchin,@Charmatzis https://github.com/locationtech/geotrellis/blob/master/spark/src/main/scala/geotrellis/spark/io/slippy/HadoopSlippyTileWriter.scala,3/13/17,1
Charmatzis,:-) ,3/13/17,1
Charmatzis,"Hi, I am buffering a seq of Lines and I get the error ""Exception in thread ""main"" java.lang.RuntimeException: Unexpected result for Line buffer: MultiPolygon ""I believe that is beacause I get Polygons and at some time I get a MultiPolygon and the error appears.How can I the result to be a Seq[MultiPolygon] and not raising tha error?",3/13/17,1
Charmatzis,https://github.com/locationtech/geotrellis/blob/a00c35b928e96083188d91734252d66574e3b4a7/vector/src/main/scala/geotrellis/vector/Line.scala#L314 How can I escape this error?,3/13/17,1
pomadchin,@Charmatzis https://github.com/locationtech/geotrellis/pull/2057,3/13/17,1
metasim,"@lossyrob et al: If #2036 provides a path toward moving the `raster-test`, `s3-test`, and `vector-test`, etc. back into their parent packages, do you want an issue filed for that work? Or is that not the direction you want to go with the build structure?",3/13/17,1
pomadchin,"@metasim if it_Ñés not hard for you, than it would be great to get rid of `-test` projects",3/13/17,1
pomadchin,:tada: :tada: :tada:,3/13/17,1
echeipesh,"We_Ñére really not getting any benefit from having those as separate projects, it would be great to combine them with source and that method looks like it will work.",3/13/17,1
metasim,Filed #2058. I'll look into it once #2036 is merged.,3/13/17,1
lossyrob,I_Ñém :+1: on that idea,3/13/17,1
basking2,Hi all. I was in the scala room and they mentioned that 2.11.9 was to be their last 2.11 release. Is there an expected scala 2.12 build of geotrellis expected or should I put some effort into building that build into our internal process? https://contributors.scala-lang.org/t/upcoming-scala-releases-2-11-9-2-12-2-and-2-13-0-m1/599,3/13/17,1
pomadchin,@basking2 hi! we have an issue: https://github.com/locationtech/geotrellis/issues/1751,3/13/17,1
pomadchin,we are blocked by Spark :),3/13/17,1
basking2,You guys are fantastic. Thank you!,3/13/17,1
dorukozturk,"Hi, I am trying to complete missing pyramid levels using this code (I am actually trying to produce zoom level 7 from 8 for now). When I zoom to level 7 they appear on different part of the world though. How do I fix it? ```scalaobject Main {  def completePyramid(bucket: String, prefix: String, layerName: String, lastZoom: Integer) (implicit sc: SparkContext) {    val layoutScheme: ZoomedLayoutScheme = ZoomedLayoutScheme(WebMercator, tileSize=256)    val layerReader: S3LayerReader = S3LayerReader(bucket, prefix)    val layerWriter: S3LayerWriter = S3LayerWriter(bucket, prefix)    val keyIndex: KeyIndexMethod[SpatialKey] = ZCurveKeyIndexMethod    val layer = layerReader.read[SpatialKey, Tile, TileLayerMetadata[SpatialKey]](LayerId(layerName, lastZoom))    for ((zoom, rdd) <- Pyramid.levelStream(layer, layoutScheme, lastZoom-1, 7)) {      // write will force collect and will iterate on RDD      layerWriter.write(LayerId(layerName, zoom), rdd, keyIndex)    }  }  def main(args: Array[String]): Unit = {    val layerName = ""weld11""    val bucket = ""kitware-catalog""    val prefix = ""full-catalog""    val lastZoom = 8    val conf =      new SparkConf()        .setMaster(""local[*]"")        .setAppName(""Spark Pyramid"")        .set(""spark.executor.memory"", ""6g"")        .set(""spark.driver.memory"", ""6g"")    val sc = new SparkContext(conf)    completePyramid(bucket, prefix, layerName, lastZoom) (sc)  }}``` ",3/13/17,1
dorukozturk,and level 8 and level 7 has same number of tiles which is suspicios :),3/13/17,1
dorukozturk,level 8 has 7876 tiles level 7 should have ~7876/4 right?,3/13/17,1
echeipesh,"`Pyramid.levelStream(layer, layoutScheme, lastZoom-1, 7))` that seems very suspitious, what are you trying to do there ?",3/13/17,1
echeipesh,Why `lastZoom-1` ?,3/13/17,1
dorukozturk,when I used lastZoom (which means last zoom level I have) It told me I have zoom level 8 already,3/13/17,1
dorukozturk,so I started that from 7 is that wrong :),3/13/17,1
echeipesh,"Yeah, you_Ñére giving it level at zoom 8, telling it its zoom level 7 and asking it to pyramid to 7, at which poing it goes _ÑÒGreat! Job done!_Ñù https://github.com/locationtech/geotrellis/blob/master/spark/src/main/scala/geotrellis/spark/pyramid/Pyramid.scala#L140",3/13/17,1
echeipesh,"So you don_Ñét want to save 8, because you already have it, this should do the trick: `Pyramid.levelStream(layer, layoutScheme, lastZoom, 7)).tail` - that will drop the intial, zoom 8, layer since you already have it.",3/13/17,1
dorukozturk,now I got it,3/13/17,1
echeipesh,Sorry I didn_Ñét think of that case when giving you the snippet :(,3/13/17,1
dorukozturk,that is fine thanks for your help :),3/13/17,1
echeipesh,welcome :),3/13/17,1
dorukozturk,"and if I want all the zoom levels after 7 to 1 I would just do Pyramid.levelStream(layer, layoutScheme, 8, 1)) in that for loop right? ",3/13/17,1
echeipesh,yep,3/13/17,1
dorukozturk,ok I am trying that right now,3/13/17,1
dorukozturk,"ok I got this  Layer(name = ""weld11"", zoom = 8) already exists",3/13/17,1
dorukozturk,sorry it is monday :) what am i missing?,3/13/17,1
echeipesh,did you add the `.tail` to outoupt of `levelStream` ?,3/13/17,1
dorukozturk,ok quick question isn't .tail the same thing as starting from zoomlevel -1 ?,3/13/17,1
echeipesh,Nope. Each item in the sequence undergoes a pyramid transformation. When you lie about zoom level it skips the transformation which is why zoom level 7 looks exactly like zoom level 8.,3/13/17,1
echeipesh,"The shift on the map is because leaflet is using the layout for zoom level 7 but in reality its just fetching tiles from zoom level 8, which probably means the raster got visually moved toward south-west corner.",3/13/17,1
dorukozturk,"```scala  def completePyramid(bucket: String, prefix: String, layerName: String, lastZoom: Integer) (implicit sc: SparkContext) {    val layoutScheme: ZoomedLayoutScheme = ZoomedLayoutScheme(WebMercator, tileSize=256)    val layerReader: S3LayerReader = S3LayerReader(bucket, prefix)    val layerWriter: S3LayerWriter = S3LayerWriter(bucket, prefix)    val keyIndex: KeyIndexMethod[SpatialKey] = ZCurveKeyIndexMethod    val layer = layerReader.read[SpatialKey, Tile, TileLayerMetadata[SpatialKey]](LayerId(layerName, lastZoom))    for ((zoom, rdd) <- Pyramid.levelStream(layer, layoutScheme, lastZoom, 1)).tail {      // write will force collect and will iterate on RDD      layerWriter.write(LayerId(layerName, zoom), rdd, keyIndex)    }  }```",3/13/17,1
dorukozturk,should it be something like this?,3/13/17,1
echeipesh,LGTM,3/13/17,1
dorukozturk,this is not compiling because of the tail,3/13/17,1
dorukozturk,got this : illegal start of simple expression,3/13/17,1
echeipesh,"oh right, got to iterate over new stream: `for ((zoom, rdd) <- Pyramid.levelStream(layer, layoutScheme, lastZoom, 1)).tail.foreach`",3/13/17,1
dorukozturk,no the tail should be in paranthesis so sorry,3/13/17,1
echeipesh,"Sigh, clearly monday morning here too.",3/13/17,1
dorukozturk,ok that worked thanks :),3/13/17,1
lossyrob,@fosskers do you have an idea where https://github.com/locationtech/geotrellis/issues/1574 left off?,3/13/17,1
lossyrob,"nvm, solved it",3/14/17,1
fosskers,"Ah, so that was us after all",3/14/17,1
lossyrob,yup. The tiff tag 4-bytes values-in-an-offset strikes again,3/14/17,1
kthompson,anyone away of any extensions for geojson to support  custom data within a Feature for example? I am looking at planet's api and they basically use geojson with some extra fields ,3/14/17,1
kthompson,id rather not rewrite all of the geojson formatters that already exist in geotrellis,3/14/17,1
lossyrob,can you point to an example of the json?,3/14/17,1
kthompson,yea one sec,3/14/17,1
kthompson,https://gist.github.com/kthompson/43c0da08f8d97368fafb67f300f4fae6,3/14/17,1
kthompson,also apparently gist supports geojson :D,3/14/17,1
lossyrob,also gist in gitter. whoa,3/14/17,1
lossyrob,that_Ñés the first time I saw a map in a gitter room,3/14/17,1
kthompson,haha :D,3/14/17,1
lossyrob,I get what you_Ñére saying - how can we parse with the GeoJson parsers but also have access to that other info,3/14/17,1
kthompson,yea exactly,3/14/17,1
kthompson,like the `_links` fields for example,3/14/17,1
lossyrob,"trying to think it through, that seems a bit challenging",3/14/17,1
lossyrob,It_Ñés possible to lean on GeoTrellis JsonFormats while writing JsonFormats for container objects,3/14/17,1
kthompson,or the `properties` bit. Yea I am not sure if these are legit extensions for geojson either ,3/14/17,1
lossyrob,"so to parse out the geometry, you can just use the GeoTrellis stuff",3/14/17,1
lossyrob,"but for the feature data, you would have to write your own JsonFormat",3/14/17,1
lossyrob,Like it does here https://github.com/locationtech/geotrellis/blob/master/vector/src/main/scala/geotrellis/vector/io/json/FeatureFormats.scala#L49,3/14/17,1
kthompson,i was thinking of maybe writing custom JsonFormatters for `Feature` that pulls the extra data bits out seperately,3/14/17,1
kthompson,oh nice,3/14/17,1
lossyrob,"so you end up redoing a bit of the Feature format, but for the complicated Geometry stuff you can still lean on GeoTrellis",3/14/17,1
kthompson,that would get me pretty close already just need to pull the `_` fields. Awesome thanks for the tips,3/14/17,1
lossyrob,no problem,3/14/17,1
lossyrob,"I_Ñéve thought of making a `geotrellis-planet` or `scala-planet-api` or something similar for interacting with Planet_Ñés API; this would be good code for that, if you feel like putting it in a Gist or sharing it in some way",3/14/17,1
kthompson,:thumbsup: ,3/14/17,1
kthompson,oh btw not too long ago we found an issue with the S3LayerDeleter im not sure if we reported it or not though,3/14/17,1
kthompson,hrm looking at it it does not appear to be fixed,3/14/17,1
lossyrob,oof - what was the issue?,3/14/17,1
kthompson,ill make a PR its a simple fix,3/14/17,1
lossyrob,neither https://github.com/locationtech/geotrellis/pull/2040 or https://github.com/locationtech/geotrellis/pull/2039 solves?,3/14/17,1
lossyrob,"Ok, that would be great. I_Ñém cutting 1.1.0-RC1 right now",3/14/17,1
lossyrob,do you have an Eclipse CLA?,3/14/17,1
metasim,"I have a chunk of code like this:``` (tile: IntConstantNoDataArrayTile).renderPng(ColorRamps.Viridis).write(...)```where the cell values are in the range 0 to 16. I'm getting this exception:``` Exception in thread ""main"" java.util.NoSuchElementException: key not found: 1140937983	at scala.collection.MapLike$class.default(MapLike.scala:228)	at scala.collection.AbstractMap.default(Map.scala:59)	at scala.collection.MapLike$class.apply(MapLike.scala:141)	at scala.collection.AbstractMap.apply(Map.scala:59)	at geotrellis.raster.render.IntCachedColorMap$$anonfun$mapColorsToIndex$1.apply$mcVI$sp(ColorMap.scala:264)	at geotrellis.raster.histogram.FastMapHistogram.foreachValue$mcI$sp(FastMapHistogram.scala:264)	at geotrellis.raster.render.IntCachedColorMap.mapColorsToIndex(ColorMap.scala:264)	at geotrellis.raster.render.png.IndexedPngEncoding.convertColorMap(PngColorEncoding.scala:49)	at geotrellis.raster.render.PngRenderMethods$class.renderPng(PngRenderMethods.scala:49)	at geotrellis.raster.package$withTileMethods.renderPng(package.scala:53)	at geotrellis.raster.render.PngRenderMethods$class.renderPng(PngRenderMethods.scala:60)	at geotrellis.raster.package$withTileMethods.renderPng(package.scala:53)```Thoughts on what might be causing this?",3/14/17,1
kthompson,@lossyrob no those dont solve it and yea im pretty sure i did the CLA,3/14/17,1
kthompson,https://github.com/locationtech/geotrellis/pull/2070,3/14/17,1
kthompson,its actually kind of a gross bug so you may want to test it out and get it in your RC,3/14/17,1
lossyrob,"yeah, I see that",3/14/17,1
lossyrob,I_Ñém going to include it,3/14/17,1
kthompson,:thumbsup: ,3/14/17,1
lossyrob,looks like you edited directly in GitHub. Unfortunately this sets the email address on the commit to a github email; can you push a change with the email address that the CLA is under?,3/14/17,1
kthompson,oh hrm. let me verify,3/14/17,1
kthompson,I repushed my patch branch with the correct email,3/14/17,1
kthompson,i *think* the PR gets updated automatically but let me know if not,3/14/17,1
lossyrob,"It does, and I checked the CLA is good",3/14/17,1
lossyrob,thanks!,3/14/17,1
kthompson,np,3/14/17,1
lossyrob,that was really just in the nick of time to make 1.1.0-RC1 :),3/14/17,1
kthompson,haha nice,3/14/17,1
lossyrob,__1.1.0-RC1 is released__. Please check it out and help test this release candidate!,3/15/17,1
metasim,Congratulations!! :clap: ,3/15/17,1
Charmatzis,:smile: ,3/15/17,1
dorukozturk,in geopyspark I am doing a catalog.query() with a polygon . As far as I understand it is returning all the tiles that touches the given polygon. Is there a clip kind of functionality? Or how can I get the extent of the returned rdd to clip the array myself? I see in the returned metadata there are 2 extents but they are not the extents of the returned rdd. ,3/15/17,1
echeipesh,"That_Ñés a good point, query will return intersecting tiles, which will cover larger area than the initial polygon. We haven_Ñét implemented clipping in python yet, but the way it would be handled and is handled in spark is to mask each tile by the query geometry, which will turn all the cells outside the geometry into NODATA.",3/15/17,1
dorukozturk,"himm so there is no easy way of getting the whole extent of the returned rdds? Because if I knew what was returned, even they were big I could have done some rasterio magic and mask the array ",3/15/17,1
echeipesh,"No, there really isn_Ñét. It would be simple enough to calculate that on scala side, but python geopyspark doesn_Ñét really expose that much functionality yet. Do you think this is something that generally be included in query responsose or is this basically a work around for not being able to do clipping in the first place ?",3/15/17,1
dorukozturk,"I think both clipping and intersecting are very handy and they are useful different use cases.  In our case it would be great if we could get back a clipped rdd, extent thing is only for the workaround ",3/15/17,1
echeipesh,"@metasim Sorry to have missed you question, is that still outstanding ?",3/15/17,1
metasim,"NP. I just worked around it by using an `IntColorMap`. I tried debugging it to see why a `ColorRamp` wouldn't work, but to no avail.",3/15/17,1
echeipesh,Just from a glance the code in `IntCachedColorMap` seems suspect. Was this a huge tile like landsat ?,3/15/17,1
metasim,Not that big.,3/15/17,1
metasim,Maybe ~0.5M pixels.,3/15/17,1
metasim,It only had 16 discrete values plus NODATA.,3/15/17,1
metasim,If you think it's a bug I can try to dig deeper.,3/15/17,1
echeipesh,"Don_Ñét want to send you chasing after shadows, but that value is `0x440154FF` which is the first value in `Viridis` ColorRamp. Kind of weird to be doing lookup by color rather than by value or count, so something smells fishy.",3/15/17,1
metasim,"This is shooting in the dark, but `Viridis` is pretty new, and may be the largest ramp in the library. Perhaps the histogram generating code wasnt' tested against a table that big?",3/15/17,1
Charmatzis,"Hi guys, I am trying to create Line from  Array of Points, but I have few problems that lines that crosses the dateline. ref. http://gisforthought.com/clipping-datasets-to-the-dateline-in-ogr2ogr/  Is the a way to split that line in the dateline?",3/16/17,1
Charmatzis,"I have seen that there is spatial4j which does something like that, https://github.com/locationtech/spatial4j/blob/master/src/main/java/org/locationtech/spatial4j/shape/jts/JtsGeometry.java#L421 however did you ever come around these kind of problems?",3/16/17,1
dorukozturk,"Hi, So I am trying to complete my ingests which raised and error on zoom level 10 so I assembled this:```scalaobject Main {  def completePyramid(bucket: String, prefix: String, layerName: String, lastZoom: Integer) (implicit sc: SparkContext) {    val layoutScheme: ZoomedLayoutScheme = ZoomedLayoutScheme(WebMercator, tileSize=256)    val layerReader: S3LayerReader = S3LayerReader(bucket, prefix)    val layerWriter: S3LayerWriter = S3LayerWriter(bucket, prefix)    val keyIndex: KeyIndexMethod[SpatialKey] = ZCurveKeyIndexMethod    val layer = layerReader.read[SpatialKey, Tile, TileLayerMetadata[SpatialKey]](LayerId(layerName, lastZoom))    for ((zoom, rdd) <- Pyramid.levelStream(layer, layoutScheme, lastZoom, 1).tail) {      // write will force collect and will iterate on RDD      layerWriter.write(LayerId(layerName, zoom), rdd, keyIndex)    }  }  def main(args: Array[String]): Unit = {    val layerName = ""weld""    val bucket = ""my-bucket""    val prefix = ""full-catalog""    val lastZoom = 11    val conf = new SparkConf(true)    implicit val sc = SparkUtils.createSparkContext(""Complete Pyramid"", conf)    completePyramid(bucket, prefix, layerName, lastZoom) (sc)  }}```I tried this locally and it worked perfectly. Then I tried to run it on a EMR cluster with a bigger dataset which was also incomplete in terms of pyramid levels. What I am experiencing is it is using 10 GB of memory and only 1 core. I checked the environment tab on the SPARK UI and everything looks correct. ```shDRIVER_MEMORY := 4200MDRIVER_CORES := 2EXECUTOR_MEMORY := 4200MEXECUTOR_CORES := 2YARN_OVERHEAD := 700``` These are my configs and I was using M4.2xlarge machines. So why am I failing distributing this job over the cluster? Is it the code that is written poorly so that it will be serial?",3/16/17,1
dorukozturk,My cluster had 30 nodes and as I said in the environment tab of Spark UI I saw those 4200M numbers and other configs that I set up,3/16/17,1
pomadchin,@dorukozturk have you looked at your executors? mb out of memory?,3/16/17,1
pomadchin,what was the shuffle size?,3/16/17,1
pomadchin,what_Ñés the err? just out of mem (ram i mean)?,3/16/17,1
dorukozturk,Actually there are no errors,3/16/17,1
pomadchin,and what happens?,3/16/17,1
pomadchin,what ganglia says?,3/16/17,1
dorukozturk,The cluster has 750 gb of memory and using only 10,3/16/17,1
dorukozturk,And 2 executors,3/16/17,1
pomadchin,w0w,3/16/17,1
pomadchin,"sry mb i'm repeating everything already was written by you, but can you post here the full command you are trying to run?",3/16/17,1
dorukozturk,"@pomadchin ```shspark-submit --master yarn-cluster --verbose --class demo.Main --driver-memory 4200M --driver-cores 2 --executor-memory 4200M --executor-cores 2 --conf spark.dynamicAllocation.enabled=true --conf spark.driver.maxResultSize=4g --conf spark.yarn.executor.memoryOverhead=700 --conf spark.yarn.driver.memoryOverhead=700 s3://kitware-geotrellis-demo/geotrellis-sbt-template-assembly-0.1.0.jar```",3/16/17,1
dorukozturk,Any ideas on my issue ?,3/17/17,1
echeipesh,@dorukozturk I wonder if the `spark.default.parallelism` is too low. If the catalog read does not get `numPartitions` explicitly it uses `SparkContext.defaultParallelism`. You would know if you look at the task execution in the spark web UI and you have low number of partitions in the tasks. Without enough partitions the YARN cluster manager would not be able to scale up the executors with `spark.dynamicAllocation.enabled=true`.,3/17/17,1
echeipesh,@Charmatzis I haven_Ñét run into that problem but I_Ñém pretty sure JTS has no support for geodetic operations and dateline wrapping. Looks from that link like you can unwrap the JTS geoms and work with spatial4j but it wouldn_Ñét be pretty code.,3/17/17,1
metasim,"@Charmatzis FWIW, we just ran into a similar problem reprojecting MODIS data.",3/17/17,1
dorukozturk,@echeipesh Thank you very much I will double check that now.,3/17/17,1
dorukozturk,@echeipesh you were right that was the problem,3/17/17,1
dorukozturk,it was defaulting to 2 somehow :),3/17/17,1
Charmatzis,@metasim and howdid you solved it?,3/17/17,1
pomadchin,@metasim what modis product have you ingested? as i don_Ñét remember probelms with mod09a1 ; interesting (i ingested in native sinusoidal projection on a webmercator layout),3/17/17,1
metasim,"@Charmatzis We switched from ""buffer"" ETL mode to ""per-tile"" and the problem went away. The cause was that when we reprojected the oringinal granule there was enough rounding error to create a tile of zero area (that' my understanding; the work was being done by someone else).",3/17/17,1
metasim,@pomadchin It was [MCD43A4](https://lpdaac.usgs.gov/dataset_discovery/modis/modis_products_table/mcd43a4).,3/17/17,1
pomadchin,"@metasim it makes sense, i ingested the same data in times there were no buffered tiles support",3/17/17,1
pomadchin,sounds like a ~bug~ in buffered method?,3/17/17,1
metasim,"We also had to use an (approximate?) ESRI CRS code, as [SR-ORG:6974](http://spatialreference.org/ref/sr-org/6974/) isn't available in GT.... I can't find any info out there on who the authority is on SR-ORG codes. You guys knowanything about it?",3/17/17,1
pomadchin,"@metasim instead of not perfect ETL you can write your own ingest, and geotrellis would be pretty happy with just a correct proj4 string",3/17/17,1
metasim,"Yes, but there's just enough machinery in the ETL stuff to make that a difficult cost/benefit decision for us. #2072 was our hoped-for solution :)",3/17/17,1
metasim,"Would you guys accept a PR on that, or does it open a can of worms somewhere else?",3/17/17,1
pomadchin,that would be a part of https://github.com/locationtech/geotrellis/issues/1767,3/17/17,1
pomadchin,"but you can create a PR against current ETL, mb that idea would be moved into refactored ETL",3/17/17,1
metasim,"Actually, if my exploration is corret, the code to handle it  is isolated to `CRS.fromName`.",3/17/17,1
metasim,I'm assuming the `backend.json` processing just passes the string through.,3/17/17,1
metasim,(I mean `output.json`),3/17/17,1
pomadchin,that's totaly can and would be done; but in theory you don_Ñét even need to provide output crs if you don_Ñét want to reproject the input source (that_Ñés how it should work),3/17/17,1
pomadchin,and the crs can be grabed from the input data,3/17/17,1
pomadchin,hm,3/17/17,1
metasim,maybe we misunderstood that.,3/17/17,1
metasim,I'll ask Matt.,3/17/17,1
pomadchin,"eh, nope; in the current etl ouput crs is obligatory ): https://github.com/locationtech/geotrellis/blob/master/spark-etl/src/main/scala/geotrellis/spark/etl/Etl.scala#L145",3/17/17,1
pomadchin,"think that the quickest workaround / solution to make that field optional, and to take crs from metadata if that field is not availble ",3/17/17,1
metasim,"Yeh, that's the second nasty call to `getCrs.get` that has bit us (the other was fixed in the Avro CRS PR).",3/17/17,1
pomadchin,hm,3/17/17,1
pomadchin,ok,3/17/17,1
metasim,We need an sbt warning for all calls to `Option.get`.,3/17/17,1
metasim,(IMHO),3/17/17,1
pomadchin,https://github.com/wartremover/wartremover,3/17/17,1
pomadchin,checkout that link ^',3/17/17,1
metasim,`OptionPartial` FTW,3/17/17,1
pomadchin,http://www.wartremover.org/doc/install-setup.html,3/17/17,1
pomadchin,`wartremoverErrors ++= Warts.unsafe` is what you want,3/17/17,1
pomadchin,O:,3/17/17,1
metasim,Only if I wanted to spend all day with it! ;-),3/17/17,1
pomadchin,:D,3/17/17,1
metasim,"@lossyrob @pomadchin @echeipesh Do we need to sync up on the avro2spark inititiative? At some point I'd like to bring you up to date on ML-oriented `Transformer`s [such as these](https://gist.github.com/metasim/8c63bd53246b42004b9d4975e0b37a8d), and see here where things are with schema conversion.",3/17/17,1
metasim,(My assumption has been that you've been focused on backlog and release),3/17/17,1
metasim,"@pomadchin FYI, just enabling `OptionPartial` results in [3288 warnings](https://gist.github.com/anonymous/8e04d015c643683b8d4c720d82250d56) :-(",3/17/17,1
pomadchin,):,3/17/17,1
pomadchin,life is a mystery (c),3/17/17,1
metasim,"Yep, therefore don't call `Option[Life].get` :laughing: ",3/17/17,1
pomadchin,":D sometimes, it called _ÑÉrisk' i guess?",3/17/17,1
metasim,AKA non-referential transparency,3/17/17,1
echeipesh,"@metasim Yeah, sorry about letting that pretty trivial task slide, threw up PR with something that surprisingly works. [This](https://gist.github.com/metasim/8c63bd53246b42004b9d4975e0b37a8d#file-gtexpanders-scala-L46) in particular looks really interesting: ",3/17/17,1
metasim,@echeipesh FYI: https://bintray.com/s22s/maven/avro2spark/0.1.4,3/20/17,1
echeipesh,Nice!,3/20/17,1
pomadchin,Cool!,3/20/17,1
kklt,Hi«_Åhttps://github.com/geotrellis/geotrellis-chatta-demo     I want to know how to use this demo,3/21/17,1
kklt,èèêèçÔç_Ë‘¢ó_ÉË‘Ç‚__ˆ«_ª,3/21/17,1
pomadchin,hi @kklt ! replace [here](https://github.com/geotrellis/geotrellis-chatta-demo/blob/master/geotrellis/build.sbt#L19) gt version to `1.1.0-RC1` and follow instructions ,3/21/17,1
kklt,"@pomadchin what  is your email? I  have  some  problems  about   geotrellis  and  geotrellis-chatta-demo   want to learn from you...",3/22/17,1
kklt,"Hi«_Åhttps://github.com/geotrellis/geotrellis-chatta-demo     I want to know how to run this demo successfully.   I  down IDEA and SBT  in  ubuntu  ,  and  use sbt  download  the  dependency   libraries.   but  the demo  can not  run successfully.  If it  need  geodocker? ",3/22/17,1
pomadchin,"@kklt yo! We are pretty active here, and not only I can help you; + results of our conversation can be useful for everybody so drop your questions here",3/22/17,1
pomadchin,What did you try to do? What doesn't work? What errors do you have?,3/22/17,1
karalas,"hello all, I am trying to run the landsat-emr-demo locally but with cassandra as my background. I have managed to ingest successfully into cassandra, since I can see that the table I created has entries (key, name, zoom, value), but when I run the ```server```, I can't return a json representation of the catalog resulting in ```{""layers"":[]}``` when I hit ```http://localhost:8899/catalog```. ",3/22/17,1
pomadchin,how do you start your server?,3/22/17,1
pomadchin,"that should be done smth like: ```spark-submit --name ""Service"" --master ""local"" --driver-memory 1G \${SERVER_ASSEMBLY} cassandra ${ZOOS} ${MASTER}```[source](https://github.com/geotrellis/geotrellis-landsat-emr-demo/blob/master/server/src/main/scala/demo/Main.scala#L63-L68)",3/22/17,1
metasim,"I'm using Avro 1.8.1 in my code, but the Spark 2.1.0 chain pulls in Avro 1.7.6. Even though I'm bundling 1.8.1 in my assembly, when I run `spark-submit` I get `java.lang.NoSuchMethodError: org.apache.avro.Schema$Field.<init>(Ljava/lang/String;Lorg/apache/avro/Schema;Ljava/lang/String;Ljava/lang/Object;)V` (a method added in avro 1.8). Have you guys ever dealt with this sort of thing?",3/22/17,1
pomadchin,@metasim avro is in hadoop classpath and it depends on 1.6.x ,3/22/17,1
metasim,"Right, that's what I meant.",3/22/17,1
metasim,I'd like to use Avro 1.8.x... do you think am I out of luck?,3/22/17,1
pomadchin,"in fact there are three ways: you may shade your avro version (untested though may work), update avro in hadoop classpath, and the third - use old avro (1.7.x should be 99% compatabile)",3/22/17,1
metasim,about to try shading.... spooky magic IMO,3/22/17,1
pomadchin,[example](https://github.com/geotrellis/geotrellis-chatta-demo/blob/master/geotrellis/build.sbt#L33-L42) of shading,3/22/17,1
metasim,:+1: ,3/22/17,1
pomadchin,"no magic at all, it just renames links using jarjar through changing your assembly bytecode",3/22/17,1
metasim,"""changing your assembly bytecode"" == ""spooky""",3/22/17,1
metasim,:),3/22/17,1
pomadchin,"btw, off docs: https://github.com/sbt/sbt-assembly/#shading",3/22/17,1
pomadchin,"but the example link should be enough :D (that_Ñés what you need, just replace guava with avro and other libs with libs where you want to have shaded version)",3/22/17,1
metasim,thx,3/22/17,1
kthompson,is there a `geotrellis-spark` version that syncs with the `1.1.0-RC1` version?,3/22/17,1
kthompson,nevermind i forgot to update to `org.locationtech.geotrellis`,3/22/17,1
pomadchin,:D,3/22/17,1
karalas,"I did not start the server correctly indeed. I am now trying something like```local-tile-server: ZOOS=""localhost:2181""local-tile-server: MASTER=""localhost:7077""local-tile-server: ${SERVER_ASSEMBLY}	spark-submit --name ""${NAME} Service"" --master ""local"" --driver-memory 1G \${SERVER_ASSEMBLY} cassandra ${ZOOS} ${MASTER}```but I get a ```IllegalArgumentException: localhost:2181: Name or service not known```. How am I supposed to call it?",3/22/17,1
pomadchin,@karalas without port,3/22/17,1
pomadchin,*try without port**,3/22/17,1
karalas,"ok, thanks",3/22/17,1
karalas,"so, this ``` local-tile-server: ZOOS=localhostlocal-tile-server: MASTER=localhostlocal-tile-server: ${SERVER_ASSEMBLY}	spark-submit --name ""${NAME} Service"" --master ""local"" --driver-memory 1G \${SERVER_ASSEMBLY} cassandra ${ZOOS} ${MASTER}```works, but I get ```Error during processing of request: 'Attribute times not found for layer Layer(name = ""sentineltest"", zoom = 0)'``` which means that ```times``` has not been ingested correctly to database? ",3/22/17,1
pomadchin,"Yes, it means there is no times attribute; what ingest code did you use?",3/22/17,1
pomadchin,Can you print metadata table?,3/22/17,1
karalas,"```implicit val sc = SparkUtils.createSparkContext(""GeoTrellis ETL SinglebandIngest"", new SparkConf(true))  try {    EtlConf(args) foreach { conf =>      val etl = Etl(conf, Etl.defaultModules)      val sourceTiles = etl.load[TemporalProjectedExtent, Tile]      val (zoom, tiled) = etl.tile[TemporalProjectedExtent, Tile, SpaceTimeKey](sourceTiles)      val saveAction: Etl.SaveAction[SpaceTimeKey, Tile, TileLayerMetadata[SpaceTimeKey]] =        (attributeStore, writer, id, rdd) => {          writer.write(id, rdd)          if(id.zoom == 0) {            attributeStore.write(id, ""times"",              rdd                .map(_._1.instant)                .countByValue                .keys.toArray                .sorted)            attributeStore.write(id, ""extent"",              (rdd.metadata.extent, rdd.metadata.crs))          }        }      etl.save[SpaceTimeKey, Tile](LayerId(etl.input.name, zoom), tiled, saveAction)    }  } finally {    sc.stop()  }```",3/22/17,1
pomadchin,"Ye, that's what I thrower you good",3/22/17,1
karalas,yes,3/22/17,1
pomadchin,And what tables are there in Cassandra?,3/22/17,1
karalas,there is a table called ```catalog```,3/22/17,1
karalas,with a query like this,3/22/17,1
karalas,"```select * from my.cataloglimit 10allow filtering```",3/22/17,1
karalas,I can see the entries,3/22/17,1
pomadchin,is there a metadata table?,3/22/17,1
karalas,no,3/22/17,1
karalas,in the keyspace ```my``` there only the ```catalog``` table,3/22/17,1
karalas,but there is a table ```metadata``` under ```geotrellis``` keyspace,3/22/17,1
pomadchin,"Ha, that's the issue",3/22/17,1
pomadchin,Different key paces,3/22/17,1
pomadchin,Interesting,3/22/17,1
pomadchin,So the easiest idea to create catalog in geotrellis keysoace,3/22/17,1
pomadchin,The correct solution to change metadata keyspace in your app,3/22/17,1
pomadchin,https://github.com/locationtech/geotrellis/blob/master/cassandra/src/main/resources/reference.conf#L17,3/22/17,1
pomadchin,"Change it to my, in your ingest and server subproject ; just add this setting into conf file",3/22/17,1
pomadchin,"geotrellis.cassandra.keyspace = ""my""",3/22/17,1
pomadchin,in both cases you need to reingest dats,3/22/17,1
karalas,"ok I will try, thx",3/22/17,1
kthompson,any idea what the reason behind the GridBounds limitation is?,3/22/17,1
kthompson,ie `width.toLong * height.toLong > Int.MaxValue`,3/22/17,1
kthompson,looks like it was added with this commit https://github.com/locationtech/geotrellis/commit/a8904b827eefdad00c478666a7128c5ecc178b0a#diff-ecf633f8538a1cb6e6bd8e8fecc7b2acR44,3/22/17,1
karalas,"@pomadchin this solution finally worked! :) I tried also the more straightforward to me to do the ingest with ```""path"": ""geotrellis.catalog""``` instead of ```""path"": ""my.catalog""``` in ```output.json```, but this didn't work  ",3/22/17,1
Scootz99,"Hey, can i run geotrellis with windows? I didn't see anything in the documents about that. Just how to use it with mac or linux. ",3/22/17,1
kthompson,@Scootz99 ive been using on windows successfully for development,3/22/17,1
kthompson,our production environment uses linux though,3/22/17,1
kthompson,"Anyone know if I have a streaming GeoTiff, is the only way to get at a specific subsection would be to crop it?",3/22/17,1
kthompson,like actually reading points from the tile without downloading the whole tile,3/22/17,1
pomadchin,"@karalas eh, Ok I'll have a look; wondering why it didn't work? in what keyspace your tiles were ingested then (when you used that straightforward way)? ",3/23/17,1
lossyrob,@kthompson crop would be the way - was there another way you were thinking of approaching It?,3/23/17,1
peterhbr,@lossyrob i believe he's asking about things like byte range requests,3/23/17,1
peterhbr,like vsicurl,3/23/17,1
karalas,"@pomadchin  forget it, maybe I did something wrongly or there was a mess in cassandra with my tests.. I cleared all keyspaces and tables from cassandra and tried a fresh ingest and it worked without problems",3/23/17,1
pomadchin,@karalas wwwwwwwwww0000h,3/23/17,1
pomadchin,but i think there needs to be some clarification in docs / in code,3/23/17,1
pomadchin,i need to think about the design more; should we keep attribute store neccesarily in the same keyspace or not,3/23/17,1
pomadchin,and how it_Ñés done; as i think there is an inconsistence somewhere (just a guess :D),3/23/17,1
pomadchin,ah; it_Ñés fine; yes you need to change keysapce in config too; my idea was that probably you may want to store everything in different places; like all mds in a separate keyspace,3/23/17,1
pomadchin,i can add an additional method to make easier overloads :D but that seems correct.,3/23/17,1
karalas,nice :+1: ,3/23/17,1
pomadchin,btw so what was your soultion finally?,3/23/17,1
pomadchin,just reingest?,3/23/17,1
pomadchin,md and catalog are in separate tables?,3/23/17,1
pomadchin,hey @karalas i tried your last commit with reverted metadata keyspace changes: https://github.com/pomadchin/geotrellis-sentinel-demo/commit/f0a4a64c3d7c4e60d3c7599baade0fde377ad902,3/23/17,1
pomadchin,and it worked O:,3/23/17,1
pomadchin,[![Screen Shot 2017-03-23 at 12.41.14.png](https://files.gitter.im/geotrellis/geotrellis/NWnH/thumb/Screen-Shot-2017-03-23-at-12.41.14.png)](https://files.gitter.im/geotrellis/geotrellis/NWnH/Screen-Shot-2017-03-23-at-12.41.14.png),3/23/17,1
pomadchin,"```cqlsh> describe tables;...Keyspace geotrellis-------------------metadata...Keyspace my-----------catalog```",3/23/17,1
pomadchin,the only inconviniece i discovered that you have to create keyspace by hands.,3/23/17,1
pomadchin,looks like just smth weird happend to cassandra :D / or smth weird with ingest / whatever in your case.,3/23/17,1
karalas,"hey @pomadchin , yes probably it was something weird with cassandra.. and yes I have to create the keyspace by hand before the ingest",3/23/17,1
karalas,thank you,3/23/17,1
pomadchin,@karalas thanks for a test case: https://github.com/locationtech/geotrellis/pull/2083 in 1.1.0 that would be not necessary to create keyspace by hands (:,3/23/17,1
pomadchin,in 2.0.0 Cassandra along with HBase instances API may be refactored; issue is already created ^^',3/23/17,1
karalas,"i am back with a new question regarding the ingest process and the ordering of the catalog.. as far as i understand each new image i want to ingest refers to a new layer named with ```""name"": ""AAA""``` in the ```input.json```? This way, images belonging to the same area (tile according to the satellite) but are of different time instances, have a different layer name. Also, images that belong to a neighboring (other) tile, have again a different layer name. Is this how it works, or all images of the same area (satellite tile) can be defined ""under the same folder"" descriminated by different time instances, and another area has its own folder respectively?  ",3/23/17,1
pomadchin,so do you talk about updating already existing layer?,3/23/17,1
pomadchin,you can ingest not one by one images,3/23/17,1
pomadchin,but a whole folder to ingest during write,3/23/17,1
pomadchin,"ofc you can upload / overwrite existing images on the existing layer; to add new tiles, etc; you can use LayerUpdater for these purposes",3/23/17,1
pomadchin,i wrote a simple example how to do it: https://gist.github.com/pomadchin/c2c8346ab0ef1ece11cf4432a87d14ec,3/23/17,1
pomadchin,current geotrellis-etl project doesnt support it; but in gt 2.0.0 there would be done significant improvements O: that would allow you to update/reindex/whatever with your rasters,3/23/17,1
pomadchin,): just plans,3/23/17,1
pomadchin,and currently you have to write updater your own,3/23/17,1
Charmatzis,"Hi, guys. After working with scala spark and geotrellis for two months now. I am starting to see the end of the project that I develop.",3/23/17,1
Charmatzis,My last step is to calculate a path from A to B from a raster layer and exported to vector.,3/23/17,1
Charmatzis,"I have seen the cost distance method, this returns back a raster , how can I convert that to a path and then to a vector?",3/23/17,1
kthompson,"so i am trying to do some thingks with streaming geotiffs, and am getting files that look like noise. all i am doing now is:```    val tiff = GeoTiffReader.readMultiband(""input.tif"")    tiff.write(""output.tiff"")```And the output is still just noise. Am I missing a step or is this an issue with the RC maybe? I am going to try with 1.0 now to see if the issue is there as wellI am looking at the tiffs in qgis",3/23/17,1
pomadchin,@kthompson interesting; mb you can throw here your tile?,3/23/17,1
pomadchin,because i generated tiffs using snapshot and rc gt versions this week and it looks good,3/23/17,1
kthompson,yea I think so. uploading them now,3/23/17,1
pomadchin,cool,3/23/17,1
kthompson,@pomadchin i DM you those files,3/23/17,1
pomadchin,thanks!,3/23/17,1
kthompson,sure thing ill try with 1.0 now,3/23/17,1
pomadchin,im trying rc1 and snapshot,3/23/17,1
kthompson,ok i havent tried snapshot only `1.1.0-RC1`,3/23/17,1
kthompson,but let me know if you can reproduce,3/23/17,1
pomadchin,@kthompson i could not reproduce it on `1.1.0-RC1`,3/23/17,1
pomadchin,try to delete `ouput.tif` before writing into it,3/23/17,1
pomadchin,mb data is somehow corrupted,3/23/17,1
pomadchin,"what i did: ```Welcome to Scala 2.11.8 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_45).Type in expressions for evaluation. Or try :help.scala> import geotrellis.raster.io.geotiff.reader.GeoTiffReaderimport geotrellis.raster.io.geotiff.reader.GeoTiffReaderscala> val tiff = GeoTiffReader.readMultiband(""/data/input.tif"")tiff: geotrellis.raster.io.geotiff.MultibandGeoTiff = MultibandGeoTiff(geotrellis.raster.ArrayMultibandTile@1d8c1037,Extent(691500.0, 4151500.0, 716500.0, 4176500.0),geotrellis.proj4.CRS$$anon$3@94eb0799,Tags(Map(TIFFTAG_DATETIME -> 2016:04:30 19:27:02, AREA_OR_POINT -> AREA),List(Map(), Map(), Map(), Map(), Map())),GeoTiffOptions(geotrellis.raster.io.geotiff.Striped@389ba6d6,geotrellis.raster.io.geotiff.compression.DeflateCompression$@45eb145a,2,None))scala> tiff.write(""/data/output.tiff"")```",3/23/17,1
kthompson,hrm dang alright let me try again,3/23/17,1
pomadchin,btw,3/23/17,1
pomadchin,@kthompson i tried your output,3/23/17,1
pomadchin,it looks ok,3/23/17,1
pomadchin,"where is the bug, mb i just didn_Ñét notice smth?",3/23/17,1
pomadchin,i looked at images using qgis; decided to check gdal and gdalinfo with `-mm` calculation eqals on input and on ouput,3/23/17,1
kthompson,oh really? hrm maybe its an issue with QGIS,3/23/17,1
kthompson,i just updated it ill check the image in the updated version,3/23/17,1
kthompson,ahh screwy it looks like it was QGIS. thanks for the help,3/23/17,1
pomadchin,^^_Ñé),3/23/17,1
echeipesh,"@Charmatzis Thats interesting question. I suppose if the CD raster is small enough (which it probably isn_Ñét) you can just use some existing shortest path library to search over it, treating each pixel as a vertex. If you_Ñére trying to build a shortest path on tiled raster it looks like GraphX has shortest path algorithm, but it would probably do something aweful to job memory requirements to unroll a raster like that.",3/23/17,1
echeipesh,Although if its a single source CD raster you can just greedily choose the next pixel with lowest value and walk the raster to the source that way.,3/23/17,1
Charmatzis,"@echeipesh  wait a second, I believe there a function for that ",3/23/17,1
Charmatzis,https://github.com/locationtech/geotrellis/blob/master/raster-test/src/test/scala/geotrellis/raster/costdistance/CostDistanceWithPathsSpec.scala ,3/23/17,1
kthompson,@pomadchin is it possible to to streaming with a compressed geotiff?,3/23/17,1
kthompson,"it looks like when i do `GeoTiffReader.readMultiband(reader, true, true)` it actually downloads the whole geotiff immediately",3/23/17,1
echeipesh,@kthompson This is something that got fixed in 1.1.0-RC1 for multiband streaming: https://github.com/locationtech/geotrellis/pull/1905,3/23/17,1
kthompson,@echeipesh is that post RC1? or part of RC1?,3/23/17,1
echeipesh,It_Ñés part of RC1,3/23/17,1
kthompson,i *think* there is still an issue. im working on making something to make it reproducable,3/23/17,1
pomadchin,"@kthompson that would be helpful, thank you!",3/23/17,1
kthompson,alright here you go: https://gist.github.com/kthompson/7462b7169a6982f44be5629a8f5ed8af,3/23/17,1
lossyrob,"So this is an issue with http reading w/ signed urls, not with the s3 stream reading, correct?",3/23/17,1
kthompson,yea i havent tried using the normal S3 way,3/23/17,1
kthompson,"i did try doing ""streaming"" from a local file though and that seemed to work",3/23/17,1
lossyrob,"@kthompson gridbounds needs an integer size, so codified that. Like rasterextent, which is a grid extent, perhaps there should be a less restrictive bounds",3/23/17,1
kthompson,yea we are working with some really big geotiff(whole US)  and trying to stream it but I think for now we will just tile it with gdal first,3/23/17,1
lossyrob,Ok that's good to know - is the http reader in the gist a fix?,3/23/17,1
lossyrob,"If it's possible to stream, the http should do it correctly ",3/23/17,1
kthompson,the httpreader in the gist is a workaround as S3 doesnt seems to allow HEAD requests on signed urls,3/23/17,1
lossyrob,So id want to fix for 1.1,3/23/17,1
lossyrob,Does s3 just return the whole tile in that case?,3/23/17,1
kthompson,it seems a bit lame of S3 to allow a GET but not a HEAD request,3/23/17,1
kthompson,no it does support the range request and it is returning chunks,3/23/17,1
kthompson,the cropped tile looks vaguely similar so its working to some degree,3/23/17,1
lossyrob,"Ok, gotcha. So we need to support byte range reading on http that doesn't support the HEAD request technique that we currently use, and uses GET instead",3/23/17,1
lossyrob,Can you write an issue for that pointing to your gist? That should go into RC2,3/23/17,1
kthompson,sure but also the cropped part of that tiff is all wrong,3/23/17,1
kthompson,should i specify that in the same issue as well?,3/23/17,1
lossyrob,Yeah. Probably there's some fixes to that code that's required,3/23/17,1
kthompson,:+1: ,3/23/17,1
lossyrob,If you want to keep going on it the effort is appreciated ,3/23/17,1
kthompson,yea i dont know that i could get up to speed on the geotiff bits as well as you guys :),3/23/17,1
lossyrob,"True, it's a pretty complicated setup, but the more people that know the better :)",3/23/17,1
kthompson,haha yea :smile: ,3/23/17,1
kthompson,"does this seem odd or incorrect `originalExtent.buffer(-Math.min(originalExtent.height, originalExtent.width) / 4)`",3/23/17,1
kthompson,im basically trying to shrink the extent by some amount,3/23/17,1
lossyrob,Doesn't seem odd to me ,3/23/17,1
kthompson,ok thanks :thumbsup: ,3/23/17,1
kthompson,so just to be a bit more thourough it also looks like when i use the `S3RangeReader` i get a similar output that is incorrect as well,3/23/17,1
lossyrob,That's not good,3/23/17,1
kthompson,yea :/,3/23/17,1
lossyrob,that_Ñés with that same `small.tif?`,3/23/17,1
kthompson,"yea basically nearly identical as the gist i posted except with `val streamer = S3RangeReader(""https://kevin-test-bucket1.s3-us-west-2.amazonaws.com/input.tif"", S3Client.DEFAULT)` and the small.tif isnt looking correct",3/23/17,1
lossyrob,[![Screen Shot 2017-03-23 at 3.07.07 PM.png](https://files.gitter.im/geotrellis/geotrellis/3JRs/thumb/Screen-Shot-2017-03-23-at-3.07.07-PM.png)](https://files.gitter.im/geotrellis/geotrellis/3JRs/Screen-Shot-2017-03-23-at-3.07.07-PM.png),3/23/17,1
lossyrob,this is what the tiff looks like to me in qgis just from grabbing it from s3,3/23/17,1
lossyrob,w/ `wget`,3/23/17,1
lossyrob,that doesn_Ñét look right_Ñ_how was this written?,3/23/17,1
kthompson,that was created by the code in the gist,3/23/17,1
lossyrob,"oh so that_Ñés the result, not the input",3/23/17,1
lossyrob,gotcha,3/23/17,1
kthompson,correct,3/23/17,1
lossyrob,I_Ñém getting a 403 forbidden for the wget on the url (with sig) from the gist,3/23/17,1
lossyrob,"oh wait I_Ñùm pasting wrong, one sec",3/23/17,1
kthompson,:thumbsup: ,3/23/17,1
lossyrob,"and you said you can read windows fine with that file and crop, when it_Ñés on the filesystem?",3/23/17,1
kthompson,yes,3/23/17,1
kthompson,the source tiff is compressed to so that probably plays into it somehow,3/23/17,1
lossyrob,it shouldn_Ñét at all,3/23/17,1
lossyrob,compression happens at the geotiff segment level,3/23/17,1
lossyrob,inner-segment,3/23/17,1
kthompson,oh ok,3/23/17,1
lossyrob,the cropping stuff happens in terms of segments,3/23/17,1
kthompson,so `GeoTiffReader.readMultiband` has a `decompress` parameter is that something unrelated to the geotiff compression?,3/23/17,1
lossyrob,it does decompress,3/23/17,1
kthompson,"ie its ""decompressing"" immediately rather than lazily",3/23/17,1
lossyrob,"but the mechanisms that do the streaming, and the mechanisms that do decompression, are orthogonal ",3/23/17,1
lossyrob,yeah it decompresses the whole segment once it has it,3/23/17,1
lossyrob,it_Ñés the fetching of the segments and determining what those segments mean as far as contributing pixels) is what seems to be mucking up,3/23/17,1
kthompson,yea,3/23/17,1
lossyrob,"ok, I have the image. looking into it now",3/23/17,1
lossyrob,"@kthompson can you try with `    val tiff = MultibandGeoTiff.streaming(reader, false, true)` ?",3/23/17,1
lossyrob,wait nvm,3/23/17,1
lossyrob,I misread the code,3/23/17,1
kthompson,yea i think `MultibandGeoTiff.streaming` already calls it that way,3/23/17,1
kthompson,or calls `GeoTiffReader.readMultiband` rather,3/23/17,1
lossyrob,hmm I_Ñém getting the same looking error from the file crop,3/23/17,1
lossyrob,just from disk,3/23/17,1
lossyrob,"which is good, easier to debug",3/23/17,1
kthompson,haha well that sgood i suppose,3/23/17,1
kthompson,"ok so if i do `GeoTiffReader.readMultiband(""input.tif"", true, true)` locally it works",3/23/17,1
lossyrob,decompress I think doesn_Ñét work with the streaming option,3/23/17,1
lossyrob,or with the cropping. I might be wrong,3/23/17,1
kthompson,"`GeoTiffReader.readMultiband(""input.tif"", false, true)` give the weird output",3/23/17,1
lossyrob,"if you say decompress, it won_Ñét lazy crop",3/23/17,1
lossyrob,that_Ñés actually an API problem,3/23/17,1
kthompson,ok yea thats what it seemed like earlier when i was testing that,3/23/17,1
lossyrob,looksl like we missed testing pixel interleave multiband geotiffs. looking for the fix now,3/23/17,1
kthompson,awesome,3/23/17,1
Charmatzis,"Hi, is there a project or example where you have used the Terrain Tiles on AWS  ? (https://aws.amazon.com/public-datasets/terrain/ )",3/24/17,1
karalas,"hi all, I want to ingest many tiffs in cassandra of about 500MB each. I have placed my tiffs under the same folder which i give as input to ```path``` in ```input.json```.  I can ingest max 2 such tiffs, from 3 and above i get an ```ERROR Executor: Exception in task 2.0 in stage 1.0 (TID 5)java.lang.OutOfMemoryError: Java heap space```. I am running spark in a single machine and calling ```spark-submit``` with ```--driver-memory 7G --executor-memory 8G``` parameters. ",3/24/17,1
karalas,"so the process in etl tries to ingest all the images in memory at once and that's why my memory explodes? what can i do, ingest one by one?",3/24/17,1
Charmatzis,"@karalas try to insert these parameters ```""-XX:MaxPermSize=384M"", ""-d64"", ""-Xms512m"", ""-Xmx3g""```",3/24/17,1
Charmatzis,these are for the java,3/24/17,1
Charmatzis,think of it like when you executing sbt the default params are,3/24/17,1
Charmatzis,https://github.com/locationtech/geotrellis/blob/master/sbt#L24 ,3/24/17,1
karalas,"@Charmatzis  thank you, that indeed worked with 3 images, but when i tried with 15 images for example, i got the same error.  I set ```javaOptions += ""-XX:MaxPermSize=512M -d64 -Xms512m -Xmx8g""``` in ```build.sbt``` of both root and ingest folder.",3/24/17,1
Charmatzis, @karalas  I believe you should inject that in the spar-submit as an option,3/24/17,1
Charmatzis,like java-opts,3/24/17,1
pomadchin,@karalas yo! how many nods do you have?,3/24/17,1
pomadchin,how do you submit your job? (params),3/24/17,1
Charmatzis,@karalas  like that ,3/24/17,1
Charmatzis,http://stackoverflow.com/a/21157374 ,3/24/17,1
pomadchin,it can be done in a bit more human usable way :D,3/24/17,1
pomadchin,like this: https://github.com/geotrellis/geotrellis-landsat-emr-demo/blob/master/Makefile#L91-L98,3/24/17,1
pomadchin,"during the `spark-submit` launch you can specify memory, cores, etc, per each executor and for driver",3/24/17,1
pomadchin,so there are several options: in the `output.json` you can add a `_ÑÒpartitions_Ñù:_Ñù5000_Ñù` parameter to make calculations more spark usable (sure if you have more than one node),3/24/17,1
pomadchin,the partitions you have the easier it should work,3/24/17,1
pomadchin,spark would be less restricted in memory shuffle,3/24/17,1
pomadchin,and there is a chance that it would resolve the task even on low memory nodes,3/24/17,1
Charmatzis,"Hi, I am using ```tile.costDistanceWithPaths(Int, Int)```, but I am stuck a little bit. How can I get the pixel (int, int) in a tile having a Point(lat,long)?",3/24/17,1
pomadchin,@Charmatzis do you have tile extent and projection?,3/24/17,1
Charmatzis,@pomadchin  yeap,3/24/17,1
pomadchin,cool,3/24/17,1
pomadchin,sec,3/24/17,1
pomadchin,"```scalaval tmd: TileLayerMetadata[SpatialKey] = ???val point: Point = ??? // point in a latlngval raster: Raster[Tile] = ???val pointProjected = point.reproject(LatLng, tmd.crs)val (col, row) = raster.rasterExtent.mapToGrid(pointProjected)raster.get(col, row)```",3/24/17,1
pomadchin,there is also `gridToMap` function if that would be interesting for you,3/24/17,1
Charmatzis,@pomadchin  thx!,3/24/17,1
karalas,"i still have not find the correct way to call ```spark-submit```.. I use smth like ```local-ingest: JAVA_OPTS=""-XX:MaxPermSize=512M -d64 -Xms512m -Xmx8g""local-ingest: ${INGEST_ASSEMBLY}	spark-submit --name ""${NAME} Ingest"" --master ""local[4]"" --driver-memory 7G --executor-memory 8G \${INGEST_ASSEMBLY} ${JAVA_OPTS} \```but it does not compile (```unknown option ...```). I have seen also the ```extraJavaOptions``` in spark configuration but I see it's not for xmx. Which is the correct way?",3/24/17,1
karalas,i have one node for the time being,3/24/17,1
pomadchin,http://spark.apache.org/docs/latest/configuration.html#application-properties,3/24/17,1
pomadchin,The example is here https://github.com/geotrellis/geotrellis-landsat-emr-demo/blob/master/Makefile#L91-L98,3/24/17,1
pomadchin,But remove commas,3/24/17,1
karalas,ok,3/24/17,1
karalas,so the correct way is,3/24/17,1
karalas,"```local-ingest: ${INGEST_ASSEMBLY}	spark-submit --name ""${NAME} Ingest"" --master ""local[4]"" --driver-memory 8G \--driver-cores 2 \--executor-memory 8G \--executor-cores 2 \```",3/24/17,1
karalas,but still get ```java.lang.OutOfMemoryError: Java heap space``` error,3/24/17,1
Charmatzis,"How can I use ```tile.costDistanceWithPaths(int, int)``` if I have as an input ```RDD[(ProjectedExtent, Tile)]```or ```RDD[(SpatialKey, Tile)]```?",3/24/17,1
Charmatzis,i mean how can I move from rdd to tile ?,3/24/17,1
kthompson,Hey @lossyrob  not sure if you did anything with this yet but i have a workaround for the no HEAD support on signed S3 links (https://github.com/locationtech/geotrellis/issues/2084) ,3/24/17,1
kthompson,"you can basically do ``` request      .method(""GET"")      .execute { is =>        """"      }.headers(""Content-Length"").headOption``` to get the content length without downloading the whole thing",3/24/17,1
lossyrob,"@kthompson still tracking down the underlying pixel-interleave problem, and haven_Ñét gotten to that yet, so this is great thanks",3/24/17,1
lossyrob,@Charmatzis you_Ñére in a perfect place to test https://github.com/locationtech/geotrellis/pull/1999 :),3/24/17,1
kthompson,@lossyrob yea idk about the pixel interleave issue but i did put in a PR for a change to the `HttpRangeReader`,3/24/17,1
lossyrob,nice!,3/24/17,1
kthompson,were you going to use #2084 to track the pixel interleaving issue?,3/24/17,1
kthompson,"if so i will remove the ""Closes #2084"" bit from my PR",3/24/17,1
lossyrob,"I was, but your PR is more appropriate to solve that one. I_Ñéll write up another issue for the pixel interleave problem",3/24/17,1
kthompson,ok sounds good :thumbsup: ,3/24/17,1
lossyrob,"Such a small point, but I can_Ñét decide if the useHeadRequest should be default to true in the class. I see that it is in the object, but putting it in the class would not break API if someone used the new constructor_Ñ_but it begs the question, when you have a class with default arguments, do you put them in the object apply and leave them bare on the class or use default params in the class? /cc @echeipesh @moradology ",3/24/17,1
kthompson,yea I thought about doing that. I dont mind making the change. just let me know.,3/24/17,1
kklt,@pomadchin Hi «_Å I  am here,3/25/17,1
pomadchin,Yo!,3/25/17,1
pomadchin,So what are the problems? Do you use the latest Chattanooga demo master branch?,3/25/17,1
kklt,No,3/25/17,1
kklt,my  CDH  is 5.5,3/25/17,1
kklt,and  spark1.5,3/25/17,1
kklt,can not  run  the  DEMO,3/25/17,1
kklt,https://github.com/geotrellis/geotrellis-chatta-demo   who  have run  this DEMO successfully? ,3/25/17,1
pomadchin,what branch / sha commit of this demo do you use?,3/25/17,1
pomadchin,and have you tried to pull down the latest chatta demo branch?,3/25/17,1
pomadchin,\+ I don't think gt 1.0.0 + would really run successful on Spark 1.5 as we officially support Spark 2+ only,3/25/17,1
kklt,I  download  geotrellis-chatta-demo  and  download the  dependecy  via  SBT  .  ,3/25/17,1
pomadchin,can you throw here your build.sbt file?,3/25/17,1
kklt,"import scala.util.Propertiesname := ""GeoTrellis-Tutorial-Project""scalaVersion := Properties.propOrElse(""scala.version"", ""2.10.6"")mainClass in (Compile,run) := Some(""src.main.scala.geotrellis.chatta.Main"")crossScalaVersions := Seq(""2.11.8"", ""2.10.6"")organization := ""com.azavea""licenses := Seq(""Apache-2.0"" -> url(""http://www.apache.org/licenses/LICENSE-2.0.html""))scalacOptions ++= Seq(  ""-deprecation"",  ""-unchecked"",  ""-Yinline-warnings"",  ""-language:implicitConversions"",  ""-language:reflectiveCalls"",  ""-language:higherKinds"",  ""-language:postfixOps"",  ""-language:existentials"",  ""-feature"")publishMavenStyle := truepublishArtifact in Test := falsepomIncludeRepository := { _ => false }resolvers += Resolver.bintrayRepo(""azavea"", ""geotrellis"")val gtVersion = ""1.0.0-2077839""val geotrellis = Seq(  ""com.azavea.geotrellis"" %% ""geotrellis-accumulo""  % gtVersion,  ""com.azavea.geotrellis"" %% ""geotrellis-hbase""     % gtVersion,  ""com.azavea.geotrellis"" %% ""geotrellis-cassandra"" % gtVersion,  ""com.azavea.geotrellis"" %% ""geotrellis-s3""        % gtVersion,  ""com.azavea.geotrellis"" %% ""geotrellis-spark""     % gtVersion,  ""com.azavea.geotrellis"" %% ""geotrellis-spark-etl"" % gtVersion)libraryDependencies ++= (((Seq(  ""org.apache.spark""  %% ""spark-core""    % ""1.5.2"",  ""io.spray""          %% ""spray-routing"" % ""1.3.3"",  ""io.spray""          %% ""spray-can""     % ""1.3.3"",  ""org.apache.hadoop""  % ""hadoop-client"" % ""2.7.1"") ++ geotrellis) map (_ exclude(""com.google.guava"", ""guava""))) ++ Seq(""com.google.guava"" % ""guava"" % ""16.0.1""))ivyScala := ivyScala.value map { _.copy(overrideScalaVersion = true) }test in assembly := {}assemblyMergeStrategy in assembly := {  case ""reference.conf"" => MergeStrategy.concat  case ""application.conf"" => MergeStrategy.concat  case ""META-INF/MANIFEST.MF"" => MergeStrategy.discard  case ""META-INF\\MANIFEST.MF"" => MergeStrategy.discard  case ""META-INF/ECLIPSEF.RSA"" => MergeStrategy.discard  case ""META-INF/ECLIPSEF.SF"" => MergeStrategy.discard  case _ => MergeStrategy.first}",3/25/17,1
pomadchin,"Ok, so go into chatta demo dir and update it up to master",3/25/17,1
pomadchin,"btw, to insert code here, you can use GitHub markdown (wrap your code into \``` code here \```",3/25/17,1
kklt,[![blob](https://files.gitter.im/geotrellis/geotrellis/TwTP/thumb/blob.png)](https://files.gitter.im/geotrellis/geotrellis/TwTP/blob),3/25/17,1
kklt,I  click  Download ZIP  to  download  the source code  to my  disk,3/25/17,1
pomadchin,eh; than redownload it ._.' ,3/25/17,1
kklt,OK  ,3/25/17,1
pomadchin,but to interact with GitHub you can you git: https://en.wikipedia.org/wiki/Git,3/25/17,1
kklt,I   import the  source code into  IDEA,3/25/17,1
kklt,[![blob](https://files.gitter.im/geotrellis/geotrellis/gVM2/thumb/blob.png)](https://files.gitter.im/geotrellis/geotrellis/gVM2/blob),3/25/17,1
kklt,import  the  dependecy  jars  via  SBT ,3/25/17,1
kklt,[![blob](https://files.gitter.im/geotrellis/geotrellis/EKPD/thumb/blob.png)](https://files.gitter.im/geotrellis/geotrellis/EKPD/blob),3/25/17,1
pomadchin,seems ok; how s your build.sbt looks now?,3/25/17,1
pomadchin,From screenshots I can guess it's not the latest master ,3/25/17,1
kklt,"yes  ,  it  is  really  not  the  latest  master ",3/25/17,1
pomadchin,"Btw, if that would crash during the ingest (I think the probability of it is really high, as you are trying to launch it on the old spark and currently can't switch to the latest) you can try this commit: https://github.com/geotrellis/geotrellis-chatta-demo/tree/1bab1ddef4cc40a71f7a0c07536fdedcc3a5b11f",3/25/17,1
kklt,I  have   put  the  old  chatta-demo  in  my  CDH5.5,3/25/17,1
kklt,"in  my  CDH  , I   ececute  ./sbt  compile  and  ./sbt  package  success .  but  then  execute  ./ingest.sh   ,   it  execute  rong  ",3/25/17,1
pomadchin,"ok, try the version above :o it should support Spark 1.5.x+ ",3/25/17,1
kklt,"I  have  try  this  version  for  6  days ,  is  this  version  support  spark1.5?",3/25/17,1
kklt,this  is my  build.sbt,3/25/17,1
kklt,"import scala.util.Propertiesname := ""GeoTrellis-Tutorial-Project""scalaVersion := Properties.propOrElse(""scala.version"", ""2.10.6"")mainClass in (Compile,run) := Some(""src.main.scala.geotrellis.chatta.Main"")crossScalaVersions := Seq(""2.11.8"", ""2.10.6"")organization := ""com.azavea""licenses := Seq(""Apache-2.0"" -> url(""http://www.apache.org/licenses/LICENSE-2.0.html""))scalacOptions ++= Seq(  ""-deprecation"",  ""-unchecked"",  ""-Yinline-warnings"",  ""-language:implicitConversions"",  ""-language:reflectiveCalls"",  ""-language:higherKinds"",  ""-language:postfixOps"",  ""-language:existentials"",  ""-feature"")publishMavenStyle := truepublishArtifact in Test := falsepomIncludeRepository := { _ => false }resolvers += Resolver.bintrayRepo(""azavea"", ""geotrellis"")val gtVersion = ""1.0.0-2077839""val geotrellis = Seq(  ""com.azavea.geotrellis"" %% ""geotrellis-accumulo""  % gtVersion,  ""com.azavea.geotrellis"" %% ""geotrellis-hbase""     % gtVersion,  ""com.azavea.geotrellis"" %% ""geotrellis-cassandra"" % gtVersion,  ""com.azavea.geotrellis"" %% ""geotrellis-s3""        % gtVersion,  ""com.azavea.geotrellis"" %% ""geotrellis-spark""     % gtVersion,  ""com.azavea.geotrellis"" %% ""geotrellis-spark-etl"" % gtVersion)libraryDependencies ++= (((Seq(  ""org.apache.spark""  %% ""spark-core""    % ""1.5.2"",  ""io.spray""          %% ""spray-routing"" % ""1.3.3"",  ""io.spray""          %% ""spray-can""     % ""1.3.3"",  ""org.apache.hadoop""  % ""hadoop-client"" % ""2.7.1"") ++ geotrellis) map (_ exclude(""com.google.guava"", ""guava""))) ++ Seq(""com.google.guava"" % ""guava"" % ""16.0.1""))ivyScala := ivyScala.value map { _.copy(overrideScalaVersion = true) }test in assembly := {}assemblyMergeStrategy in assembly := {  case ""reference.conf"" => MergeStrategy.concat  case ""application.conf"" => MergeStrategy.concat  case ""META-INF/MANIFEST.MF"" => MergeStrategy.discard  case ""META-INF\\MANIFEST.MF"" => MergeStrategy.discard  case ""META-INF/ECLIPSEF.RSA"" => MergeStrategy.discard  case ""META-INF/ECLIPSEF.SF"" => MergeStrategy.discard  case _ => MergeStrategy.first}",3/25/17,1
kklt,is  this  verison  can  run  successfully  on  my  CDH5.5(with  spark1.5)?,3/25/17,1
kklt,[![blob](https://files.gitter.im/geotrellis/geotrellis/eWB2/thumb/blob.png)](https://files.gitter.im/geotrellis/geotrellis/eWB2/blob),3/25/17,1
kklt,this  version  can  not   run  success  in  my  IDEA   in  windows ,3/25/17,1
pomadchin,This one: https://github.com/geotrellis/geotrellis-chatta-demo/tree/1bab1ddef4cc40a71f7a0c07536fdedcc3a5b11f,3/25/17,1
kklt,[![blob](https://files.gitter.im/geotrellis/geotrellis/SaOC/thumb/blob.png)](https://files.gitter.im/geotrellis/geotrellis/SaOC/blob),3/25/17,1
kklt,"when   I   execute  ./sbt   compile  and  ./sbt   package   success,  then   I  execute   ./ingest.sh  .   it   not  run   success.   the  screenshots   is   here",3/25/17,1
pomadchin,"Ah, try ./sbt assembly",3/25/17,1
kklt,./sbt  assembly    also   execute   success,3/25/17,1
kklt,"@pomadchin    https://github.com/geotrellis/geotrellis-chatta-demo/tree/1bab1ddef4cc40a71f7a0c07536fdedcc3a5b11f       this  one  can  run  success   in  spark1.5?    in  this  version  , the  application.conf  file   is  like  this  ",3/25/17,1
pomadchin,yes,3/25/17,1
pomadchin,Did you read the README.md? ,3/25/17,1
pomadchin,"So there are two parts of the app, the first one is the ingest, data gots ingested into backed you specified (Accunulo, hadoop, etc). the second is a server part, to view the ingested weighted overlay in webui",3/25/17,1
kklt,I   kown     what   the  app   want  to  do.   ,3/25/17,1
kklt,[![blob](https://files.gitter.im/geotrellis/geotrellis/ecjf/thumb/blob.png)](https://files.gitter.im/geotrellis/geotrellis/ecjf/blob),3/25/17,1
kklt,"@pomadchin   I  use   this   version   https://github.com/geotrellis/geotrellis-chatta-demo/tree/1bab1ddef4cc40a71f7a0c07536fdedcc3a5b11f           execute  ./sbt assembly    success,   then  execute  ./ingest.sh    ,   but   it   run  fault  ,   the  screenshot   is     above",3/25/17,1
kklt,Caused by: java.io.FileNotFoundException: File file:/home/liukai/lunwen/test/geotrellis-chatta-demo-spark15/geotrellis/data/arg_wm/PrimeAgriculturalSoilsNotForestedOrFarmland.tiff does not exist,3/25/17,1
kklt,"in  fact,   those  files   are   in  /data/arg_wm  ",3/25/17,1
kklt,[![blob](https://files.gitter.im/geotrellis/geotrellis/uD1W/thumb/blob.png)](https://files.gitter.im/geotrellis/geotrellis/uD1W/blob),3/25/17,1
kklt,"Job aborted due to stage failure: Task 0 in stage 0.0 failed 4 times, most recent failure: Lost task 0.3 in stage 0.0 (TID 3, h4): java.io.FileNotFoundException: File file:/home/liukai/lunwen/test/geotrellis-chatta-demo-spark15/geotrellis/data/arg_wm/DevelopedLand.tiff does not exist",3/25/17,1
kklt,[![blob](https://files.gitter.im/geotrellis/geotrellis/vTJE/thumb/blob.png)](https://files.gitter.im/geotrellis/geotrellis/vTJE/blob),3/25/17,1
kklt,why  is  it?,3/25/17,1
pomadchin,@just change ingest paths to correct ones,3/25/17,1
kklt,Caused by: akka.remote.transport.netty.NettyTransport$$anonfun$associate$1$$anon$2: Connection refused: h5/192.168.1.125:34608,3/26/17,1
kklt,How  to  solve   this   problem?,3/26/17,1
kklt,this  demo   can   run  in   geodocker?   ,3/26/17,1
Scootz99,"hey, im trying to run the chatta-demo with intellij on windows. When i want to debug it, there is always the error main class not found with geotrellis.chatta.Main is not found or cant be loaded. I checked it in the run/debug configuration but cant find the main class in the project structure i downloaded here: https://github.com/geotrellis/geotrellis-chatta-demo Can anyone help me with that problem?",3/26/17,1
kklt,@Scootz99  this  demo   has  two  main,3/27/17,1
kklt,[![blob](https://files.gitter.im/geotrellis/geotrellis/KFdS/thumb/blob.png)](https://files.gitter.im/geotrellis/geotrellis/KFdS/blob),3/27/17,1
karalas,"hello, I want to ingest a lot of tiffs into cassandra but I have memory problems with spark. I am calling ```spark-submit``` as follows:```local-ingest: ${INGEST_ASSEMBLY}	spark-submit --name ""${NAME} Ingest"" --master ""local[4]"" --driver-memory 10G \	--driver-cores 1 \	--executor-memory 8g \	--executor-cores 1 \	--conf spark.yarn.executor.memoryOverhead=1g \	--conf spark.yarn.driver.memoryOverhead=1g \	--conf spark.network.timeout=240s \	--conf spark.driver.maxResultSize=5g \	--driver-java-options ""-XX:+UseCompressedOops -XX:MaxPermSize=2g -d64 -Xms1g"" \```I have tried also to incorporate Kryo serializer  in main ingest process```val conf =    new SparkConf()      .setMaster(""local[*]"")      .setAppName(""GeoTrellis ETL SinglebandIngest"")      .set(""spark.serializer"", ""org.apache.spark.serializer.KryoSerializer"")      .set(""spark.kryo.registrator"", ""geotrellis.spark.io.kryo.KryoRegistrator"")  implicit val sc = new SparkContext(conf)  try {    EtlConf(args) foreach { conf =>      val etl = Etl(conf, Etl.defaultModules) ...```I am testing now with only 15 tiffs of about 0.5g each,  but get either this```17/03/27 09:57:20 ERROR Executor: Exception in task 13.0 in stage 0.0 (TID 13)java.lang.OutOfMemoryError```or  this error```17/03/27 09:28:12 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 10.0.2.15:46412 in memory (size: 2.2 KB, free: 5.2 GB)Java HotSpot(TM) 64-Bit Server VM warning: INFO: os::commit_memory(0x0000000657500000, 2472542208, 0) failed; error='Cannot allocate memory' (errno=12)## There is insufficient memory for the Java Runtime Environment to continue.# Native memory allocation (mmap) failed to map 2472542208 bytes for committing reserved memory.# An error report file with more information is saved as:# /home/kkaralas/Documents/geotrellis-sentinel-demo/hs_err_pid5351.logmake: *** [local-ingest] Error 1```I can see that both memory and swap are full in runtime. Is there a way to restrict somehow the memory usage and do the ingest? thank you",3/27/17,1
pomadchin,@karalas sry i don_Ñét remember; how many nodes do you have?,3/27/17,1
karalas,i have only one node,3/27/17,1
Charmatzis,"@karalas  can you try something like this ?```",3/27/17,1
Charmatzis,"```   --conf ""spark.driver.extraJavaOptions=-XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:MaxHeapFreeRatio=70 "" ```",3/27/17,1
pomadchin,@karalas i think you need significantly more memory; as 8 gigs would be enough just to load input into spark memory,3/27/17,1
pomadchin,`0.5 * 15`,3/27/17,1
karalas,"one dummy question, if I ingest one-by-one the images and not the whole folder at once, then would it be ok? ",3/27/17,1
karalas,@Charmatzis  i have tried but without sucess,3/27/17,1
pomadchin,"@karalas sure; but for that way, you need to use layer updater instead of layer writer, to _update_ already existent image layerso that would be smth like: 1. normal ingest (you ingest any of your iamges at first) 2. ingest using layer updater",3/27/17,1
Charmatzis,@karalas  same error?,3/27/17,1
karalas,@Charmatzis yes,3/27/17,1
pomadchin,"@Charmatzis sure, a simple math; 8 gigs enough just to load all tiffs into spark mem at once without doing any operations on them",3/27/17,1
pomadchin,@karalas https://gist.github.com/pomadchin/c2c8346ab0ef1ece11cf4432a87d14ec,3/27/17,1
pomadchin,looks like it can be a bit outdated but it should work smth like that,3/27/17,1
pomadchin,(the idea is the following),3/27/17,1
pomadchin,I_Ñéll check how it compiles with GT-RC2 a bit later,3/27/17,1
Charmatzis,"hi guys. Have you ever worked with ```costDistanceWithPaths```  function? Did you experience a performance issues? I have worked with a tiny tileset 512x512 and it doesn't return results and times out when I give input like pointA(100, 100) to pointB(200, 200)",3/27/17,1
kklt,why  the  latest   chatta-demo  do   not  have   the  two  file  ingest.sh and  run-server.sh?,3/27/17,1
pomadchin,it has a make file,3/27/17,1
pomadchin,https://github.com/geotrellis/geotrellis-chatta-demo#usage,3/27/17,1
kklt,thank  you,3/27/17,1
kklt,I  use  Docker  to  run this  demo  ,3/27/17,1
kklt,have  downloaded   the  four  docker images,3/27/17,1
pomadchin,@Charmatzis yo! I think @echeipesh can give you some comments on it,3/27/17,1
kklt,[![blob](https://files.gitter.im/geotrellis/geotrellis/1VEX/thumb/blob.png)](https://files.gitter.im/geotrellis/geotrellis/1VEX/blob),3/27/17,1
kklt,I  use   the   latest  chatta-demo.   and  spark2.10  scala 2.11.8,3/27/17,1
pomadchin,Replace [this](https://github.com/geotrellis/geotrellis-chatta-demo/blob/master/geotrellis/build.sbt#L19) with `1.1.0-RC2`,3/27/17,1
kklt,"when   exexcute    ./sbt   assembly    ,       it    run   falss",3/27/17,1
kklt,"@pomadchin   I   do  not   know   what  you   mean ,    sorry   ,,  please   explain   it   detailly",3/27/17,1
kklt,[![blob](https://files.gitter.im/geotrellis/geotrellis/jwbz/thumb/blob.png)](https://files.gitter.im/geotrellis/geotrellis/jwbz/blob),3/27/17,1
kklt,"./sbt  compile           also   run   false   ,   I  have    already   used   the    latest    geotrellis-chatta-demo ,",3/27/17,1
pomadchin,@kklt update it again; I applied changes to make demo work,3/27/17,1
pomadchin,That_Ñés https://github.com/geotrellis/geotrellis-chatta-demo/commit/efcbcc2621ca73550ac363e1114ff7e4980ae9ef what i meant by a replace (:,3/27/17,1
kthompson,is there some docs somewhere on combining a `Geometry` with a `Tile`? I have a Tile that is a essentially a bit mask and i want to add the Geometry to it to get a new bit mask/Tile. or maybe there is an alternate way,3/27/17,1
echeipesh,You_Ñére trying to burn a geomery mask into a tile cell mask ?,3/27/17,1
kthompson,yea,3/27/17,1
kthompson,"ultimately i need to remove a bunch of pixels from a tile. but i have two masks to use, one is a Geometry and the other is a Tile",3/27/17,1
echeipesh,There isn_Ñét anything super sophisticated you can make tile mutable (which will just be re-wrap probably) and use https://github.com/locationtech/geotrellis/blob/master/raster/src/main/scala/geotrellis/raster/rasterize/Rasterizer.scala#L110,3/27/17,1
kthompson,ok cool ill take a look at that thanks,3/27/17,1
lossyrob,Yeah that seems like a straightforward rasterizer use case,3/27/17,1
kklt,[![blob](https://files.gitter.im/geotrellis/geotrellis/glYd/thumb/blob.png)](https://files.gitter.im/geotrellis/geotrellis/glYd/blob),3/28/17,1
kklt,"the     demo     do    not    have    the     two    files    ingest.sh     and     run-server.sh ,    ",3/28/17,1
kklt,How   can   I    copy   the    two   files    into   docker?,3/28/17,1
kklt,[![blob](https://files.gitter.im/geotrellis/geotrellis/zB02/thumb/blob.png)](https://files.gitter.im/geotrellis/geotrellis/zB02/blob),3/28/17,1
kklt,what    is   wrong   ?,3/28/17,1
pomadchin,"why do you need `ingest.sh` and `run-server.sh`? you have a `Makefile` here, you can do `make ingest` and `make server` instead. Old `.sh` files won_Ñét work with the current demo as GT API changed significnatly.This error means, that path [/data/chatta-demo](https://github.com/geotrellis/geotrellis-chatta-demo/blob/master/geotrellis/conf/output.json#L4) already contains the layer, before ingest i recommend you to remove data from this folder. Or you can look into the folder: it should contain all [these](https://github.com/geotrellis/geotrellis-chatta-demo/blob/master/geotrellis/conf/input.json#L3) folders (layer names would correspond to folder names).",3/28/17,1
kklt,[![blob](https://files.gitter.im/geotrellis/geotrellis/bmaY/thumb/blob.png)](https://files.gitter.im/geotrellis/geotrellis/bmaY/blob),3/28/17,1
kklt,[![blob](https://files.gitter.im/geotrellis/geotrellis/nG0c/thumb/blob.png)](https://files.gitter.im/geotrellis/geotrellis/nG0c/blob),3/28/17,1
kklt,[![blob](https://files.gitter.im/geotrellis/geotrellis/ucQ4/thumb/blob.png)](https://files.gitter.im/geotrellis/geotrellis/ucQ4/blob),3/28/17,1
pomadchin,looks like a weak machine,3/28/17,1
pomadchin,how much memory and cpu do you have?,3/28/17,1
pomadchin,to workaround you may try to increase spark hartbeat,3/28/17,1
pomadchin,set `spark.network.timeout 10000000` in spark-defaults.conf,3/28/17,1
pomadchin,"that_Ñés how @karalas did, and it worked",3/28/17,1
kklt,I   have   32G   memory,3/28/17,1
kklt,"I have   three  machine  run  CDH,    I  use   one   machine   to   run   this  demo",3/28/17,1
kklt,[![blob](https://files.gitter.im/geotrellis/geotrellis/AYeM/thumb/blob.png)](https://files.gitter.im/geotrellis/geotrellis/AYeM/blob),3/28/17,1
kklt,[![blob](https://files.gitter.im/geotrellis/geotrellis/1U6Q/thumb/blob.png)](https://files.gitter.im/geotrellis/geotrellis/1U6Q/blob),3/28/17,1
pomadchin,"how many hdfs nodes are alive btw? ",3/28/17,1
pomadchin,`serveraddress:50070`,3/28/17,1
pomadchin,ok i noticed smth,3/28/17,1
pomadchin,you can_Ñét use the old chatta demo with the current geodocker cluster,3/28/17,1
pomadchin,from screenshots I noticed that you use the old one,3/28/17,1
pomadchin,so I am not surprised by spark failures,3/28/17,1
pomadchin,"i thought i described you rather clear that the chatta demo would work with geodocker cluster; the old one not. Don_Ñét use the old one at all, it_Ñés not really maintained :)",3/28/17,1
pomadchin,Especially for you i updated current chatta demo to base not on geotrellis-snapshot but on rc2,3/28/17,1
pomadchin,"just do the following: ```bashgit clone https://github.com/geotrellis/geotrellis-chatta-democd https://github.com/geotrellis/geotrellis-chatta-demomake ingest-docker && make server```",3/28/17,1
pomadchin,https://github.com/geotrellis/geotrellis-chatta-demo#usage,3/28/17,1
kklt,"No,   I   have  used  the  new  one",3/28/17,1
kklt,I   have  used  the   latest  one ,3/28/17,1
pomadchin,@kklt please double check it ,3/28/17,1
pomadchin,"as chatta demo, geotrellis and Spark 2 in a geodocker cluster support scala 2.11.x",3/28/17,1
pomadchin,[![Screen Shot 2017-03-28 at 10.51.23.png](https://files.gitter.im/geotrellis/geotrellis/TiZB/thumb/Screen-Shot-2017-03-28-at-10.51.23.png)](https://files.gitter.im/geotrellis/geotrellis/TiZB/Screen-Shot-2017-03-28-at-10.51.23.png),3/28/17,1
kklt,[![blob](https://files.gitter.im/geotrellis/geotrellis/A5Nv/thumb/blob.png)](https://files.gitter.im/geotrellis/geotrellis/A5Nv/blob),3/28/17,1
kklt,I  execute  the  command  like   this,3/28/17,1
pomadchin,ah i see,3/28/17,1
pomadchin,a typoe in docs. ok,3/28/17,1
kklt,what?  ,3/28/17,1
pomadchin,"so have you tried to do just `make ingest-docker` from your host machine (not form inside arised geodocker setup)?",3/28/17,1
pomadchin,because you don_Ñét need the whole geodocker env to run this demo,3/28/17,1
kklt,[![blob](https://files.gitter.im/geotrellis/geotrellis/KmCT/thumb/blob.png)](https://files.gitter.im/geotrellis/geotrellis/KmCT/blob),3/28/17,1
pomadchin,that was `make ingest-docker` ?,3/28/17,1
kklt,"when  i   execute   the command  docker-compose up,it   start   false",3/28/17,1
kklt,No   ,3/28/17,1
pomadchin,"ok, so run `make ingest-docker`",3/28/17,1
pomadchin,i_Ñéll follow steps with `docker-compose up` and fix missing / weird readme docs.,3/28/17,1
kklt,OK,3/28/17,1
kklt,I   have   already   downloaded   the   fourh   docker images,3/28/17,1
kklt,[![blob](https://files.gitter.im/geotrellis/geotrellis/5GSg/thumb/blob.png)](https://files.gitter.im/geotrellis/geotrellis/5GSg/blob),3/28/17,1
pomadchin,and what_Ñés the result of `make ingest-docker ` ?,3/28/17,1
kklt,[![blob](https://files.gitter.im/geotrellis/geotrellis/Gfv9/thumb/blob.png)](https://files.gitter.im/geotrellis/geotrellis/Gfv9/blob),3/28/17,1
pomadchin,why it_Ñés interrupted?,3/28/17,1
kklt,"when  execute  the command make ingest-docker , it  begins  to download  the  docker images ,  but   I  have  already downloaded the fourh docker images",3/28/17,1
pomadchin,this command was based on a different base docker image to make it more lightweight. it downloads a base docker image and builds a new local container on top of it.,3/28/17,1
pomadchin,https://github.com/geotrellis/geotrellis-chatta-demo/blob/master/geotrellis/Dockerfile#L1,3/28/17,1
kklt,I   still  can not  run  it successfully,3/28/17,1
pomadchin,@kklt i could do it by just `make ingest-docker && make server` on a clean machine,3/28/17,1
pomadchin,without geodocker env,3/28/17,1
pomadchin,"plus i fixed README with geodocker usage description, it should work though it_Ñés still more complicated; i recommend you to switch to geodocker after figuring out how to work with docker",3/28/17,1
Charmatzis,"hi, have you ever consider to do raster analysis with tiles that are intersecting the dayline? I mean like there would be two tilesets e.g. ( tile1 = Extent (179, 90, 180, -90), tile2 = Extent (-179, 90, -180, -90))",3/28/17,1
lossyrob,"@Charmatzis we have considered it, but honestly it hasn_Ñét come up. JTS is our underlying vector library, and it_Ñés famous for not handling the dateline well. I would suggest looking into ways that JTS users have handled dateline issues - we_Ñéd be happy to take suggestions/get feature request issues/bug issues, but this is def an area we haven_Ñét explored a lot",3/28/17,1
kthompson,if i have a raster that i want to ingest but i want the layer it gets ingested into to have a bigger extent than that raster what do i need to do before ingesting it?,3/28/17,1
kthompson,would i use reproject for that?,3/28/17,1
kthompson,all of the reproject methods seem to take a CRS or transform but i dont think i actually need to transform the data. ,3/28/17,1
kthompson,"does this seem right? ```   def fillRaster(raster: Raster[Tile], newExtent: Extent): Option[Raster[Tile]] = {    if (newExtent.contains(raster.extent)) {      Some {        val re      = RasterExtent(newExtent, CellSize(256, 256))        val newTile = DoubleArrayTile.ofDim(re.cols, re.rows)        val colOffset = re.mapXToGrid(raster.extent.xmin)        val rowOffset = re.mapXToGrid(raster.extent.ymin)        newTile.update(colOffset, rowOffset, raster)        Raster(newTile, newExtent)      }    } else {      None    }  }```",3/28/17,1
kthompson,looks like i have a typo with thw rowOffset using mapX instead of mapY,3/28/17,1
pomadchin,"@kthompson are you talking about the following?1. ingest a tile / set of tiles into layer2. add more tiles that would be out of already ingested data ",3/28/17,1
lossyrob,you could use `merge`,3/28/17,1
kthompson,basically yes. so if we have information that says a tile needs to be updated i need to go back and update it.,3/28/17,1
lossyrob,"```scalaval re      = RasterExtent(newExtent, CellSize(256, 256))val newTile = DoubleArrayTile.ofDim(re.cols, re.rows)Raster(newTile, newExtent).merge(raster)```",3/28/17,1
kthompson,hrm ok thats a bit simpler :),3/28/17,1
lossyrob,https://geotrellis.github.io/scaladocs/latest/index.html#geotrellis.raster.merge.RasterMergeMethods@merge(other:geotrellis.raster.Raster[T]):geotrellis.raster.Raster[T],3/28/17,1
pomadchin,:D functions on any life-case ,3/28/17,1
lossyrob,it_Ñéll only burn values if the left raster has a nodata value,3/28/17,1
lossyrob,which in this case is all the values,3/28/17,1
kthompson,yea ok,3/28/17,1
lossyrob,it_Ñés how we do pyramiding and the like,3/28/17,1
kthompson,oh nice that works with multiband tiles too,3/28/17,1
kthompson,"is there a way to generate an empty multiband tiles with the correct number of bands similar to `DoubleArrayTile.ofDim(re.cols, re.rows)`",3/28/17,1
kthompson,i guess something like ,3/28/17,1
kthompson,"```MultibandTile(          (0 to raster.tile.bandCount)            .map(_ => DoubleArrayTile.ofDim(re.cols, re.rows)))```should work",3/28/17,1
lossyrob,yeah. that_Ñés a bit of missing API I_Ñéd say,3/28/17,1
kthompson,"so i got ```  private def fillRaster(raster: Raster[MultibandTile], newExtent: Extent): Option[Raster[MultibandTile]] = {    if (newExtent.contains(raster.extent)) {      val RasterExtent(_, cellWidth, cellHeight, _, _) = raster.rasterExtent      Some {        val re = RasterExtent(newExtent, CellSize(cellWidth, cellHeight))        val newTile = MultibandTile(          (0 to raster.tile.bandCount)            .map(_ => DoubleArrayTile.ofDim(re.cols, re.rows)))        Raster(newTile, newExtent).merge(raster)      }    } else {      None    }  }```",3/28/17,1
kthompson,gitter formatting isnt as forgiving as slack :),3/28/17,1
lossyrob,"I would do `val re = RasterExtent(newExtent, rasterExtent.cellSize)` but yeah",3/28/17,1
lossyrob,well hm,3/28/17,1
lossyrob,"DoubleArrayTile.ofDim(re.cols, re.rows) doesn_Ñét have a NoData defined",3/28/17,1
lossyrob,and will be 0_Ñés,3/28/17,1
lossyrob,"do you want the outer cells to be NoData (Double.NaN), 0, or 0 and also considered NoData (`DoubleUserDefinedNoDataCellType(0.0)`)",3/28/17,1
kthompson,NoData (Double.NaN),3/28/17,1
lossyrob,"use `DoubleArrayTile.empty(re.cols, re.rows)`",3/28/17,1
kthompson,:+1:  ,3/28/17,1
kthompson,looks like they both use `DoubleConstantNoDataCellType` but empty i think is better semantically,3/28/17,1
lossyrob,well `ofDim` will fill it with 0_Ñés,3/28/17,1
lossyrob,"so they won_Ñét be NoData, they_Ñéll be 0.0 values",3/28/17,1
lossyrob,`empty` will fill it with Double.NaN,3/28/17,1
kthompson,ahh yes i see now,3/28/17,1
kthompson,thanks,3/28/17,1
lossyrob,"Did I announce alreay that __1.1.0-RC2__ is out? If not, whoops! It_Ñés out!",3/28/17,1
kthompson,i saw someone else reference it so I updated to that locally :),3/28/17,1
kthompson,is this issue https://github.com/locationtech/geotrellis/issues/2100 required for streaming interleaved geotiffs?,3/28/17,1
lossyrob,"that issue is for writing, not for reading",3/28/17,1
lossyrob,the interleave geotiff read issues should be solved in RC2,3/28/17,1
kthompson,oh nice. ill test it out.,3/28/17,1
kthompson,so im trying out my test https://gist.github.com/kthompson/7462b7169a6982f44be5629a8f5ed8af and it seems like the extent i specify im downloading about 33mb but even if i make it smaller by buffering it more it still downloads 33mb. and the `crop` method does appear to be returning.,3/28/17,1
kthompson,ignore the NOTES bit,3/28/17,1
kthompson,i am trying with just a local file now to see if there is any difference,3/28/17,1
kthompson,the output shows looking how youd expect but it does not appear to be cropped,3/28/17,1
kthompson,ie the image isnt pixelated or anything but its the same size as the original,3/28/17,1
kthompson,"Here is my log output:```Original extent: Extent(691500.0, 4151500.0, 716500.0, 4176500.0)New extent: Extent(691500.09765625, 4151500.09765625, 716499.90234375, 4176499.90234375)cropping tiff DEBUG[main] UInt16GeoTiffMultibandTile - Cropping GridBounds(0,0,4999,4999) uses 5000 out of 5000 pixel-interleave segments```",3/28/17,1
kthompson,ugh my cropped extent is dumb,3/28/17,1
kthompson,ok so ignore everything I said streaming with interleaved tiles is looking good :thumbsup:  its downloading about half the size of the original geotiff when i request an extent that is about half the size. so nice work,3/28/17,1
Charmatzis,"@lossyrob Well I have search for JTS and how it handles the dayline issues and I didn't found a solution, even though I have develped a solution for vectors but now I am dealing with rasters. Maybe I should find my own solution. ",3/29/17,1
karalas,"hi, I try to use layer updater with cassandra in order to ingest a lot of tiffs of the same area but with different dates. The code I am trying is here https://github.com/karalas/geotrellis-sentinel-demo/blob/master/ingest/src/main/scala/demo/SentinelIngestMain.scala, but I can see the my mem (~10g) bumps up again and goes to swap. Then I get a ```com.datastax.driver.core.exceptions.OperationTimedOutException: [localhost/127.0.0.1:9042] Timed out waiting for server response```.  Obviously I am doing smth wrong..",3/29/17,1
pomadchin,"@karalas so the idea of a snippet i throwed you in a following: let it be 10 tiles to ingest:1.  ingest a first tile using layerWriter (and with [fixed](https://github.com/karalas/geotrellis-sentinel-demo/blob/master/ingest/src/main/scala/demo/SentinelIngestMain.scala#L153-L158) temporal keybounds for the ingested layer)2. ingest other tiles with layerUpdater one by one (in separate jobs)",3/29/17,1
pomadchin,"how layer updater works: grabs layer you want to ignest keybounds, makes a db lookup layer (if there would be some intersections, in your case - no intersections), merges if there are some intersections, writes",3/29/17,1
pomadchin,how do you work with it?,3/29/17,1
karalas,@pomadchin I think that until https://github.com/karalas/geotrellis-sentinel-demo/blob/master/ingest/src/main/scala/demo/SentinelIngestMain.scala#L161 is my initial ingest and then with https://github.com/karalas/geotrellis-sentinel-demo/blob/master/ingest/src/main/scala/demo/SentinelIngestMain.scala#L178-L186 I try to ingest the second image in order to update the layer. Is this what you asked me for? Maybe you mean that here https://github.com/karalas/geotrellis-sentinel-demo/blob/master/ingest/src/main/scala/demo/SentinelIngestMain.scala#L178 I need to call a ```new SparkContext```?,3/29/17,1
lossyrob,@Charmatzis `spatial4j` (another locationtech project) handles this,3/29/17,1
lossyrob,"it_Ñés a library that is by a solar/lucene committer, who works on thier spatial stuff",3/29/17,1
pomadchin,"@karalas that_Ñés correct; just split that into two separate jobs; one for update, second one for ingest",3/29/17,1
pomadchin,and launch two different spark-submit jobs,3/29/17,1
pomadchin,hey @karalas do you need some additional clarification?,3/29/17,1
kthompson,is SpatialKey in grid coordinates? ,3/29/17,1
pomadchin,"eh, yes; if you meant by grid your layout: https://wiki.osgeo.org/wiki/Tile_Map_Service_Specification",3/29/17,1
kthompson,yea trying to learn all of the lingo to understand a bit better,3/29/17,1
pomadchin,no problem at all :D ,3/29/17,1
kthompson,well like RasterExtent is to convert map coordinates to grid coordinates,3/29/17,1
kthompson,"are those ""grid"" coordinates the same concept as SpatialKey?",3/29/17,1
pomadchin,"ah, in context of raster extent: toGrid => converts your spatial coords into a certain tile grid coords (col, row); fromGrid convert grid coords to spatial coordsspatial coords in this context are real world coordinates, like some coords in lat lng, web mercator, etc",3/29/17,1
kthompson,yea i think i understand those bits and that all pertains to a Tile in geotrellis,3/29/17,1
kthompson,and I know that SpatialKey indicates the position of a Tile within a layer,3/29/17,1
kthompson,"but im not sure what col/row actually mean in a SpatialKey or like what ""unit"" they are in",3/29/17,1
pomadchin,integers;,3/29/17,1
pomadchin,https://wiki.osgeo.org/images/e/e7/Tms.png,3/29/17,1
kthompson,but they are not literally the tile col/row within a layer right? they correspond to some higher level maybe the whole world?,3/29/17,1
pomadchin,"probably i'm just a bit misunderstanding a problem O: they are a col / row in terms of layer but there are laws; it_Ñés a quad tree, so on zoom level 0 you have only 1 tile, on zoom leve 2 you would have 4 tiles, _Ñ_, etc",3/29/17,1
pomadchin,"and these keys always correspond tile extent; you always have `SpatialKey <=> Extent` functions o:",3/29/17,1
pomadchin,"a good example of a spatial key is any leaflet map, or google earth; you can notice these squares on a map when you zoom in / zoom out; these tiles correspond directly to geotrellis SpatialKey (we are talking about zoomed layout scheme in gt)",3/29/17,1
kthompson,ok cool,3/29/17,1
jfbourgon,"I'm trying to explore Geotrellis capacity about PointCloud data and recently found a project related to that on [Geotrellis Github](https://github.com/locationtech/geotrellis/tree/milestone/pointcloud/pointcloud).I'm wondering if you could point me out to some code snippet illustrating how to use these libraries. For now, I built the subproject using sbt but i'm failing to use it properly into Spark 2.1.0. Here is an output of the issue I'm facing:``` import geotrellis.pointcloud.spark.io.hadoop._import geotrellis.pointcloud.spark.json._import geotrellis.spark._import geotrellis.spark.io._import geotrellis.spark.io.hadoop._import org.apache.hadoop.fs._val rddc = HadoopPointCloudRDD(new Path(""/dev/VILLE_MONTREAL/VILLE_MONTREAL/18_E_5_52/POINT_CLOUD""))(sc)java.lang.NoSuchMethodError: shapeless.Witness$.mkWitness(Ljava/lang/Object;)Lshapeless/Witness;  at geotrellis.pointcloud.pipeline.json.Implicits$$anonfun$2$$anonfun$apply$1$anon$exportEncoder$macro$3067$1.inst$macro$1$lzycompute(Implicits.scala:38)  at geotrellis.pointcloud.pipeline.json.Implicits$$anonfun$2$$anonfun$apply$1$anon$exportEncoder$macro$3067$1.inst$macro$1(Implicits.scala:38)  at geotrellis.pointcloud.pipeline.json.Implicits$$anonfun$2$$anonfun$apply$1.apply(Implicits.scala:38)  at geotrellis.pointcloud.pipeline.json.Implicits$$anonfun$2$$anonfun$apply$1.apply(Implicits.scala:40)  at scala.collection.immutable.List.map(List.scala:273)  at geotrellis.pointcloud.pipeline.json.Implicits$$anonfun$2.apply(Implicits.scala:37)  at geotrellis.pointcloud.pipeline.json.Implicits$$anonfun$2.apply(Implicits.scala:34)  at io.circe.Encoder$$anon$13.apply(Encoder.scala:84)  at io.circe.syntax.package$EncoderOps$.asJson$extension(package.scala:8)  at geotrellis.pointcloud.pipeline.package$.pipelineConstructorToJson(package.scala:31)  at geotrellis.pointcloud.spark.io.hadoop.HadoopPointCloudRDD$.apply(HadoopPointCloudRDD.scala:61)  ... 72 elided``` ",3/29/17,1
pomadchin,@jfbourgon good questions you ask; ,3/29/17,1
pomadchin,Example project: https://github.com/geotrellis/geotrellis-pointcloud-demo,3/29/17,1
pomadchin,I think import geotrellis.pointcloud.spark._ should cover all your needs,3/29/17,1
pomadchin,"This can be used as an ingest sample: demonstrates point cloud to dem conversion and ingest aftetall, optionally with pyramid: https://github.com/geotrellis/geotrellis-pointcloud-demo/tree/master/src/app-backend/ingest/src/main/scala/com/azavea/pointcloud/ingest",3/29/17,1
pomadchin,Any questions related to its usage are appreciated,3/29/17,1
kthompson,is there a built in way to get the extent of a tile where there is data pixels?,3/29/17,1
kthompson,or do i just need to iterate through all the cells?,3/29/17,1
pomadchin,do you mean you need to check if your tile is non empty and get extent of only data cells?,3/29/17,1
ASellers,"Hi, I'm trying to determine if Geotrellis can support ShapeFiles with Spark processing out-of-the-box.  How does one convert ShapeFiles to RDD for Spark processing?  Thanks.",3/29/17,1
lossyrob,"no, but the built in way would do the thing you are thinking of. Iterating over each cell and keeping track of xmin,xmax,ymin,ymax as you go",3/29/17,1
lossyrob,should that be a lib method?,3/29/17,1
pomadchin,sounds like a useful method O:,3/29/17,1
lossyrob,"@ASellers not quite out-of-the-box. But it is possible - do you mean _ÑÒI have a set of Shapefiles on HDFS or S3, and I want to pull out feature data from them into an RDD of feature data_Ñù?",3/29/17,1
ASellers,"Hi, @lossyrob - thanks for the help.  That's right.  I have a collection of ShapeFiles that we want to be able to parse at scale to leverage spatial context in a predictive analytics application that requires us to join them with other data sets that contain (for example) point in space information, and answer geo-queries efficiently.",3/29/17,1
ASellers,Appreciate any insight about a possible approach.,3/29/17,1
lossyrob,"We have an `InputFormat` that can read binary data from Hadoop or S3. It would require writing a custom subclass of that, to produce the required geometries. We have a wrapper over GeoTools that allows reading shapefiles - unfortuantely last time I looked I couldn_Ñét figure out how to get it to read a shapefile from an `Array[Byte]` (which you would have from reading via the InputFormat), so that would have to be figured out. Once you are there, it_Ñés pretty simple to use that InputFormat to create a spark RDD of the things that InputFormat produces: in this case it can be our feature datatype, `Feature[Geometry, Map[String,Object]]`, so in the end you have an `RDD[Feature[Geometry, Map[String,Object]]]`. After that, you can start to work with the geometry data e.g. use our [VectorJoin](https://github.com/locationtech/geotrellis/blob/master/spark/src/main/scala/geotrellis/spark/join/VectorJoin.scala#L30) to do efficient joins of the data (requiring spatial partitioning beforehand), or using [SpatialJoin](https://github.com/locationtech/geotrellis/blob/master/spark/src/main/scala/geotrellis/spark/join/SpatialJoin.scala#L27) to gather geometries that have the same tile key on a grid layout. The join part is totally do-able, but because we_Ñéve focused a lot on raster data there will certainly be bumps in the road using that API",3/29/17,1
kthompson,@pomadchin yea basically just get the area of the tile that has cells with data(as opposed to NoData),3/29/17,1
kthompson,ill do the iteration bits but i think it would be useful for like when you want to crop an image to its data like for thumbnails for instance,3/29/17,1
ASellers,@lossyrob - that's helpful.  Thank you very much.,3/29/17,1
lossyrob,"makes sense. If you threw that into a `Methods` trait on Tile/MultibandTile so that it is implicitly added to the core types, that would make a good PR",3/29/17,1
lossyrob,@ASellers no problem,3/29/17,1
kthompson,:thumbsup: ,3/29/17,1
Charmatzis,"@lossyrob  I have already check that, but I will develop a solution by self",3/30/17,1
Charmatzis,"Hi, guys.Do you consider to merge that pull request https://github.com/locationtech/geotrellis/pull/1999 in 1.1.0 version?",3/30/17,1
pomadchin,yo @Charmatzis ; 1.2.0 version (you can notice a milestone label),3/30/17,1
Charmatzis,"OK, thx",3/30/17,1
karalas,"hi @pomadchin  and thanks for the interest! I have updated my code to https://github.com/karalas/geotrellis-sentinel-demo/blob/master/ingest/src/main/scala/demo/SentinelIngestMain.scala. I am having trouble to write correctly the ```times``` tag since if I run only the first ingest (```run```function), I cannot read the catalog when I run subsequently the ```server``` (resulting in ```Attribute times not found for layer ...```). Also, how can I instantiate multiple ```spark-submits``` within the same script? I am only calling multiple ```SparkContext```s.. Is the general idea of this script ok? thank you",3/30/17,1
karalas,It seems to me also that this type of ingest is slower than the initial we had with the etl tool. Is it because of the ```updatedKeyIndex``` https://github.com/karalas/geotrellis-sentinel-demo/blob/master/ingest/src/main/scala/demo/SentinelIngestMain.scala#L143-L154 ?,3/30/17,1
pomadchin,hm; im wondering what are you trying to do ._. because that code looks similar to landsat ndvi demo,3/30/17,1
pomadchin,What_Ñés the problem with https://gist.github.com/pomadchin/c2c8346ab0ef1ece11cf4432a87d14ec#file-layerupdatersample-scala-L16-L40 ?,3/30/17,1
pomadchin,"say source1 = path to the first tiff to be ingested; source2 = path to 2, 3 ,4 ...",3/30/17,1
pomadchin,"The problem with `times` and `extent` attributes you experience because you use lc emr demo; and this demo requires to write https://github.com/karalas/geotrellis-sentinel-demo/blob/ca2f0ba8f65f5447d4ac2aaef64e9003ebb98456/ingest/src/main/scala/demo/SentinelIngestMain.scala#L37-L47",3/30/17,1
pomadchin,"I want to say that during the update, the process would be a bit more complicated; as you would need to read times / extent, delete it, write new on the zero zoom level.",3/30/17,1
pomadchin,"```scala// do similar thing with extent but instead of ++ use extent combine functionval times = attributeStore.read[Array[Long]](id, ""times"")attributeStore.delete(id, ""times"")attributeStore.write(id, ""times"",              (times ++ rdd                .map(_._1.instant)                .countByValue                .keys.toArray).sorted)```",3/30/17,1
karalas,@pomadchin  ok I will try to make it according to the above steps and inform you! ,3/30/17,1
pomadchin,cool!,3/30/17,1
karalas,"@pomadchin  for the first part, with source1",3/30/17,1
karalas,"```val layerId = LayerId(layerName, zoom)  val id = LayerId(layerName, 0)  Pyramid.upLevels(tiled, layoutScheme, zoom, 1) { (rdd, z) =>    // writing a layer with larger than default keyIndex space    writer.write[SpaceTimeKey, Tile, TileLayerMetadata[SpaceTimeKey]](layerId, reprojected, updatedKeyIndex)    if (z == 0) {      attributeStore.write(id, ""times"",        rdd          .map(_._1.instant)          .countByValue          .keys.toArray          .sorted)      attributeStore.write(id, ""extent"",        (md.extent, md.crs))    }  }```",3/30/17,1
karalas,in ```attributeStore.write``` line i get ``` could not find implicit value for evidence parameter of type spray.json.JsonFormat[Array[Long]]```. Do I need this code to be inside ```Pyramid```? As a understand these lines are to write ```times``` and ```extent``` tags to attributeStore? ,3/30/17,1
pomadchin,"You just need to import spray codecs as usual:```scalaimport spray.json._import spray.json.DefaultJsonProtocol._```",3/30/17,1
pomadchin,https://github.com/karalas/geotrellis-sentinel-demo/blob/ca2f0ba8f65f5447d4ac2aaef64e9003ebb98456/ingest/src/main/scala/demo/SentinelIngestMain.scala#L11,3/30/17,1
karalas,"@pomadchin  you are right, I had an issue with the imports, but still this",3/30/17,1
karalas,"```Pyramid.upLevels(tiled, layoutScheme, zoom, 1) { (rdd, z) =>    val layerId = LayerId(layerName, z)    // writing a layer with larger than default keyIndex space    writer.write[SpaceTimeKey, Tile, TileLayerMetadata[SpaceTimeKey]](layerId, reprojected, updatedKeyIndex)    if (layerId.zoom == 0) {      val id = LayerId(layerName, 0)      attributeStore.write(id, ""times"",        rdd          .map(_._1.instant)          .countByValue          .keys.toArray          .sorted)      attributeStore.write(id, ""extent"",        (md.extent, md.crs))    }  }```throws ```geotrellis.raster.GeoAttrsError: invalid rows: 0``` error",3/30/17,1
karalas,any idea about this? I noticed in the code above that I have to run from zoom 0 but it's not that,3/30/17,1
pomadchin,I believe there is smth wrong with projections,3/30/17,1
pomadchin,"yes, fix `Pyramid.upLevels(tiled, layoutScheme, zoom, 1)` => `Pyramid.upLevels(tiled, layoutScheme, zoom, 0)`",3/30/17,1
pomadchin,that would build pyramid up from `zoom` up to 0 lvl,3/30/17,1
karalas,"```writer.write[SpaceTimeKey, Tile, TileLayerMetadata[SpaceTimeKey]](LayerId(layerName, zoom), reprojected, updatedKeyIndex)  Pyramid.upLevels(tiled, layoutScheme, 0, 0, Bilinear) { (rdd, z) =>    val layerId = LayerId(layerName, z)    if (layerId.zoom == 0) {      val id = LayerId(layerName, 0)      attributeStore.write(id, ""times"",        rdd          .map(_._1.instant)          .countByValue          .keys.toArray          .sorted)      attributeStore.write(id, ""extent"",        (md.extent, md.crs))    }  }```",3/30/17,1
pomadchin,eh; [here](https://github.com/karalas/geotrellis-sentinel-demo/blob/master/ingest/src/main/scala/demo/SentinelIngestMain.scala#L59) is zoom level calculated,3/30/17,1
karalas,yes,3/30/17,1
karalas,zoom level is calculated 14 for this image,3/30/17,1
karalas,with the above code I don't get ```GeoAttrsError``` ,3/30/17,1
pomadchin,"eh, i have no time to have a more close look into it right now",3/30/17,1
karalas,but ```times``` also is not written,3/30/17,1
pomadchin,i_Ñéll do it a bit later,3/30/17,1
pomadchin,can you add in comments file name you are trying to ingest? (as you sent me dropbox folder i probably can pick it up),3/30/17,1
karalas,ok ,3/30/17,1
pomadchin,but im pretty sure there is some typo somewhere,3/30/17,1
pomadchin,as etl worked for you,3/30/17,1
karalas,I also believe that it's smth simlpe,3/30/17,1
lossyrob,"@Charmatzis if there was user push to move it into 1.1, we would consider it for sure. Would that be very valuable to you?",3/30/17,1
Charmatzis,"it is very very usefull, if you can, please go for it!!!!",3/30/17,1
pomadchin,@lossyrob it_Ñés time to review :D :D ,3/30/17,1
lossyrob,Did you review and +1?,3/30/17,1
pomadchin,"yes, all changes were satisified. The only thing i like `cfor`s more then `while` loops (in cases we walk through all rasters cols/rows), but it_Ñés just a code style thing :D ",3/30/17,1
lossyrob,"Ok. I think there's some additional performance discussions, otherwise that's good enough for me",3/30/17,1
lossyrob,If there are no API breaks we can put it into RC3,3/30/17,1
pomadchin,"no API breaks, the same benchmark suite i ran on SNAPSHOT and on 1.0.0 ",3/30/17,1
lossyrob,"@ASellers @echeipesh just reminded me that we did do some Shapefile -> RDD work for a comparative analysis effort we did last year, which read shapefiles off of S3 via HTTP https://github.com/azavea/geowave-geomesa-comparative-analysis/blob/master/empirical-data/common/src/main/scala/com/azavea/ingest/common/shp/HydrateRDD.scala#L34",3/30/17,1
ASellers,@lossyrob - Thanks!  That's really helpful.,3/30/17,1
echeipesh,@Charmatzis About the dateline problem you_Ñére having. I understand the problem conceptually but not sure what concretely should be there but isn_Ñét in GeoTrellis API. The work you_Ñére doing do you think its a feature that should exist in GeoTrellis but doesn_Ñét or its very specific to your application ?,3/30/17,1
kthompson,is there a way to combine two rasters that have different tile dimentions?,3/30/17,1
echeipesh,You would have to resample one of them to do it. Is that the intent or something else?,3/30/17,1
kthompson,yea thats what i was thinking,3/30/17,1
kthompson,oh i was thinking reproject actually resample looks much more appropriate,3/30/17,1
kthompson,thanks :smile: ,3/30/17,1
echeipesh,"While I have you here can I poke you about #2001 ? I assume it was useful for you to do many small scale ingests, but I just want to be clear",3/30/17,1
metasim,"Any reason there's not a `RasterCodec`, other than noone's needed it yet?",3/30/17,1
echeipesh,"I think nobody has needed it yet, generally we store keys that can be turned to an extent using metadata and we haven_Ñét hit on the case where extent should be with the tile.",3/30/17,1
metasim,"I'm trying to decide that if we had a primary Spark SQL UDT for GT, would it be a `Raster[MultibandTile]` or a `MultibandTile`. I'm leaning toward the former.",3/30/17,1
metasim,(we're not limited to one... just prioritizing),3/30/17,1
echeipesh,"I would agree that it should be raster, its way more flexible and the cost of the extra data duplication if you include it with grid key is minimal.",3/30/17,1
echeipesh,Plus there is way more useful functions you can write for a raster rather than a tile.,3/30/17,1
metasim,"yeh, that resonates.",3/30/17,1
metasim,Turns out that I can write a UDT very easily if there's an `AvroRecordCodec[T]` for it :),3/30/17,1
echeipesh,"^ Teaser trailer, in scalac Spring of 2017!",3/30/17,1
kthompson,@echeipesh i need to do some more testing on that internally if you want to close it for now i can update it once i have been able to put more testing into it,3/30/17,1
echeipesh,"I don_Ñét mind keeping it open if you plan to push it, just needs a little guidance in the docs for when/why to use itand then tests where it makes sense.",3/30/17,1
kthompson,yea mainly the idea for it was to just allow some more parallelism but I am not sure when I will have time to get back to it.,3/30/17,1
echeipesh,Is it a pattern that you_Ñéve used in your app and seen a benefit ?,3/30/17,1
kthompson,not yet I had just put in the initial effort to write it up but havent had the chance to do any testing or put it into production,3/30/17,1
echeipesh,"Ah alright, I_Ñéll try to find time to poke at it with a small dataset to see what I can see.",3/30/17,1
kthompson,ok. I added it to our backlog so hopefully if i get some time soon i can put some more effort into testing it,3/30/17,1
kthompson,is there a from of BitArrayTile where the zeros are treated as noData?,3/30/17,1
echeipesh,"No, we made a call that that didn_Ñét seem to make sense as a data type (https://github.com/locationtech/geotrellis/blob/master/raster/src/main/scala/geotrellis/raster/CellType.scala#L343) because there would only be one possible data value. Is it causing ugly code ?",3/30/17,1
kthompson,well for example if you call toVector on a Raster that has a BitArrayTile you end up with a Polygon the size of the extent,3/30/17,1
kthompson,even if all the values are zero,3/30/17,1
kthompson,which if its a bitmask im expecting an empty polygon,3/30/17,1
echeipesh,"hmm, thats opposite of good.",3/30/17,1
kthompson,haha,3/30/17,1
kthompson,so i guess im going to need to do some custom cases where if cellType==BitCellType && value == 0 then do the thing,3/30/17,1
echeipesh,"That_Ñés an embarassing problem, I_Ñém having hard time coming up with a graceful work-around. Probably if there was a class that wrapped `Tile` interface and map `0 => NODATA` is probably the easiest patch that doesn_Ñét envolve cell widening on the tile.",3/30/17,1
kthompson,yea i am guessing creating a new CellType would probably not be trivial either,3/30/17,1
echeipesh,"well that_Ñés not really a problem, but hooking it into `BitArrayTile` won_Ñét happen since its hard coded.",3/30/17,1
kthompson,yea,3/30/17,1
echeipesh,`Vectorize` is also not that big and could use an extra option to work around this check: https://github.com/locationtech/geotrellis/blob/master/raster/src/main/scala/geotrellis/raster/vectorize/Vectorize.scala#L84,3/30/17,1
kthompson,yea,3/30/17,1
kthompson,i dont actually need to vectorize the raster but i was using it to verify how my raster actually looked,3/30/17,1
kthompson,i think i will be ok i just have to make a few checks for cases when im using the `BitCellType`,3/30/17,1
echeipesh,"Made an issue to sort this out next time we break the API, don_Ñét think we can do anything until then https://github.com/locationtech/geotrellis/issues/2111",3/30/17,1
kthompson,:thumbsup: ,3/30/17,1
kthompson,yea i wonder how BitArrayTile is commonly used,3/30/17,1
kthompson,our use case is basically to mask pixels on other tiles so we need it just to say whether or not we want to save pixels in another tile,3/30/17,1
echeipesh,Hard for me to say what _ÑÒcommon_Ñù means :) Usually if it_Ñés used in `combine` matching on 1/0 is fine.,3/30/17,1
kthompson,yea i hear you :),3/30/17,1
echeipesh,But bit rasters pretty much always end up being a special case. Reading them from GeoTiff is strange as well.,3/30/17,1
kthompson,yea makes sense as it would seems its only purpose is to be compressed really well,3/30/17,1
Charmatzis,@echeipesh  basically all that are basic GIS problems that in a GIS project a anlyst will face.,3/31/17,1
Charmatzis,Ho to everyone. I beleive a good extention on working with Lat Lng data would be to use  (https://en.wikipedia.org/wiki/Haversine_formula),3/31/17,1
Charmatzis,https://rosettacode.org/wiki/Haversine_formula#Scala ,3/31/17,1
Charmatzis,if you are intereting a I can develop a pull request,3/31/17,1
pomadchin,@Charmatzis any PRs are welcome :tada:  :tada:,3/31/17,1
pomadchin,feel free to do it :D ,3/31/17,1
lossyrob,@Charmatzis we actually use it here https://github.com/locationtech/geotrellis/blob/master/spark/src/main/scala/geotrellis/spark/tiling/ZoomedLayoutScheme.scala#L71,3/31/17,1
lossyrob,What would be a good interface for generalized usage?,3/31/17,1
Charmatzis,@lossyrob yeap.  If it was more generalized it would be much better to use it and in other cases. I believe it is a function that is needed.,3/31/17,1
Charmatzis,"one general question, how did you came up using jts? i am searching now and have found that ESRI has opened their geometry-api for java",3/31/17,1
Charmatzis,https://github.com/Esri/geometry-api-java ,3/31/17,1
lossyrob,"JTS has been around forever, and is a widely respected and used library. There_Ñés even ports of it_Ñés code to other languages, like the GEOS project, which PostGIS uses. I don_Ñét see an advantage of using the ESRI stuff, is there?",3/31/17,1
metasim,"(Aside: I'm often suspicious of ESRI doing the ""embrace and extend"" strategy that Microsoft has been famous for.)",3/31/17,1
metasim,Is there a programmatic way of getting the list of strings valid to pass [to this method](https://github.com/locationtech/geotrellis/blob/master/raster/src/main/scala/geotrellis/raster/CellType.scala#L422)?,3/31/17,1
fosskers,"(morning meeting atm @metasim , answer forthcoming)",3/31/17,1
metasim,Short of using reflection to find all the subtypes of `CellType` and calling `name`?,3/31/17,1
pomadchin,"@metasim sure, you can even get it in compile time",3/31/17,1
metasim,Cool.,3/31/17,1
pomadchin,"as types are sealed traits, that means you can use macro to go through all types, and get each seled trait `name` value",3/31/17,1
pomadchin,"on a standup, would be glad to help you on that if you_Ñéll have questions",3/31/17,1
metasim,"Oh, that's not exactly what I meant.... more like, is there a GT API for that. :)",3/31/17,1
pomadchin,ahahah :D no,3/31/17,1
pomadchin,do you think that would be useful?,3/31/17,1
metasim,"At least for testing, I think there should be a call to list all of the `CellType` that are standard.  From there I can get the string names.",3/31/17,1
metasim,I have another use use for them that I'll fill you in later. ;),3/31/17,1
dorukozturk,"Hi,  what kind of formula is used to specify the tilenames in the catalog for spatialkey under each zoom level? ",4/1/17,1
dorukozturk,looks like tile's filename includes both the x and y coordinates,4/1/17,1
lossyrob,Formula is the space filling curve index of the configured sfc for that layer. I can elaborate more later,4/2/17,1
dorukozturk,"great, that is enough information already",4/2/17,1
Charmatzis,"Hi, I am trying to serialize a csv dataset with Lat and Lon and I get this error ",4/3/17,1
Charmatzis,"Exception in thread ""main"" java.lang.UnsupportedOperationException: No Encoder found for com.vividsolutions.jts.geom.Point ",4/3/17,1
pomadchin,are you using datasets?,4/3/17,1
Charmatzis,"yeap",4/3/17,1
Charmatzis,is there a quick solution for that?,4/3/17,1
pomadchin,"import that in scope (`import KryoEncoders._`)```scalaobject KryoEncoders {  def single[A: ClassTag] = Encoders.kryo[A](classTag[A])  implicit def pointEncoder = single[Point]}```",4/3/17,1
pomadchin,a more generic thing (includeing tuples and auto derivation): https://github.com/pomadchin/geotrellis/blob/ffe5b33dcb41d09f2ea72800d2a8ca8b25af3507/spark-sql/src/main/scala/geotrellis/spark/sql/KryoEncoderImplicits.scala#L28-L42,4/3/17,1
Charmatzis,@pomadchin  which point class should I reference? the geotrellis.vector.Point or the jts.geom.Point?,4/3/17,1
pomadchin,you have an exception `No Encoder found for com.vividsolutions.jts.geom.Point` => that means for `com.vividsolutions.jts.geom.Point`,4/3/17,1
metasim,"If I have  a `t: Tile` with a floating point `CellType`, what's the most efficient way of iterating over the cells? `forEachDouble()`?",4/3/17,1
metasim,Some `cfor` construct?,4/3/17,1
echeipesh,"@metasim in general you can_Ñét get any lower than `get` or `getDouble` call on a Tile so `cfor` will give you the best optimization, but I_Ñém not sure its going to be much in the end.",4/3/17,1
metasim,"@echeipesh  Thanks... not having a problem, just don't want to create one. Figured out how to do an explode via catalyst expression, and want to do that loop right",4/3/17,1
echeipesh,"Awesome, what did it end up taking ?",4/3/17,1
metasim,A `Generator` I'll send a link when I commit.,4/3/17,1
metasim,"There's a deserialization step involved, but should be much less painful (I hope).",4/3/17,1
Charmatzis,"@pomadchin  well I did manage to use the kryo encoder, but when I try to deserialize I get a byte array ...... (not another problem!!!!!) ",4/3/17,1
metasim,"@echeipesh Here's the POC:* [Exploder](https://github.com/s22s/avro2spark/blob/feature/spark-udt/src/main/scala/org/apache/spark/sql/GTSQLFunctions.scala#L50)* [Test](https://github.com/s22s/avro2spark/blob/feature/spark-udt/src/test/scala/astraea/spark/avro/GTSQLSpec.scala#L57)Next I'm going to do multiple tile columns at once.",4/3/17,1
echeipesh,"@Charmatzis Thats what the DataFrame column it creates, just an array, but if you_Ñére using `DataSet.as[Point]` it should pickup that encoder and give you back points.",4/3/17,1
echeipesh,"@metasim That function returns `TraversableOnce` I would be willing to bet that if you can express that in terms of `Iterator` it will chooch much better, that is generally the case when creating subclasses of `RDD`.",4/3/17,1
metasim,I'll try it. `TraversableOnce` is what the built-in `Explode` uses.,4/3/17,1
echeipesh,"Right and `Iterator extends TraversableOnce` (as well as Seq), the point is to avoid allocating a fixed memory structure like an array: https://github.com/s22s/avro2spark/blob/feature/spark-udt/src/main/scala/org/apache/spark/sql/GTSQLFunctions.scala#L61",4/3/17,1
metasim,So how would you rewrite that? With some lazy construct?,4/3/17,1
metasim,`for` comprehension?,4/3/17,1
echeipesh,probably for comprehension on iterator would be the only way to do it,4/3/17,1
metasim,cool.,4/3/17,1
echeipesh,"```scala    override def eval(input: InternalRow): TraversableOnce[InternalRow] = {      // Expect CCE?      val tileRow = child.eval(input).asInstanceOf[InternalRow]      val tile = TileUDT.deserialize(tileRow)      for {        col <- Iterator.range(0, tile.cols)        row <- Iterator.range(0, tile.rows)      } yield InternalRow(tile.getDouble(col, row))    }```Something like that.",4/3/17,1
metasim,heh... you beat me. awesome,4/3/17,1
metasim,I was trying to remember if it was cols or rows that should be in the outer loop.,4/3/17,1
metasim,I'm seeing examples of both in GT.,4/3/17,1
metasim,"It's column major, right?",4/3/17,1
echeipesh,"yeah, I got that backwords",4/3/17,1
metasim,"(it works, BTW)",4/3/17,1
echeipesh,"cool. Wish there was some kind of rigging that would test for _ÑÒthat job completed with X less memory""",4/3/17,1
metasim,"Yeh.... I suppose I could try that with jvisualvm snapshots, but that's always been a rabbit hole for me.",4/3/17,1
metasim,Thanks for that feedback... glad you caught that sloppiness.,4/3/17,1
jattnalpanga,"hi, Sorry for bringing this question up here, I started using Geotrellis. I would like to know if I can use it for JPEG  raster format ? I know it works for GeoTiff format. ",4/4/17,1
lossyrob,"Hi @jattnalpanga, GeoTrellis can_Ñét do that natively, but there is work done to integrate it with GDAL here: https://github.com/geotrellis/geotrellis-gdal",4/4/17,1
lossyrob,that would allow you to read in whatever raster formats GDAL can,4/4/17,1
lossyrob,it_Ñés a little behind on work though,4/4/17,1
lossyrob,"__GeoTrellis 1.1.0-RC3 is released!__ Please let me know if you experience problems; if we find anything we_Ñéll release an RC4, otherwise this will be promoted to the full release in a week or two.",4/4/17,1
pomadchin,"@jattnalpanga in addition to @lossyrob words, have a look into: https://github.com/geotrellis/geotrellis-gdal/pull/1 or here: https://github.com/pomadchin/geotrellis-gdal/tree/versionupgrades (it includes some minor fixes, related to sbt build process and of some libs versions upgrade)",4/5/17,1
aaron-santos,"hey, I'm working on some optimizations and I'm looking for some validation to see if they pass the smell test or if there might be better ways to accomplish the same thing",4/5/17,1
aaron-santos,"the first is that some of the satellite imagery I'm ingesting has bands at different resolutions. I'm thinking about ingesting the highest resolution bands first and  as the zoom level matches the native resolution of the low-res bands, modifying the rdd being ingested to include the new bands",4/5/17,1
aaron-santos,"this might mean that a multibandtile at zoom=16 might have 5 bands, while a multibandtile at zoom=12 might have 10 bands and one at zoom=10 might have 13 bands. The application would necessarily have to sort this out during tile retrieval",4/5/17,1
aaron-santos,The other idea (and not necessarily mutually exclusive) is writing an avrocodec for multiband tiles that stores band offsets as part of a header and turning off avro compression. But it would mean that I could read these offsets and then perform range requests (to S3) in this case to just get bands that I care about. I'll need to select different bands for different use cases,4/5/17,1
echeipesh,"@aaron-santos seems like those two options are optimizing for different things, what is the thing you most care about or are you just trying to sort out _ÑÒgenerally good_Ñù way to store and query multi-resolution multi-band data ?",4/5/17,1
aaron-santos,"yes, one for reducing storage size, and the other for reducing transfer cost",4/5/17,1
aaron-santos,I'm trying to improve on the system I have now that downsamples low-res bands to match the hi-res bands and retrieves all the bands even if I just need one or two. Optimizing two different things for sure,4/5/17,1
echeipesh,"Its definatly a problem that needs to be solved in the API, currently we completely punted on it. Doing a sub-set band reads is always awesome. The first option seems like you would get for free by just storing two layers (that would mess with your appilication logic though). Hmm..",4/5/17,1
aaron-santos,I'm willing to add quite a bit of complexity too.  :),4/5/17,1
echeipesh,From the application standpoint would it be better to store tiny tiles for low rez images with only pixels that overlap the area or store full sized tiles at higher zoom levels ?,4/5/17,1
aaron-santos,Can you explain that a little more?,4/5/17,1
echeipesh,"Well, potentially there can be a tile type that holds different resolution images. In order for it to be true to extent bounds it would need to have less pixels per low-res band vs high-res band for the same zoom level. Just wondering if from standpoint of your application that is even a useful behavior or if going up the zoom levels to fetch first _ÑÒ256x256_Ñù tile for low-res band is actually the prefered way of handling it.",4/5/17,1
aaron-santos,"that's an interesting idea! I was thinking I'd have to pull from different zooms if I wanted to ""overzoom"" a band which was not available for the zoom level I was interested in",4/5/17,1
aaron-santos,"I think that could work, but wow that sounds like a complicated ingest process",4/5/17,1
echeipesh,"Going to try to sketch it out, does sound like a doozy.",4/5/17,1
lossyrob,I think an appropraite thing to mention here is the idea of a GeoTiff layer,4/5/17,1
lossyrob,which is something I_Ñéve been working on thinking through,4/5/17,1
lossyrob,"where instead of avro encoded files on the backend, we would have cloud optimized geotiffs",4/5/17,1
lossyrob,https://trac.osgeo.org/gdal/wiki/CloudOptimizedGeoTIFF,4/5/17,1
lossyrob,the band offsetting thing you describe is something geotiffs already can do,4/5/17,1
lossyrob,"and I have a WIP PR for being able to do that with our geotiff reader, on band interleave geotiffs https://github.com/locationtech/geotrellis/pull/2115",4/5/17,1
lossyrob,"the cloud optimized geotiff has an idea of overviews, which can snap to the power-of-two zoom levels",4/5/17,1
lossyrob,"the idea is that we would have web mercator tiles who_Ñés internal tiling was 256x256, which match the TMS tiles we currently store all as separate files",4/5/17,1
lossyrob,"but because of byte range requests, we can get away with only pulling the data we need",4/5/17,1
lossyrob,"but in case we need a lot of data, we only have to make a small number of GET requests, whereas now we have to make a lot",4/5/17,1
lossyrob,"as for the multiresolution thing_Ñ_i see what you mean by _ÑÒMultibandTile with multiple zooms_Ñù, but that sounds like storing more data than you have to, say if you had zoom 13 + zoom 17, your storing zoom 13 pixels for each of zoom 14..17, which are just duplications",4/5/17,1
lossyrob,"we could just split the ingest, and then represent one _ÑÒlayer_Ñù as having multiple tiles, and pulling as needed. that might be worthwhile library functionality ",4/5/17,1
aaron-santos,"yeah, it would def depend on a speed/space tradeoff",4/5/17,1
aaron-santos,/complexity :),4/5/17,1
kthompson,how do i properly deal with `LayerOutOfKeyBoundsError`?,4/5/17,1
kthompson,i dont really know my full bounds on initial write and it seems i cant update the bounds as i write new layer tiles,4/5/17,1
kthompson,https://geotrellis.readthedocs.io/en/latest/guide/spark.html#reindexing-layers talks about using the LayerReindexer but that seems super inefficient,4/5/17,1
kthompson,my one layer could be the whole earth potentially,4/5/17,1
lossyrob,You need to index it to the whole earth on the first ingest then. ZoomedLayoutScheme doesnt have this problem because it alwaus encompasses the whoke,4/6/17,1
lossyrob,Whole earth,4/6/17,1
kthompson,yea i must be doing something wrong. i am using ZoomedLayoutScheme,4/6/17,1
kthompson,but im not ingesting everything all at once,4/6/17,1
kthompson,i have multiple basetiles that i need to ingest seperately,4/6/17,1
pomadchin,hey @kthompson https://gist.github.com/pomadchin/c2c8346ab0ef1ece11cf4432a87d14ec ,4/6/17,1
pomadchin,it_Ñés a small example of layerupdater usage,4/6/17,1
pomadchin,"in case of hilber curve usage, you probably need to define keyIndex for each zoom level (for optimizations)",4/6/17,1
pomadchin,"and you can calculate the exact `KeyBounds` of the entire dataset, to set them as `KeyBounds` of the `KeyIndex`",4/6/17,1
asmith26,"Hi there,Does Geotrellis support anything like [scikit-image blob detection](http://scikit-image.org/docs/dev/auto_examples/features_detection/plot_blob.html):![scikit-image blob detection](http://scikit-image.org/docs/dev/_images/sphx_glr_plot_blob_001.png)",4/6/17,1
asmith26,(I couldn't find anything in particular in the docs),4/6/17,1
kthompson,Thanks @pomadchin I'll try that out today,4/6/17,1
pomadchin,"@kthompson cool, there can be some typos in that gist, so I appreciate any comments",4/6/17,1
kthompson,_ª¥ç,4/6/17,1
lossyrob,"@asmith26 we don_Ñét support it directly, but with our python bindings iniative, we should be able to use scikit-image directly",4/6/17,1
lossyrob,"we_Ñére working on a demo for pulling geotrellis-ingested tiles from a backend and running blob detection for ship counting, inside a jupyter notebook",4/6/17,1
kthompson,@pomadchin your sample code looks good for my purposes only thing i really did a bit differently is i just explicitly supplied the KeyBounds to the createIndex method,4/6/17,1
kthompson,i didnt realize that existed though so thanks,4/6/17,1
pomadchin,"@kthompson be carefull with bounds because probably you may want to have different keybouds for a different zoom lvl (it_Ñés a common thing)however ye it_Ñés just a sample to demonstatrate that it_Ñés possible; thanks for a feedback :tada: ",4/6/17,1
kthompson,ok cool,4/6/17,1
kthompson,i ended up using `ZoomedLayoutScheme.layoutColsForZoom` and `layoutRowsForZoom` which is basically the same as in your example but it felt nicer to me :),4/6/17,1
kthompson,"ie `math.pow(2, level)`",4/6/17,1
pomadchin,"haha, sure, your solution is better :D ",4/6/17,1
kthompson,:D,4/6/17,1
asmith26,@lossyrob many thanks!,4/6/17,1